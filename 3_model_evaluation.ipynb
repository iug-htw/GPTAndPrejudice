{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83cc8f5-bd99-4599-9f06-24efef365043",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d738963d-0ffa-484b-a135-29fada2d16a5",
   "metadata": {},
   "source": [
    "This notebook documents the evaluation of the **GPT & Prejudice** language model.  \n",
    "It focuses exclusively on assessing the **base GPT model** using both quantitative and qualitative methods.\n",
    "\n",
    "### Overview\n",
    "The evaluation covers two main components:\n",
    "\n",
    "#### **1. Quantitative Evaluation**\n",
    "The notebook loads the trained GPT model along with its **validation** and **evaluation** datasets, then computes a set of key performance metrics:\n",
    "- **Perplexity (PPL):** measures how well the model predicts unseen text.  \n",
    "- **WEAT (Word Embedding Association Test):** probes for implicit social and gender biases in the model’s learned representations.  \n",
    "- **BLEU and ROUGE:** evaluate text generation quality and overlap with reference sentences.\n",
    "\n",
    "#### **2. Qualitative Analysis**\n",
    "To complement the metrics, the model is prompted with various **starter sentences** inspired by the corpus.  \n",
    "These completions are examined for narrative coherence, tone consistency, and stylistic fidelity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IuG_Lap1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import combinations\n",
    "import tiktoken\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from gpt_model import DEFAULT_CFG\n",
    "from utils.model import load_GPT_model\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = load_GPT_model(path=\"model_896_14_8_256.pth\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccfe2eef-47b0-40f0-8b71-8d3b1d501451",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def encode(full_text):\n",
    "    return tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d501e-87f9-4da3-99e9-681c44ad31c6",
   "metadata": {},
   "source": [
    "### Load validation and evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcfce182-1d71-4b11-b06b-7f9881c25093",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_path = './datasets/val_text_data.txt'\n",
    "\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=DEFAULT_CFG[\"context_length\"],\n",
    "    stride=DEFAULT_CFG[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6106a26c-295c-442b-9603-4540b98d60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file_path = './datasets/eval_text_data.txt'\n",
    "\n",
    "with open(eval_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    eval_data = file.read()\n",
    "\n",
    "eval_loader = create_dataloader_v1(\n",
    "    eval_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=DEFAULT_CFG[\"context_length\"],\n",
    "    stride=DEFAULT_CFG[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ee03d-6d01-4dea-a56c-165793d31631",
   "metadata": {},
   "source": [
    "## 1.1 Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d523e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)  # Forward pass\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * target_ids.numel()\n",
    "            total_tokens += target_ids.numel()\n",
    "\n",
    "    perplexity = np.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78860520-1e1a-4d48-a476-c1b1eaf72471",
   "metadata": {},
   "source": [
    "##### Perplixity on the validation set (subset of the corpus used for evaluation during trainig):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837f7534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(60.410590339687815)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b815c6da-9fab-4ec8-997c-3cff9b4033f9",
   "metadata": {},
   "source": [
    "##### Perplixity on the evaluation set (never seen before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b21a76d-16d0-4d4d-8352-240155f6a850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(72.36422471622937)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(model, eval_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76ad46f-c4d0-42ef-b070-cb911261fcfa",
   "metadata": {},
   "source": [
    "On the held-out validation set it achieved a perplexity of **60.4**, showing strong fit to its training domain. To test generalization, we evaluated on three novels by female authors contemporary to those in the training set—Harriet Martineau, Julia Kavanagh, and Mary Brunton—where perplexity rose to **72.4**.\n",
    "\n",
    "For context, perplexity values in prior work range from 78.4 on Penn Tree Bank with LSTMs (Grave et al., 2017) to 29.41 on WikiText-2 with GPT-2 Small (117M parameters) (Radford et al., 2019), with GPT-2 also reporting 99.3 on WikiText-2. These comparisons are not direct baselines, but they help situate the scale of our results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce2021-7f46-4b55-9fbc-f9c9b2505ea0",
   "metadata": {},
   "source": [
    "## 1.2 WEAT (Word Embedding Association Test) score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674517d0-253f-4fc0-be62-42d2a7cc96bd",
   "metadata": {},
   "source": [
    "This section evaluates **bias in the model’s learned word embeddings** using the Word Embedding Association Test (WEAT).  \n",
    "WEAT measures whether the model’s internal representations associate certain **target groups** (e.g., *male* vs. *female terms*) more closely with specific **attributes** (e.g., *career* vs. *family*, or *honour* vs. *beauty*).\n",
    "\n",
    "#### **How it works**\n",
    "- The function `weat_score()` computes cosine similarities between embeddings of the target and attribute words.  \n",
    "- For each group, it calculates how much more one group (e.g., *male*) is associated with one set of attributes (e.g., *honour, duty, independence*) compared to another (e.g., *grace, beauty, delicacy*).  \n",
    "- The resulting **WEAT score** reflects the **direction and magnitude** of bias:\n",
    "  - A **positive** score indicates stronger association of the first target group with the first attribute set.  \n",
    "  - A **negative** score indicates stronger association of the second target group with the first attribute set.  \n",
    "  - A score near **zero** suggests no strong directional bias.\n",
    "\n",
    "#### **In this notebook**\n",
    "- The model’s embedding layer is used directly to obtain word vectors for gender-related and virtue-related terms.  \n",
    "- We test whether male-coded words align more with *duty and independence* while female-coded words align more with *grace and beauty* — reflecting potential literary or cultural biases absorbed from the training corpus.  \n",
    "- The output (e.g., `-0.0466`) represents the overall bias direction, forming part of the model’s fairness and bias analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44c61225-1077-4019-984a-564aa7ba6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "def weat_score(model, target_words_1, target_words_2, attribute_words_1, attribute_words_2, tokenizer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Measures bias by comparing how close different groups of words are in embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding(word):\n",
    "        token_id = tokenizer.encode(word, allowed_special={'<|endoftext|>'})[0]\n",
    "        with torch.no_grad():\n",
    "            embed = model.tok_emb(torch.tensor([token_id], device=device)).cpu().numpy()\n",
    "        return embed.flatten()\n",
    "\n",
    "    # Get embeddings\n",
    "    target_1_embs = [get_embedding(w) for w in target_words_1]\n",
    "    target_2_embs = [get_embedding(w) for w in target_words_2]\n",
    "    attr_1_embs = [get_embedding(w) for w in attribute_words_1]\n",
    "    attr_2_embs = [get_embedding(w) for w in attribute_words_2]\n",
    "\n",
    "    def association(t, A, B):\n",
    "        return np.mean([cosine_similarity(t, a) for a in A]) - np.mean([cosine_similarity(t, b) for b in B])\n",
    "\n",
    "    # Compute WEAT score\n",
    "    s1 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_1_embs])\n",
    "    s2 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_2_embs])\n",
    "    \n",
    "    weat_score = s1 - s2\n",
    "    return weat_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd7a1406-0bd9-409a-9fd8-8b4f343d2c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.046634555)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_male = [\"gentleman\", \"officer\", \"clergyman\", \"husband\", \"captain\"]\n",
    "target_female = [\"lady\", \"governess\", \"girl\", \"wife\", \"widow\"]\n",
    "\n",
    "attribute_male = [\"honour\", \"duty\", \"wisdom\", \"fortitude\", \"independence\"]\n",
    "attribute_female = [\"grace\", \"affection\", \"beauty\", \"delicacy\", \"modesty\"]\n",
    "\n",
    "weat_score(model, target_male, target_female, attribute_male, attribute_female, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6d43e-ad11-4de0-a360-7ecbf0b03bbd",
   "metadata": {},
   "source": [
    "The WEAT result of **−0.0466** suggests a **slight inverse association**. Female-related words in the model’s embedding space appear marginally closer to traditionally male-coded attributes such as *honour*, *duty*, and *independence*. Although the magnitude of this bias is very small, it is noteworthy given that the training corpus was written within a strongly gendered social context.  \n",
    "\n",
    "This subtle inversion may reflect the nuanced way language portrays female characters in the 19th century corpus. Their moral strength, intellect, and independence is often emphasized alongside traditional feminine traits. The low absolute value indicates that **the model does not exhibit a strong polarized bias** in this particular pairing, but rather captures a more balanced or contextually blended representation of gendered virtues in the source texts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e749c-1b0d-4069-8815-95a68d4fecb7",
   "metadata": {},
   "source": [
    "### 1.3 BLEU and ROUGE Evaluation\n",
    "\n",
    "This section evaluates the **text generation quality** of the model using two complementary metrics: **BLEU** and **ROUGE-L**.\n",
    "\n",
    "#### **Purpose**\n",
    "While perplexity measures the model’s internal predictive confidence, BLEU and ROUGE focus on **surface-level similarity** between generated text and reference continuations showing how fluently and faithfully the model completes partial sentences from the validation set.\n",
    "\n",
    "#### **Procedure**\n",
    "1. Sentences from the validation dataset are filtered for medium length (5–60 tokens) to ensure coherent prompts.  \n",
    "2. Each sentence is split into two halves: the **first half** is used as a prompt, and the **second half** serves as the **reference continuation**.  \n",
    "3. The model generates its own completion for each first half.  \n",
    "4. BLEU and ROUGE-L scores are computed between the generated text and the true second half.  \n",
    "\n",
    "- **BLEU** measures *n-gram precision* — how many words and short phrases from the reference appear in the generated text.  \n",
    "- **ROUGE-L** measures *recall* based on the longest common subsequence, capturing overall content overlap.\n",
    "\n",
    "Together, these metrics assess both **syntactic fluency** and **semantic fidelity** in the model’s completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cdcce2e-326f-4cc6-8033-e77e0a2ba270",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d845171-c8f1-47a0-a60b-1f9847a22590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import re\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_bleu_rouge_from_val(model, device=\"cpu\"):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    # Step 1: Load the validation set\n",
    "    with open('./datasets/val_text_data.txt', 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Step 2: Split into sentences & filter\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', data)\n",
    "    filtered_sentences = [s.strip() for s in sentences if 5 <= len(s.split()) <= 60]\n",
    "    filtered_sentences = filtered_sentences[:1000]\n",
    "\n",
    "    # Step 3: Split each sentence into two halves and store as tuples\n",
    "    sentence_tuples = []\n",
    "    for sent in filtered_sentences:\n",
    "        words = sent.split()\n",
    "        mid = len(words) // 2\n",
    "        first_half = ' '.join(words[:mid])\n",
    "        second_half = ' '.join(words[mid:])\n",
    "        sentence_tuples.append((first_half, second_half))\n",
    "\n",
    "    # Step 4: For each (first_half, second_half), generate prediction\n",
    "    for first_half, second_half in sentence_tuples:\n",
    "        generated_text = generate(\n",
    "            model=model, prompt=first_half,\n",
    "            max_new_tokens=30, context_size=DEFAULT_CFG['context_length'],\n",
    "            device=device,\n",
    "            temperature=0.7,\n",
    "            top_k=50\n",
    "        )\n",
    "\n",
    "        # Build reference and prediction\n",
    "        reference = first_half + \" \" + second_half\n",
    "        prediction = generated_text\n",
    "\n",
    "        references.append(reference)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Step 5-6: Compute BLEU and ROUGE\n",
    "    # Format references correctly for BLEU\n",
    "    references_formatted = [[ref] for ref in references]\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references_formatted)['bleu']\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}, ROUGE-L Score: {rouge_score['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0404bbea-7821-4b52-a5e9-f240bf5c01d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.3445, ROUGE-L Score: 0.4292\n"
     ]
    }
   ],
   "source": [
    "compute_bleu_rouge_from_val(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b46ec9-c2a2-4b39-a4e4-f2c12392f6da",
   "metadata": {},
   "source": [
    "The model achieved a **BLEU score of 0.3445** and a **ROUGE-L score of 0.4292**, indicating that its generated continuations align reasonably well with the reference text.  \n",
    "These mid-range scores suggest the model produces grammatically coherent and stylistically consistent text but does not perfectly reproduce the exact continuations.  \n",
    "\n",
    "Given the literary domain of the novels, where sentence structure and vocabulary are highly variable, these results reflect a strong grasp of **language style and tone** rather than strict word-for-word predictability. The model demonstrates good narrative continuity and plausible completions, even when its predictions diverge lexically from the reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc60e3f-b10c-4a90-ac56-852f15edffb6",
   "metadata": {},
   "source": [
    "## 2. Qualitative analysis: Sentence generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adb0c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss Bennet has inherited the estate from her aunt, so she must have been able to have found a match for Jane's; and she has had the goodness to pay her a visit with her, with all her friends, and has promised to do it she may find out as very agreeable a man as herself.\"\n",
      "\n",
      "==================================================\n",
      "Mr. Darcy has inherited the estate from his aunt, so he must be able to be able to pay for her. I hope you will be able to settle in the house this morning, if you will stay with us; and if you will give me time to stay, I will give you my letter. Adieu\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Miss Bennet has inherited the estate from her aunt, so she must\",\n",
    "    max_new_tokens=50, context_size=DEFAULT_CFG['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Mr. Darcy has inherited the estate from his aunt, so he must\",\n",
    "    max_new_tokens=50, context_size=DEFAULT_CFG['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81220f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wife is a sensible girl, and I am sure I could not have been so happy as I do.\"\n",
      "\"I am glad you have been so very kind\n",
      "==================================================\n",
      "A husband is, and I am sure I can't help it. I'm sure I'm very sorry for you. I'm sure I'm not much obliged to\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"A wife is\",\n",
    "    max_new_tokens=30, context_size=DEFAULT_CFG['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"A husband is\",\n",
    "    max_new_tokens=30, context_size=DEFAULT_CFG['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "714f6606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I shall now go to bed at night,\" said he. \"I shall go home to-morrow. I shall be at home, and I shall take care of the\n",
      "==================================================\n",
      "He said, in a low voice, \"You are right, Mr Delvile, and I hope you will not be wrong.\"\n",
      "\"I know not\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"I shall now go\",\n",
    "    max_new_tokens=30, context_size=DEFAULT_CFG['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"He said\",\n",
    "    max_new_tokens=30, context_size=DEFAULT_CFG['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78dc249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She was very fond of having a sort of good heart to do, and had a great deal of good to think that she had done wrong, and was fond of being able to do anything wrong.\n",
      "But there was no danger of her being so good-natured and agreeable as Mrs. Dashwood, and so much more disposed to say, that she felt herself at the very time so much that she could never have done anything very wrong; and at that time she was so pleased that she could never find out her own mind before, and the pleasure of being in a tolerable manner, of knowing how much or what she had to say, was all that she could do for her in this light, which she had said of Mrs. Jennings's having said of her, and in such a manner as that of her being married, she was sure she should not have believed it.\n",
      "When the first thing was over, it was a very great measure for her to be at last in her way\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"She was\",\n",
    "    max_new_tokens=200, context_size=DEFAULT_CFG['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb810-cfc6-49fe-bfd0-66c035e0707e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
