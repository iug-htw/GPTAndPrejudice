# **Circuit Patching in a Small GPT (Jane Austen model)**

**Summary of Methods, Pitfalls, and Improvements**

---

## **1\. What we are trying to do**

We want to uncover **causal circuits** inside our small GPT model trained on Jane Austen’s novels.  
 The idea is simple: run the model on two inputs:

* **Clean run**: the model sees a “good” prompt (e.g., *Elizabeth … Without hesitation,*).   
  → favors *“she”*  
* **Corrupt run**: the model sees a “bad” prompt (e.g., *Darcy … Without hesitation,*).  
  → favors *“he”*

Then we test: *if we swap the clean hidden state into the corrupt run at a **certain layer**, does the corrupt model behave more like the clean one?*

This is called **activation patching** (or causal tracing).

---

## **2\. First attempts (why they fell short)**

### **a) Target–distractor logit difference**

* Follows approach in paper: arXiv:2211.00593v1   
* We test if the model predicts a certain token. (e.g. When John and Mary walked into the ball, John gave a letter to … \[it should predict Mary\]).  
* Metric: *logit(target) – logit(distractor)*.  
* Works in large GPTs, but here the Austen model often doesn’t prefer the “correct” token (e.g., “she” vs. “he”).  
* The model is too small, not very smart, likely doesn’t detect the exact token we are looking for.  
* **Result:** very small, noisy gains.

### **b) Full-layer replacement patching**

* At each layer L, we replaced the **entire residual stream** of the corrupt run with the clean run.  
* Problem: once replaced, the rest of the forward pass becomes **identical to the clean run**, no matter which layer we patch.  
* **Result:** every layer looks equally important (flat gains).

These methods failed because they were **too strict** (requiring correctness) or **too strong** (overwriting the whole trajectory).

### **c) Gender probing**

* Designed a **gender-controlled prompt set** (e.g., *“She is …”*, *“He is …”*, *“The lady is …”*, *“The gentleman is …”*).  
* Collected the model’s **next-token predictions** and measured the probability mass on predefined categories:  
  * *appearance* (e.g., beautiful, pretty, handsome),  
  * *intelligence* (clever, foolish, wise),  
  * *virtue* (kind, proud, modest),  
  * *female roles* (woman, wife, sister),  
  * *male roles* (man, husband, brother).  
* First checked the **immediate next token** (1-step). Then extended to **n-step rollouts** (e.g., 5 tokens), averaging category mass across those steps to capture descriptors that usually appear after function words like “not” or “very.”  
* Logged both the **quantitative category probabilities** and the **actual continuations** generated by the model.

#### **What we found**

* **At 1-step:**  
  * The model almost never predicted category words directly (next token was usually “not,” punctuation, or function words).  
  * Category probability mass was very low and differences between female/male prompts were weak.

* **At 5-step average:**  
  * Clear **gendered patterns emerged**:  
    * Female prompts had higher mass for *appearance, intelligence, virtue,* and *female-role* words.  
    * Male prompts had higher mass for *male-role* words.  
  * Continuations matched this bias qualitatively: women were described by looks, virtue, or female roles, while men were described in terms of roles/status.  
* The **bias signal only surfaced when averaging across multiple steps**, not at the single next token.

#### **Why it falls short**

* Bias is subtle at the single-token level — only visible once you roll out several tokens.  
* The lexicons were restricted to single tokens, missing many Austen-style multi-token descriptors (e.g., “very beautiful”).  
* Continuations were repetitive and formulaic, reflecting the small model’s limited training, so the bias is present but not richly expressed.

---

## **3\. The new approach (KL-to-Clean \+ partial patching)**

### **a) KL-to-Clean metric**

* Instead of requiring the model to predict the right token,  
* we measure how close the **entire corrupt output distribution** is to the clean output distribution.  
* Gain(L) \= KL(p\_clean || p\_corrupt) − KL(p\_clean || p\_patched).  
* Positive gain ⇒ patching layer L reduces divergence, nudging corrupt closer to clean.

### **b) Position-masked patching**

* We patch **only the final token position**, not all tokens.  
* This localizes the intervention and avoids rewriting the whole sequence.

### **c) Epsilon mixing**

* Instead of hard replacement, we blend:  
   `patched = (1 − eps) * corrupt + eps * clean` (e.g. eps \= 0.1).

* This is a **gentle nudge**, not a full reset.

→ **patching is now:**

* **localized** (only last token position), and  
* **gentle** (just 10% clean mixed in),

---

## **4\. Why this works better**

* Clean vs corrupt differences are no longer “wiped out” by full replacement.  
* By only nudging a single position, we can measure which layers actually move the corrupt run toward the clean run.  
* Result: **non-flat gains** across layers, peaking in the later layers (5–7).

This reveals *where in the model* the decisive information (e.g., gender signal) is being read out and used.

---

## **5\. What the new results mean**

![][image1]

For the gender swap task:

* **Layers 5–7 show the largest gains** → these layers are causally important for turning gender features into predictions.  
* Earlier layers have smaller gains → they encode some information but don’t drive the final decision.

This aligns with circuit intuition:

* Early: encode lexical features.  
* Middle: route information.  
* Late: read out into logits.

---

## **6\. Why this is the right direction**

* The old methods either produced noise (logit diff) or trivial flat bars (full replacement).  
* The new method (KL-to-Clean \+ masked/mixed patching) finally shows **layer-specific causal importance**.  
* This sets the stage for **head-level patching** and **SAE-feature patching**, which will identify the actual fine-grained circuits.

---

**In one line:**  
We moved from *“everything looks the same”* (flat, trivial results) to *“we can see which layers matter”* by making the interventions smaller, more local, and evaluating similarity to clean rather than correctness.

---

## **Next steps (KL-to-clean approach)**

Those are general suggested steps. However, the main goal now is to zoom into each layer and see if we can identify sub-graphs of nodes (circuits) responsible for the variance of gain across the layers.

### **1\) Zoom in: head-level patching (same KL metric)**

Suggested updates:

1. Expose per-head `z` inside the `TransformerBlock`  
   *Note:* Ensure your block returns this info when `enable_cache=True`, e.g. via attributes the outer model reads.  
2. Cache it in `GPTModel.forward`  
3. Add a head-scan in `circuits.py`

### **2\) Extend the CLI to scan heads at a target layer**

### **3\) Path-patching (Q/K/V) to understand *how* heads work**

Once a few heads in (say) layer 6 pop:

* Add plan classes that replace only **K** or **V** for that head from clean→corrupt.  
* Score with KL again.  
  * Big gain when patching **K** → the head’s **addressing** (what to read) is critical.  
  * Big gain when patching **V** → the **content** it writes forward is critical.

### **4\) SAE overlay at those peak layers**

Two quick experiments:

* **Correlation**: encode `resid_pre[L]` with your SAE; compute top latents activated when those high-gain heads are patched. Are they “female,” “marriage,” etc.?

* **Causal**: decode a single latent (e.g., “female”) and **add** it at `resid_pre[L]` while observing:  
  * Head logit contributions (if you compute them), or  
  * Overall KL to clean.

### **5\) Robustness \+ controls (so you trust the story)**

* Vary `eps` (0.05, 0.1, 0.2): peak layers/heads should be stable.  
* Try multiple prompt templates (gender swap variants).  
* Negative control: patch random layer/head or add random Gaussian direction → tiny gains.  
* Shuffle positions: if the head is supposed to read the subject token, masking that token should nuke its gain.  
* Try different tasks (ioi, duty vs emotion…etc)

---

## **Main code folders and scripts:**

```
circuits_analysis___experiment/
├─ algorithms/
│  ├─ circuit_discovery.py   # High-level orchestration for circuit runs (loops over layers, saves per-layer metrics)
│  ├─ patching.py            # Activation patching utilities (clean/corrupt runs, KL-to-clean layer sweeps)
│  └─ metrics.py             # Metrics: KL divergence, logit diff, accuracy deltas, per-row probabilities
│
├─ data/
│  └─ ioi_data.py            # IOI prompt generation & loaders (names, templates, corrupt/clean splits)
│
├─ utils/
│  └─ tensors.py             # Small tensor helpers: batching, device moves, masking, softmax/logit ops
│
├─ viz/
│  └─  plotting.py            # Plots for layer gains, per-layer curves, bar/line charts, save to PNG
│
├─ out/                      # KL-to-clean & patching results, merged predictions
│  ├─ kl2clean_gender_swap_layer_gains.json
│  ├─ kl2clean_gender_swap_layer_gains.png
│  ├─ layer_gains.json
│  ├─ layer_gains.png
│  ├─ layer_patching_pre.csv
│  ├─ predictions_clean.csv
│  ├─ predictions_corrupt.csv
│  ├─ predictions_merged.csv
│  └─ summary.json
│
├─ out_gender/               # Gender probing outputs
│  ├─ continuations_next1.txt
│  ├─ continuations_next5.txt
│  ├─ per_example.csv
│  ├─ per_row_probs.csv
│  ├─ per_row_probs_next1.csv
│  ├─ per_row_probs_next5.csv
│  └─ summary.json
│
├─ circuit_tasks.py       # Generators for clean/corrupt minimal-pair prompts to probe circuits
├─ circuits.py            # Utilities for activation caching, patching, and causal metrics (must be moved to root directory)
├─ circuits_cli.py        # CLI entry point wrapping circuits.py  (must be moved to root directory)
├─ gender_probe.py        # v1 gender probing (next-token); writes per-example/prob CSVs (must be moved to root directory)
├─gender_probe_v2.py     # v2 with next-k continuation averaging & more robust sampling (must be moved to root directory)
└─ README.md
```

Perfect — now we can add **usage instructions** for the two main entry-point scripts (`circuits_cli.py` and `gender_probe_v2.py`) directly into your README.

Here’s the updated section to append under **Repo layout** in your README:

---

## How to Run

### 1. Causal Circuit Scans (`circuits_cli.py`)

Run clean vs. corrupt minimal-pair experiments with KL-to-clean or logit-diff metrics.

**Basic usage:**

```bash
python circuits_cli.py \
  --ckpt path/to/model.pth \
  --task [gender_swap|ioi|marriage_vs_wealth|emotion_vs_duty] \
  --pairs 128 \
  --metric kl \
  --final_pos_only \
  --eps 0.1 \
  --out out/
```

**Arguments:**

* `--ckpt` (required): Path to model checkpoint (`.pth`).
* `--task`: One of `gender_swap`, `ioi`, `marriage_vs_wealth`, `emotion_vs_duty`.
* `--pairs`: Number of clean/corrupt pairs (default: 128).
* `--metric`: `kl` (default) or `logit_diff`.
* `--final_pos_only`: Patch only the final position (recommended).
* `--eps`: Mixing strength (0.0 = hard replace, 0.1 = gentle nudge).
* `--out`: Output directory for JSON/PNG results.

**Outputs:**

* `layer_gains.json` – per-layer gains.
* `layer_gains.png` – bar plot of layer importance.

---

### 2. Gender Probing (`gender_probe_v2.py`)

Quantify gender bias in continuations with averaged probability mass over multiple tokens.

**Basic usage (greedy next-token rollout):**

```bash
python gender_probe_v2.py \
  --ckpt path/to/model.pth \
  --device cpu \
  --n 64 \
  --horizon 5 \
  --policy greedy \
  --outdir out_gender/
```

**With top-k sampling:**

```bash
python gender_probe_v2.py \
  --ckpt path/to/model.pth \
  --device cuda \
  --n 100 \
  --horizon 8 \
  --policy topk \
  --topk 50 \
  --temperature 1.0 \
  --outdir out_gender/
```

**Arguments:**

* `--ckpt` (required): Path to model checkpoint (`.pth`).
* `--device`: `cpu` (default) or `cuda`.
* `--n`: Prompts per gender (total prompts = 2 × n).
* `--horizon`: Number of tokens to roll out (default: 1).
* `--policy`: `greedy` (default) or `topk`.
* `--topk`: Top-k filter size (used if `policy=topk`).
* `--temperature`: Temperature for sampling (default: 1.0).
* `--outdir`: Output directory (default: `out_gender`).

**Outputs:**

* `per_example.csv` – top-k next-token predictions.
* `per_row_probs_next{N}.csv` – averaged category probabilities over N steps.
* `continuations_next{N}.txt` – generated continuations for inspection.
* `summary.json` – aggregated statistics.


[image1]: <data:image/png;base64,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>