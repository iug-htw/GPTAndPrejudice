{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d339",
   "metadata": {},
   "source": [
    "### Detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d281a",
   "metadata": {},
   "source": [
    "### Set up model configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72561797-a3f0-4d84-9883-64c447482389",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768, #768,         # Embedding dimension\n",
    "    \"n_heads\": 12, #12,          # Number of attention heads\n",
    "    \"n_layers\": 12, #12,         # Number of layers\n",
    "    \"drop_rate\": 0.2,       # Dropout rate\n",
    "    \"qkv_bias\": False,      # Query-Key-Value bias\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fbf11",
   "metadata": {},
   "source": [
    "### Load training and validation data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b562de-efe1-40d2-a5ba-350b1edb7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data.txt'\n",
    "val_file_path = 'val_text_data.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf608bf",
   "metadata": {},
   "source": [
    "### Initialize data loaders for training\n",
    "Data loaders implementation can be found in `./data_loader_v1.py`.\n",
    "\n",
    "This implementation follows the omplementation detailed in _Raschka, Sebastian. Build a Large Language Model (From Scratch). Manning Publications, 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bddf6dae-302d-4fc7-853b-2806a0c7d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860e138",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a472b92-340a-46c8-8526-d0ab1a59fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 404065\n",
      "Tokens: 126804\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "#tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "total_characters = len(train_data + val_data)\n",
    "total_tokens = len(tokenizer.encode(train_data + val_data, allowed_special={'<|endoftext|>'}))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50baeca2",
   "metadata": {},
   "source": [
    "Check which tokens are actuall used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da08e76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 0 -> '!'\n",
      "Token ID: 1 -> '\"'\n",
      "Token ID: 6 -> '''\n",
      "Token ID: 7 -> '('\n",
      "Token ID: 8 -> ')'\n",
      "Token ID: 40967 -> 'enh'\n",
      "Token ID: 11 -> ','\n",
      "Token ID: 12 -> '-'\n",
      "Token ID: 13 -> '.'\n",
      "Token ID: 16395 -> 'ANK'\n",
      "Token ID: 8207 -> 'dt'\n",
      "Token ID: 15 -> '0'\n",
      "Token ID: 81934 -> ' Erl'\n",
      "Token ID: 18 -> '3'\n",
      "Token ID: 8211 -> ' Sil'\n",
      "Token ID: 8212 -> 'rf'\n",
      "Token ID: 21 -> '6'\n",
      "Token ID: 16 -> '1'\n",
      "Token ID: 23 -> '8'\n",
      "Token ID: 24 -> '9'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read and tokenize Rilke text\n",
    "with open(\"train_text_data.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "# Convert tokens back to unique words\n",
    "unique_tokens = set(tokens)\n",
    "decoded_tokens = {token: tokenizer.decode([token]) for token in unique_tokens}\n",
    "\n",
    "# Print sample\n",
    "for token, word in list(decoded_tokens.items())[:20]:  # Show first 20 tokens\n",
    "    print(f\"Token ID: {token} -> '{word}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21b092",
   "metadata": {},
   "source": [
    "Print dictionary created by tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24439f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: b'!' -> ID: 0\n",
      "Token: b'\"' -> ID: 1\n",
      "Token: b'#' -> ID: 2\n",
      "Token: b'$' -> ID: 3\n",
      "Token: b'%' -> ID: 4\n",
      "Token: b'&' -> ID: 5\n",
      "Token: b\"'\" -> ID: 6\n",
      "Token: b'(' -> ID: 7\n",
      "Token: b')' -> ID: 8\n",
      "Token: b'*' -> ID: 9\n",
      "Token: b'+' -> ID: 10\n",
      "Token: b',' -> ID: 11\n",
      "Token: b'-' -> ID: 12\n",
      "Token: b'.' -> ID: 13\n",
      "Token: b'/' -> ID: 14\n",
      "Token: b'0' -> ID: 15\n",
      "Token: b'1' -> ID: 16\n",
      "Token: b'2' -> ID: 17\n",
      "Token: b'3' -> ID: 18\n",
      "Token: b'4' -> ID: 19\n"
     ]
    }
   ],
   "source": [
    "# Get the token dictionary (token -> token_id mapping)\n",
    "token_dict = tokenizer._mergeable_ranks\n",
    "\n",
    "# Print the first few tokens and their IDs\n",
    "for token, token_id in list(token_dict.items())[:20]:  # Adjust number to see more\n",
    "    print(f\"Token: {repr(token)} -> ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9229d2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " deadly\n"
     ]
    }
   ],
   "source": [
    "#Decode a Specific Token ID\n",
    "print(tokenizer.decode([25114]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6fdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the Full Vocabulary (Sorted by ID)\n",
    "sorted_vocab = sorted(token_dict.items(), key=lambda x: x[1])\n",
    "for token, token_id in sorted_vocab[:500]:  # Adjust number to see more\n",
    "    print(f\"ID: {token_id} -> Token: {repr(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864f121",
   "metadata": {},
   "source": [
    "<h2>Different tokenizer without prefixed vocabulary</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7abdb03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Im', '▁Park', '▁ist', '▁eine', '▁Rose', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: train_text_data.txt\n",
      "  input_format: \n",
      "  model_prefix: rilke_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 2\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: train_text_data.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 9344 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=320395\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9513% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=78\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999513\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 9343 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 26026 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 9343\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 13963\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 13963 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=9131 obj=11.5237 num_tokens=28534 num_tokens/piece=3.12496\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8217 obj=9.55957 num_tokens=28713 num_tokens/piece=3.49434\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=6159 obj=9.67986 num_tokens=30849 num_tokens/piece=5.00877\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=6154 obj=9.60013 num_tokens=30857 num_tokens/piece=5.01414\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=5499 obj=9.70418 num_tokens=32017 num_tokens/piece=5.82233\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=5499 obj=9.67012 num_tokens=32018 num_tokens/piece=5.82251\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: rilke_tokenizer.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: rilke_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train a new tokenizer based on Rilke texts\n",
    "spm.SentencePieceTrainer.train(input=\"train_text_data.txt\", model_prefix=\"rilke_tokenizer\", vocab_size=5000,num_threads=2)\n",
    "\n",
    "# Load trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"rilke_tokenizer.model\")\n",
    "\n",
    "# Tokenize text\n",
    "text = \"Im Park ist eine Rose.\"\n",
    "\n",
    "tokens = sp.encode(text, out_type=str)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25807b78",
   "metadata": {},
   "source": [
    "<h3>Sanity Check</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb273e3a-602f-45b0-a404-89fa88b6aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1f969edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean(): \n",
    "    \"\"\"\n",
    "    This is a function for GPU data claening before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    \n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.mps.empty_cache()  # Attempt to release MPS memory\n",
    "    \n",
    "    # Move tensors to CPU\n",
    "    for tensor in list(globals().values()):\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device == torch.device(\"mps\"):\n",
    "            tensor.to(\"cpu\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    del tensor\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "    print(\"Allocated Memory:\", torch.mps.current_allocated_memory() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da82d2c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5f2c9bfd-5c57-4af6-98e8-5da47988d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train import train_model_simple\n",
    "import time\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\n",
    "def train(train_loader, val_loader,\n",
    "          num_epochs=10, eval_iter=5, \n",
    "          sample_text=\"Every effort moves you\",\n",
    "          checkpoint_path=\"model_and_optimizer.pth\"):\n",
    "\n",
    "    global train_losses, val_losses, track_tokens_seen  # Ensure these are updated globally\n",
    "\n",
    "    if device == \"mps\":\n",
    "        clean()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.05)\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=num_epochs, eval_iter=eval_iter,\n",
    "        start_context=sample_text, cfg=GPT_CONFIG_124M,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        print(50 * \"=\")\n",
    "        clean()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae6fc",
   "metadata": {},
   "source": [
    "### Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dda45148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.901, Val loss 4.879\n",
      "Ep 1 (Step 000025): Train loss 2.628, Val loss 4.636\n",
      "Ep 1 (Step 000050): Train loss 2.604, Val loss 4.544\n",
      "Ep 1 (Step 000075): Train loss 2.428, Val loss 4.542\n",
      "Im Park ist und was uns noch ein Läche, ein Ding wenn sie ihn die stummergleicht das wirklichkeiten und aus dem verwie fünd, j\n",
      "Ep 2 (Step 000100): Train loss 2.457, Val loss 4.520\n",
      "Ep 2 (Step 000125): Train loss 2.320, Val loss 4.486\n",
      "Ep 2 (Step 000150): Train loss 2.317, Val loss 4.506\n",
      "Im Park ist und wurden ich um bist heim, an die Alter? Aberall, du deinen Hächauen Stunden, nur dich der sein; jetzt,— als\n",
      "Ep 3 (Step 000175): Train loss 2.261, Val loss 4.567\n",
      "Ep 3 (Step 000200): Train loss 2.108, Val loss 4.512\n",
      "Ep 3 (Step 000225): Train loss 2.196, Val loss 4.554\n",
      "Im Park ist daß ich, ist –ern, welcheln? Dinge [chel mein Händen\n",
      "Ep 4 (Step 000250): Train loss 2.059, Val loss 4.557\n",
      "Ep 4 (Step 000275): Train loss 1.747, Val loss 4.522\n",
      "Ep 4 (Step 000300): Train loss 1.944, Val loss 4.554\n",
      "Im Park ist; die Schoß er gebühren Häumen, aber wenn er das, ein selben Gesichtet, wie ein seiner sein Bildern, dieser Blauf\n",
      "Ep 5 (Step 000325): Train loss 1.930, Val loss 4.555\n",
      "Ep 5 (Step 000350): Train loss 1.817, Val loss 4.585\n",
      "Ep 5 (Step 000375): Train loss 1.796, Val loss 4.607\n",
      "Im Park ist in den Balindern ist du den Bliche wurden, das letzt von dem Todauen, die Tür ein Blute sich aufgeln das: um gesch so\n",
      "Ep 6 (Step 000400): Train loss 1.628, Val loss 4.628\n",
      "Ep 6 (Step 000425): Train loss 1.432, Val loss 4.640\n",
      "Ep 6 (Step 000450): Train loss 1.589, Val loss 4.718\n",
      "Ep 6 (Step 000475): Train loss 1.456, Val loss 4.701\n",
      "Im Park ist; er Dich die Sonne vorbe werden Himmeln\n",
      "Ep 7 (Step 000500): Train loss 1.370, Val loss 4.775\n",
      "Ep 7 (Step 000525): Train loss 1.477, Val loss 4.766\n",
      "Ep 7 (Step 000550): Train loss 1.290, Val loss 4.771\n",
      "Im Park ist das Meer alsich schwagend, ging sie zänden nahr stauen, die Tür ein Herz hin, aufstieben über dem Einsamen \n",
      "Training completed in 16.40 minutes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(train_loader, val_loader, num_epochs=7,\n",
    "      eval_iter=25, sample_text=\"Im Park ist\",\n",
    "      checkpoint_path=\"model_and_optimizer_5.pth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6651aada",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_and_optimizer_6.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=\u001b[32m0.0004\u001b[39m, weight_decay=\u001b[32m0.1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_and_optimizer_6.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m model.load_state_dict(checkpoint[\u001b[33m\"\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      7\u001b[39m optimizer.load_state_dict(checkpoint[\u001b[33m\"\u001b[39m\u001b[33moptimizer_state_dict\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/Cramming_example/lib/python3.13/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/Cramming_example/lib/python3.13/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/Cramming_example/lib/python3.13/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'model_and_optimizer_6.pth'"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer_6.pth\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0c982",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Er ruftender bist der Schritte den Sohn\n",
      "die würde,\n",
      "kür eines verwie leise seine Sehlt\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, prompt=\"Er ruft\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=1,\n",
    "    top_k=40,\n",
    "    eos_id=13\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a25f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cramming_example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
