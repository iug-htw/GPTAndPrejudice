{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d339",
   "metadata": {},
   "source": [
    "### Detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d281a",
   "metadata": {},
   "source": [
    "### Set up model configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72561797-a3f0-4d84-9883-64c447482389",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.2,       # Dropout rate\n",
    "    \"qkv_bias\": False,      # Query-Key-Value bias\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8fb1a",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767d9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 1820039\n",
      "Tokens: 415577\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fbf11",
   "metadata": {},
   "source": [
    "### Load training and validation data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b562de-efe1-40d2-a5ba-350b1edb7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data.txt'\n",
    "val_file_path = 'val_text_data.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf608bf",
   "metadata": {},
   "source": [
    "### Initialize data loaders for training\n",
    "Data loaders implementation can be found in `./data_loader_v1.py`.\n",
    "\n",
    "This implementation follows the omplementation detailed in _Raschka, Sebastian. Build a Large Language Model (From Scratch). Manning Publications, 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf6dae-302d-4fc7-853b-2806a0c7d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae321436",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = len(train_data + val_data)\n",
    "total_tokens = len(tokenizer.encode(train_data + val_data, allowed_special={'<|endoftext|>'}))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb273e3a-602f-45b0-a404-89fa88b6aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f969edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean(): \n",
    "    \"\"\"\n",
    "    This is a function for GPU data claening before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    \n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.mps.empty_cache()  # Attempt to release MPS memory\n",
    "    \n",
    "    # Move tensors to CPU\n",
    "    for tensor in list(globals().values()):\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device == torch.device(\"mps\"):\n",
    "            tensor.to(\"cpu\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    del tensor\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "    print(\"Allocated Memory:\", torch.mps.current_allocated_memory() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da82d2c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f2c9bfd-5c57-4af6-98e8-5da47988d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train import train_model_simple\n",
    "import time\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\n",
    "def train(train_loader, val_loader,\n",
    "          num_epochs=10, eval_iter=5, \n",
    "          sample_text=\"Every effort moves you\",\n",
    "          checkpoint_path=\"model_and_optimizer.pth\"):\n",
    "\n",
    "    global train_losses, val_losses, track_tokens_seen  # Ensure these are updated globally\n",
    "\n",
    "    if device == \"mps\":\n",
    "        clean()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.05)\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=num_epochs, eval_iter=eval_iter,\n",
    "        start_context=sample_text, cfg=GPT_CONFIG_124M,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        print(50 * \"=\")\n",
    "        clean()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae6fc",
   "metadata": {},
   "source": [
    "### Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dda45148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS Available: True\n",
      "Allocated Memory: 0.0 MB\n",
      "==================================================\n",
      "Starting training...\n",
      "Ep 1 (Step 000000): Train loss 10.521, Val loss 10.502\n",
      "Ep 1 (Step 000025): Train loss 7.593, Val loss 7.567\n",
      "Ep 1 (Step 000050): Train loss 6.531, Val loss 6.613\n",
      "Ep 1 (Step 000075): Train loss 6.171, Val loss 6.316\n",
      "Ep 1 (Step 000100): Train loss 5.955, Val loss 6.079\n",
      "Ep 1 (Step 000125): Train loss 5.765, Val loss 5.947\n",
      "Ep 1 (Step 000150): Train loss 5.593, Val loss 5.798\n",
      "Ep 1 (Step 000175): Train loss 5.504, Val loss 5.699\n",
      "Ep 1 (Step 000200): Train loss 5.366, Val loss 5.609\n",
      "Ep 1 (Step 000225): Train loss 5.361, Val loss 5.547\n",
      "Ep 1 (Step 000250): Train loss 5.231, Val loss 5.508\n",
      "Ep 1 (Step 000275): Train loss 5.203, Val loss 5.447\n",
      "Ep 1 (Step 000300): Train loss 5.135, Val loss 5.394\n",
      "Ep 1 (Step 000325): Train loss 5.125, Val loss 5.357\n",
      "Ep 1 (Step 000350): Train loss 5.033, Val loss 5.329\n",
      "The horses are a way on such a family\n",
      "Ep 2 (Step 000375): Train loss 5.000, Val loss 5.297\n",
      "Ep 2 (Step 000400): Train loss 4.942, Val loss 5.271\n",
      "Ep 2 (Step 000425): Train loss 4.892, Val loss 5.245\n",
      "Ep 2 (Step 000450): Train loss 4.872, Val loss 5.209\n",
      "Ep 2 (Step 000475): Train loss 4.873, Val loss 5.207\n",
      "Ep 2 (Step 000500): Train loss 4.787, Val loss 5.187\n",
      "Ep 2 (Step 000525): Train loss 4.773, Val loss 5.171\n",
      "Ep 2 (Step 000550): Train loss 4.800, Val loss 5.139\n",
      "Ep 2 (Step 000575): Train loss 4.724, Val loss 5.135\n",
      "Ep 2 (Step 000600): Train loss 4.719, Val loss 5.124\n",
      "Ep 2 (Step 000625): Train loss 4.660, Val loss 5.098\n",
      "Ep 2 (Step 000650): Train loss 4.663, Val loss 5.085\n",
      "Ep 2 (Step 000675): Train loss 4.604, Val loss 5.088\n",
      "Ep 2 (Step 000700): Train loss 4.653, Val loss 5.051\n",
      "Ep 2 (Step 000725): Train loss 4.564, Val loss 5.049\n",
      "The horses are, I must be done for it would the most people so happy to be no one of the very good good in the time\n",
      "Ep 3 (Step 000750): Train loss 4.590, Val loss 5.039\n",
      "Ep 3 (Step 000775): Train loss 4.492, Val loss 5.040\n",
      "Ep 3 (Step 000800): Train loss 4.569, Val loss 5.022\n",
      "Ep 3 (Step 000825): Train loss 4.565, Val loss 5.019\n",
      "Ep 3 (Step 000850): Train loss 4.469, Val loss 4.998\n",
      "Ep 3 (Step 000875): Train loss 4.520, Val loss 4.987\n",
      "Ep 3 (Step 000900): Train loss 4.471, Val loss 4.978\n",
      "Ep 3 (Step 000925): Train loss 4.454, Val loss 4.962\n",
      "Ep 3 (Step 000950): Train loss 4.431, Val loss 4.974\n",
      "Ep 3 (Step 000975): Train loss 4.419, Val loss 4.967\n",
      "Ep 3 (Step 001000): Train loss 4.416, Val loss 4.949\n",
      "Ep 3 (Step 001025): Train loss 4.426, Val loss 4.939\n",
      "Ep 3 (Step 001050): Train loss 4.363, Val loss 4.925\n",
      "Ep 3 (Step 001075): Train loss 4.296, Val loss 4.918\n",
      "The horses are quite on one or in the other, a few minutes were to the other point of course of her brother, which had no other\n",
      "Ep 4 (Step 001100): Train loss 4.329, Val loss 4.912\n",
      "Ep 4 (Step 001125): Train loss 4.314, Val loss 4.918\n",
      "Ep 4 (Step 001150): Train loss 4.278, Val loss 4.903\n",
      "Ep 4 (Step 001175): Train loss 4.349, Val loss 4.912\n",
      "Ep 4 (Step 001200): Train loss 4.298, Val loss 4.900\n",
      "Ep 4 (Step 001225): Train loss 4.330, Val loss 4.907\n",
      "Ep 4 (Step 001250): Train loss 4.264, Val loss 4.889\n",
      "Ep 4 (Step 001275): Train loss 4.205, Val loss 4.884\n",
      "Ep 4 (Step 001300): Train loss 4.223, Val loss 4.882\n",
      "Ep 4 (Step 001325): Train loss 4.146, Val loss 4.871\n",
      "Ep 4 (Step 001350): Train loss 4.219, Val loss 4.872\n",
      "Ep 4 (Step 001375): Train loss 4.200, Val loss 4.855\n",
      "Ep 4 (Step 001400): Train loss 4.211, Val loss 4.860\n",
      "Ep 4 (Step 001425): Train loss 4.127, Val loss 4.852\n",
      "Ep 4 (Step 001450): Train loss 4.191, Val loss 4.842\n",
      "The horses are engaged to her the whole of her mother, and this, and with some very well with him\n",
      "Ep 5 (Step 001475): Train loss 4.072, Val loss 4.851\n",
      "Ep 5 (Step 001500): Train loss 4.104, Val loss 4.849\n",
      "Ep 5 (Step 001525): Train loss 4.063, Val loss 4.852\n",
      "Ep 5 (Step 001550): Train loss 4.103, Val loss 4.851\n",
      "Ep 5 (Step 001575): Train loss 4.080, Val loss 4.853\n",
      "Ep 5 (Step 001600): Train loss 4.025, Val loss 4.837\n",
      "Ep 5 (Step 001625): Train loss 3.986, Val loss 4.830\n",
      "Ep 5 (Step 001650): Train loss 3.990, Val loss 4.826\n",
      "Ep 5 (Step 001675): Train loss 4.000, Val loss 4.826\n",
      "Ep 5 (Step 001700): Train loss 3.993, Val loss 4.815\n",
      "Ep 5 (Step 001725): Train loss 4.008, Val loss 4.816\n",
      "Ep 5 (Step 001750): Train loss 3.960, Val loss 4.815\n",
      "Ep 5 (Step 001775): Train loss 3.974, Val loss 4.816\n",
      "Ep 5 (Step 001800): Train loss 3.902, Val loss 4.810\n",
      "The horses are full of his mind\n",
      "Ep 6 (Step 001825): Train loss 3.923, Val loss 4.792\n",
      "Ep 6 (Step 001850): Train loss 3.918, Val loss 4.797\n",
      "Ep 6 (Step 001875): Train loss 3.897, Val loss 4.815\n",
      "Ep 6 (Step 001900): Train loss 3.914, Val loss 4.804\n",
      "Ep 6 (Step 001925): Train loss 3.830, Val loss 4.806\n",
      "Ep 6 (Step 001950): Train loss 3.878, Val loss 4.792\n",
      "Ep 6 (Step 001975): Train loss 3.879, Val loss 4.789\n",
      "Ep 6 (Step 002000): Train loss 3.795, Val loss 4.792\n",
      "Ep 6 (Step 002025): Train loss 3.835, Val loss 4.815\n",
      "Ep 6 (Step 002050): Train loss 3.786, Val loss 4.810\n",
      "Ep 6 (Step 002075): Train loss 3.753, Val loss 4.797\n",
      "Ep 6 (Step 002100): Train loss 3.756, Val loss 4.798\n",
      "Ep 6 (Step 002125): Train loss 3.753, Val loss 4.787\n",
      "Ep 6 (Step 002150): Train loss 3.728, Val loss 4.786\n",
      "Ep 6 (Step 002175): Train loss 3.726, Val loss 4.786\n",
      "The horses are not so; and if he walked on his wife\n",
      "Ep 7 (Step 002200): Train loss 3.727, Val loss 4.796\n",
      "Ep 7 (Step 002225): Train loss 3.677, Val loss 4.788\n",
      "Ep 7 (Step 002250): Train loss 3.667, Val loss 4.819\n",
      "Ep 7 (Step 002275): Train loss 3.647, Val loss 4.807\n",
      "Ep 7 (Step 002300): Train loss 3.584, Val loss 4.807\n",
      "Ep 7 (Step 002325): Train loss 3.592, Val loss 4.808\n",
      "Ep 7 (Step 002350): Train loss 3.620, Val loss 4.801\n",
      "Ep 7 (Step 002375): Train loss 3.603, Val loss 4.802\n",
      "Ep 7 (Step 002400): Train loss 3.579, Val loss 4.791\n",
      "Ep 7 (Step 002425): Train loss 3.537, Val loss 4.802\n",
      "Ep 7 (Step 002450): Train loss 3.500, Val loss 4.800\n",
      "Ep 7 (Step 002475): Train loss 3.536, Val loss 4.782\n",
      "Ep 7 (Step 002500): Train loss 3.498, Val loss 4.791\n",
      "Ep 7 (Step 002525): Train loss 3.485, Val loss 4.785\n",
      "Ep 7 (Step 002550): Train loss 3.489, Val loss 4.799\n",
      "The horses are going to be kept them on board\n",
      "Training completed in 46.72 minutes.\n",
      "==================================================\n",
      "MPS Available: True\n",
      "Allocated Memory: 2943.4462890625 MB\n"
     ]
    }
   ],
   "source": [
    "# train model on 3 books\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=7,\n",
    "      eval_iter=25, sample_text=\"The horses are\",\n",
    "      checkpoint_path=\"model_and_optimizer_5.pth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6651aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer_5.pth\", weights_only=True, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6327510-497c-4077-8fb9-d2cbf6508399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import combinations\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU and ROUGE metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "### 1. Perplexity Calculation ###\n",
    "def compute_perplexity(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)  # Forward pass\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * target_ids.numel()\n",
    "            total_tokens += target_ids.numel()\n",
    "\n",
    "    perplexity = np.exp(total_loss / total_tokens)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "### 2. Word Embedding Association Test (WEAT) ###\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "def weat_score(model, target_words_1, target_words_2, attribute_words_1, attribute_words_2, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Measures bias by comparing how close different groups of words are in embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding(word):\n",
    "        token_id = tokenizer.encode(word, allowed_special={'<|endoftext|>'})[0]  # Get token ID\n",
    "        with torch.no_grad():\n",
    "            embed = model.tok_emb(torch.tensor([token_id], device=device)).cpu().numpy()\n",
    "        return embed.flatten()\n",
    "\n",
    "    # Get embeddings\n",
    "    target_1_embs = [get_embedding(w) for w in target_words_1]\n",
    "    target_2_embs = [get_embedding(w) for w in target_words_2]\n",
    "    attr_1_embs = [get_embedding(w) for w in attribute_words_1]\n",
    "    attr_2_embs = [get_embedding(w) for w in attribute_words_2]\n",
    "\n",
    "    def association(t, A, B):\n",
    "        return np.mean([cosine_similarity(t, a) for a in A]) - np.mean([cosine_similarity(t, b) for b in B])\n",
    "\n",
    "    # Compute WEAT score\n",
    "    s1 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_1_embs])\n",
    "    s2 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_2_embs])\n",
    "    \n",
    "    weat_score = s1 - s2  # Difference in associations\n",
    "    return weat_score\n",
    "\n",
    "\n",
    "### 3. BLEU & ROUGE Score Calculation ###\n",
    "def compute_bleu_rouge(references, predictions):\n",
    "    \"\"\"\n",
    "    Compute BLEU and ROUGE scores between reference texts and model-generated texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenizing correctly for BLEU\n",
    "    references = [[ref] for ref in references]  # Each reference sentence must be a list of words inside another list\n",
    "    predictions = [pred for pred in predictions]  # Tokenize predictions\n",
    "    \n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references)['bleu']\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=[[r] for r in references])  # Wrap in another list\n",
    "\n",
    "    return bleu_score, rouge_score[\"rougeL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e944baa-ad49-46fa-90f4-0ffb87369f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_perplexity(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097d63a-0a44-4191-bc81-f4ab0c83f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_male = [\"gentleman\", \"husband\", \"clergyman\", \"brother\", \"captain\"]\n",
    "target_female = [\"lady\", \"governess\", \"sister\", \"wife\", \"widow\"]\n",
    "\n",
    "attribute_male = [\"honour\", \"duty\", \"wisdom\", \"fortitude\", \"independence\"]\n",
    "attribute_female = [\"grace\", \"affection\", \"beauty\", \"delicacy\", \"modesty\"]\n",
    "\n",
    "\n",
    "weat_score(model, target_male, target_female, attribute_male, attribute_female, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed05f8b-0033-41cc-8476-36adfb17b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "predictions = []\n",
    "\n",
    "# Example test prompts\n",
    "test_sentences = [\n",
    "    (\"A single man in possession of a good fortune, must be ___\", \"in want of a wife\"),\n",
    "    (\"The servants, I suppose, forgot to tell you that Mr. Palmer was ___\", \"not in the house.\"),\n",
    "    (\"He rose up, and ___\", \"walked across the room.\"),\n",
    "    (\"I should be undeserving of the confidence ___\", \"you have honoured me with\"),\n",
    "    (\"A few months had seen the beginning and ___\", \"the end of their acquaintance\")\n",
    "]\n",
    "\n",
    "for element in test_sentences:\n",
    "    sentence = element[0]\n",
    "    expected = element[1]\n",
    "    \n",
    "    encoded = tokenizer.encode(sentence.replace(\" ___\", \"\"), allowed_special={'<|endoftext|>'})\n",
    "    input_ids = torch.tensor(encoded).unsqueeze(0)\n",
    "    \n",
    "    # Generate text\n",
    "    # logits = model(input_ids)\n",
    "    # logits = logits[:, -1, :]\n",
    "    # idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    # flat = idx_next.squeeze(0) # remove batch dimension\n",
    "    \n",
    "    # # Decode predictions & reference text\n",
    "    # generated_text = tokenizer.decode(flat.tolist())\n",
    "    \n",
    "    generated_text = generate(\n",
    "        model=model, tokenizer=tokenizer,\n",
    "        prompt=sentence.replace(\" ___\", \"\"),\n",
    "        max_new_tokens=5, context_size=GPT_CONFIG_124M['context_length'],\n",
    "        device=\"cpu\",\n",
    "        temperature=0.5,\n",
    "        top_k=40,\n",
    "    )\n",
    "    \n",
    "    # Append reference (ground truth completion) & model output\n",
    "    references.append(sentence.replace(\"___\", expected))  # Example correct completion\n",
    "    predictions.append(generated_text)\n",
    "\n",
    "print(\"references:\", references)\n",
    "print(\"predictions:\", predictions)\n",
    "print(50*\"=\")\n",
    "\n",
    "# Compute BLEU and ROUGE scores\n",
    "bleu, rouge = compute_bleu_rouge(references, predictions)\n",
    "print(f\"BLEU Score: {bleu:.4f}, ROUGE-L Score: {rouge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2caf7-feea-4fad-aed0-6cd4122f40e1",
   "metadata": {},
   "source": [
    "### Example text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adb0c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am glad to have the greatest character, when we had to leave them.\"\n",
      "But Sir John and said to her the room, and she spoke at first she would scarcely any body\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    prompt=\"Hello, I am\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=1,\n",
    "    top_k=40,\n",
    "    eos_id=13\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS Available: True\n",
      "Allocated Memory: 2113.8642578125 MB\n"
     ]
    }
   ],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a25f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
