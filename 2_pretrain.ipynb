{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d339",
   "metadata": {},
   "source": [
    "### Detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d281a",
   "metadata": {},
   "source": [
    "### Set up model configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "72561797-a3f0-4d84-9883-64c447482389",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768, #768,         # Embedding dimension\n",
    "    \"n_heads\": 12, #12,          # Number of attention heads\n",
    "    \"n_layers\": 12, #12,         # Number of layers\n",
    "    \"drop_rate\": 0.2,       # Dropout rate\n",
    "    \"qkv_bias\": False,      # Query-Key-Value bias\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fbf11",
   "metadata": {},
   "source": [
    "### Load training and validation data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e2b562de-efe1-40d2-a5ba-350b1edb7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data.txt'\n",
    "val_file_path = 'val_text_data.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf608bf",
   "metadata": {},
   "source": [
    "### Initialize data loaders for training\n",
    "Data loaders implementation can be found in `./data_loader_v1.py`.\n",
    "\n",
    "This implementation follows the omplementation detailed in _Raschka, Sebastian. Build a Large Language Model (From Scratch). Manning Publications, 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bddf6dae-302d-4fc7-853b-2806a0c7d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860e138",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5a472b92-340a-46c8-8526-d0ab1a59fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 246550\n",
      "Tokens: 76005\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "#tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "total_characters = len(train_data + val_data)\n",
    "total_tokens = len(tokenizer.encode(train_data + val_data, allowed_special={'<|endoftext|>'}))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21b092",
   "metadata": {},
   "source": [
    "Print dictionary created by tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "24439f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: b'!' -> ID: 0\n",
      "Token: b'\"' -> ID: 1\n",
      "Token: b'#' -> ID: 2\n",
      "Token: b'$' -> ID: 3\n",
      "Token: b'%' -> ID: 4\n",
      "Token: b'&' -> ID: 5\n",
      "Token: b\"'\" -> ID: 6\n",
      "Token: b'(' -> ID: 7\n",
      "Token: b')' -> ID: 8\n",
      "Token: b'*' -> ID: 9\n",
      "Token: b'+' -> ID: 10\n",
      "Token: b',' -> ID: 11\n",
      "Token: b'-' -> ID: 12\n",
      "Token: b'.' -> ID: 13\n",
      "Token: b'/' -> ID: 14\n",
      "Token: b'0' -> ID: 15\n",
      "Token: b'1' -> ID: 16\n",
      "Token: b'2' -> ID: 17\n",
      "Token: b'3' -> ID: 18\n",
      "Token: b'4' -> ID: 19\n"
     ]
    }
   ],
   "source": [
    "# Get the token dictionary (token -> token_id mapping)\n",
    "token_dict = tokenizer._mergeable_ranks\n",
    "\n",
    "# Print the first few tokens and their IDs\n",
    "for token, token_id in list(token_dict.items())[:20]:  # Adjust number to see more\n",
    "    print(f\"Token: {repr(token)} -> ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9229d2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " head\n"
     ]
    }
   ],
   "source": [
    "#Decode a Specific Token ID\n",
    "print(tokenizer.decode([2010]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7e6fdcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0 -> Token: b'!'\n",
      "ID: 1 -> Token: b'\"'\n",
      "ID: 2 -> Token: b'#'\n",
      "ID: 3 -> Token: b'$'\n",
      "ID: 4 -> Token: b'%'\n",
      "ID: 5 -> Token: b'&'\n",
      "ID: 6 -> Token: b\"'\"\n",
      "ID: 7 -> Token: b'('\n",
      "ID: 8 -> Token: b')'\n",
      "ID: 9 -> Token: b'*'\n",
      "ID: 10 -> Token: b'+'\n",
      "ID: 11 -> Token: b','\n",
      "ID: 12 -> Token: b'-'\n",
      "ID: 13 -> Token: b'.'\n",
      "ID: 14 -> Token: b'/'\n",
      "ID: 15 -> Token: b'0'\n",
      "ID: 16 -> Token: b'1'\n",
      "ID: 17 -> Token: b'2'\n",
      "ID: 18 -> Token: b'3'\n",
      "ID: 19 -> Token: b'4'\n",
      "ID: 20 -> Token: b'5'\n",
      "ID: 21 -> Token: b'6'\n",
      "ID: 22 -> Token: b'7'\n",
      "ID: 23 -> Token: b'8'\n",
      "ID: 24 -> Token: b'9'\n",
      "ID: 25 -> Token: b':'\n",
      "ID: 26 -> Token: b';'\n",
      "ID: 27 -> Token: b'<'\n",
      "ID: 28 -> Token: b'='\n",
      "ID: 29 -> Token: b'>'\n",
      "ID: 30 -> Token: b'?'\n",
      "ID: 31 -> Token: b'@'\n",
      "ID: 32 -> Token: b'A'\n",
      "ID: 33 -> Token: b'B'\n",
      "ID: 34 -> Token: b'C'\n",
      "ID: 35 -> Token: b'D'\n",
      "ID: 36 -> Token: b'E'\n",
      "ID: 37 -> Token: b'F'\n",
      "ID: 38 -> Token: b'G'\n",
      "ID: 39 -> Token: b'H'\n",
      "ID: 40 -> Token: b'I'\n",
      "ID: 41 -> Token: b'J'\n",
      "ID: 42 -> Token: b'K'\n",
      "ID: 43 -> Token: b'L'\n",
      "ID: 44 -> Token: b'M'\n",
      "ID: 45 -> Token: b'N'\n",
      "ID: 46 -> Token: b'O'\n",
      "ID: 47 -> Token: b'P'\n",
      "ID: 48 -> Token: b'Q'\n",
      "ID: 49 -> Token: b'R'\n",
      "ID: 50 -> Token: b'S'\n",
      "ID: 51 -> Token: b'T'\n",
      "ID: 52 -> Token: b'U'\n",
      "ID: 53 -> Token: b'V'\n",
      "ID: 54 -> Token: b'W'\n",
      "ID: 55 -> Token: b'X'\n",
      "ID: 56 -> Token: b'Y'\n",
      "ID: 57 -> Token: b'Z'\n",
      "ID: 58 -> Token: b'['\n",
      "ID: 59 -> Token: b'\\\\'\n",
      "ID: 60 -> Token: b']'\n",
      "ID: 61 -> Token: b'^'\n",
      "ID: 62 -> Token: b'_'\n",
      "ID: 63 -> Token: b'`'\n",
      "ID: 64 -> Token: b'a'\n",
      "ID: 65 -> Token: b'b'\n",
      "ID: 66 -> Token: b'c'\n",
      "ID: 67 -> Token: b'd'\n",
      "ID: 68 -> Token: b'e'\n",
      "ID: 69 -> Token: b'f'\n",
      "ID: 70 -> Token: b'g'\n",
      "ID: 71 -> Token: b'h'\n",
      "ID: 72 -> Token: b'i'\n",
      "ID: 73 -> Token: b'j'\n",
      "ID: 74 -> Token: b'k'\n",
      "ID: 75 -> Token: b'l'\n",
      "ID: 76 -> Token: b'm'\n",
      "ID: 77 -> Token: b'n'\n",
      "ID: 78 -> Token: b'o'\n",
      "ID: 79 -> Token: b'p'\n",
      "ID: 80 -> Token: b'q'\n",
      "ID: 81 -> Token: b'r'\n",
      "ID: 82 -> Token: b's'\n",
      "ID: 83 -> Token: b't'\n",
      "ID: 84 -> Token: b'u'\n",
      "ID: 85 -> Token: b'v'\n",
      "ID: 86 -> Token: b'w'\n",
      "ID: 87 -> Token: b'x'\n",
      "ID: 88 -> Token: b'y'\n",
      "ID: 89 -> Token: b'z'\n",
      "ID: 90 -> Token: b'{'\n",
      "ID: 91 -> Token: b'|'\n",
      "ID: 92 -> Token: b'}'\n",
      "ID: 93 -> Token: b'~'\n",
      "ID: 94 -> Token: b'\\xa1'\n",
      "ID: 95 -> Token: b'\\xa2'\n",
      "ID: 96 -> Token: b'\\xa3'\n",
      "ID: 97 -> Token: b'\\xa4'\n",
      "ID: 98 -> Token: b'\\xa5'\n",
      "ID: 99 -> Token: b'\\xa6'\n",
      "ID: 100 -> Token: b'\\xa7'\n",
      "ID: 101 -> Token: b'\\xa8'\n",
      "ID: 102 -> Token: b'\\xa9'\n",
      "ID: 103 -> Token: b'\\xaa'\n",
      "ID: 104 -> Token: b'\\xab'\n",
      "ID: 105 -> Token: b'\\xac'\n",
      "ID: 106 -> Token: b'\\xae'\n",
      "ID: 107 -> Token: b'\\xaf'\n",
      "ID: 108 -> Token: b'\\xb0'\n",
      "ID: 109 -> Token: b'\\xb1'\n",
      "ID: 110 -> Token: b'\\xb2'\n",
      "ID: 111 -> Token: b'\\xb3'\n",
      "ID: 112 -> Token: b'\\xb4'\n",
      "ID: 113 -> Token: b'\\xb5'\n",
      "ID: 114 -> Token: b'\\xb6'\n",
      "ID: 115 -> Token: b'\\xb7'\n",
      "ID: 116 -> Token: b'\\xb8'\n",
      "ID: 117 -> Token: b'\\xb9'\n",
      "ID: 118 -> Token: b'\\xba'\n",
      "ID: 119 -> Token: b'\\xbb'\n",
      "ID: 120 -> Token: b'\\xbc'\n",
      "ID: 121 -> Token: b'\\xbd'\n",
      "ID: 122 -> Token: b'\\xbe'\n",
      "ID: 123 -> Token: b'\\xbf'\n",
      "ID: 124 -> Token: b'\\xc0'\n",
      "ID: 125 -> Token: b'\\xc1'\n",
      "ID: 126 -> Token: b'\\xc2'\n",
      "ID: 127 -> Token: b'\\xc3'\n",
      "ID: 128 -> Token: b'\\xc4'\n",
      "ID: 129 -> Token: b'\\xc5'\n",
      "ID: 130 -> Token: b'\\xc6'\n",
      "ID: 131 -> Token: b'\\xc7'\n",
      "ID: 132 -> Token: b'\\xc8'\n",
      "ID: 133 -> Token: b'\\xc9'\n",
      "ID: 134 -> Token: b'\\xca'\n",
      "ID: 135 -> Token: b'\\xcb'\n",
      "ID: 136 -> Token: b'\\xcc'\n",
      "ID: 137 -> Token: b'\\xcd'\n",
      "ID: 138 -> Token: b'\\xce'\n",
      "ID: 139 -> Token: b'\\xcf'\n",
      "ID: 140 -> Token: b'\\xd0'\n",
      "ID: 141 -> Token: b'\\xd1'\n",
      "ID: 142 -> Token: b'\\xd2'\n",
      "ID: 143 -> Token: b'\\xd3'\n",
      "ID: 144 -> Token: b'\\xd4'\n",
      "ID: 145 -> Token: b'\\xd5'\n",
      "ID: 146 -> Token: b'\\xd6'\n",
      "ID: 147 -> Token: b'\\xd7'\n",
      "ID: 148 -> Token: b'\\xd8'\n",
      "ID: 149 -> Token: b'\\xd9'\n",
      "ID: 150 -> Token: b'\\xda'\n",
      "ID: 151 -> Token: b'\\xdb'\n",
      "ID: 152 -> Token: b'\\xdc'\n",
      "ID: 153 -> Token: b'\\xdd'\n",
      "ID: 154 -> Token: b'\\xde'\n",
      "ID: 155 -> Token: b'\\xdf'\n",
      "ID: 156 -> Token: b'\\xe0'\n",
      "ID: 157 -> Token: b'\\xe1'\n",
      "ID: 158 -> Token: b'\\xe2'\n",
      "ID: 159 -> Token: b'\\xe3'\n",
      "ID: 160 -> Token: b'\\xe4'\n",
      "ID: 161 -> Token: b'\\xe5'\n",
      "ID: 162 -> Token: b'\\xe6'\n",
      "ID: 163 -> Token: b'\\xe7'\n",
      "ID: 164 -> Token: b'\\xe8'\n",
      "ID: 165 -> Token: b'\\xe9'\n",
      "ID: 166 -> Token: b'\\xea'\n",
      "ID: 167 -> Token: b'\\xeb'\n",
      "ID: 168 -> Token: b'\\xec'\n",
      "ID: 169 -> Token: b'\\xed'\n",
      "ID: 170 -> Token: b'\\xee'\n",
      "ID: 171 -> Token: b'\\xef'\n",
      "ID: 172 -> Token: b'\\xf0'\n",
      "ID: 173 -> Token: b'\\xf1'\n",
      "ID: 174 -> Token: b'\\xf2'\n",
      "ID: 175 -> Token: b'\\xf3'\n",
      "ID: 176 -> Token: b'\\xf4'\n",
      "ID: 177 -> Token: b'\\xf5'\n",
      "ID: 178 -> Token: b'\\xf6'\n",
      "ID: 179 -> Token: b'\\xf7'\n",
      "ID: 180 -> Token: b'\\xf8'\n",
      "ID: 181 -> Token: b'\\xf9'\n",
      "ID: 182 -> Token: b'\\xfa'\n",
      "ID: 183 -> Token: b'\\xfb'\n",
      "ID: 184 -> Token: b'\\xfc'\n",
      "ID: 185 -> Token: b'\\xfd'\n",
      "ID: 186 -> Token: b'\\xfe'\n",
      "ID: 187 -> Token: b'\\xff'\n",
      "ID: 188 -> Token: b'\\x00'\n",
      "ID: 189 -> Token: b'\\x01'\n",
      "ID: 190 -> Token: b'\\x02'\n",
      "ID: 191 -> Token: b'\\x03'\n",
      "ID: 192 -> Token: b'\\x04'\n",
      "ID: 193 -> Token: b'\\x05'\n",
      "ID: 194 -> Token: b'\\x06'\n",
      "ID: 195 -> Token: b'\\x07'\n",
      "ID: 196 -> Token: b'\\x08'\n",
      "ID: 197 -> Token: b'\\t'\n",
      "ID: 198 -> Token: b'\\n'\n",
      "ID: 199 -> Token: b'\\x0b'\n",
      "ID: 200 -> Token: b'\\x0c'\n",
      "ID: 201 -> Token: b'\\r'\n",
      "ID: 202 -> Token: b'\\x0e'\n",
      "ID: 203 -> Token: b'\\x0f'\n",
      "ID: 204 -> Token: b'\\x10'\n",
      "ID: 205 -> Token: b'\\x11'\n",
      "ID: 206 -> Token: b'\\x12'\n",
      "ID: 207 -> Token: b'\\x13'\n",
      "ID: 208 -> Token: b'\\x14'\n",
      "ID: 209 -> Token: b'\\x15'\n",
      "ID: 210 -> Token: b'\\x16'\n",
      "ID: 211 -> Token: b'\\x17'\n",
      "ID: 212 -> Token: b'\\x18'\n",
      "ID: 213 -> Token: b'\\x19'\n",
      "ID: 214 -> Token: b'\\x1a'\n",
      "ID: 215 -> Token: b'\\x1b'\n",
      "ID: 216 -> Token: b'\\x1c'\n",
      "ID: 217 -> Token: b'\\x1d'\n",
      "ID: 218 -> Token: b'\\x1e'\n",
      "ID: 219 -> Token: b'\\x1f'\n",
      "ID: 220 -> Token: b' '\n",
      "ID: 221 -> Token: b'\\x7f'\n",
      "ID: 222 -> Token: b'\\x80'\n",
      "ID: 223 -> Token: b'\\x81'\n",
      "ID: 224 -> Token: b'\\x82'\n",
      "ID: 225 -> Token: b'\\x83'\n",
      "ID: 226 -> Token: b'\\x84'\n",
      "ID: 227 -> Token: b'\\x85'\n",
      "ID: 228 -> Token: b'\\x86'\n",
      "ID: 229 -> Token: b'\\x87'\n",
      "ID: 230 -> Token: b'\\x88'\n",
      "ID: 231 -> Token: b'\\x89'\n",
      "ID: 232 -> Token: b'\\x8a'\n",
      "ID: 233 -> Token: b'\\x8b'\n",
      "ID: 234 -> Token: b'\\x8c'\n",
      "ID: 235 -> Token: b'\\x8d'\n",
      "ID: 236 -> Token: b'\\x8e'\n",
      "ID: 237 -> Token: b'\\x8f'\n",
      "ID: 238 -> Token: b'\\x90'\n",
      "ID: 239 -> Token: b'\\x91'\n",
      "ID: 240 -> Token: b'\\x92'\n",
      "ID: 241 -> Token: b'\\x93'\n",
      "ID: 242 -> Token: b'\\x94'\n",
      "ID: 243 -> Token: b'\\x95'\n",
      "ID: 244 -> Token: b'\\x96'\n",
      "ID: 245 -> Token: b'\\x97'\n",
      "ID: 246 -> Token: b'\\x98'\n",
      "ID: 247 -> Token: b'\\x99'\n",
      "ID: 248 -> Token: b'\\x9a'\n",
      "ID: 249 -> Token: b'\\x9b'\n",
      "ID: 250 -> Token: b'\\x9c'\n",
      "ID: 251 -> Token: b'\\x9d'\n",
      "ID: 252 -> Token: b'\\x9e'\n",
      "ID: 253 -> Token: b'\\x9f'\n",
      "ID: 254 -> Token: b'\\xa0'\n",
      "ID: 255 -> Token: b'\\xad'\n",
      "ID: 256 -> Token: b'  '\n",
      "ID: 257 -> Token: b'    '\n",
      "ID: 258 -> Token: b'in'\n",
      "ID: 259 -> Token: b' t'\n",
      "ID: 260 -> Token: b'        '\n",
      "ID: 261 -> Token: b'er'\n",
      "ID: 262 -> Token: b'   '\n",
      "ID: 263 -> Token: b'on'\n",
      "ID: 264 -> Token: b' a'\n",
      "ID: 265 -> Token: b're'\n",
      "ID: 266 -> Token: b'at'\n",
      "ID: 267 -> Token: b'st'\n",
      "ID: 268 -> Token: b'en'\n",
      "ID: 269 -> Token: b'or'\n",
      "ID: 270 -> Token: b' th'\n",
      "ID: 271 -> Token: b'\\n\\n'\n",
      "ID: 272 -> Token: b' c'\n",
      "ID: 273 -> Token: b'le'\n",
      "ID: 274 -> Token: b' s'\n",
      "ID: 275 -> Token: b'it'\n",
      "ID: 276 -> Token: b'an'\n",
      "ID: 277 -> Token: b'ar'\n",
      "ID: 278 -> Token: b'al'\n",
      "ID: 279 -> Token: b' the'\n",
      "ID: 280 -> Token: b';\\n'\n",
      "ID: 281 -> Token: b' p'\n",
      "ID: 282 -> Token: b' f'\n",
      "ID: 283 -> Token: b'ou'\n",
      "ID: 284 -> Token: b' ='\n",
      "ID: 285 -> Token: b'is'\n",
      "ID: 286 -> Token: b'       '\n",
      "ID: 287 -> Token: b'ing'\n",
      "ID: 288 -> Token: b'es'\n",
      "ID: 289 -> Token: b' w'\n",
      "ID: 290 -> Token: b'ion'\n",
      "ID: 291 -> Token: b'ed'\n",
      "ID: 292 -> Token: b'ic'\n",
      "ID: 293 -> Token: b' b'\n",
      "ID: 294 -> Token: b' d'\n",
      "ID: 295 -> Token: b'et'\n",
      "ID: 296 -> Token: b' m'\n",
      "ID: 297 -> Token: b' o'\n",
      "ID: 298 -> Token: b'\\t\\t'\n",
      "ID: 299 -> Token: b'ro'\n",
      "ID: 300 -> Token: b'as'\n",
      "ID: 301 -> Token: b'el'\n",
      "ID: 302 -> Token: b'ct'\n",
      "ID: 303 -> Token: b'nd'\n",
      "ID: 304 -> Token: b' in'\n",
      "ID: 305 -> Token: b' h'\n",
      "ID: 306 -> Token: b'ent'\n",
      "ID: 307 -> Token: b'id'\n",
      "ID: 308 -> Token: b' n'\n",
      "ID: 309 -> Token: b'am'\n",
      "ID: 310 -> Token: b'           '\n",
      "ID: 311 -> Token: b' to'\n",
      "ID: 312 -> Token: b' re'\n",
      "ID: 313 -> Token: b'--'\n",
      "ID: 314 -> Token: b' {'\n",
      "ID: 315 -> Token: b' of'\n",
      "ID: 316 -> Token: b'om'\n",
      "ID: 317 -> Token: b');\\n'\n",
      "ID: 318 -> Token: b'im'\n",
      "ID: 319 -> Token: b'\\r\\n'\n",
      "ID: 320 -> Token: b' ('\n",
      "ID: 321 -> Token: b'il'\n",
      "ID: 322 -> Token: b'//'\n",
      "ID: 323 -> Token: b' and'\n",
      "ID: 324 -> Token: b'ur'\n",
      "ID: 325 -> Token: b'se'\n",
      "ID: 326 -> Token: b' l'\n",
      "ID: 327 -> Token: b'ex'\n",
      "ID: 328 -> Token: b' S'\n",
      "ID: 329 -> Token: b'ad'\n",
      "ID: 330 -> Token: b' \"'\n",
      "ID: 331 -> Token: b'ch'\n",
      "ID: 332 -> Token: b'ut'\n",
      "ID: 333 -> Token: b'if'\n",
      "ID: 334 -> Token: b'**'\n",
      "ID: 335 -> Token: b' }'\n",
      "ID: 336 -> Token: b'em'\n",
      "ID: 337 -> Token: b'ol'\n",
      "ID: 338 -> Token: b'                '\n",
      "ID: 339 -> Token: b'th'\n",
      "ID: 340 -> Token: b')\\n'\n",
      "ID: 341 -> Token: b' {\\n'\n",
      "ID: 342 -> Token: b' g'\n",
      "ID: 343 -> Token: b'ig'\n",
      "ID: 344 -> Token: b'iv'\n",
      "ID: 345 -> Token: b',\\n'\n",
      "ID: 346 -> Token: b'ce'\n",
      "ID: 347 -> Token: b'od'\n",
      "ID: 348 -> Token: b' v'\n",
      "ID: 349 -> Token: b'ate'\n",
      "ID: 350 -> Token: b' T'\n",
      "ID: 351 -> Token: b'ag'\n",
      "ID: 352 -> Token: b'ay'\n",
      "ID: 353 -> Token: b' *'\n",
      "ID: 354 -> Token: b'ot'\n",
      "ID: 355 -> Token: b'us'\n",
      "ID: 356 -> Token: b' C'\n",
      "ID: 357 -> Token: b' st'\n",
      "ID: 358 -> Token: b' I'\n",
      "ID: 359 -> Token: b'un'\n",
      "ID: 360 -> Token: b'ul'\n",
      "ID: 361 -> Token: b'ue'\n",
      "ID: 362 -> Token: b' A'\n",
      "ID: 363 -> Token: b'ow'\n",
      "ID: 364 -> Token: b\" '\"\n",
      "ID: 365 -> Token: b'ew'\n",
      "ID: 366 -> Token: b' <'\n",
      "ID: 367 -> Token: b'ation'\n",
      "ID: 368 -> Token: b'()'\n",
      "ID: 369 -> Token: b' for'\n",
      "ID: 370 -> Token: b'ab'\n",
      "ID: 371 -> Token: b'ort'\n",
      "ID: 372 -> Token: b'um'\n",
      "ID: 373 -> Token: b'ame'\n",
      "ID: 374 -> Token: b' is'\n",
      "ID: 375 -> Token: b'pe'\n",
      "ID: 376 -> Token: b'tr'\n",
      "ID: 377 -> Token: b'ck'\n",
      "ID: 378 -> Token: b'\\xe2\\x80'\n",
      "ID: 379 -> Token: b' y'\n",
      "ID: 380 -> Token: b'ist'\n",
      "ID: 381 -> Token: b'----'\n",
      "ID: 382 -> Token: b'.\\n\\n'\n",
      "ID: 383 -> Token: b'he'\n",
      "ID: 384 -> Token: b' e'\n",
      "ID: 385 -> Token: b'lo'\n",
      "ID: 386 -> Token: b' M'\n",
      "ID: 387 -> Token: b' be'\n",
      "ID: 388 -> Token: b'ers'\n",
      "ID: 389 -> Token: b' on'\n",
      "ID: 390 -> Token: b' con'\n",
      "ID: 391 -> Token: b'ap'\n",
      "ID: 392 -> Token: b'ub'\n",
      "ID: 393 -> Token: b' P'\n",
      "ID: 394 -> Token: b'               '\n",
      "ID: 395 -> Token: b'ass'\n",
      "ID: 396 -> Token: b'int'\n",
      "ID: 397 -> Token: b'>\\n'\n",
      "ID: 398 -> Token: b'ly'\n",
      "ID: 399 -> Token: b'urn'\n",
      "ID: 400 -> Token: b' $'\n",
      "ID: 401 -> Token: b';\\n\\n'\n",
      "ID: 402 -> Token: b'av'\n",
      "ID: 403 -> Token: b'port'\n",
      "ID: 404 -> Token: b'ir'\n",
      "ID: 405 -> Token: b'->'\n",
      "ID: 406 -> Token: b'nt'\n",
      "ID: 407 -> Token: b'ction'\n",
      "ID: 408 -> Token: b'end'\n",
      "ID: 409 -> Token: b' de'\n",
      "ID: 410 -> Token: b'00'\n",
      "ID: 411 -> Token: b'ith'\n",
      "ID: 412 -> Token: b'out'\n",
      "ID: 413 -> Token: b'turn'\n",
      "ID: 414 -> Token: b'our'\n",
      "ID: 415 -> Token: b'     '\n",
      "ID: 416 -> Token: b'lic'\n",
      "ID: 417 -> Token: b'res'\n",
      "ID: 418 -> Token: b'pt'\n",
      "ID: 419 -> Token: b'=='\n",
      "ID: 420 -> Token: b' this'\n",
      "ID: 421 -> Token: b' wh'\n",
      "ID: 422 -> Token: b' if'\n",
      "ID: 423 -> Token: b' D'\n",
      "ID: 424 -> Token: b'ver'\n",
      "ID: 425 -> Token: b'age'\n",
      "ID: 426 -> Token: b' B'\n",
      "ID: 427 -> Token: b'ht'\n",
      "ID: 428 -> Token: b'ext'\n",
      "ID: 429 -> Token: b'=\"'\n",
      "ID: 430 -> Token: b' that'\n",
      "ID: 431 -> Token: b'****'\n",
      "ID: 432 -> Token: b' R'\n",
      "ID: 433 -> Token: b' it'\n",
      "ID: 434 -> Token: b'ess'\n",
      "ID: 435 -> Token: b' F'\n",
      "ID: 436 -> Token: b' r'\n",
      "ID: 437 -> Token: b'os'\n",
      "ID: 438 -> Token: b'and'\n",
      "ID: 439 -> Token: b' as'\n",
      "ID: 440 -> Token: b'ect'\n",
      "ID: 441 -> Token: b'ke'\n",
      "ID: 442 -> Token: b'rom'\n",
      "ID: 443 -> Token: b' //'\n",
      "ID: 444 -> Token: b'con'\n",
      "ID: 445 -> Token: b' L'\n",
      "ID: 446 -> Token: b'(\"'\n",
      "ID: 447 -> Token: b'qu'\n",
      "ID: 448 -> Token: b'lass'\n",
      "ID: 449 -> Token: b' with'\n",
      "ID: 450 -> Token: b'iz'\n",
      "ID: 451 -> Token: b'de'\n",
      "ID: 452 -> Token: b' N'\n",
      "ID: 453 -> Token: b' al'\n",
      "ID: 454 -> Token: b'op'\n",
      "ID: 455 -> Token: b'up'\n",
      "ID: 456 -> Token: b'get'\n",
      "ID: 457 -> Token: b' }\\n'\n",
      "ID: 458 -> Token: b'ile'\n",
      "ID: 459 -> Token: b' an'\n",
      "ID: 460 -> Token: b'ata'\n",
      "ID: 461 -> Token: b'ore'\n",
      "ID: 462 -> Token: b'ri'\n",
      "ID: 463 -> Token: b' pro'\n",
      "ID: 464 -> Token: b';\\r\\n'\n",
      "ID: 465 -> Token: b'\\t\\t\\t\\t'\n",
      "ID: 466 -> Token: b'ter'\n",
      "ID: 467 -> Token: b'ain'\n",
      "ID: 468 -> Token: b' W'\n",
      "ID: 469 -> Token: b' E'\n",
      "ID: 470 -> Token: b' com'\n",
      "ID: 471 -> Token: b' return'\n",
      "ID: 472 -> Token: b'art'\n",
      "ID: 473 -> Token: b' H'\n",
      "ID: 474 -> Token: b'ack'\n",
      "ID: 475 -> Token: b'import'\n",
      "ID: 476 -> Token: b'ublic'\n",
      "ID: 477 -> Token: b' or'\n",
      "ID: 478 -> Token: b'est'\n",
      "ID: 479 -> Token: b'ment'\n",
      "ID: 480 -> Token: b' G'\n",
      "ID: 481 -> Token: b'able'\n",
      "ID: 482 -> Token: b' -'\n",
      "ID: 483 -> Token: b'ine'\n",
      "ID: 484 -> Token: b'ill'\n",
      "ID: 485 -> Token: b'ind'\n",
      "ID: 486 -> Token: b'ere'\n",
      "ID: 487 -> Token: b'::'\n",
      "ID: 488 -> Token: b'ity'\n",
      "ID: 489 -> Token: b' +'\n",
      "ID: 490 -> Token: b' tr'\n",
      "ID: 491 -> Token: b'elf'\n",
      "ID: 492 -> Token: b'ight'\n",
      "ID: 493 -> Token: b\"('\"\n",
      "ID: 494 -> Token: b'orm'\n",
      "ID: 495 -> Token: b'ult'\n",
      "ID: 496 -> Token: b'str'\n",
      "ID: 497 -> Token: b'..'\n",
      "ID: 498 -> Token: b'\",'\n",
      "ID: 499 -> Token: b' you'\n"
     ]
    }
   ],
   "source": [
    "#View the Full Vocabulary (Sorted by ID)\n",
    "sorted_vocab = sorted(token_dict.items(), key=lambda x: x[1])\n",
    "for token, token_id in sorted_vocab[:500]:  # Adjust number to see more\n",
    "    print(f\"ID: {token_id} -> Token: {repr(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25807b78",
   "metadata": {},
   "source": [
    "<h3>Sanity Check</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eb273e3a-602f-45b0-a404-89fa88b6aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1f969edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean(): \n",
    "    \"\"\"\n",
    "    This is a function for GPU data claening before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    \n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.mps.empty_cache()  # Attempt to release MPS memory\n",
    "    \n",
    "    # Move tensors to CPU\n",
    "    for tensor in list(globals().values()):\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device == torch.device(\"mps\"):\n",
    "            tensor.to(\"cpu\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    del tensor\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "    print(\"Allocated Memory:\", torch.mps.current_allocated_memory() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da82d2c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5f2c9bfd-5c57-4af6-98e8-5da47988d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train import train_model_simple\n",
    "import time\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\n",
    "def train(train_loader, val_loader,\n",
    "          num_epochs=10, eval_iter=5, \n",
    "          sample_text=\"Every effort moves you\",\n",
    "          checkpoint_path=\"model_and_optimizer.pth\"):\n",
    "\n",
    "    global train_losses, val_losses, track_tokens_seen  # Ensure these are updated globally\n",
    "\n",
    "    if device == \"mps\":\n",
    "        clean()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.05)\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=num_epochs, eval_iter=eval_iter,\n",
    "        start_context=sample_text, cfg=GPT_CONFIG_124M,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        print(50 * \"=\")\n",
    "        clean()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae6fc",
   "metadata": {},
   "source": [
    "### Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dda45148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.901, Val loss 4.879\n",
      "Ep 1 (Step 000025): Train loss 2.628, Val loss 4.636\n",
      "Ep 1 (Step 000050): Train loss 2.604, Val loss 4.544\n",
      "Ep 1 (Step 000075): Train loss 2.428, Val loss 4.542\n",
      "Im Park ist und was uns noch ein Läche, ein Ding wenn sie ihn die stummergleicht das wirklichkeiten und aus dem verwie fünd, j\n",
      "Ep 2 (Step 000100): Train loss 2.457, Val loss 4.520\n",
      "Ep 2 (Step 000125): Train loss 2.320, Val loss 4.486\n",
      "Ep 2 (Step 000150): Train loss 2.317, Val loss 4.506\n",
      "Im Park ist und wurden ich um bist heim, an die Alter? Aberall, du deinen Hächauen Stunden, nur dich der sein; jetzt,— als\n",
      "Ep 3 (Step 000175): Train loss 2.261, Val loss 4.567\n",
      "Ep 3 (Step 000200): Train loss 2.108, Val loss 4.512\n",
      "Ep 3 (Step 000225): Train loss 2.196, Val loss 4.554\n",
      "Im Park ist daß ich, ist –ern, welcheln? Dinge [chel mein Händen\n",
      "Ep 4 (Step 000250): Train loss 2.059, Val loss 4.557\n",
      "Ep 4 (Step 000275): Train loss 1.747, Val loss 4.522\n",
      "Ep 4 (Step 000300): Train loss 1.944, Val loss 4.554\n",
      "Im Park ist; die Schoß er gebühren Häumen, aber wenn er das, ein selben Gesichtet, wie ein seiner sein Bildern, dieser Blauf\n",
      "Ep 5 (Step 000325): Train loss 1.930, Val loss 4.555\n",
      "Ep 5 (Step 000350): Train loss 1.817, Val loss 4.585\n",
      "Ep 5 (Step 000375): Train loss 1.796, Val loss 4.607\n",
      "Im Park ist in den Balindern ist du den Bliche wurden, das letzt von dem Todauen, die Tür ein Blute sich aufgeln das: um gesch so\n",
      "Ep 6 (Step 000400): Train loss 1.628, Val loss 4.628\n",
      "Ep 6 (Step 000425): Train loss 1.432, Val loss 4.640\n",
      "Ep 6 (Step 000450): Train loss 1.589, Val loss 4.718\n",
      "Ep 6 (Step 000475): Train loss 1.456, Val loss 4.701\n",
      "Im Park ist; er Dich die Sonne vorbe werden Himmeln\n",
      "Ep 7 (Step 000500): Train loss 1.370, Val loss 4.775\n",
      "Ep 7 (Step 000525): Train loss 1.477, Val loss 4.766\n",
      "Ep 7 (Step 000550): Train loss 1.290, Val loss 4.771\n",
      "Im Park ist das Meer alsich schwagend, ging sie zänden nahr stauen, die Tür ein Herz hin, aufstieben über dem Einsamen \n",
      "Training completed in 16.40 minutes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(train_loader, val_loader, num_epochs=7,\n",
    "      eval_iter=25, sample_text=\"Im Park ist\",\n",
    "      checkpoint_path=\"model_and_optimizer_5.pth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6651aada",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_and_optimizer_6.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=\u001b[32m0.0004\u001b[39m, weight_decay=\u001b[32m0.1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_and_optimizer_6.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m model.load_state_dict(checkpoint[\u001b[33m\"\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      7\u001b[39m optimizer.load_state_dict(checkpoint[\u001b[33m\"\u001b[39m\u001b[33moptimizer_state_dict\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/Cramming_example/lib/python3.13/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/Cramming_example/lib/python3.13/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/Cramming_example/lib/python3.13/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'model_and_optimizer_6.pth'"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer_6.pth\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0c982",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Er ruftender bist der Schritte den Sohn\n",
      "die würde,\n",
      "kür eines verwie leise seine Sehlt\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, prompt=\"Er ruft\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=1,\n",
    "    top_k=40,\n",
    "    eos_id=13\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a25f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cramming_example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
