{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IuG_Lap1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import combinations\n",
    "import tiktoken\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel, DEFAULT_CFG\n",
    "from utils.model import load_GPT_model\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = load_GPT_model(path=\"model_896_14_8_256.pth\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccfe2eef-47b0-40f0-8b71-8d3b1d501451",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def encode(full_text):\n",
    "    return tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcfce182-1d71-4b11-b06b-7f9881c25093",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_path = './dataset/val_text_data.txt'\n",
    "\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=DEFAULT_CFG[\"context_length\"],\n",
    "    stride=DEFAULT_CFG[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6106a26c-295c-442b-9603-4540b98d60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file_path = './dataset/eval_text_data.txt'\n",
    "\n",
    "with open(eval_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    eval_data = file.read()\n",
    "\n",
    "eval_loader = create_dataloader_v1(\n",
    "    eval_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=DEFAULT_CFG[\"context_length\"],\n",
    "    stride=DEFAULT_CFG[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d523e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)  # Forward pass\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * target_ids.numel()\n",
    "            total_tokens += target_ids.numel()\n",
    "\n",
    "    perplexity = np.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837f7534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(60.410590339687815)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21a76d-16d0-4d4d-8352-240155f6a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_perplexity(model, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44c61225-1077-4019-984a-564aa7ba6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "def weat_score(model, target_words_1, target_words_2, attribute_words_1, attribute_words_2, tokenizer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Measures bias by comparing how close different groups of words are in embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding(word):\n",
    "        token_id = tokenizer.encode(word, allowed_special={'<|endoftext|>'})[0]\n",
    "        with torch.no_grad():\n",
    "            embed = model.tok_emb(torch.tensor([token_id], device=device)).cpu().numpy()\n",
    "        return embed.flatten()\n",
    "\n",
    "    # Get embeddings\n",
    "    target_1_embs = [get_embedding(w) for w in target_words_1]\n",
    "    target_2_embs = [get_embedding(w) for w in target_words_2]\n",
    "    attr_1_embs = [get_embedding(w) for w in attribute_words_1]\n",
    "    attr_2_embs = [get_embedding(w) for w in attribute_words_2]\n",
    "\n",
    "    def association(t, A, B):\n",
    "        return np.mean([cosine_similarity(t, a) for a in A]) - np.mean([cosine_similarity(t, b) for b in B])\n",
    "\n",
    "    # Compute WEAT score\n",
    "    s1 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_1_embs])\n",
    "    s2 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_2_embs])\n",
    "    \n",
    "    weat_score = s1 - s2\n",
    "    return weat_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd7a1406-0bd9-409a-9fd8-8b4f343d2c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.046634555)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_male = [\"gentleman\", \"officer\", \"clergyman\", \"husband\", \"captain\"]\n",
    "target_female = [\"lady\", \"governess\", \"girl\", \"wife\", \"widow\"]\n",
    "\n",
    "attribute_male = [\"honour\", \"duty\", \"wisdom\", \"fortitude\", \"independence\"]\n",
    "attribute_female = [\"grace\", \"affection\", \"beauty\", \"delicacy\", \"modesty\"]\n",
    "\n",
    "weat_score(model, target_male, target_female, attribute_male, attribute_female, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cdcce2e-326f-4cc6-8033-e77e0a2ba270",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d845171-c8f1-47a0-a60b-1f9847a22590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import re\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_bleu_rouge_from_val(model, device=\"cpu\"):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    # Step 1: Load the validation set\n",
    "    with open('val_text_data_all_txt.txt', 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Step 2: Split into sentences & filter\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', data)\n",
    "    filtered_sentences = [s.strip() for s in sentences if 5 <= len(s.split()) <= 60]\n",
    "    filtered_sentences = filtered_sentences[:1000]\n",
    "\n",
    "    # Step 3: Split each sentence into two halves and store as tuples\n",
    "    sentence_tuples = []\n",
    "    for sent in filtered_sentences:\n",
    "        words = sent.split()\n",
    "        mid = len(words) // 2\n",
    "        first_half = ' '.join(words[:mid])\n",
    "        second_half = ' '.join(words[mid:])\n",
    "        sentence_tuples.append((first_half, second_half))\n",
    "\n",
    "    # Step 4: For each (first_half, second_half), generate prediction\n",
    "    for first_half, second_half in sentence_tuples:\n",
    "        generated_text = generate(\n",
    "            model=model, prompt=first_half,\n",
    "            max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "            device=device,\n",
    "            temperature=0.7,\n",
    "            top_k=50\n",
    "        )\n",
    "\n",
    "        # Build reference and prediction\n",
    "        reference = first_half + \" \" + second_half\n",
    "        prediction = generated_text\n",
    "\n",
    "        references.append(reference)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Step 5-6: Compute BLEU and ROUGE\n",
    "    # Format references correctly for BLEU\n",
    "    references_formatted = [[ref] for ref in references]\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references_formatted)['bleu']\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}, ROUGE-L Score: {rouge_score['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0404bbea-7821-4b52-a5e9-f240bf5c01d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT_CONFIG_124M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcompute_bleu_rouge_from_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m, in \u001b[0;36mcompute_bleu_rouge_from_val\u001b[1;34m(model, device)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Step 4: For each (first_half, second_half), generate prediction\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m first_half, second_half \u001b[38;5;129;01min\u001b[39;00m sentence_tuples:\n\u001b[0;32m     32\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m generate(\n\u001b[0;32m     33\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel, prompt\u001b[38;5;241m=\u001b[39mfirst_half,\n\u001b[1;32m---> 34\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, context_size\u001b[38;5;241m=\u001b[39m\u001b[43mGPT_CONFIG_124M\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     35\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     36\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[0;32m     37\u001b[0m         top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     38\u001b[0m     )\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Build reference and prediction\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     reference \u001b[38;5;241m=\u001b[39m first_half \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m second_half\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GPT_CONFIG_124M' is not defined"
     ]
    }
   ],
   "source": [
    "compute_bleu_rouge_from_val(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Miss Bennet has inherited the estate from her aunt, so she must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Mr. Darcy has inherited the estate from his aunt, so he must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81220f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"A wife is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"A husband is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "714f6606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I shall now go on with her on the morrow. You will not be surprised to be on the way of making any change of manner in this way; you will\n",
      "==================================================\n",
      "He said, \"I am not sure of any consequence. I am sure you would not have been angry with me, if I had not been so ungr\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"I shall now go\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"He said\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78dc249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She was so much obliged to leave her to go, that she would not stay till night.\n",
      "<|endoftext|>\n",
      "Mrs. Jennings was not a creature to be known to her, nor had she been able to see her friends, nor of her family; nor could she possibly have found a place in the world more than any thing else, in any of the family. She was a young woman, a very sensible woman, and of her husband; but she was not one to herself, except a young woman in a family, who, as her husband, she had been brought home with them, she had not been in her company with him, and had not been long so much obliged to her, or with her aunt, to have taken up a great deal of conversation with her; but she was now a little surprised, and had the satisfaction of seeing her sister, and of her being so long as to have seen a sister so very young, and so very good-humoured, that she was so\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"She was\",\n",
    "    max_new_tokens=200, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb810-cfc6-49fe-bfd0-66c035e0707e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78fd8b69-ae98-4300-b876-93bf84fa0d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to be the only one to whom I am so much obliged to you. I am sure I am not so glad to hear it. But I am very'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "583fd4d9-4273-435a-a2fe-651162acb64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to be a relief to me, and I am no longer so happy as I am now to be. I am not so happy as I am, that'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c8a060b-6d8a-4dc8-ade9-bfde78b38900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The gentleman is not a gentleman, I am sure, but I am sure he is a very good man, and I am sure I should not like him. I'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"The gentleman is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba961e03-e482-4601-bef7-daab0c1dac77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when Jane and Edward went to the house, he gave a drink to his mother a small income at Donwell. He was invited in to London, and Jane was to take some news that could never have been made to'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"when Jane and Edward went to the house, he gave a drink to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=1,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f99b81-a4be-4994-9c00-8898a408f06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a2767-efc9-4167-ad8e-6290a4d6d411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
