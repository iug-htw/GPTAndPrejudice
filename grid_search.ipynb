{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf6d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from pre_train import train_model_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d96666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e79f7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_grid = {\n",
    "    \"vocab_size\": [10000],\n",
    "    \"context_length\": [256, 512],\n",
    "    \"emb_dim\": [512, 768, 1024],\n",
    "    \"n_heads\": [4, 6, 8, 12, 16],\n",
    "    \"n_layers\": [6, 8, 12, 16],\n",
    "    \"drop_rate\": [0.3, 0.2],\n",
    "    \"qkv_bias\": [False, True],\n",
    "    \"device\": [device],\n",
    "}\n",
    "\n",
    "optimizer_grid = {\n",
    "    \"lr\": [0.0002, 0.0004],\n",
    "    \"betas\": [(0.9, 0.95), (0.9, 0.98)],\n",
    "    \"eps\": [1e-8],\n",
    "    \"weight_decay\": [0.01, 0.05, 0.001]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e6b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data.txt'\n",
    "val_file_path = 'val_text_data.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165fe083",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18767567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def train_func(model, optimizer, train_loader, val_loader, cfg):\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen = [],[],[]\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=1, eval_iter=10,\n",
    "        cfg=cfg,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen,\n",
    "        start_context=\"\",\n",
    "        generate_sample_text=False\n",
    "    )\n",
    "    \n",
    "    if val_losses:\n",
    "        final_val_loss = val_losses[-1]\n",
    "    else:\n",
    "        final_val_loss = float('inf')  # Fallback if val_losses is empty\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return final_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fa16b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_gpt(config_grid, optimizer_grid, train_func):\n",
    "    \"\"\"\n",
    "    Perform grid search over GPT config and optimizer parameters.\n",
    "\n",
    "    Args:\n",
    "        train_loader: Your PyTorch training DataLoader\n",
    "        val_loader: Your PyTorch validation DataLoader\n",
    "        config_grid: Dictionary of GPT config hyperparameters to search\n",
    "        optimizer_grid: Dictionary of optimizer hyperparameters to search\n",
    "        train_func: Function that accepts (model, optimizer, train_loader, val_loader) and returns val_loss\n",
    "\n",
    "    Returns:\n",
    "        Best config, optimizer params, and the lowest validation loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Create all possible combinations of config and optimizer params\n",
    "    config_combos = list(itertools.product(*config_grid.values()))\n",
    "    optimizer_combos = list(itertools.product(*optimizer_grid.values()))\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_config = None\n",
    "    best_optimizer_params = None\n",
    "\n",
    "    for config_values in config_combos:\n",
    "        gpt_cfg = deepcopy(dict(zip(config_grid.keys(), config_values)))\n",
    "\n",
    "        for opt_values in optimizer_combos:\n",
    "            opt_cfg = dict(zip(optimizer_grid.keys(), opt_values))\n",
    "            \n",
    "            if gpt_cfg[\"emb_dim\"] % gpt_cfg[\"n_heads\"] != 0:\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            train_loader = create_dataloader_v1(\n",
    "                train_data,\n",
    "                batch_size=4,\n",
    "                max_length=gpt_cfg[\"context_length\"],\n",
    "                stride=gpt_cfg[\"context_length\"],\n",
    "                drop_last=True,\n",
    "                shuffle=True,\n",
    "                num_workers=0\n",
    "            )\n",
    "\n",
    "            val_loader = create_dataloader_v1(\n",
    "                val_data,\n",
    "                batch_size=4,\n",
    "                max_length=gpt_cfg[\"context_length\"],\n",
    "                stride=gpt_cfg[\"context_length\"],\n",
    "                drop_last=False,\n",
    "                shuffle=False,\n",
    "                num_workers=0\n",
    "            )\n",
    "\n",
    "            # Build your model dynamically here (pseudo-code, replace with your model call)\n",
    "            model = GPTModel(gpt_cfg).to(gpt_cfg['device'])\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=opt_cfg['lr'],\n",
    "                betas=opt_cfg['betas'],\n",
    "                eps=opt_cfg['eps'],\n",
    "                weight_decay=opt_cfg['weight_decay']\n",
    "            )\n",
    "\n",
    "            # Run one training + validation loop and get val loss (your training logic)\n",
    "            val_loss = train_func(model, optimizer, train_loader, val_loader, gpt_cfg)\n",
    "\n",
    "            print(50*\"=\")\n",
    "            print(f\"Config: {gpt_cfg}, Optimizer: {opt_cfg}, Val Loss: {val_loss:.4f}\")\n",
    "            print(50*\"=\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_config = deepcopy(gpt_cfg)\n",
    "                best_optimizer_params = deepcopy(opt_cfg)\n",
    "\n",
    "    return best_config, best_optimizer_params, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586f462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.169, Val loss 9.168\n",
      "Ep 1 (Step 000010): Train loss 7.584, Val loss 7.536\n",
      "Ep 1 (Step 000020): Train loss 6.960, Val loss 6.854\n",
      "Ep 1 (Step 000030): Train loss 6.540, Val loss 6.515\n",
      "Ep 1 (Step 000040): Train loss 6.423, Val loss 6.410\n",
      "Ep 1 (Step 000050): Train loss 6.453, Val loss 6.383\n",
      "Ep 1 (Step 000060): Train loss 6.324, Val loss 6.346\n",
      "Ep 1 (Step 000070): Train loss 6.261, Val loss 6.257\n",
      "Ep 1 (Step 000080): Train loss 6.213, Val loss 6.190\n",
      "Ep 1 (Step 000090): Train loss 6.090, Val loss 6.092\n",
      "Ep 1 (Step 000100): Train loss 6.044, Val loss 6.014\n",
      "Ep 1 (Step 000110): Train loss 5.891, Val loss 5.972\n",
      "Ep 1 (Step 000120): Train loss 5.903, Val loss 5.929\n",
      "Ep 1 (Step 000130): Train loss 5.739, Val loss 5.876\n",
      "Ep 1 (Step 000140): Train loss 5.793, Val loss 5.832\n",
      "Ep 1 (Step 000150): Train loss 5.729, Val loss 5.783\n",
      "Ep 1 (Step 000160): Train loss 5.743, Val loss 5.758\n",
      "Ep 1 (Step 000170): Train loss 5.622, Val loss 5.708\n",
      "Ep 1 (Step 000180): Train loss 5.611, Val loss 5.694\n",
      "Ep 1 (Step 000190): Train loss 5.597, Val loss 5.665\n",
      "Ep 1 (Step 000200): Train loss 5.596, Val loss 5.664\n",
      "Ep 1 (Step 000210): Train loss 5.578, Val loss 5.631\n",
      "Ep 1 (Step 000220): Train loss 5.477, Val loss 5.603\n",
      "Ep 1 (Step 000230): Train loss 5.500, Val loss 5.581\n",
      "Ep 1 (Step 000240): Train loss 5.388, Val loss 5.547\n",
      "Ep 1 (Step 000250): Train loss 5.460, Val loss 5.553\n",
      "Ep 1 (Step 000260): Train loss 5.349, Val loss 5.520\n",
      "Ep 1 (Step 000270): Train loss 5.386, Val loss 5.511\n",
      "Ep 1 (Step 000280): Train loss 5.427, Val loss 5.476\n",
      "Ep 1 (Step 000290): Train loss 5.344, Val loss 5.467\n",
      "Ep 1 (Step 000300): Train loss 5.374, Val loss 5.449\n",
      "Ep 1 (Step 000310): Train loss 5.305, Val loss 5.436\n",
      "Ep 1 (Step 000320): Train loss 5.305, Val loss 5.428\n",
      "Ep 1 (Step 000330): Train loss 5.348, Val loss 5.425\n",
      "Ep 1 (Step 000340): Train loss 5.301, Val loss 5.420\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.4197\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.137, Val loss 9.134\n",
      "Ep 1 (Step 000010): Train loss 7.483, Val loss 7.482\n",
      "Ep 1 (Step 000020): Train loss 6.824, Val loss 6.838\n",
      "Ep 1 (Step 000030): Train loss 6.477, Val loss 6.503\n",
      "Ep 1 (Step 000040): Train loss 6.421, Val loss 6.408\n",
      "Ep 1 (Step 000050): Train loss 6.397, Val loss 6.380\n",
      "Ep 1 (Step 000060): Train loss 6.338, Val loss 6.339\n",
      "Ep 1 (Step 000070): Train loss 6.262, Val loss 6.257\n",
      "Ep 1 (Step 000080): Train loss 6.164, Val loss 6.132\n",
      "Ep 1 (Step 000090): Train loss 6.048, Val loss 6.072\n",
      "Ep 1 (Step 000100): Train loss 6.065, Val loss 5.998\n",
      "Ep 1 (Step 000110): Train loss 5.933, Val loss 5.930\n",
      "Ep 1 (Step 000120): Train loss 5.929, Val loss 5.929\n",
      "Ep 1 (Step 000130): Train loss 5.905, Val loss 5.855\n",
      "Ep 1 (Step 000140): Train loss 5.800, Val loss 5.815\n",
      "Ep 1 (Step 000150): Train loss 5.707, Val loss 5.787\n",
      "Ep 1 (Step 000160): Train loss 5.705, Val loss 5.748\n",
      "Ep 1 (Step 000170): Train loss 5.660, Val loss 5.709\n",
      "Ep 1 (Step 000180): Train loss 5.587, Val loss 5.678\n",
      "Ep 1 (Step 000190): Train loss 5.578, Val loss 5.655\n",
      "Ep 1 (Step 000200): Train loss 5.590, Val loss 5.628\n",
      "Ep 1 (Step 000210): Train loss 5.461, Val loss 5.617\n",
      "Ep 1 (Step 000220): Train loss 5.526, Val loss 5.596\n",
      "Ep 1 (Step 000230): Train loss 5.498, Val loss 5.567\n",
      "Ep 1 (Step 000240): Train loss 5.451, Val loss 5.548\n",
      "Ep 1 (Step 000250): Train loss 5.443, Val loss 5.521\n",
      "Ep 1 (Step 000260): Train loss 5.474, Val loss 5.508\n",
      "Ep 1 (Step 000270): Train loss 5.356, Val loss 5.488\n",
      "Ep 1 (Step 000280): Train loss 5.373, Val loss 5.480\n",
      "Ep 1 (Step 000290): Train loss 5.305, Val loss 5.473\n",
      "Ep 1 (Step 000300): Train loss 5.415, Val loss 5.465\n",
      "Ep 1 (Step 000310): Train loss 5.362, Val loss 5.443\n",
      "Ep 1 (Step 000320): Train loss 5.239, Val loss 5.436\n",
      "Ep 1 (Step 000330): Train loss 5.299, Val loss 5.419\n",
      "Ep 1 (Step 000340): Train loss 5.332, Val loss 5.406\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.4059\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.158, Val loss 9.156\n",
      "Ep 1 (Step 000010): Train loss 7.563, Val loss 7.511\n",
      "Ep 1 (Step 000020): Train loss 6.885, Val loss 6.838\n",
      "Ep 1 (Step 000030): Train loss 6.550, Val loss 6.495\n",
      "Ep 1 (Step 000040): Train loss 6.390, Val loss 6.389\n",
      "Ep 1 (Step 000050): Train loss 6.356, Val loss 6.364\n",
      "Ep 1 (Step 000060): Train loss 6.303, Val loss 6.337\n",
      "Ep 1 (Step 000070): Train loss 6.248, Val loss 6.256\n",
      "Ep 1 (Step 000080): Train loss 6.126, Val loss 6.146\n",
      "Ep 1 (Step 000090): Train loss 6.109, Val loss 6.073\n",
      "Ep 1 (Step 000100): Train loss 6.018, Val loss 6.008\n",
      "Ep 1 (Step 000110): Train loss 5.916, Val loss 5.945\n",
      "Ep 1 (Step 000120): Train loss 5.845, Val loss 5.891\n",
      "Ep 1 (Step 000130): Train loss 5.874, Val loss 5.857\n",
      "Ep 1 (Step 000140): Train loss 5.839, Val loss 5.816\n",
      "Ep 1 (Step 000150): Train loss 5.727, Val loss 5.778\n",
      "Ep 1 (Step 000160): Train loss 5.739, Val loss 5.761\n",
      "Ep 1 (Step 000170): Train loss 5.638, Val loss 5.728\n",
      "Ep 1 (Step 000180): Train loss 5.634, Val loss 5.678\n",
      "Ep 1 (Step 000190): Train loss 5.640, Val loss 5.656\n",
      "Ep 1 (Step 000200): Train loss 5.552, Val loss 5.623\n",
      "Ep 1 (Step 000210): Train loss 5.440, Val loss 5.621\n",
      "Ep 1 (Step 000220): Train loss 5.430, Val loss 5.596\n",
      "Ep 1 (Step 000230): Train loss 5.528, Val loss 5.566\n",
      "Ep 1 (Step 000240): Train loss 5.526, Val loss 5.550\n",
      "Ep 1 (Step 000250): Train loss 5.436, Val loss 5.519\n",
      "Ep 1 (Step 000260): Train loss 5.394, Val loss 5.513\n",
      "Ep 1 (Step 000270): Train loss 5.334, Val loss 5.494\n",
      "Ep 1 (Step 000280): Train loss 5.328, Val loss 5.472\n",
      "Ep 1 (Step 000290): Train loss 5.347, Val loss 5.471\n",
      "Ep 1 (Step 000300): Train loss 5.304, Val loss 5.455\n",
      "Ep 1 (Step 000310): Train loss 5.303, Val loss 5.444\n",
      "Ep 1 (Step 000320): Train loss 5.353, Val loss 5.432\n",
      "Ep 1 (Step 000330): Train loss 5.355, Val loss 5.427\n",
      "Ep 1 (Step 000340): Train loss 5.277, Val loss 5.410\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.4104\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.150, Val loss 9.156\n",
      "Ep 1 (Step 000010): Train loss 7.483, Val loss 7.472\n",
      "Ep 1 (Step 000020): Train loss 6.863, Val loss 6.806\n",
      "Ep 1 (Step 000030): Train loss 6.453, Val loss 6.496\n",
      "Ep 1 (Step 000040): Train loss 6.412, Val loss 6.415\n",
      "Ep 1 (Step 000050): Train loss 6.387, Val loss 6.384\n",
      "Ep 1 (Step 000060): Train loss 6.382, Val loss 6.354\n",
      "Ep 1 (Step 000070): Train loss 6.281, Val loss 6.296\n",
      "Ep 1 (Step 000080): Train loss 6.229, Val loss 6.192\n",
      "Ep 1 (Step 000090): Train loss 6.099, Val loss 6.102\n",
      "Ep 1 (Step 000100): Train loss 6.015, Val loss 6.028\n",
      "Ep 1 (Step 000110): Train loss 5.984, Val loss 5.965\n",
      "Ep 1 (Step 000120): Train loss 5.918, Val loss 5.921\n",
      "Ep 1 (Step 000130): Train loss 5.878, Val loss 5.878\n",
      "Ep 1 (Step 000140): Train loss 5.799, Val loss 5.829\n",
      "Ep 1 (Step 000150): Train loss 5.791, Val loss 5.790\n",
      "Ep 1 (Step 000160): Train loss 5.727, Val loss 5.748\n",
      "Ep 1 (Step 000170): Train loss 5.631, Val loss 5.702\n",
      "Ep 1 (Step 000180): Train loss 5.596, Val loss 5.688\n",
      "Ep 1 (Step 000190): Train loss 5.577, Val loss 5.652\n",
      "Ep 1 (Step 000200): Train loss 5.638, Val loss 5.636\n",
      "Ep 1 (Step 000210): Train loss 5.519, Val loss 5.595\n",
      "Ep 1 (Step 000220): Train loss 5.513, Val loss 5.576\n",
      "Ep 1 (Step 000230): Train loss 5.470, Val loss 5.549\n",
      "Ep 1 (Step 000240): Train loss 5.477, Val loss 5.521\n",
      "Ep 1 (Step 000250): Train loss 5.399, Val loss 5.513\n",
      "Ep 1 (Step 000260): Train loss 5.424, Val loss 5.505\n",
      "Ep 1 (Step 000270): Train loss 5.379, Val loss 5.476\n",
      "Ep 1 (Step 000280): Train loss 5.421, Val loss 5.473\n",
      "Ep 1 (Step 000290): Train loss 5.342, Val loss 5.439\n",
      "Ep 1 (Step 000300): Train loss 5.331, Val loss 5.447\n",
      "Ep 1 (Step 000310): Train loss 5.305, Val loss 5.418\n",
      "Ep 1 (Step 000320): Train loss 5.254, Val loss 5.413\n",
      "Ep 1 (Step 000330): Train loss 5.340, Val loss 5.403\n",
      "Ep 1 (Step 000340): Train loss 5.283, Val loss 5.377\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3768\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.176, Val loss 9.160\n",
      "Ep 1 (Step 000010): Train loss 7.525, Val loss 7.517\n",
      "Ep 1 (Step 000020): Train loss 6.869, Val loss 6.838\n",
      "Ep 1 (Step 000030): Train loss 6.536, Val loss 6.481\n",
      "Ep 1 (Step 000040): Train loss 6.377, Val loss 6.392\n",
      "Ep 1 (Step 000050): Train loss 6.367, Val loss 6.371\n",
      "Ep 1 (Step 000060): Train loss 6.391, Val loss 6.335\n",
      "Ep 1 (Step 000070): Train loss 6.301, Val loss 6.286\n",
      "Ep 1 (Step 000080): Train loss 6.195, Val loss 6.182\n",
      "Ep 1 (Step 000090): Train loss 6.052, Val loss 6.107\n",
      "Ep 1 (Step 000100): Train loss 6.044, Val loss 6.028\n",
      "Ep 1 (Step 000110): Train loss 5.993, Val loss 5.958\n",
      "Ep 1 (Step 000120): Train loss 5.944, Val loss 5.918\n",
      "Ep 1 (Step 000130): Train loss 5.842, Val loss 5.903\n",
      "Ep 1 (Step 000140): Train loss 5.786, Val loss 5.846\n",
      "Ep 1 (Step 000150): Train loss 5.704, Val loss 5.785\n",
      "Ep 1 (Step 000160): Train loss 5.690, Val loss 5.749\n",
      "Ep 1 (Step 000170): Train loss 5.727, Val loss 5.722\n",
      "Ep 1 (Step 000180): Train loss 5.588, Val loss 5.678\n",
      "Ep 1 (Step 000190): Train loss 5.668, Val loss 5.657\n",
      "Ep 1 (Step 000200): Train loss 5.566, Val loss 5.620\n",
      "Ep 1 (Step 000210): Train loss 5.535, Val loss 5.606\n",
      "Ep 1 (Step 000220): Train loss 5.472, Val loss 5.575\n",
      "Ep 1 (Step 000230): Train loss 5.427, Val loss 5.555\n",
      "Ep 1 (Step 000240): Train loss 5.556, Val loss 5.531\n",
      "Ep 1 (Step 000250): Train loss 5.418, Val loss 5.519\n",
      "Ep 1 (Step 000260): Train loss 5.412, Val loss 5.501\n",
      "Ep 1 (Step 000270): Train loss 5.412, Val loss 5.485\n",
      "Ep 1 (Step 000280): Train loss 5.399, Val loss 5.480\n",
      "Ep 1 (Step 000290): Train loss 5.387, Val loss 5.471\n",
      "Ep 1 (Step 000300): Train loss 5.372, Val loss 5.443\n",
      "Ep 1 (Step 000310): Train loss 5.374, Val loss 5.429\n",
      "Ep 1 (Step 000320): Train loss 5.308, Val loss 5.426\n",
      "Ep 1 (Step 000330): Train loss 5.286, Val loss 5.406\n",
      "Ep 1 (Step 000340): Train loss 5.286, Val loss 5.396\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3961\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.155, Val loss 9.127\n",
      "Ep 1 (Step 000010): Train loss 7.534, Val loss 7.514\n",
      "Ep 1 (Step 000020): Train loss 6.876, Val loss 6.854\n",
      "Ep 1 (Step 000030): Train loss 6.523, Val loss 6.494\n",
      "Ep 1 (Step 000040): Train loss 6.395, Val loss 6.397\n",
      "Ep 1 (Step 000050): Train loss 6.391, Val loss 6.383\n",
      "Ep 1 (Step 000060): Train loss 6.317, Val loss 6.326\n",
      "Ep 1 (Step 000070): Train loss 6.268, Val loss 6.275\n",
      "Ep 1 (Step 000080): Train loss 6.181, Val loss 6.168\n",
      "Ep 1 (Step 000090): Train loss 6.025, Val loss 6.085\n",
      "Ep 1 (Step 000100): Train loss 6.036, Val loss 6.027\n",
      "Ep 1 (Step 000110): Train loss 5.879, Val loss 5.969\n",
      "Ep 1 (Step 000120): Train loss 5.843, Val loss 5.929\n",
      "Ep 1 (Step 000130): Train loss 5.756, Val loss 5.869\n",
      "Ep 1 (Step 000140): Train loss 5.799, Val loss 5.816\n",
      "Ep 1 (Step 000150): Train loss 5.707, Val loss 5.785\n",
      "Ep 1 (Step 000160): Train loss 5.687, Val loss 5.751\n",
      "Ep 1 (Step 000170): Train loss 5.619, Val loss 5.709\n",
      "Ep 1 (Step 000180): Train loss 5.623, Val loss 5.672\n",
      "Ep 1 (Step 000190): Train loss 5.573, Val loss 5.661\n",
      "Ep 1 (Step 000200): Train loss 5.545, Val loss 5.628\n",
      "Ep 1 (Step 000210): Train loss 5.500, Val loss 5.609\n",
      "Ep 1 (Step 000220): Train loss 5.543, Val loss 5.586\n",
      "Ep 1 (Step 000230): Train loss 5.508, Val loss 5.549\n",
      "Ep 1 (Step 000240): Train loss 5.471, Val loss 5.519\n",
      "Ep 1 (Step 000250): Train loss 5.414, Val loss 5.510\n",
      "Ep 1 (Step 000260): Train loss 5.457, Val loss 5.504\n",
      "Ep 1 (Step 000270): Train loss 5.408, Val loss 5.480\n",
      "Ep 1 (Step 000280): Train loss 5.392, Val loss 5.466\n",
      "Ep 1 (Step 000290): Train loss 5.366, Val loss 5.463\n",
      "Ep 1 (Step 000300): Train loss 5.369, Val loss 5.443\n",
      "Ep 1 (Step 000310): Train loss 5.272, Val loss 5.435\n",
      "Ep 1 (Step 000320): Train loss 5.410, Val loss 5.411\n",
      "Ep 1 (Step 000330): Train loss 5.317, Val loss 5.407\n",
      "Ep 1 (Step 000340): Train loss 5.257, Val loss 5.386\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3859\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.940, Val loss 8.924\n",
      "Ep 1 (Step 000010): Train loss 6.914, Val loss 6.897\n",
      "Ep 1 (Step 000020): Train loss 6.455, Val loss 6.493\n",
      "Ep 1 (Step 000030): Train loss 6.400, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.426, Val loss 6.412\n",
      "Ep 1 (Step 000050): Train loss 6.325, Val loss 6.375\n",
      "Ep 1 (Step 000060): Train loss 6.128, Val loss 6.163\n",
      "Ep 1 (Step 000070): Train loss 6.025, Val loss 6.055\n",
      "Ep 1 (Step 000080): Train loss 5.826, Val loss 5.953\n",
      "Ep 1 (Step 000090): Train loss 5.884, Val loss 5.852\n",
      "Ep 1 (Step 000100): Train loss 5.795, Val loss 5.796\n",
      "Ep 1 (Step 000110): Train loss 5.690, Val loss 5.747\n",
      "Ep 1 (Step 000120): Train loss 5.623, Val loss 5.688\n",
      "Ep 1 (Step 000130): Train loss 5.639, Val loss 5.654\n",
      "Ep 1 (Step 000140): Train loss 5.539, Val loss 5.612\n",
      "Ep 1 (Step 000150): Train loss 5.536, Val loss 5.578\n",
      "Ep 1 (Step 000160): Train loss 5.547, Val loss 5.546\n",
      "Ep 1 (Step 000170): Train loss 5.499, Val loss 5.528\n",
      "Ep 1 (Step 000180): Train loss 5.437, Val loss 5.506\n",
      "Ep 1 (Step 000190): Train loss 5.299, Val loss 5.497\n",
      "Ep 1 (Step 000200): Train loss 5.325, Val loss 5.473\n",
      "Ep 1 (Step 000210): Train loss 5.303, Val loss 5.465\n",
      "Ep 1 (Step 000220): Train loss 5.291, Val loss 5.446\n",
      "Ep 1 (Step 000230): Train loss 5.264, Val loss 5.419\n",
      "Ep 1 (Step 000240): Train loss 5.271, Val loss 5.406\n",
      "Ep 1 (Step 000250): Train loss 5.248, Val loss 5.405\n",
      "Ep 1 (Step 000260): Train loss 5.239, Val loss 5.361\n",
      "Ep 1 (Step 000270): Train loss 5.277, Val loss 5.345\n",
      "Ep 1 (Step 000280): Train loss 5.158, Val loss 5.348\n",
      "Ep 1 (Step 000290): Train loss 5.103, Val loss 5.324\n",
      "Ep 1 (Step 000300): Train loss 5.100, Val loss 5.314\n",
      "Ep 1 (Step 000310): Train loss 5.299, Val loss 5.305\n",
      "Ep 1 (Step 000320): Train loss 5.122, Val loss 5.303\n",
      "Ep 1 (Step 000330): Train loss 5.172, Val loss 5.290\n",
      "Ep 1 (Step 000340): Train loss 5.141, Val loss 5.278\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2781\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.926, Val loss 8.926\n",
      "Ep 1 (Step 000010): Train loss 6.902, Val loss 6.894\n",
      "Ep 1 (Step 000020): Train loss 6.495, Val loss 6.490\n",
      "Ep 1 (Step 000030): Train loss 6.440, Val loss 6.488\n",
      "Ep 1 (Step 000040): Train loss 6.373, Val loss 6.421\n",
      "Ep 1 (Step 000050): Train loss 6.349, Val loss 6.311\n",
      "Ep 1 (Step 000060): Train loss 6.191, Val loss 6.195\n",
      "Ep 1 (Step 000070): Train loss 5.996, Val loss 6.094\n",
      "Ep 1 (Step 000080): Train loss 5.895, Val loss 5.975\n",
      "Ep 1 (Step 000090): Train loss 5.908, Val loss 5.939\n",
      "Ep 1 (Step 000100): Train loss 5.822, Val loss 5.854\n",
      "Ep 1 (Step 000110): Train loss 5.642, Val loss 5.775\n",
      "Ep 1 (Step 000120): Train loss 5.646, Val loss 5.721\n",
      "Ep 1 (Step 000130): Train loss 5.617, Val loss 5.690\n",
      "Ep 1 (Step 000140): Train loss 5.567, Val loss 5.629\n",
      "Ep 1 (Step 000150): Train loss 5.537, Val loss 5.606\n",
      "Ep 1 (Step 000160): Train loss 5.401, Val loss 5.539\n",
      "Ep 1 (Step 000170): Train loss 5.454, Val loss 5.507\n",
      "Ep 1 (Step 000180): Train loss 5.379, Val loss 5.474\n",
      "Ep 1 (Step 000190): Train loss 5.389, Val loss 5.479\n",
      "Ep 1 (Step 000200): Train loss 5.273, Val loss 5.448\n",
      "Ep 1 (Step 000210): Train loss 5.281, Val loss 5.427\n",
      "Ep 1 (Step 000220): Train loss 5.364, Val loss 5.432\n",
      "Ep 1 (Step 000230): Train loss 5.356, Val loss 5.396\n",
      "Ep 1 (Step 000240): Train loss 5.287, Val loss 5.379\n",
      "Ep 1 (Step 000250): Train loss 5.261, Val loss 5.378\n",
      "Ep 1 (Step 000260): Train loss 5.322, Val loss 5.365\n",
      "Ep 1 (Step 000270): Train loss 5.327, Val loss 5.345\n",
      "Ep 1 (Step 000280): Train loss 5.157, Val loss 5.337\n",
      "Ep 1 (Step 000290): Train loss 5.141, Val loss 5.325\n",
      "Ep 1 (Step 000300): Train loss 5.315, Val loss 5.323\n",
      "Ep 1 (Step 000310): Train loss 5.205, Val loss 5.298\n",
      "Ep 1 (Step 000320): Train loss 5.222, Val loss 5.293\n",
      "Ep 1 (Step 000330): Train loss 5.115, Val loss 5.290\n",
      "Ep 1 (Step 000340): Train loss 5.114, Val loss 5.284\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2845\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.908, Val loss 8.934\n",
      "Ep 1 (Step 000010): Train loss 6.904, Val loss 6.898\n",
      "Ep 1 (Step 000020): Train loss 6.493, Val loss 6.469\n",
      "Ep 1 (Step 000030): Train loss 6.452, Val loss 6.471\n",
      "Ep 1 (Step 000040): Train loss 6.418, Val loss 6.399\n",
      "Ep 1 (Step 000050): Train loss 6.264, Val loss 6.251\n",
      "Ep 1 (Step 000060): Train loss 6.067, Val loss 6.116\n",
      "Ep 1 (Step 000070): Train loss 6.022, Val loss 6.018\n",
      "Ep 1 (Step 000080): Train loss 5.892, Val loss 5.904\n",
      "Ep 1 (Step 000090): Train loss 5.779, Val loss 5.832\n",
      "Ep 1 (Step 000100): Train loss 5.776, Val loss 5.774\n",
      "Ep 1 (Step 000110): Train loss 5.653, Val loss 5.739\n",
      "Ep 1 (Step 000120): Train loss 5.628, Val loss 5.663\n",
      "Ep 1 (Step 000130): Train loss 5.571, Val loss 5.637\n",
      "Ep 1 (Step 000140): Train loss 5.513, Val loss 5.587\n",
      "Ep 1 (Step 000150): Train loss 5.484, Val loss 5.580\n",
      "Ep 1 (Step 000160): Train loss 5.501, Val loss 5.532\n",
      "Ep 1 (Step 000170): Train loss 5.464, Val loss 5.523\n",
      "Ep 1 (Step 000180): Train loss 5.398, Val loss 5.507\n",
      "Ep 1 (Step 000190): Train loss 5.429, Val loss 5.504\n",
      "Ep 1 (Step 000200): Train loss 5.362, Val loss 5.462\n",
      "Ep 1 (Step 000210): Train loss 5.331, Val loss 5.457\n",
      "Ep 1 (Step 000220): Train loss 5.303, Val loss 5.433\n",
      "Ep 1 (Step 000230): Train loss 5.266, Val loss 5.426\n",
      "Ep 1 (Step 000240): Train loss 5.334, Val loss 5.409\n",
      "Ep 1 (Step 000250): Train loss 5.311, Val loss 5.390\n",
      "Ep 1 (Step 000260): Train loss 5.266, Val loss 5.376\n",
      "Ep 1 (Step 000270): Train loss 5.266, Val loss 5.365\n",
      "Ep 1 (Step 000280): Train loss 5.186, Val loss 5.358\n",
      "Ep 1 (Step 000290): Train loss 5.196, Val loss 5.360\n",
      "Ep 1 (Step 000300): Train loss 5.186, Val loss 5.348\n",
      "Ep 1 (Step 000310): Train loss 5.158, Val loss 5.322\n",
      "Ep 1 (Step 000320): Train loss 5.124, Val loss 5.323\n",
      "Ep 1 (Step 000330): Train loss 5.080, Val loss 5.303\n",
      "Ep 1 (Step 000340): Train loss 5.185, Val loss 5.281\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2809\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.936, Val loss 8.936\n",
      "Ep 1 (Step 000010): Train loss 6.865, Val loss 6.840\n",
      "Ep 1 (Step 000020): Train loss 6.521, Val loss 6.463\n",
      "Ep 1 (Step 000030): Train loss 6.515, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.423, Val loss 6.423\n",
      "Ep 1 (Step 000050): Train loss 6.293, Val loss 6.316\n",
      "Ep 1 (Step 000060): Train loss 6.126, Val loss 6.147\n",
      "Ep 1 (Step 000070): Train loss 6.024, Val loss 6.040\n",
      "Ep 1 (Step 000080): Train loss 5.880, Val loss 5.962\n",
      "Ep 1 (Step 000090): Train loss 5.781, Val loss 5.899\n",
      "Ep 1 (Step 000100): Train loss 5.685, Val loss 5.790\n",
      "Ep 1 (Step 000110): Train loss 5.630, Val loss 5.737\n",
      "Ep 1 (Step 000120): Train loss 5.602, Val loss 5.683\n",
      "Ep 1 (Step 000130): Train loss 5.563, Val loss 5.656\n",
      "Ep 1 (Step 000140): Train loss 5.491, Val loss 5.601\n",
      "Ep 1 (Step 000150): Train loss 5.474, Val loss 5.574\n",
      "Ep 1 (Step 000160): Train loss 5.432, Val loss 5.550\n",
      "Ep 1 (Step 000170): Train loss 5.433, Val loss 5.527\n",
      "Ep 1 (Step 000180): Train loss 5.400, Val loss 5.479\n",
      "Ep 1 (Step 000190): Train loss 5.392, Val loss 5.473\n",
      "Ep 1 (Step 000200): Train loss 5.396, Val loss 5.445\n",
      "Ep 1 (Step 000210): Train loss 5.294, Val loss 5.425\n",
      "Ep 1 (Step 000220): Train loss 5.270, Val loss 5.420\n",
      "Ep 1 (Step 000230): Train loss 5.263, Val loss 5.416\n",
      "Ep 1 (Step 000240): Train loss 5.254, Val loss 5.400\n",
      "Ep 1 (Step 000250): Train loss 5.154, Val loss 5.376\n",
      "Ep 1 (Step 000260): Train loss 5.224, Val loss 5.356\n",
      "Ep 1 (Step 000270): Train loss 5.190, Val loss 5.339\n",
      "Ep 1 (Step 000280): Train loss 5.134, Val loss 5.325\n",
      "Ep 1 (Step 000290): Train loss 5.241, Val loss 5.311\n",
      "Ep 1 (Step 000300): Train loss 5.164, Val loss 5.298\n",
      "Ep 1 (Step 000310): Train loss 5.209, Val loss 5.287\n",
      "Ep 1 (Step 000320): Train loss 5.111, Val loss 5.275\n",
      "Ep 1 (Step 000330): Train loss 5.077, Val loss 5.271\n",
      "Ep 1 (Step 000340): Train loss 5.006, Val loss 5.245\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2451\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.974, Val loss 8.952\n",
      "Ep 1 (Step 000010): Train loss 6.894, Val loss 6.841\n",
      "Ep 1 (Step 000020): Train loss 6.483, Val loss 6.444\n",
      "Ep 1 (Step 000030): Train loss 6.411, Val loss 6.475\n",
      "Ep 1 (Step 000040): Train loss 6.377, Val loss 6.416\n",
      "Ep 1 (Step 000050): Train loss 6.319, Val loss 6.291\n",
      "Ep 1 (Step 000060): Train loss 6.091, Val loss 6.129\n",
      "Ep 1 (Step 000070): Train loss 5.999, Val loss 6.010\n",
      "Ep 1 (Step 000080): Train loss 5.955, Val loss 5.957\n",
      "Ep 1 (Step 000090): Train loss 5.860, Val loss 5.874\n",
      "Ep 1 (Step 000100): Train loss 5.813, Val loss 5.794\n",
      "Ep 1 (Step 000110): Train loss 5.655, Val loss 5.748\n",
      "Ep 1 (Step 000120): Train loss 5.687, Val loss 5.730\n",
      "Ep 1 (Step 000130): Train loss 5.547, Val loss 5.652\n",
      "Ep 1 (Step 000140): Train loss 5.540, Val loss 5.605\n",
      "Ep 1 (Step 000150): Train loss 5.539, Val loss 5.591\n",
      "Ep 1 (Step 000160): Train loss 5.378, Val loss 5.552\n",
      "Ep 1 (Step 000170): Train loss 5.394, Val loss 5.526\n",
      "Ep 1 (Step 000180): Train loss 5.374, Val loss 5.504\n",
      "Ep 1 (Step 000190): Train loss 5.417, Val loss 5.472\n",
      "Ep 1 (Step 000200): Train loss 5.443, Val loss 5.459\n",
      "Ep 1 (Step 000210): Train loss 5.284, Val loss 5.469\n",
      "Ep 1 (Step 000220): Train loss 5.286, Val loss 5.440\n",
      "Ep 1 (Step 000230): Train loss 5.316, Val loss 5.390\n",
      "Ep 1 (Step 000240): Train loss 5.292, Val loss 5.372\n",
      "Ep 1 (Step 000250): Train loss 5.290, Val loss 5.341\n",
      "Ep 1 (Step 000260): Train loss 5.209, Val loss 5.342\n",
      "Ep 1 (Step 000270): Train loss 5.177, Val loss 5.354\n",
      "Ep 1 (Step 000280): Train loss 5.190, Val loss 5.342\n",
      "Ep 1 (Step 000290): Train loss 5.274, Val loss 5.310\n",
      "Ep 1 (Step 000300): Train loss 5.136, Val loss 5.320\n",
      "Ep 1 (Step 000310): Train loss 5.178, Val loss 5.283\n",
      "Ep 1 (Step 000320): Train loss 5.181, Val loss 5.281\n",
      "Ep 1 (Step 000330): Train loss 5.117, Val loss 5.258\n",
      "Ep 1 (Step 000340): Train loss 5.090, Val loss 5.244\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2439\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.927, Val loss 8.893\n",
      "Ep 1 (Step 000010): Train loss 6.914, Val loss 6.901\n",
      "Ep 1 (Step 000020): Train loss 6.529, Val loss 6.474\n",
      "Ep 1 (Step 000030): Train loss 6.454, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.361, Val loss 6.424\n",
      "Ep 1 (Step 000050): Train loss 6.276, Val loss 6.294\n",
      "Ep 1 (Step 000060): Train loss 6.120, Val loss 6.170\n",
      "Ep 1 (Step 000070): Train loss 5.941, Val loss 6.023\n",
      "Ep 1 (Step 000080): Train loss 5.846, Val loss 5.967\n",
      "Ep 1 (Step 000090): Train loss 5.834, Val loss 5.852\n",
      "Ep 1 (Step 000100): Train loss 5.797, Val loss 5.790\n",
      "Ep 1 (Step 000110): Train loss 5.673, Val loss 5.727\n",
      "Ep 1 (Step 000120): Train loss 5.469, Val loss 5.674\n",
      "Ep 1 (Step 000130): Train loss 5.547, Val loss 5.636\n",
      "Ep 1 (Step 000140): Train loss 5.514, Val loss 5.589\n",
      "Ep 1 (Step 000150): Train loss 5.437, Val loss 5.570\n",
      "Ep 1 (Step 000160): Train loss 5.498, Val loss 5.541\n",
      "Ep 1 (Step 000170): Train loss 5.354, Val loss 5.515\n",
      "Ep 1 (Step 000180): Train loss 5.405, Val loss 5.485\n",
      "Ep 1 (Step 000190): Train loss 5.382, Val loss 5.450\n",
      "Ep 1 (Step 000200): Train loss 5.294, Val loss 5.421\n",
      "Ep 1 (Step 000210): Train loss 5.369, Val loss 5.419\n",
      "Ep 1 (Step 000220): Train loss 5.311, Val loss 5.386\n",
      "Ep 1 (Step 000230): Train loss 5.316, Val loss 5.385\n",
      "Ep 1 (Step 000240): Train loss 5.233, Val loss 5.363\n",
      "Ep 1 (Step 000250): Train loss 5.193, Val loss 5.356\n",
      "Ep 1 (Step 000260): Train loss 5.117, Val loss 5.327\n",
      "Ep 1 (Step 000270): Train loss 5.248, Val loss 5.308\n",
      "Ep 1 (Step 000280): Train loss 5.200, Val loss 5.320\n",
      "Ep 1 (Step 000290): Train loss 5.172, Val loss 5.299\n",
      "Ep 1 (Step 000300): Train loss 5.183, Val loss 5.284\n",
      "Ep 1 (Step 000310): Train loss 5.107, Val loss 5.271\n",
      "Ep 1 (Step 000320): Train loss 5.144, Val loss 5.255\n",
      "Ep 1 (Step 000330): Train loss 5.251, Val loss 5.256\n",
      "Ep 1 (Step 000340): Train loss 5.093, Val loss 5.236\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2360\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.127, Val loss 9.129\n",
      "Ep 1 (Step 000010): Train loss 7.494, Val loss 7.505\n",
      "Ep 1 (Step 000020): Train loss 6.810, Val loss 6.810\n",
      "Ep 1 (Step 000030): Train loss 6.513, Val loss 6.482\n",
      "Ep 1 (Step 000040): Train loss 6.452, Val loss 6.410\n",
      "Ep 1 (Step 000050): Train loss 6.355, Val loss 6.394\n",
      "Ep 1 (Step 000060): Train loss 6.331, Val loss 6.364\n",
      "Ep 1 (Step 000070): Train loss 6.254, Val loss 6.269\n",
      "Ep 1 (Step 000080): Train loss 6.184, Val loss 6.192\n",
      "Ep 1 (Step 000090): Train loss 6.038, Val loss 6.100\n",
      "Ep 1 (Step 000100): Train loss 6.015, Val loss 6.035\n",
      "Ep 1 (Step 000110): Train loss 5.908, Val loss 5.980\n",
      "Ep 1 (Step 000120): Train loss 5.908, Val loss 5.911\n",
      "Ep 1 (Step 000130): Train loss 5.845, Val loss 5.877\n",
      "Ep 1 (Step 000140): Train loss 5.786, Val loss 5.827\n",
      "Ep 1 (Step 000150): Train loss 5.706, Val loss 5.808\n",
      "Ep 1 (Step 000160): Train loss 5.735, Val loss 5.768\n",
      "Ep 1 (Step 000170): Train loss 5.695, Val loss 5.723\n",
      "Ep 1 (Step 000180): Train loss 5.694, Val loss 5.695\n",
      "Ep 1 (Step 000190): Train loss 5.601, Val loss 5.655\n",
      "Ep 1 (Step 000200): Train loss 5.569, Val loss 5.632\n",
      "Ep 1 (Step 000210): Train loss 5.523, Val loss 5.609\n",
      "Ep 1 (Step 000220): Train loss 5.644, Val loss 5.590\n",
      "Ep 1 (Step 000230): Train loss 5.496, Val loss 5.570\n",
      "Ep 1 (Step 000240): Train loss 5.426, Val loss 5.547\n",
      "Ep 1 (Step 000250): Train loss 5.514, Val loss 5.525\n",
      "Ep 1 (Step 000260): Train loss 5.492, Val loss 5.499\n",
      "Ep 1 (Step 000270): Train loss 5.391, Val loss 5.493\n",
      "Ep 1 (Step 000280): Train loss 5.342, Val loss 5.479\n",
      "Ep 1 (Step 000290): Train loss 5.361, Val loss 5.459\n",
      "Ep 1 (Step 000300): Train loss 5.346, Val loss 5.466\n",
      "Ep 1 (Step 000310): Train loss 5.344, Val loss 5.442\n",
      "Ep 1 (Step 000320): Train loss 5.212, Val loss 5.425\n",
      "Ep 1 (Step 000330): Train loss 5.313, Val loss 5.415\n",
      "Ep 1 (Step 000340): Train loss 5.305, Val loss 5.414\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.4144\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.161, Val loss 9.150\n",
      "Ep 1 (Step 000010): Train loss 7.564, Val loss 7.517\n",
      "Ep 1 (Step 000020): Train loss 6.828, Val loss 6.829\n",
      "Ep 1 (Step 000030): Train loss 6.476, Val loss 6.482\n",
      "Ep 1 (Step 000040): Train loss 6.392, Val loss 6.397\n",
      "Ep 1 (Step 000050): Train loss 6.367, Val loss 6.385\n",
      "Ep 1 (Step 000060): Train loss 6.373, Val loss 6.348\n",
      "Ep 1 (Step 000070): Train loss 6.278, Val loss 6.283\n",
      "Ep 1 (Step 000080): Train loss 6.249, Val loss 6.185\n",
      "Ep 1 (Step 000090): Train loss 6.130, Val loss 6.088\n",
      "Ep 1 (Step 000100): Train loss 5.984, Val loss 6.018\n",
      "Ep 1 (Step 000110): Train loss 5.952, Val loss 5.968\n",
      "Ep 1 (Step 000120): Train loss 5.905, Val loss 5.917\n",
      "Ep 1 (Step 000130): Train loss 5.814, Val loss 5.889\n",
      "Ep 1 (Step 000140): Train loss 5.798, Val loss 5.835\n",
      "Ep 1 (Step 000150): Train loss 5.710, Val loss 5.789\n",
      "Ep 1 (Step 000160): Train loss 5.684, Val loss 5.768\n",
      "Ep 1 (Step 000170): Train loss 5.643, Val loss 5.735\n",
      "Ep 1 (Step 000180): Train loss 5.643, Val loss 5.705\n",
      "Ep 1 (Step 000190): Train loss 5.525, Val loss 5.671\n",
      "Ep 1 (Step 000200): Train loss 5.523, Val loss 5.645\n",
      "Ep 1 (Step 000210): Train loss 5.565, Val loss 5.618\n",
      "Ep 1 (Step 000220): Train loss 5.474, Val loss 5.601\n",
      "Ep 1 (Step 000230): Train loss 5.536, Val loss 5.572\n",
      "Ep 1 (Step 000240): Train loss 5.425, Val loss 5.556\n",
      "Ep 1 (Step 000250): Train loss 5.444, Val loss 5.539\n",
      "Ep 1 (Step 000260): Train loss 5.398, Val loss 5.534\n",
      "Ep 1 (Step 000270): Train loss 5.366, Val loss 5.498\n",
      "Ep 1 (Step 000280): Train loss 5.371, Val loss 5.490\n",
      "Ep 1 (Step 000290): Train loss 5.360, Val loss 5.475\n",
      "Ep 1 (Step 000300): Train loss 5.460, Val loss 5.458\n",
      "Ep 1 (Step 000310): Train loss 5.284, Val loss 5.435\n",
      "Ep 1 (Step 000320): Train loss 5.382, Val loss 5.423\n",
      "Ep 1 (Step 000330): Train loss 5.279, Val loss 5.398\n",
      "Ep 1 (Step 000340): Train loss 5.220, Val loss 5.411\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.4112\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.145, Val loss 9.146\n",
      "Ep 1 (Step 000010): Train loss 7.519, Val loss 7.468\n",
      "Ep 1 (Step 000020): Train loss 6.863, Val loss 6.798\n",
      "Ep 1 (Step 000030): Train loss 6.514, Val loss 6.491\n",
      "Ep 1 (Step 000040): Train loss 6.411, Val loss 6.410\n",
      "Ep 1 (Step 000050): Train loss 6.351, Val loss 6.395\n",
      "Ep 1 (Step 000060): Train loss 6.383, Val loss 6.345\n",
      "Ep 1 (Step 000070): Train loss 6.259, Val loss 6.246\n",
      "Ep 1 (Step 000080): Train loss 6.135, Val loss 6.164\n",
      "Ep 1 (Step 000090): Train loss 6.162, Val loss 6.093\n",
      "Ep 1 (Step 000100): Train loss 6.048, Val loss 6.018\n",
      "Ep 1 (Step 000110): Train loss 5.927, Val loss 5.967\n",
      "Ep 1 (Step 000120): Train loss 5.971, Val loss 5.903\n",
      "Ep 1 (Step 000130): Train loss 5.868, Val loss 5.859\n",
      "Ep 1 (Step 000140): Train loss 5.758, Val loss 5.829\n",
      "Ep 1 (Step 000150): Train loss 5.761, Val loss 5.784\n",
      "Ep 1 (Step 000160): Train loss 5.710, Val loss 5.758\n",
      "Ep 1 (Step 000170): Train loss 5.664, Val loss 5.755\n",
      "Ep 1 (Step 000180): Train loss 5.589, Val loss 5.715\n",
      "Ep 1 (Step 000190): Train loss 5.752, Val loss 5.673\n",
      "Ep 1 (Step 000200): Train loss 5.584, Val loss 5.656\n",
      "Ep 1 (Step 000210): Train loss 5.495, Val loss 5.622\n",
      "Ep 1 (Step 000220): Train loss 5.555, Val loss 5.615\n",
      "Ep 1 (Step 000230): Train loss 5.435, Val loss 5.577\n",
      "Ep 1 (Step 000240): Train loss 5.497, Val loss 5.570\n",
      "Ep 1 (Step 000250): Train loss 5.522, Val loss 5.549\n",
      "Ep 1 (Step 000260): Train loss 5.340, Val loss 5.521\n",
      "Ep 1 (Step 000270): Train loss 5.401, Val loss 5.511\n",
      "Ep 1 (Step 000280): Train loss 5.363, Val loss 5.501\n",
      "Ep 1 (Step 000290): Train loss 5.439, Val loss 5.478\n",
      "Ep 1 (Step 000300): Train loss 5.367, Val loss 5.484\n",
      "Ep 1 (Step 000310): Train loss 5.320, Val loss 5.459\n",
      "Ep 1 (Step 000320): Train loss 5.355, Val loss 5.443\n",
      "Ep 1 (Step 000330): Train loss 5.287, Val loss 5.434\n",
      "Ep 1 (Step 000340): Train loss 5.345, Val loss 5.412\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.4117\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.154, Val loss 9.130\n",
      "Ep 1 (Step 000010): Train loss 7.478, Val loss 7.460\n",
      "Ep 1 (Step 000020): Train loss 6.849, Val loss 6.804\n",
      "Ep 1 (Step 000030): Train loss 6.515, Val loss 6.502\n",
      "Ep 1 (Step 000040): Train loss 6.399, Val loss 6.419\n",
      "Ep 1 (Step 000050): Train loss 6.427, Val loss 6.390\n",
      "Ep 1 (Step 000060): Train loss 6.326, Val loss 6.359\n",
      "Ep 1 (Step 000070): Train loss 6.317, Val loss 6.317\n",
      "Ep 1 (Step 000080): Train loss 6.155, Val loss 6.204\n",
      "Ep 1 (Step 000090): Train loss 6.092, Val loss 6.108\n",
      "Ep 1 (Step 000100): Train loss 5.984, Val loss 6.032\n",
      "Ep 1 (Step 000110): Train loss 5.923, Val loss 5.983\n",
      "Ep 1 (Step 000120): Train loss 5.834, Val loss 5.934\n",
      "Ep 1 (Step 000130): Train loss 5.766, Val loss 5.875\n",
      "Ep 1 (Step 000140): Train loss 5.858, Val loss 5.832\n",
      "Ep 1 (Step 000150): Train loss 5.783, Val loss 5.788\n",
      "Ep 1 (Step 000160): Train loss 5.664, Val loss 5.753\n",
      "Ep 1 (Step 000170): Train loss 5.615, Val loss 5.731\n",
      "Ep 1 (Step 000180): Train loss 5.611, Val loss 5.687\n",
      "Ep 1 (Step 000190): Train loss 5.605, Val loss 5.659\n",
      "Ep 1 (Step 000200): Train loss 5.517, Val loss 5.628\n",
      "Ep 1 (Step 000210): Train loss 5.620, Val loss 5.604\n",
      "Ep 1 (Step 000220): Train loss 5.484, Val loss 5.575\n",
      "Ep 1 (Step 000230): Train loss 5.348, Val loss 5.558\n",
      "Ep 1 (Step 000240): Train loss 5.364, Val loss 5.531\n",
      "Ep 1 (Step 000250): Train loss 5.423, Val loss 5.515\n",
      "Ep 1 (Step 000260): Train loss 5.478, Val loss 5.521\n",
      "Ep 1 (Step 000270): Train loss 5.418, Val loss 5.483\n",
      "Ep 1 (Step 000280): Train loss 5.439, Val loss 5.489\n",
      "Ep 1 (Step 000290): Train loss 5.410, Val loss 5.454\n",
      "Ep 1 (Step 000300): Train loss 5.311, Val loss 5.440\n",
      "Ep 1 (Step 000310): Train loss 5.337, Val loss 5.424\n",
      "Ep 1 (Step 000320): Train loss 5.293, Val loss 5.415\n",
      "Ep 1 (Step 000330): Train loss 5.294, Val loss 5.405\n",
      "Ep 1 (Step 000340): Train loss 5.341, Val loss 5.385\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3854\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.123, Val loss 9.120\n",
      "Ep 1 (Step 000010): Train loss 7.486, Val loss 7.459\n",
      "Ep 1 (Step 000020): Train loss 6.780, Val loss 6.808\n",
      "Ep 1 (Step 000030): Train loss 6.517, Val loss 6.507\n",
      "Ep 1 (Step 000040): Train loss 6.440, Val loss 6.411\n",
      "Ep 1 (Step 000050): Train loss 6.383, Val loss 6.378\n",
      "Ep 1 (Step 000060): Train loss 6.363, Val loss 6.342\n",
      "Ep 1 (Step 000070): Train loss 6.289, Val loss 6.293\n",
      "Ep 1 (Step 000080): Train loss 6.249, Val loss 6.202\n",
      "Ep 1 (Step 000090): Train loss 6.054, Val loss 6.111\n",
      "Ep 1 (Step 000100): Train loss 5.957, Val loss 6.038\n",
      "Ep 1 (Step 000110): Train loss 5.954, Val loss 5.976\n",
      "Ep 1 (Step 000120): Train loss 5.845, Val loss 5.933\n",
      "Ep 1 (Step 000130): Train loss 5.812, Val loss 5.863\n",
      "Ep 1 (Step 000140): Train loss 5.837, Val loss 5.832\n",
      "Ep 1 (Step 000150): Train loss 5.807, Val loss 5.774\n",
      "Ep 1 (Step 000160): Train loss 5.730, Val loss 5.724\n",
      "Ep 1 (Step 000170): Train loss 5.702, Val loss 5.715\n",
      "Ep 1 (Step 000180): Train loss 5.588, Val loss 5.669\n",
      "Ep 1 (Step 000190): Train loss 5.551, Val loss 5.630\n",
      "Ep 1 (Step 000200): Train loss 5.562, Val loss 5.611\n",
      "Ep 1 (Step 000210): Train loss 5.550, Val loss 5.584\n",
      "Ep 1 (Step 000220): Train loss 5.448, Val loss 5.575\n",
      "Ep 1 (Step 000230): Train loss 5.425, Val loss 5.553\n",
      "Ep 1 (Step 000240): Train loss 5.461, Val loss 5.533\n",
      "Ep 1 (Step 000250): Train loss 5.462, Val loss 5.518\n",
      "Ep 1 (Step 000260): Train loss 5.391, Val loss 5.512\n",
      "Ep 1 (Step 000270): Train loss 5.492, Val loss 5.480\n",
      "Ep 1 (Step 000280): Train loss 5.380, Val loss 5.478\n",
      "Ep 1 (Step 000290): Train loss 5.376, Val loss 5.462\n",
      "Ep 1 (Step 000300): Train loss 5.310, Val loss 5.458\n",
      "Ep 1 (Step 000310): Train loss 5.347, Val loss 5.432\n",
      "Ep 1 (Step 000320): Train loss 5.388, Val loss 5.425\n",
      "Ep 1 (Step 000330): Train loss 5.312, Val loss 5.409\n",
      "Ep 1 (Step 000340): Train loss 5.297, Val loss 5.387\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3873\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.168, Val loss 9.148\n",
      "Ep 1 (Step 000010): Train loss 7.541, Val loss 7.508\n",
      "Ep 1 (Step 000020): Train loss 6.872, Val loss 6.833\n",
      "Ep 1 (Step 000030): Train loss 6.485, Val loss 6.492\n",
      "Ep 1 (Step 000040): Train loss 6.460, Val loss 6.405\n",
      "Ep 1 (Step 000050): Train loss 6.378, Val loss 6.374\n",
      "Ep 1 (Step 000060): Train loss 6.307, Val loss 6.345\n",
      "Ep 1 (Step 000070): Train loss 6.317, Val loss 6.304\n",
      "Ep 1 (Step 000080): Train loss 6.153, Val loss 6.175\n",
      "Ep 1 (Step 000090): Train loss 6.107, Val loss 6.111\n",
      "Ep 1 (Step 000100): Train loss 6.036, Val loss 6.026\n",
      "Ep 1 (Step 000110): Train loss 5.884, Val loss 5.988\n",
      "Ep 1 (Step 000120): Train loss 5.910, Val loss 5.929\n",
      "Ep 1 (Step 000130): Train loss 5.872, Val loss 5.864\n",
      "Ep 1 (Step 000140): Train loss 5.840, Val loss 5.840\n",
      "Ep 1 (Step 000150): Train loss 5.712, Val loss 5.768\n",
      "Ep 1 (Step 000160): Train loss 5.681, Val loss 5.729\n",
      "Ep 1 (Step 000170): Train loss 5.621, Val loss 5.711\n",
      "Ep 1 (Step 000180): Train loss 5.578, Val loss 5.678\n",
      "Ep 1 (Step 000190): Train loss 5.558, Val loss 5.640\n",
      "Ep 1 (Step 000200): Train loss 5.560, Val loss 5.633\n",
      "Ep 1 (Step 000210): Train loss 5.501, Val loss 5.598\n",
      "Ep 1 (Step 000220): Train loss 5.515, Val loss 5.581\n",
      "Ep 1 (Step 000230): Train loss 5.472, Val loss 5.558\n",
      "Ep 1 (Step 000240): Train loss 5.378, Val loss 5.516\n",
      "Ep 1 (Step 000250): Train loss 5.384, Val loss 5.509\n",
      "Ep 1 (Step 000260): Train loss 5.376, Val loss 5.488\n",
      "Ep 1 (Step 000270): Train loss 5.427, Val loss 5.486\n",
      "Ep 1 (Step 000280): Train loss 5.359, Val loss 5.474\n",
      "Ep 1 (Step 000290): Train loss 5.388, Val loss 5.456\n",
      "Ep 1 (Step 000300): Train loss 5.350, Val loss 5.461\n",
      "Ep 1 (Step 000310): Train loss 5.331, Val loss 5.424\n",
      "Ep 1 (Step 000320): Train loss 5.346, Val loss 5.405\n",
      "Ep 1 (Step 000330): Train loss 5.317, Val loss 5.408\n",
      "Ep 1 (Step 000340): Train loss 5.340, Val loss 5.398\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3982\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.960, Val loss 8.930\n",
      "Ep 1 (Step 000010): Train loss 6.953, Val loss 6.893\n",
      "Ep 1 (Step 000020): Train loss 6.451, Val loss 6.439\n",
      "Ep 1 (Step 000030): Train loss 6.412, Val loss 6.450\n",
      "Ep 1 (Step 000040): Train loss 6.399, Val loss 6.396\n",
      "Ep 1 (Step 000050): Train loss 6.294, Val loss 6.264\n",
      "Ep 1 (Step 000060): Train loss 6.173, Val loss 6.136\n",
      "Ep 1 (Step 000070): Train loss 6.004, Val loss 6.036\n",
      "Ep 1 (Step 000080): Train loss 5.912, Val loss 5.957\n",
      "Ep 1 (Step 000090): Train loss 5.892, Val loss 5.910\n",
      "Ep 1 (Step 000100): Train loss 5.730, Val loss 5.814\n",
      "Ep 1 (Step 000110): Train loss 5.684, Val loss 5.768\n",
      "Ep 1 (Step 000120): Train loss 5.684, Val loss 5.699\n",
      "Ep 1 (Step 000130): Train loss 5.595, Val loss 5.650\n",
      "Ep 1 (Step 000140): Train loss 5.529, Val loss 5.611\n",
      "Ep 1 (Step 000150): Train loss 5.553, Val loss 5.564\n",
      "Ep 1 (Step 000160): Train loss 5.543, Val loss 5.545\n",
      "Ep 1 (Step 000170): Train loss 5.385, Val loss 5.538\n",
      "Ep 1 (Step 000180): Train loss 5.391, Val loss 5.511\n",
      "Ep 1 (Step 000190): Train loss 5.357, Val loss 5.473\n",
      "Ep 1 (Step 000200): Train loss 5.385, Val loss 5.447\n",
      "Ep 1 (Step 000210): Train loss 5.196, Val loss 5.423\n",
      "Ep 1 (Step 000220): Train loss 5.321, Val loss 5.406\n",
      "Ep 1 (Step 000230): Train loss 5.317, Val loss 5.401\n",
      "Ep 1 (Step 000240): Train loss 5.243, Val loss 5.381\n",
      "Ep 1 (Step 000250): Train loss 5.277, Val loss 5.388\n",
      "Ep 1 (Step 000260): Train loss 5.235, Val loss 5.384\n",
      "Ep 1 (Step 000270): Train loss 5.258, Val loss 5.373\n",
      "Ep 1 (Step 000280): Train loss 5.260, Val loss 5.353\n",
      "Ep 1 (Step 000290): Train loss 5.217, Val loss 5.357\n",
      "Ep 1 (Step 000300): Train loss 5.189, Val loss 5.323\n",
      "Ep 1 (Step 000310): Train loss 5.169, Val loss 5.333\n",
      "Ep 1 (Step 000320): Train loss 5.222, Val loss 5.316\n",
      "Ep 1 (Step 000330): Train loss 5.141, Val loss 5.294\n",
      "Ep 1 (Step 000340): Train loss 5.111, Val loss 5.293\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2932\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.896, Val loss 8.879\n",
      "Ep 1 (Step 000010): Train loss 6.906, Val loss 6.819\n",
      "Ep 1 (Step 000020): Train loss 6.515, Val loss 6.478\n",
      "Ep 1 (Step 000030): Train loss 6.494, Val loss 6.483\n",
      "Ep 1 (Step 000040): Train loss 6.465, Val loss 6.391\n",
      "Ep 1 (Step 000050): Train loss 6.191, Val loss 6.258\n",
      "Ep 1 (Step 000060): Train loss 6.080, Val loss 6.092\n",
      "Ep 1 (Step 000070): Train loss 6.041, Val loss 6.004\n",
      "Ep 1 (Step 000080): Train loss 5.887, Val loss 5.916\n",
      "Ep 1 (Step 000090): Train loss 5.829, Val loss 5.849\n",
      "Ep 1 (Step 000100): Train loss 5.738, Val loss 5.793\n",
      "Ep 1 (Step 000110): Train loss 5.680, Val loss 5.731\n",
      "Ep 1 (Step 000120): Train loss 5.527, Val loss 5.698\n",
      "Ep 1 (Step 000130): Train loss 5.614, Val loss 5.662\n",
      "Ep 1 (Step 000140): Train loss 5.491, Val loss 5.616\n",
      "Ep 1 (Step 000150): Train loss 5.446, Val loss 5.590\n",
      "Ep 1 (Step 000160): Train loss 5.530, Val loss 5.554\n",
      "Ep 1 (Step 000170): Train loss 5.473, Val loss 5.543\n",
      "Ep 1 (Step 000180): Train loss 5.381, Val loss 5.543\n",
      "Ep 1 (Step 000190): Train loss 5.366, Val loss 5.486\n",
      "Ep 1 (Step 000200): Train loss 5.395, Val loss 5.473\n",
      "Ep 1 (Step 000210): Train loss 5.268, Val loss 5.435\n",
      "Ep 1 (Step 000220): Train loss 5.315, Val loss 5.431\n",
      "Ep 1 (Step 000230): Train loss 5.252, Val loss 5.422\n",
      "Ep 1 (Step 000240): Train loss 5.342, Val loss 5.399\n",
      "Ep 1 (Step 000250): Train loss 5.264, Val loss 5.392\n",
      "Ep 1 (Step 000260): Train loss 5.269, Val loss 5.371\n",
      "Ep 1 (Step 000270): Train loss 5.295, Val loss 5.354\n",
      "Ep 1 (Step 000280): Train loss 5.269, Val loss 5.344\n",
      "Ep 1 (Step 000290): Train loss 5.171, Val loss 5.321\n",
      "Ep 1 (Step 000300): Train loss 5.114, Val loss 5.313\n",
      "Ep 1 (Step 000310): Train loss 5.179, Val loss 5.330\n",
      "Ep 1 (Step 000320): Train loss 5.287, Val loss 5.304\n",
      "Ep 1 (Step 000330): Train loss 5.222, Val loss 5.290\n",
      "Ep 1 (Step 000340): Train loss 5.164, Val loss 5.267\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2672\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.914, Val loss 8.902\n",
      "Ep 1 (Step 000010): Train loss 6.846, Val loss 6.829\n",
      "Ep 1 (Step 000020): Train loss 6.493, Val loss 6.448\n",
      "Ep 1 (Step 000030): Train loss 6.467, Val loss 6.484\n",
      "Ep 1 (Step 000040): Train loss 6.333, Val loss 6.408\n",
      "Ep 1 (Step 000050): Train loss 6.258, Val loss 6.255\n",
      "Ep 1 (Step 000060): Train loss 6.135, Val loss 6.116\n",
      "Ep 1 (Step 000070): Train loss 6.001, Val loss 6.035\n",
      "Ep 1 (Step 000080): Train loss 5.858, Val loss 5.933\n",
      "Ep 1 (Step 000090): Train loss 5.780, Val loss 5.837\n",
      "Ep 1 (Step 000100): Train loss 5.716, Val loss 5.765\n",
      "Ep 1 (Step 000110): Train loss 5.674, Val loss 5.718\n",
      "Ep 1 (Step 000120): Train loss 5.679, Val loss 5.709\n",
      "Ep 1 (Step 000130): Train loss 5.568, Val loss 5.644\n",
      "Ep 1 (Step 000140): Train loss 5.493, Val loss 5.600\n",
      "Ep 1 (Step 000150): Train loss 5.461, Val loss 5.550\n",
      "Ep 1 (Step 000160): Train loss 5.487, Val loss 5.541\n",
      "Ep 1 (Step 000170): Train loss 5.401, Val loss 5.515\n",
      "Ep 1 (Step 000180): Train loss 5.396, Val loss 5.502\n",
      "Ep 1 (Step 000190): Train loss 5.429, Val loss 5.489\n",
      "Ep 1 (Step 000200): Train loss 5.312, Val loss 5.453\n",
      "Ep 1 (Step 000210): Train loss 5.347, Val loss 5.441\n",
      "Ep 1 (Step 000220): Train loss 5.302, Val loss 5.427\n",
      "Ep 1 (Step 000230): Train loss 5.299, Val loss 5.407\n",
      "Ep 1 (Step 000240): Train loss 5.329, Val loss 5.402\n",
      "Ep 1 (Step 000250): Train loss 5.269, Val loss 5.369\n",
      "Ep 1 (Step 000260): Train loss 5.301, Val loss 5.363\n",
      "Ep 1 (Step 000270): Train loss 5.210, Val loss 5.349\n",
      "Ep 1 (Step 000280): Train loss 5.207, Val loss 5.337\n",
      "Ep 1 (Step 000290): Train loss 5.137, Val loss 5.336\n",
      "Ep 1 (Step 000300): Train loss 5.184, Val loss 5.329\n",
      "Ep 1 (Step 000310): Train loss 5.200, Val loss 5.325\n",
      "Ep 1 (Step 000320): Train loss 5.170, Val loss 5.306\n",
      "Ep 1 (Step 000330): Train loss 5.123, Val loss 5.287\n",
      "Ep 1 (Step 000340): Train loss 5.120, Val loss 5.291\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2908\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.958, Val loss 8.954\n",
      "Ep 1 (Step 000010): Train loss 6.909, Val loss 6.871\n",
      "Ep 1 (Step 000020): Train loss 6.492, Val loss 6.481\n",
      "Ep 1 (Step 000030): Train loss 6.468, Val loss 6.461\n",
      "Ep 1 (Step 000040): Train loss 6.334, Val loss 6.393\n",
      "Ep 1 (Step 000050): Train loss 6.281, Val loss 6.280\n",
      "Ep 1 (Step 000060): Train loss 6.140, Val loss 6.132\n",
      "Ep 1 (Step 000070): Train loss 6.066, Val loss 6.033\n",
      "Ep 1 (Step 000080): Train loss 6.082, Val loss 5.968\n",
      "Ep 1 (Step 000090): Train loss 5.742, Val loss 5.856\n",
      "Ep 1 (Step 000100): Train loss 5.770, Val loss 5.812\n",
      "Ep 1 (Step 000110): Train loss 5.635, Val loss 5.730\n",
      "Ep 1 (Step 000120): Train loss 5.566, Val loss 5.688\n",
      "Ep 1 (Step 000130): Train loss 5.602, Val loss 5.630\n",
      "Ep 1 (Step 000140): Train loss 5.560, Val loss 5.600\n",
      "Ep 1 (Step 000150): Train loss 5.437, Val loss 5.557\n",
      "Ep 1 (Step 000160): Train loss 5.411, Val loss 5.528\n",
      "Ep 1 (Step 000170): Train loss 5.392, Val loss 5.497\n",
      "Ep 1 (Step 000180): Train loss 5.421, Val loss 5.467\n",
      "Ep 1 (Step 000190): Train loss 5.312, Val loss 5.442\n",
      "Ep 1 (Step 000200): Train loss 5.395, Val loss 5.452\n",
      "Ep 1 (Step 000210): Train loss 5.336, Val loss 5.427\n",
      "Ep 1 (Step 000220): Train loss 5.312, Val loss 5.425\n",
      "Ep 1 (Step 000230): Train loss 5.245, Val loss 5.383\n",
      "Ep 1 (Step 000240): Train loss 5.297, Val loss 5.377\n",
      "Ep 1 (Step 000250): Train loss 5.250, Val loss 5.381\n",
      "Ep 1 (Step 000260): Train loss 5.274, Val loss 5.336\n",
      "Ep 1 (Step 000270): Train loss 5.218, Val loss 5.319\n",
      "Ep 1 (Step 000280): Train loss 5.191, Val loss 5.299\n",
      "Ep 1 (Step 000290): Train loss 5.198, Val loss 5.282\n",
      "Ep 1 (Step 000300): Train loss 5.135, Val loss 5.299\n",
      "Ep 1 (Step 000310): Train loss 5.097, Val loss 5.267\n",
      "Ep 1 (Step 000320): Train loss 5.120, Val loss 5.257\n",
      "Ep 1 (Step 000330): Train loss 5.119, Val loss 5.257\n",
      "Ep 1 (Step 000340): Train loss 5.072, Val loss 5.236\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2355\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.926, Val loss 8.894\n",
      "Ep 1 (Step 000010): Train loss 6.896, Val loss 6.821\n",
      "Ep 1 (Step 000020): Train loss 6.482, Val loss 6.467\n",
      "Ep 1 (Step 000030): Train loss 6.510, Val loss 6.443\n",
      "Ep 1 (Step 000040): Train loss 6.462, Val loss 6.384\n",
      "Ep 1 (Step 000050): Train loss 6.285, Val loss 6.256\n",
      "Ep 1 (Step 000060): Train loss 6.190, Val loss 6.126\n",
      "Ep 1 (Step 000070): Train loss 6.007, Val loss 6.041\n",
      "Ep 1 (Step 000080): Train loss 5.900, Val loss 5.941\n",
      "Ep 1 (Step 000090): Train loss 5.882, Val loss 5.843\n",
      "Ep 1 (Step 000100): Train loss 5.755, Val loss 5.800\n",
      "Ep 1 (Step 000110): Train loss 5.671, Val loss 5.726\n",
      "Ep 1 (Step 000120): Train loss 5.555, Val loss 5.653\n",
      "Ep 1 (Step 000130): Train loss 5.583, Val loss 5.611\n",
      "Ep 1 (Step 000140): Train loss 5.486, Val loss 5.572\n",
      "Ep 1 (Step 000150): Train loss 5.473, Val loss 5.551\n",
      "Ep 1 (Step 000160): Train loss 5.448, Val loss 5.524\n",
      "Ep 1 (Step 000170): Train loss 5.410, Val loss 5.488\n",
      "Ep 1 (Step 000180): Train loss 5.386, Val loss 5.460\n",
      "Ep 1 (Step 000190): Train loss 5.343, Val loss 5.433\n",
      "Ep 1 (Step 000200): Train loss 5.318, Val loss 5.427\n",
      "Ep 1 (Step 000210): Train loss 5.372, Val loss 5.419\n",
      "Ep 1 (Step 000220): Train loss 5.265, Val loss 5.393\n",
      "Ep 1 (Step 000230): Train loss 5.270, Val loss 5.382\n",
      "Ep 1 (Step 000240): Train loss 5.288, Val loss 5.375\n",
      "Ep 1 (Step 000250): Train loss 5.199, Val loss 5.345\n",
      "Ep 1 (Step 000260): Train loss 5.280, Val loss 5.322\n",
      "Ep 1 (Step 000270): Train loss 5.226, Val loss 5.325\n",
      "Ep 1 (Step 000280): Train loss 5.127, Val loss 5.297\n",
      "Ep 1 (Step 000290): Train loss 5.158, Val loss 5.296\n",
      "Ep 1 (Step 000300): Train loss 5.163, Val loss 5.281\n",
      "Ep 1 (Step 000310): Train loss 5.135, Val loss 5.273\n",
      "Ep 1 (Step 000320): Train loss 5.194, Val loss 5.254\n",
      "Ep 1 (Step 000330): Train loss 5.137, Val loss 5.259\n",
      "Ep 1 (Step 000340): Train loss 5.077, Val loss 5.231\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2314\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.937, Val loss 8.910\n",
      "Ep 1 (Step 000010): Train loss 6.853, Val loss 6.822\n",
      "Ep 1 (Step 000020): Train loss 6.527, Val loss 6.459\n",
      "Ep 1 (Step 000030): Train loss 6.482, Val loss 6.471\n",
      "Ep 1 (Step 000040): Train loss 6.445, Val loss 6.410\n",
      "Ep 1 (Step 000050): Train loss 6.263, Val loss 6.272\n",
      "Ep 1 (Step 000060): Train loss 6.056, Val loss 6.124\n",
      "Ep 1 (Step 000070): Train loss 6.013, Val loss 6.009\n",
      "Ep 1 (Step 000080): Train loss 5.916, Val loss 5.910\n",
      "Ep 1 (Step 000090): Train loss 5.815, Val loss 5.856\n",
      "Ep 1 (Step 000100): Train loss 5.644, Val loss 5.800\n",
      "Ep 1 (Step 000110): Train loss 5.628, Val loss 5.732\n",
      "Ep 1 (Step 000120): Train loss 5.645, Val loss 5.685\n",
      "Ep 1 (Step 000130): Train loss 5.609, Val loss 5.647\n",
      "Ep 1 (Step 000140): Train loss 5.450, Val loss 5.584\n",
      "Ep 1 (Step 000150): Train loss 5.525, Val loss 5.557\n",
      "Ep 1 (Step 000160): Train loss 5.453, Val loss 5.520\n",
      "Ep 1 (Step 000170): Train loss 5.445, Val loss 5.500\n",
      "Ep 1 (Step 000180): Train loss 5.431, Val loss 5.475\n",
      "Ep 1 (Step 000190): Train loss 5.320, Val loss 5.450\n",
      "Ep 1 (Step 000200): Train loss 5.318, Val loss 5.454\n",
      "Ep 1 (Step 000210): Train loss 5.292, Val loss 5.443\n",
      "Ep 1 (Step 000220): Train loss 5.256, Val loss 5.410\n",
      "Ep 1 (Step 000230): Train loss 5.201, Val loss 5.385\n",
      "Ep 1 (Step 000240): Train loss 5.256, Val loss 5.362\n",
      "Ep 1 (Step 000250): Train loss 5.295, Val loss 5.360\n",
      "Ep 1 (Step 000260): Train loss 5.121, Val loss 5.337\n",
      "Ep 1 (Step 000270): Train loss 5.235, Val loss 5.348\n",
      "Ep 1 (Step 000280): Train loss 5.218, Val loss 5.335\n",
      "Ep 1 (Step 000290): Train loss 5.197, Val loss 5.324\n",
      "Ep 1 (Step 000300): Train loss 5.228, Val loss 5.317\n",
      "Ep 1 (Step 000310): Train loss 5.047, Val loss 5.297\n",
      "Ep 1 (Step 000320): Train loss 5.188, Val loss 5.292\n",
      "Ep 1 (Step 000330): Train loss 5.064, Val loss 5.273\n",
      "Ep 1 (Step 000340): Train loss 5.131, Val loss 5.255\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2552\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.154, Val loss 9.156\n",
      "Ep 1 (Step 000010): Train loss 7.514, Val loss 7.502\n",
      "Ep 1 (Step 000020): Train loss 6.892, Val loss 6.835\n",
      "Ep 1 (Step 000030): Train loss 6.516, Val loss 6.504\n",
      "Ep 1 (Step 000040): Train loss 6.417, Val loss 6.393\n",
      "Ep 1 (Step 000050): Train loss 6.342, Val loss 6.351\n",
      "Ep 1 (Step 000060): Train loss 6.257, Val loss 6.293\n",
      "Ep 1 (Step 000070): Train loss 6.139, Val loss 6.174\n",
      "Ep 1 (Step 000080): Train loss 6.041, Val loss 6.063\n",
      "Ep 1 (Step 000090): Train loss 5.958, Val loss 6.000\n",
      "Ep 1 (Step 000100): Train loss 5.920, Val loss 5.954\n",
      "Ep 1 (Step 000110): Train loss 5.836, Val loss 5.884\n",
      "Ep 1 (Step 000120): Train loss 5.775, Val loss 5.846\n",
      "Ep 1 (Step 000130): Train loss 5.776, Val loss 5.803\n",
      "Ep 1 (Step 000140): Train loss 5.672, Val loss 5.752\n",
      "Ep 1 (Step 000150): Train loss 5.681, Val loss 5.711\n",
      "Ep 1 (Step 000160): Train loss 5.607, Val loss 5.675\n",
      "Ep 1 (Step 000170): Train loss 5.668, Val loss 5.661\n",
      "Ep 1 (Step 000180): Train loss 5.495, Val loss 5.629\n",
      "Ep 1 (Step 000190): Train loss 5.525, Val loss 5.611\n",
      "Ep 1 (Step 000200): Train loss 5.522, Val loss 5.581\n",
      "Ep 1 (Step 000210): Train loss 5.546, Val loss 5.552\n",
      "Ep 1 (Step 000220): Train loss 5.450, Val loss 5.538\n",
      "Ep 1 (Step 000230): Train loss 5.466, Val loss 5.527\n",
      "Ep 1 (Step 000240): Train loss 5.318, Val loss 5.502\n",
      "Ep 1 (Step 000250): Train loss 5.314, Val loss 5.482\n",
      "Ep 1 (Step 000260): Train loss 5.387, Val loss 5.459\n",
      "Ep 1 (Step 000270): Train loss 5.357, Val loss 5.451\n",
      "Ep 1 (Step 000280): Train loss 5.247, Val loss 5.417\n",
      "Ep 1 (Step 000290): Train loss 5.347, Val loss 5.417\n",
      "Ep 1 (Step 000300): Train loss 5.246, Val loss 5.386\n",
      "Ep 1 (Step 000310): Train loss 5.270, Val loss 5.393\n",
      "Ep 1 (Step 000320): Train loss 5.169, Val loss 5.366\n",
      "Ep 1 (Step 000330): Train loss 5.193, Val loss 5.359\n",
      "Ep 1 (Step 000340): Train loss 5.201, Val loss 5.340\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3404\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.147, Val loss 9.149\n",
      "Ep 1 (Step 000010): Train loss 7.516, Val loss 7.523\n",
      "Ep 1 (Step 000020): Train loss 6.912, Val loss 6.850\n",
      "Ep 1 (Step 000030): Train loss 6.502, Val loss 6.519\n",
      "Ep 1 (Step 000040): Train loss 6.411, Val loss 6.393\n",
      "Ep 1 (Step 000050): Train loss 6.352, Val loss 6.362\n",
      "Ep 1 (Step 000060): Train loss 6.265, Val loss 6.311\n",
      "Ep 1 (Step 000070): Train loss 6.102, Val loss 6.169\n",
      "Ep 1 (Step 000080): Train loss 5.979, Val loss 6.087\n",
      "Ep 1 (Step 000090): Train loss 5.976, Val loss 6.009\n",
      "Ep 1 (Step 000100): Train loss 5.876, Val loss 5.945\n",
      "Ep 1 (Step 000110): Train loss 5.815, Val loss 5.899\n",
      "Ep 1 (Step 000120): Train loss 5.789, Val loss 5.830\n",
      "Ep 1 (Step 000130): Train loss 5.810, Val loss 5.803\n",
      "Ep 1 (Step 000140): Train loss 5.688, Val loss 5.755\n",
      "Ep 1 (Step 000150): Train loss 5.652, Val loss 5.707\n",
      "Ep 1 (Step 000160): Train loss 5.523, Val loss 5.675\n",
      "Ep 1 (Step 000170): Train loss 5.601, Val loss 5.627\n",
      "Ep 1 (Step 000180): Train loss 5.526, Val loss 5.597\n",
      "Ep 1 (Step 000190): Train loss 5.578, Val loss 5.576\n",
      "Ep 1 (Step 000200): Train loss 5.499, Val loss 5.545\n",
      "Ep 1 (Step 000210): Train loss 5.446, Val loss 5.540\n",
      "Ep 1 (Step 000220): Train loss 5.422, Val loss 5.515\n",
      "Ep 1 (Step 000230): Train loss 5.410, Val loss 5.498\n",
      "Ep 1 (Step 000240): Train loss 5.343, Val loss 5.472\n",
      "Ep 1 (Step 000250): Train loss 5.375, Val loss 5.462\n",
      "Ep 1 (Step 000260): Train loss 5.289, Val loss 5.449\n",
      "Ep 1 (Step 000270): Train loss 5.358, Val loss 5.437\n",
      "Ep 1 (Step 000280): Train loss 5.365, Val loss 5.424\n",
      "Ep 1 (Step 000290): Train loss 5.277, Val loss 5.405\n",
      "Ep 1 (Step 000300): Train loss 5.300, Val loss 5.408\n",
      "Ep 1 (Step 000310): Train loss 5.179, Val loss 5.386\n",
      "Ep 1 (Step 000320): Train loss 5.246, Val loss 5.384\n",
      "Ep 1 (Step 000330): Train loss 5.200, Val loss 5.362\n",
      "Ep 1 (Step 000340): Train loss 5.148, Val loss 5.358\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3584\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.118, Val loss 9.125\n",
      "Ep 1 (Step 000010): Train loss 7.478, Val loss 7.446\n",
      "Ep 1 (Step 000020): Train loss 6.791, Val loss 6.787\n",
      "Ep 1 (Step 000030): Train loss 6.480, Val loss 6.482\n",
      "Ep 1 (Step 000040): Train loss 6.422, Val loss 6.381\n",
      "Ep 1 (Step 000050): Train loss 6.319, Val loss 6.354\n",
      "Ep 1 (Step 000060): Train loss 6.243, Val loss 6.274\n",
      "Ep 1 (Step 000070): Train loss 6.155, Val loss 6.145\n",
      "Ep 1 (Step 000080): Train loss 6.031, Val loss 6.056\n",
      "Ep 1 (Step 000090): Train loss 5.882, Val loss 5.971\n",
      "Ep 1 (Step 000100): Train loss 5.871, Val loss 5.906\n",
      "Ep 1 (Step 000110): Train loss 5.909, Val loss 5.860\n",
      "Ep 1 (Step 000120): Train loss 5.758, Val loss 5.811\n",
      "Ep 1 (Step 000130): Train loss 5.746, Val loss 5.763\n",
      "Ep 1 (Step 000140): Train loss 5.644, Val loss 5.733\n",
      "Ep 1 (Step 000150): Train loss 5.687, Val loss 5.696\n",
      "Ep 1 (Step 000160): Train loss 5.538, Val loss 5.676\n",
      "Ep 1 (Step 000170): Train loss 5.583, Val loss 5.646\n",
      "Ep 1 (Step 000180): Train loss 5.494, Val loss 5.622\n",
      "Ep 1 (Step 000190): Train loss 5.503, Val loss 5.598\n",
      "Ep 1 (Step 000200): Train loss 5.433, Val loss 5.563\n",
      "Ep 1 (Step 000210): Train loss 5.434, Val loss 5.541\n",
      "Ep 1 (Step 000220): Train loss 5.477, Val loss 5.516\n",
      "Ep 1 (Step 000230): Train loss 5.382, Val loss 5.503\n",
      "Ep 1 (Step 000240): Train loss 5.472, Val loss 5.495\n",
      "Ep 1 (Step 000250): Train loss 5.394, Val loss 5.475\n",
      "Ep 1 (Step 000260): Train loss 5.375, Val loss 5.469\n",
      "Ep 1 (Step 000270): Train loss 5.305, Val loss 5.437\n",
      "Ep 1 (Step 000280): Train loss 5.372, Val loss 5.439\n",
      "Ep 1 (Step 000290): Train loss 5.314, Val loss 5.415\n",
      "Ep 1 (Step 000300): Train loss 5.256, Val loss 5.389\n",
      "Ep 1 (Step 000310): Train loss 5.230, Val loss 5.399\n",
      "Ep 1 (Step 000320): Train loss 5.263, Val loss 5.387\n",
      "Ep 1 (Step 000330): Train loss 5.187, Val loss 5.362\n",
      "Ep 1 (Step 000340): Train loss 5.256, Val loss 5.342\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3419\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.123, Val loss 9.100\n",
      "Ep 1 (Step 000010): Train loss 7.441, Val loss 7.413\n",
      "Ep 1 (Step 000020): Train loss 6.856, Val loss 6.787\n",
      "Ep 1 (Step 000030): Train loss 6.536, Val loss 6.473\n",
      "Ep 1 (Step 000040): Train loss 6.394, Val loss 6.380\n",
      "Ep 1 (Step 000050): Train loss 6.371, Val loss 6.345\n",
      "Ep 1 (Step 000060): Train loss 6.231, Val loss 6.307\n",
      "Ep 1 (Step 000070): Train loss 6.182, Val loss 6.201\n",
      "Ep 1 (Step 000080): Train loss 6.114, Val loss 6.089\n",
      "Ep 1 (Step 000090): Train loss 6.050, Val loss 6.025\n",
      "Ep 1 (Step 000100): Train loss 5.947, Val loss 5.964\n",
      "Ep 1 (Step 000110): Train loss 5.873, Val loss 5.903\n",
      "Ep 1 (Step 000120): Train loss 5.843, Val loss 5.836\n",
      "Ep 1 (Step 000130): Train loss 5.780, Val loss 5.787\n",
      "Ep 1 (Step 000140): Train loss 5.729, Val loss 5.742\n",
      "Ep 1 (Step 000150): Train loss 5.643, Val loss 5.703\n",
      "Ep 1 (Step 000160): Train loss 5.704, Val loss 5.670\n",
      "Ep 1 (Step 000170): Train loss 5.592, Val loss 5.636\n",
      "Ep 1 (Step 000180): Train loss 5.611, Val loss 5.610\n",
      "Ep 1 (Step 000190): Train loss 5.498, Val loss 5.582\n",
      "Ep 1 (Step 000200): Train loss 5.451, Val loss 5.561\n",
      "Ep 1 (Step 000210): Train loss 5.407, Val loss 5.537\n",
      "Ep 1 (Step 000220): Train loss 5.449, Val loss 5.509\n",
      "Ep 1 (Step 000230): Train loss 5.397, Val loss 5.488\n",
      "Ep 1 (Step 000240): Train loss 5.391, Val loss 5.472\n",
      "Ep 1 (Step 000250): Train loss 5.420, Val loss 5.462\n",
      "Ep 1 (Step 000260): Train loss 5.325, Val loss 5.439\n",
      "Ep 1 (Step 000270): Train loss 5.343, Val loss 5.414\n",
      "Ep 1 (Step 000280): Train loss 5.242, Val loss 5.394\n",
      "Ep 1 (Step 000290): Train loss 5.302, Val loss 5.374\n",
      "Ep 1 (Step 000300): Train loss 5.266, Val loss 5.371\n",
      "Ep 1 (Step 000310): Train loss 5.298, Val loss 5.363\n",
      "Ep 1 (Step 000320): Train loss 5.219, Val loss 5.340\n",
      "Ep 1 (Step 000330): Train loss 5.275, Val loss 5.332\n",
      "Ep 1 (Step 000340): Train loss 5.173, Val loss 5.312\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3121\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.109, Val loss 9.101\n",
      "Ep 1 (Step 000010): Train loss 7.494, Val loss 7.458\n",
      "Ep 1 (Step 000020): Train loss 6.857, Val loss 6.793\n",
      "Ep 1 (Step 000030): Train loss 6.492, Val loss 6.467\n",
      "Ep 1 (Step 000040): Train loss 6.397, Val loss 6.378\n",
      "Ep 1 (Step 000050): Train loss 6.348, Val loss 6.360\n",
      "Ep 1 (Step 000060): Train loss 6.266, Val loss 6.294\n",
      "Ep 1 (Step 000070): Train loss 6.247, Val loss 6.155\n",
      "Ep 1 (Step 000080): Train loss 6.094, Val loss 6.069\n",
      "Ep 1 (Step 000090): Train loss 5.963, Val loss 5.993\n",
      "Ep 1 (Step 000100): Train loss 5.890, Val loss 5.934\n",
      "Ep 1 (Step 000110): Train loss 5.916, Val loss 5.877\n",
      "Ep 1 (Step 000120): Train loss 5.851, Val loss 5.823\n",
      "Ep 1 (Step 000130): Train loss 5.710, Val loss 5.778\n",
      "Ep 1 (Step 000140): Train loss 5.658, Val loss 5.753\n",
      "Ep 1 (Step 000150): Train loss 5.610, Val loss 5.712\n",
      "Ep 1 (Step 000160): Train loss 5.603, Val loss 5.671\n",
      "Ep 1 (Step 000170): Train loss 5.489, Val loss 5.640\n",
      "Ep 1 (Step 000180): Train loss 5.544, Val loss 5.601\n",
      "Ep 1 (Step 000190): Train loss 5.517, Val loss 5.576\n",
      "Ep 1 (Step 000200): Train loss 5.391, Val loss 5.545\n",
      "Ep 1 (Step 000210): Train loss 5.474, Val loss 5.531\n",
      "Ep 1 (Step 000220): Train loss 5.505, Val loss 5.508\n",
      "Ep 1 (Step 000230): Train loss 5.383, Val loss 5.485\n",
      "Ep 1 (Step 000240): Train loss 5.369, Val loss 5.462\n",
      "Ep 1 (Step 000250): Train loss 5.458, Val loss 5.443\n",
      "Ep 1 (Step 000260): Train loss 5.340, Val loss 5.435\n",
      "Ep 1 (Step 000270): Train loss 5.393, Val loss 5.415\n",
      "Ep 1 (Step 000280): Train loss 5.237, Val loss 5.406\n",
      "Ep 1 (Step 000290): Train loss 5.294, Val loss 5.399\n",
      "Ep 1 (Step 000300): Train loss 5.203, Val loss 5.388\n",
      "Ep 1 (Step 000310): Train loss 5.169, Val loss 5.376\n",
      "Ep 1 (Step 000320): Train loss 5.236, Val loss 5.361\n",
      "Ep 1 (Step 000330): Train loss 5.132, Val loss 5.337\n",
      "Ep 1 (Step 000340): Train loss 5.207, Val loss 5.330\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3296\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.154, Val loss 9.169\n",
      "Ep 1 (Step 000010): Train loss 7.517, Val loss 7.501\n",
      "Ep 1 (Step 000020): Train loss 6.913, Val loss 6.811\n",
      "Ep 1 (Step 000030): Train loss 6.498, Val loss 6.486\n",
      "Ep 1 (Step 000040): Train loss 6.378, Val loss 6.386\n",
      "Ep 1 (Step 000050): Train loss 6.337, Val loss 6.353\n",
      "Ep 1 (Step 000060): Train loss 6.310, Val loss 6.288\n",
      "Ep 1 (Step 000070): Train loss 6.146, Val loss 6.183\n",
      "Ep 1 (Step 000080): Train loss 6.089, Val loss 6.070\n",
      "Ep 1 (Step 000090): Train loss 5.964, Val loss 5.989\n",
      "Ep 1 (Step 000100): Train loss 5.939, Val loss 5.925\n",
      "Ep 1 (Step 000110): Train loss 5.842, Val loss 5.867\n",
      "Ep 1 (Step 000120): Train loss 5.800, Val loss 5.834\n",
      "Ep 1 (Step 000130): Train loss 5.774, Val loss 5.792\n",
      "Ep 1 (Step 000140): Train loss 5.753, Val loss 5.755\n",
      "Ep 1 (Step 000150): Train loss 5.696, Val loss 5.731\n",
      "Ep 1 (Step 000160): Train loss 5.637, Val loss 5.672\n",
      "Ep 1 (Step 000170): Train loss 5.631, Val loss 5.642\n",
      "Ep 1 (Step 000180): Train loss 5.588, Val loss 5.615\n",
      "Ep 1 (Step 000190): Train loss 5.559, Val loss 5.598\n",
      "Ep 1 (Step 000200): Train loss 5.507, Val loss 5.571\n",
      "Ep 1 (Step 000210): Train loss 5.470, Val loss 5.547\n",
      "Ep 1 (Step 000220): Train loss 5.415, Val loss 5.538\n",
      "Ep 1 (Step 000230): Train loss 5.351, Val loss 5.515\n",
      "Ep 1 (Step 000240): Train loss 5.340, Val loss 5.484\n",
      "Ep 1 (Step 000250): Train loss 5.318, Val loss 5.462\n",
      "Ep 1 (Step 000260): Train loss 5.373, Val loss 5.443\n",
      "Ep 1 (Step 000270): Train loss 5.361, Val loss 5.412\n",
      "Ep 1 (Step 000280): Train loss 5.289, Val loss 5.401\n",
      "Ep 1 (Step 000290): Train loss 5.247, Val loss 5.392\n",
      "Ep 1 (Step 000300): Train loss 5.263, Val loss 5.383\n",
      "Ep 1 (Step 000310): Train loss 5.323, Val loss 5.377\n",
      "Ep 1 (Step 000320): Train loss 5.225, Val loss 5.354\n",
      "Ep 1 (Step 000330): Train loss 5.203, Val loss 5.349\n",
      "Ep 1 (Step 000340): Train loss 5.302, Val loss 5.338\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3379\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.956, Val loss 8.927\n",
      "Ep 1 (Step 000010): Train loss 6.977, Val loss 6.911\n",
      "Ep 1 (Step 000020): Train loss 6.510, Val loss 6.478\n",
      "Ep 1 (Step 000030): Train loss 6.427, Val loss 6.488\n",
      "Ep 1 (Step 000040): Train loss 6.355, Val loss 6.380\n",
      "Ep 1 (Step 000050): Train loss 6.217, Val loss 6.215\n",
      "Ep 1 (Step 000060): Train loss 5.976, Val loss 6.040\n",
      "Ep 1 (Step 000070): Train loss 5.899, Val loss 5.952\n",
      "Ep 1 (Step 000080): Train loss 5.881, Val loss 5.869\n",
      "Ep 1 (Step 000090): Train loss 5.704, Val loss 5.797\n",
      "Ep 1 (Step 000100): Train loss 5.724, Val loss 5.723\n",
      "Ep 1 (Step 000110): Train loss 5.658, Val loss 5.701\n",
      "Ep 1 (Step 000120): Train loss 5.569, Val loss 5.641\n",
      "Ep 1 (Step 000130): Train loss 5.536, Val loss 5.595\n",
      "Ep 1 (Step 000140): Train loss 5.439, Val loss 5.562\n",
      "Ep 1 (Step 000150): Train loss 5.428, Val loss 5.541\n",
      "Ep 1 (Step 000160): Train loss 5.472, Val loss 5.523\n",
      "Ep 1 (Step 000170): Train loss 5.451, Val loss 5.490\n",
      "Ep 1 (Step 000180): Train loss 5.314, Val loss 5.480\n",
      "Ep 1 (Step 000190): Train loss 5.341, Val loss 5.456\n",
      "Ep 1 (Step 000200): Train loss 5.269, Val loss 5.426\n",
      "Ep 1 (Step 000210): Train loss 5.288, Val loss 5.414\n",
      "Ep 1 (Step 000220): Train loss 5.264, Val loss 5.382\n",
      "Ep 1 (Step 000230): Train loss 5.218, Val loss 5.367\n",
      "Ep 1 (Step 000240): Train loss 5.227, Val loss 5.340\n",
      "Ep 1 (Step 000250): Train loss 5.223, Val loss 5.340\n",
      "Ep 1 (Step 000260): Train loss 5.149, Val loss 5.307\n",
      "Ep 1 (Step 000270): Train loss 5.188, Val loss 5.319\n",
      "Ep 1 (Step 000280): Train loss 5.095, Val loss 5.301\n",
      "Ep 1 (Step 000290): Train loss 5.183, Val loss 5.291\n",
      "Ep 1 (Step 000300): Train loss 5.160, Val loss 5.285\n",
      "Ep 1 (Step 000310): Train loss 5.081, Val loss 5.277\n",
      "Ep 1 (Step 000320): Train loss 5.105, Val loss 5.265\n",
      "Ep 1 (Step 000330): Train loss 5.103, Val loss 5.244\n",
      "Ep 1 (Step 000340): Train loss 5.000, Val loss 5.242\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2419\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.906, Val loss 8.910\n",
      "Ep 1 (Step 000010): Train loss 6.922, Val loss 6.858\n",
      "Ep 1 (Step 000020): Train loss 6.474, Val loss 6.486\n",
      "Ep 1 (Step 000030): Train loss 6.408, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.352, Val loss 6.368\n",
      "Ep 1 (Step 000050): Train loss 6.188, Val loss 6.187\n",
      "Ep 1 (Step 000060): Train loss 5.995, Val loss 6.045\n",
      "Ep 1 (Step 000070): Train loss 5.912, Val loss 5.947\n",
      "Ep 1 (Step 000080): Train loss 5.859, Val loss 5.857\n",
      "Ep 1 (Step 000090): Train loss 5.804, Val loss 5.782\n",
      "Ep 1 (Step 000100): Train loss 5.730, Val loss 5.754\n",
      "Ep 1 (Step 000110): Train loss 5.654, Val loss 5.670\n",
      "Ep 1 (Step 000120): Train loss 5.585, Val loss 5.603\n",
      "Ep 1 (Step 000130): Train loss 5.600, Val loss 5.565\n",
      "Ep 1 (Step 000140): Train loss 5.505, Val loss 5.552\n",
      "Ep 1 (Step 000150): Train loss 5.505, Val loss 5.516\n",
      "Ep 1 (Step 000160): Train loss 5.359, Val loss 5.485\n",
      "Ep 1 (Step 000170): Train loss 5.438, Val loss 5.491\n",
      "Ep 1 (Step 000180): Train loss 5.250, Val loss 5.450\n",
      "Ep 1 (Step 000190): Train loss 5.374, Val loss 5.437\n",
      "Ep 1 (Step 000200): Train loss 5.214, Val loss 5.416\n",
      "Ep 1 (Step 000210): Train loss 5.336, Val loss 5.405\n",
      "Ep 1 (Step 000220): Train loss 5.213, Val loss 5.384\n",
      "Ep 1 (Step 000230): Train loss 5.201, Val loss 5.383\n",
      "Ep 1 (Step 000240): Train loss 5.162, Val loss 5.359\n",
      "Ep 1 (Step 000250): Train loss 5.296, Val loss 5.355\n",
      "Ep 1 (Step 000260): Train loss 5.134, Val loss 5.330\n",
      "Ep 1 (Step 000270): Train loss 5.182, Val loss 5.316\n",
      "Ep 1 (Step 000280): Train loss 5.134, Val loss 5.303\n",
      "Ep 1 (Step 000290): Train loss 5.134, Val loss 5.311\n",
      "Ep 1 (Step 000300): Train loss 5.172, Val loss 5.297\n",
      "Ep 1 (Step 000310): Train loss 5.144, Val loss 5.281\n",
      "Ep 1 (Step 000320): Train loss 5.092, Val loss 5.282\n",
      "Ep 1 (Step 000330): Train loss 5.110, Val loss 5.280\n",
      "Ep 1 (Step 000340): Train loss 5.102, Val loss 5.243\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2425\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.894, Val loss 8.891\n",
      "Ep 1 (Step 000010): Train loss 6.887, Val loss 6.836\n",
      "Ep 1 (Step 000020): Train loss 6.566, Val loss 6.467\n",
      "Ep 1 (Step 000030): Train loss 6.402, Val loss 6.470\n",
      "Ep 1 (Step 000040): Train loss 6.272, Val loss 6.336\n",
      "Ep 1 (Step 000050): Train loss 6.224, Val loss 6.165\n",
      "Ep 1 (Step 000060): Train loss 6.005, Val loss 6.030\n",
      "Ep 1 (Step 000070): Train loss 5.871, Val loss 5.938\n",
      "Ep 1 (Step 000080): Train loss 5.838, Val loss 5.869\n",
      "Ep 1 (Step 000090): Train loss 5.724, Val loss 5.795\n",
      "Ep 1 (Step 000100): Train loss 5.677, Val loss 5.720\n",
      "Ep 1 (Step 000110): Train loss 5.604, Val loss 5.692\n",
      "Ep 1 (Step 000120): Train loss 5.529, Val loss 5.618\n",
      "Ep 1 (Step 000130): Train loss 5.605, Val loss 5.594\n",
      "Ep 1 (Step 000140): Train loss 5.481, Val loss 5.539\n",
      "Ep 1 (Step 000150): Train loss 5.493, Val loss 5.505\n",
      "Ep 1 (Step 000160): Train loss 5.475, Val loss 5.463\n",
      "Ep 1 (Step 000170): Train loss 5.383, Val loss 5.471\n",
      "Ep 1 (Step 000180): Train loss 5.256, Val loss 5.450\n",
      "Ep 1 (Step 000190): Train loss 5.235, Val loss 5.415\n",
      "Ep 1 (Step 000200): Train loss 5.322, Val loss 5.404\n",
      "Ep 1 (Step 000210): Train loss 5.226, Val loss 5.381\n",
      "Ep 1 (Step 000220): Train loss 5.378, Val loss 5.383\n",
      "Ep 1 (Step 000230): Train loss 5.289, Val loss 5.372\n",
      "Ep 1 (Step 000240): Train loss 5.154, Val loss 5.368\n",
      "Ep 1 (Step 000250): Train loss 5.148, Val loss 5.342\n",
      "Ep 1 (Step 000260): Train loss 5.226, Val loss 5.320\n",
      "Ep 1 (Step 000270): Train loss 5.200, Val loss 5.323\n",
      "Ep 1 (Step 000280): Train loss 5.165, Val loss 5.308\n",
      "Ep 1 (Step 000290): Train loss 5.154, Val loss 5.278\n",
      "Ep 1 (Step 000300): Train loss 5.097, Val loss 5.273\n",
      "Ep 1 (Step 000310): Train loss 5.087, Val loss 5.262\n",
      "Ep 1 (Step 000320): Train loss 4.977, Val loss 5.268\n",
      "Ep 1 (Step 000330): Train loss 5.137, Val loss 5.244\n",
      "Ep 1 (Step 000340): Train loss 5.100, Val loss 5.244\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2442\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.948, Val loss 8.924\n",
      "Ep 1 (Step 000010): Train loss 6.877, Val loss 6.830\n",
      "Ep 1 (Step 000020): Train loss 6.459, Val loss 6.458\n",
      "Ep 1 (Step 000030): Train loss 6.447, Val loss 6.441\n",
      "Ep 1 (Step 000040): Train loss 6.400, Val loss 6.360\n",
      "Ep 1 (Step 000050): Train loss 6.212, Val loss 6.218\n",
      "Ep 1 (Step 000060): Train loss 6.003, Val loss 6.084\n",
      "Ep 1 (Step 000070): Train loss 5.975, Val loss 5.988\n",
      "Ep 1 (Step 000080): Train loss 5.919, Val loss 5.936\n",
      "Ep 1 (Step 000090): Train loss 5.793, Val loss 5.812\n",
      "Ep 1 (Step 000100): Train loss 5.687, Val loss 5.754\n",
      "Ep 1 (Step 000110): Train loss 5.589, Val loss 5.685\n",
      "Ep 1 (Step 000120): Train loss 5.467, Val loss 5.620\n",
      "Ep 1 (Step 000130): Train loss 5.490, Val loss 5.576\n",
      "Ep 1 (Step 000140): Train loss 5.451, Val loss 5.529\n",
      "Ep 1 (Step 000150): Train loss 5.423, Val loss 5.506\n",
      "Ep 1 (Step 000160): Train loss 5.380, Val loss 5.481\n",
      "Ep 1 (Step 000170): Train loss 5.316, Val loss 5.465\n",
      "Ep 1 (Step 000180): Train loss 5.311, Val loss 5.449\n",
      "Ep 1 (Step 000190): Train loss 5.317, Val loss 5.410\n",
      "Ep 1 (Step 000200): Train loss 5.305, Val loss 5.387\n",
      "Ep 1 (Step 000210): Train loss 5.271, Val loss 5.365\n",
      "Ep 1 (Step 000220): Train loss 5.239, Val loss 5.358\n",
      "Ep 1 (Step 000230): Train loss 5.246, Val loss 5.343\n",
      "Ep 1 (Step 000240): Train loss 5.151, Val loss 5.330\n",
      "Ep 1 (Step 000250): Train loss 5.119, Val loss 5.317\n",
      "Ep 1 (Step 000260): Train loss 5.202, Val loss 5.296\n",
      "Ep 1 (Step 000270): Train loss 5.132, Val loss 5.276\n",
      "Ep 1 (Step 000280): Train loss 5.132, Val loss 5.271\n",
      "Ep 1 (Step 000290): Train loss 5.170, Val loss 5.269\n",
      "Ep 1 (Step 000300): Train loss 5.068, Val loss 5.257\n",
      "Ep 1 (Step 000310): Train loss 5.088, Val loss 5.236\n",
      "Ep 1 (Step 000320): Train loss 5.146, Val loss 5.239\n",
      "Ep 1 (Step 000330): Train loss 5.074, Val loss 5.214\n",
      "Ep 1 (Step 000340): Train loss 5.015, Val loss 5.217\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2172\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.951, Val loss 8.901\n",
      "Ep 1 (Step 000010): Train loss 6.932, Val loss 6.824\n",
      "Ep 1 (Step 000020): Train loss 6.467, Val loss 6.477\n",
      "Ep 1 (Step 000030): Train loss 6.397, Val loss 6.457\n",
      "Ep 1 (Step 000040): Train loss 6.327, Val loss 6.361\n",
      "Ep 1 (Step 000050): Train loss 6.159, Val loss 6.177\n",
      "Ep 1 (Step 000060): Train loss 6.019, Val loss 6.035\n",
      "Ep 1 (Step 000070): Train loss 5.906, Val loss 5.936\n",
      "Ep 1 (Step 000080): Train loss 5.825, Val loss 5.846\n",
      "Ep 1 (Step 000090): Train loss 5.647, Val loss 5.758\n",
      "Ep 1 (Step 000100): Train loss 5.605, Val loss 5.700\n",
      "Ep 1 (Step 000110): Train loss 5.658, Val loss 5.663\n",
      "Ep 1 (Step 000120): Train loss 5.496, Val loss 5.602\n",
      "Ep 1 (Step 000130): Train loss 5.491, Val loss 5.563\n",
      "Ep 1 (Step 000140): Train loss 5.483, Val loss 5.540\n",
      "Ep 1 (Step 000150): Train loss 5.414, Val loss 5.501\n",
      "Ep 1 (Step 000160): Train loss 5.349, Val loss 5.463\n",
      "Ep 1 (Step 000170): Train loss 5.292, Val loss 5.441\n",
      "Ep 1 (Step 000180): Train loss 5.363, Val loss 5.411\n",
      "Ep 1 (Step 000190): Train loss 5.326, Val loss 5.404\n",
      "Ep 1 (Step 000200): Train loss 5.294, Val loss 5.391\n",
      "Ep 1 (Step 000210): Train loss 5.248, Val loss 5.382\n",
      "Ep 1 (Step 000220): Train loss 5.331, Val loss 5.356\n",
      "Ep 1 (Step 000230): Train loss 5.241, Val loss 5.334\n",
      "Ep 1 (Step 000240): Train loss 5.261, Val loss 5.335\n",
      "Ep 1 (Step 000250): Train loss 5.141, Val loss 5.299\n",
      "Ep 1 (Step 000260): Train loss 5.133, Val loss 5.296\n",
      "Ep 1 (Step 000270): Train loss 5.105, Val loss 5.280\n",
      "Ep 1 (Step 000280): Train loss 5.158, Val loss 5.285\n",
      "Ep 1 (Step 000290): Train loss 5.134, Val loss 5.267\n",
      "Ep 1 (Step 000300): Train loss 5.171, Val loss 5.270\n",
      "Ep 1 (Step 000310): Train loss 5.220, Val loss 5.245\n",
      "Ep 1 (Step 000320): Train loss 5.008, Val loss 5.228\n",
      "Ep 1 (Step 000330): Train loss 5.021, Val loss 5.212\n",
      "Ep 1 (Step 000340): Train loss 5.019, Val loss 5.203\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2027\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.940, Val loss 8.906\n",
      "Ep 1 (Step 000010): Train loss 6.936, Val loss 6.864\n",
      "Ep 1 (Step 000020): Train loss 6.510, Val loss 6.447\n",
      "Ep 1 (Step 000030): Train loss 6.453, Val loss 6.439\n",
      "Ep 1 (Step 000040): Train loss 6.285, Val loss 6.321\n",
      "Ep 1 (Step 000050): Train loss 6.141, Val loss 6.109\n",
      "Ep 1 (Step 000060): Train loss 6.006, Val loss 6.004\n",
      "Ep 1 (Step 000070): Train loss 5.871, Val loss 5.893\n",
      "Ep 1 (Step 000080): Train loss 5.882, Val loss 5.824\n",
      "Ep 1 (Step 000090): Train loss 5.812, Val loss 5.751\n",
      "Ep 1 (Step 000100): Train loss 5.664, Val loss 5.687\n",
      "Ep 1 (Step 000110): Train loss 5.602, Val loss 5.660\n",
      "Ep 1 (Step 000120): Train loss 5.593, Val loss 5.628\n",
      "Ep 1 (Step 000130): Train loss 5.500, Val loss 5.610\n",
      "Ep 1 (Step 000140): Train loss 5.437, Val loss 5.524\n",
      "Ep 1 (Step 000150): Train loss 5.454, Val loss 5.506\n",
      "Ep 1 (Step 000160): Train loss 5.349, Val loss 5.472\n",
      "Ep 1 (Step 000170): Train loss 5.266, Val loss 5.439\n",
      "Ep 1 (Step 000180): Train loss 5.330, Val loss 5.417\n",
      "Ep 1 (Step 000190): Train loss 5.285, Val loss 5.405\n",
      "Ep 1 (Step 000200): Train loss 5.259, Val loss 5.380\n",
      "Ep 1 (Step 000210): Train loss 5.324, Val loss 5.381\n",
      "Ep 1 (Step 000220): Train loss 5.249, Val loss 5.347\n",
      "Ep 1 (Step 000230): Train loss 5.275, Val loss 5.350\n",
      "Ep 1 (Step 000240): Train loss 5.182, Val loss 5.323\n",
      "Ep 1 (Step 000250): Train loss 5.151, Val loss 5.306\n",
      "Ep 1 (Step 000260): Train loss 5.116, Val loss 5.294\n",
      "Ep 1 (Step 000270): Train loss 5.201, Val loss 5.286\n",
      "Ep 1 (Step 000280): Train loss 5.115, Val loss 5.289\n",
      "Ep 1 (Step 000290): Train loss 5.168, Val loss 5.260\n",
      "Ep 1 (Step 000300): Train loss 5.109, Val loss 5.252\n",
      "Ep 1 (Step 000310): Train loss 5.036, Val loss 5.236\n",
      "Ep 1 (Step 000320): Train loss 5.056, Val loss 5.218\n",
      "Ep 1 (Step 000330): Train loss 4.932, Val loss 5.207\n",
      "Ep 1 (Step 000340): Train loss 5.090, Val loss 5.213\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2133\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.149, Val loss 9.146\n",
      "Ep 1 (Step 000010): Train loss 7.560, Val loss 7.513\n",
      "Ep 1 (Step 000020): Train loss 6.876, Val loss 6.830\n",
      "Ep 1 (Step 000030): Train loss 6.511, Val loss 6.472\n",
      "Ep 1 (Step 000040): Train loss 6.375, Val loss 6.376\n",
      "Ep 1 (Step 000050): Train loss 6.386, Val loss 6.363\n",
      "Ep 1 (Step 000060): Train loss 6.256, Val loss 6.278\n",
      "Ep 1 (Step 000070): Train loss 6.209, Val loss 6.157\n",
      "Ep 1 (Step 000080): Train loss 6.018, Val loss 6.097\n",
      "Ep 1 (Step 000090): Train loss 5.916, Val loss 6.004\n",
      "Ep 1 (Step 000100): Train loss 5.937, Val loss 5.941\n",
      "Ep 1 (Step 000110): Train loss 5.867, Val loss 5.890\n",
      "Ep 1 (Step 000120): Train loss 5.773, Val loss 5.849\n",
      "Ep 1 (Step 000130): Train loss 5.676, Val loss 5.793\n",
      "Ep 1 (Step 000140): Train loss 5.694, Val loss 5.749\n",
      "Ep 1 (Step 000150): Train loss 5.600, Val loss 5.701\n",
      "Ep 1 (Step 000160): Train loss 5.666, Val loss 5.685\n",
      "Ep 1 (Step 000170): Train loss 5.601, Val loss 5.655\n",
      "Ep 1 (Step 000180): Train loss 5.497, Val loss 5.616\n",
      "Ep 1 (Step 000190): Train loss 5.577, Val loss 5.578\n",
      "Ep 1 (Step 000200): Train loss 5.583, Val loss 5.564\n",
      "Ep 1 (Step 000210): Train loss 5.494, Val loss 5.536\n",
      "Ep 1 (Step 000220): Train loss 5.426, Val loss 5.525\n",
      "Ep 1 (Step 000230): Train loss 5.384, Val loss 5.494\n",
      "Ep 1 (Step 000240): Train loss 5.407, Val loss 5.486\n",
      "Ep 1 (Step 000250): Train loss 5.399, Val loss 5.474\n",
      "Ep 1 (Step 000260): Train loss 5.402, Val loss 5.460\n",
      "Ep 1 (Step 000270): Train loss 5.316, Val loss 5.442\n",
      "Ep 1 (Step 000280): Train loss 5.299, Val loss 5.444\n",
      "Ep 1 (Step 000290): Train loss 5.327, Val loss 5.422\n",
      "Ep 1 (Step 000300): Train loss 5.265, Val loss 5.402\n",
      "Ep 1 (Step 000310): Train loss 5.208, Val loss 5.393\n",
      "Ep 1 (Step 000320): Train loss 5.226, Val loss 5.385\n",
      "Ep 1 (Step 000330): Train loss 5.213, Val loss 5.377\n",
      "Ep 1 (Step 000340): Train loss 5.228, Val loss 5.368\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3677\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.091, Val loss 9.077\n",
      "Ep 1 (Step 000010): Train loss 7.449, Val loss 7.445\n",
      "Ep 1 (Step 000020): Train loss 6.839, Val loss 6.788\n",
      "Ep 1 (Step 000030): Train loss 6.512, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.371, Val loss 6.367\n",
      "Ep 1 (Step 000050): Train loss 6.335, Val loss 6.331\n",
      "Ep 1 (Step 000060): Train loss 6.264, Val loss 6.256\n",
      "Ep 1 (Step 000070): Train loss 6.175, Val loss 6.174\n",
      "Ep 1 (Step 000080): Train loss 6.052, Val loss 6.067\n",
      "Ep 1 (Step 000090): Train loss 5.977, Val loss 6.002\n",
      "Ep 1 (Step 000100): Train loss 5.930, Val loss 5.943\n",
      "Ep 1 (Step 000110): Train loss 5.854, Val loss 5.888\n",
      "Ep 1 (Step 000120): Train loss 5.760, Val loss 5.846\n",
      "Ep 1 (Step 000130): Train loss 5.777, Val loss 5.811\n",
      "Ep 1 (Step 000140): Train loss 5.751, Val loss 5.769\n",
      "Ep 1 (Step 000150): Train loss 5.750, Val loss 5.727\n",
      "Ep 1 (Step 000160): Train loss 5.695, Val loss 5.687\n",
      "Ep 1 (Step 000170): Train loss 5.595, Val loss 5.656\n",
      "Ep 1 (Step 000180): Train loss 5.501, Val loss 5.633\n",
      "Ep 1 (Step 000190): Train loss 5.481, Val loss 5.597\n",
      "Ep 1 (Step 000200): Train loss 5.563, Val loss 5.576\n",
      "Ep 1 (Step 000210): Train loss 5.439, Val loss 5.550\n",
      "Ep 1 (Step 000220): Train loss 5.479, Val loss 5.524\n",
      "Ep 1 (Step 000230): Train loss 5.358, Val loss 5.506\n",
      "Ep 1 (Step 000240): Train loss 5.408, Val loss 5.487\n",
      "Ep 1 (Step 000250): Train loss 5.410, Val loss 5.474\n",
      "Ep 1 (Step 000260): Train loss 5.361, Val loss 5.456\n",
      "Ep 1 (Step 000270): Train loss 5.377, Val loss 5.442\n",
      "Ep 1 (Step 000280): Train loss 5.361, Val loss 5.416\n",
      "Ep 1 (Step 000290): Train loss 5.350, Val loss 5.418\n",
      "Ep 1 (Step 000300): Train loss 5.379, Val loss 5.417\n",
      "Ep 1 (Step 000310): Train loss 5.237, Val loss 5.399\n",
      "Ep 1 (Step 000320): Train loss 5.324, Val loss 5.388\n",
      "Ep 1 (Step 000330): Train loss 5.226, Val loss 5.374\n",
      "Ep 1 (Step 000340): Train loss 5.228, Val loss 5.368\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3676\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.162, Val loss 9.154\n",
      "Ep 1 (Step 000010): Train loss 7.538, Val loss 7.483\n",
      "Ep 1 (Step 000020): Train loss 6.850, Val loss 6.830\n",
      "Ep 1 (Step 000030): Train loss 6.522, Val loss 6.502\n",
      "Ep 1 (Step 000040): Train loss 6.409, Val loss 6.393\n",
      "Ep 1 (Step 000050): Train loss 6.321, Val loss 6.353\n",
      "Ep 1 (Step 000060): Train loss 6.345, Val loss 6.299\n",
      "Ep 1 (Step 000070): Train loss 6.166, Val loss 6.201\n",
      "Ep 1 (Step 000080): Train loss 6.101, Val loss 6.103\n",
      "Ep 1 (Step 000090): Train loss 6.001, Val loss 6.036\n",
      "Ep 1 (Step 000100): Train loss 5.971, Val loss 5.965\n",
      "Ep 1 (Step 000110): Train loss 5.851, Val loss 5.903\n",
      "Ep 1 (Step 000120): Train loss 5.817, Val loss 5.871\n",
      "Ep 1 (Step 000130): Train loss 5.710, Val loss 5.829\n",
      "Ep 1 (Step 000140): Train loss 5.731, Val loss 5.782\n",
      "Ep 1 (Step 000150): Train loss 5.633, Val loss 5.739\n",
      "Ep 1 (Step 000160): Train loss 5.594, Val loss 5.699\n",
      "Ep 1 (Step 000170): Train loss 5.636, Val loss 5.672\n",
      "Ep 1 (Step 000180): Train loss 5.557, Val loss 5.635\n",
      "Ep 1 (Step 000190): Train loss 5.494, Val loss 5.610\n",
      "Ep 1 (Step 000200): Train loss 5.531, Val loss 5.577\n",
      "Ep 1 (Step 000210): Train loss 5.412, Val loss 5.559\n",
      "Ep 1 (Step 000220): Train loss 5.391, Val loss 5.548\n",
      "Ep 1 (Step 000230): Train loss 5.460, Val loss 5.522\n",
      "Ep 1 (Step 000240): Train loss 5.383, Val loss 5.495\n",
      "Ep 1 (Step 000250): Train loss 5.373, Val loss 5.475\n",
      "Ep 1 (Step 000260): Train loss 5.380, Val loss 5.466\n",
      "Ep 1 (Step 000270): Train loss 5.287, Val loss 5.447\n",
      "Ep 1 (Step 000280): Train loss 5.289, Val loss 5.434\n",
      "Ep 1 (Step 000290): Train loss 5.323, Val loss 5.413\n",
      "Ep 1 (Step 000300): Train loss 5.306, Val loss 5.401\n",
      "Ep 1 (Step 000310): Train loss 5.302, Val loss 5.395\n",
      "Ep 1 (Step 000320): Train loss 5.283, Val loss 5.384\n",
      "Ep 1 (Step 000330): Train loss 5.258, Val loss 5.368\n",
      "Ep 1 (Step 000340): Train loss 5.287, Val loss 5.362\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3622\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.124, Val loss 9.127\n",
      "Ep 1 (Step 000010): Train loss 7.486, Val loss 7.433\n",
      "Ep 1 (Step 000020): Train loss 6.786, Val loss 6.780\n",
      "Ep 1 (Step 000030): Train loss 6.520, Val loss 6.472\n",
      "Ep 1 (Step 000040): Train loss 6.433, Val loss 6.389\n",
      "Ep 1 (Step 000050): Train loss 6.386, Val loss 6.362\n",
      "Ep 1 (Step 000060): Train loss 6.298, Val loss 6.315\n",
      "Ep 1 (Step 000070): Train loss 6.265, Val loss 6.240\n",
      "Ep 1 (Step 000080): Train loss 6.146, Val loss 6.133\n",
      "Ep 1 (Step 000090): Train loss 5.981, Val loss 6.034\n",
      "Ep 1 (Step 000100): Train loss 5.893, Val loss 5.976\n",
      "Ep 1 (Step 000110): Train loss 5.970, Val loss 5.909\n",
      "Ep 1 (Step 000120): Train loss 5.787, Val loss 5.854\n",
      "Ep 1 (Step 000130): Train loss 5.766, Val loss 5.823\n",
      "Ep 1 (Step 000140): Train loss 5.745, Val loss 5.766\n",
      "Ep 1 (Step 000150): Train loss 5.705, Val loss 5.728\n",
      "Ep 1 (Step 000160): Train loss 5.652, Val loss 5.701\n",
      "Ep 1 (Step 000170): Train loss 5.549, Val loss 5.665\n",
      "Ep 1 (Step 000180): Train loss 5.565, Val loss 5.623\n",
      "Ep 1 (Step 000190): Train loss 5.540, Val loss 5.609\n",
      "Ep 1 (Step 000200): Train loss 5.506, Val loss 5.595\n",
      "Ep 1 (Step 000210): Train loss 5.532, Val loss 5.565\n",
      "Ep 1 (Step 000220): Train loss 5.464, Val loss 5.548\n",
      "Ep 1 (Step 000230): Train loss 5.397, Val loss 5.512\n",
      "Ep 1 (Step 000240): Train loss 5.365, Val loss 5.491\n",
      "Ep 1 (Step 000250): Train loss 5.365, Val loss 5.484\n",
      "Ep 1 (Step 000260): Train loss 5.343, Val loss 5.465\n",
      "Ep 1 (Step 000270): Train loss 5.280, Val loss 5.435\n",
      "Ep 1 (Step 000280): Train loss 5.356, Val loss 5.415\n",
      "Ep 1 (Step 000290): Train loss 5.327, Val loss 5.408\n",
      "Ep 1 (Step 000300): Train loss 5.220, Val loss 5.384\n",
      "Ep 1 (Step 000310): Train loss 5.332, Val loss 5.370\n",
      "Ep 1 (Step 000320): Train loss 5.248, Val loss 5.361\n",
      "Ep 1 (Step 000330): Train loss 5.147, Val loss 5.350\n",
      "Ep 1 (Step 000340): Train loss 5.185, Val loss 5.338\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3383\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.150, Val loss 9.135\n",
      "Ep 1 (Step 000010): Train loss 7.440, Val loss 7.447\n",
      "Ep 1 (Step 000020): Train loss 6.824, Val loss 6.791\n",
      "Ep 1 (Step 000030): Train loss 6.511, Val loss 6.480\n",
      "Ep 1 (Step 000040): Train loss 6.466, Val loss 6.399\n",
      "Ep 1 (Step 000050): Train loss 6.400, Val loss 6.367\n",
      "Ep 1 (Step 000060): Train loss 6.341, Val loss 6.333\n",
      "Ep 1 (Step 000070): Train loss 6.203, Val loss 6.227\n",
      "Ep 1 (Step 000080): Train loss 6.080, Val loss 6.108\n",
      "Ep 1 (Step 000090): Train loss 5.990, Val loss 6.032\n",
      "Ep 1 (Step 000100): Train loss 5.944, Val loss 5.967\n",
      "Ep 1 (Step 000110): Train loss 5.866, Val loss 5.909\n",
      "Ep 1 (Step 000120): Train loss 5.859, Val loss 5.861\n",
      "Ep 1 (Step 000130): Train loss 5.732, Val loss 5.796\n",
      "Ep 1 (Step 000140): Train loss 5.797, Val loss 5.756\n",
      "Ep 1 (Step 000150): Train loss 5.673, Val loss 5.725\n",
      "Ep 1 (Step 000160): Train loss 5.648, Val loss 5.696\n",
      "Ep 1 (Step 000170): Train loss 5.624, Val loss 5.659\n",
      "Ep 1 (Step 000180): Train loss 5.590, Val loss 5.628\n",
      "Ep 1 (Step 000190): Train loss 5.552, Val loss 5.608\n",
      "Ep 1 (Step 000200): Train loss 5.542, Val loss 5.581\n",
      "Ep 1 (Step 000210): Train loss 5.456, Val loss 5.552\n",
      "Ep 1 (Step 000220): Train loss 5.431, Val loss 5.532\n",
      "Ep 1 (Step 000230): Train loss 5.383, Val loss 5.509\n",
      "Ep 1 (Step 000240): Train loss 5.462, Val loss 5.491\n",
      "Ep 1 (Step 000250): Train loss 5.299, Val loss 5.472\n",
      "Ep 1 (Step 000260): Train loss 5.440, Val loss 5.455\n",
      "Ep 1 (Step 000270): Train loss 5.350, Val loss 5.440\n",
      "Ep 1 (Step 000280): Train loss 5.317, Val loss 5.426\n",
      "Ep 1 (Step 000290): Train loss 5.309, Val loss 5.415\n",
      "Ep 1 (Step 000300): Train loss 5.237, Val loss 5.403\n",
      "Ep 1 (Step 000310): Train loss 5.297, Val loss 5.372\n",
      "Ep 1 (Step 000320): Train loss 5.205, Val loss 5.368\n",
      "Ep 1 (Step 000330): Train loss 5.242, Val loss 5.353\n",
      "Ep 1 (Step 000340): Train loss 5.201, Val loss 5.355\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3549\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.141, Val loss 9.148\n",
      "Ep 1 (Step 000010): Train loss 7.490, Val loss 7.434\n",
      "Ep 1 (Step 000020): Train loss 6.822, Val loss 6.774\n",
      "Ep 1 (Step 000030): Train loss 6.473, Val loss 6.479\n",
      "Ep 1 (Step 000040): Train loss 6.417, Val loss 6.407\n",
      "Ep 1 (Step 000050): Train loss 6.393, Val loss 6.353\n",
      "Ep 1 (Step 000060): Train loss 6.281, Val loss 6.300\n",
      "Ep 1 (Step 000070): Train loss 6.242, Val loss 6.204\n",
      "Ep 1 (Step 000080): Train loss 6.040, Val loss 6.084\n",
      "Ep 1 (Step 000090): Train loss 5.988, Val loss 6.005\n",
      "Ep 1 (Step 000100): Train loss 5.864, Val loss 5.945\n",
      "Ep 1 (Step 000110): Train loss 5.851, Val loss 5.895\n",
      "Ep 1 (Step 000120): Train loss 5.772, Val loss 5.832\n",
      "Ep 1 (Step 000130): Train loss 5.778, Val loss 5.786\n",
      "Ep 1 (Step 000140): Train loss 5.716, Val loss 5.754\n",
      "Ep 1 (Step 000150): Train loss 5.682, Val loss 5.704\n",
      "Ep 1 (Step 000160): Train loss 5.635, Val loss 5.686\n",
      "Ep 1 (Step 000170): Train loss 5.639, Val loss 5.646\n",
      "Ep 1 (Step 000180): Train loss 5.564, Val loss 5.624\n",
      "Ep 1 (Step 000190): Train loss 5.509, Val loss 5.582\n",
      "Ep 1 (Step 000200): Train loss 5.533, Val loss 5.570\n",
      "Ep 1 (Step 000210): Train loss 5.497, Val loss 5.545\n",
      "Ep 1 (Step 000220): Train loss 5.434, Val loss 5.524\n",
      "Ep 1 (Step 000230): Train loss 5.446, Val loss 5.493\n",
      "Ep 1 (Step 000240): Train loss 5.435, Val loss 5.478\n",
      "Ep 1 (Step 000250): Train loss 5.391, Val loss 5.465\n",
      "Ep 1 (Step 000260): Train loss 5.356, Val loss 5.443\n",
      "Ep 1 (Step 000270): Train loss 5.397, Val loss 5.420\n",
      "Ep 1 (Step 000280): Train loss 5.378, Val loss 5.395\n",
      "Ep 1 (Step 000290): Train loss 5.259, Val loss 5.386\n",
      "Ep 1 (Step 000300): Train loss 5.292, Val loss 5.380\n",
      "Ep 1 (Step 000310): Train loss 5.303, Val loss 5.362\n",
      "Ep 1 (Step 000320): Train loss 5.279, Val loss 5.353\n",
      "Ep 1 (Step 000330): Train loss 5.219, Val loss 5.340\n",
      "Ep 1 (Step 000340): Train loss 5.202, Val loss 5.343\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3432\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.931, Val loss 8.913\n",
      "Ep 1 (Step 000010): Train loss 6.926, Val loss 6.853\n",
      "Ep 1 (Step 000020): Train loss 6.514, Val loss 6.449\n",
      "Ep 1 (Step 000030): Train loss 6.524, Val loss 6.434\n",
      "Ep 1 (Step 000040): Train loss 6.328, Val loss 6.314\n",
      "Ep 1 (Step 000050): Train loss 6.118, Val loss 6.153\n",
      "Ep 1 (Step 000060): Train loss 5.989, Val loss 6.048\n",
      "Ep 1 (Step 000070): Train loss 5.886, Val loss 5.928\n",
      "Ep 1 (Step 000080): Train loss 5.750, Val loss 5.840\n",
      "Ep 1 (Step 000090): Train loss 5.765, Val loss 5.778\n",
      "Ep 1 (Step 000100): Train loss 5.685, Val loss 5.730\n",
      "Ep 1 (Step 000110): Train loss 5.616, Val loss 5.658\n",
      "Ep 1 (Step 000120): Train loss 5.478, Val loss 5.608\n",
      "Ep 1 (Step 000130): Train loss 5.520, Val loss 5.582\n",
      "Ep 1 (Step 000140): Train loss 5.482, Val loss 5.567\n",
      "Ep 1 (Step 000150): Train loss 5.431, Val loss 5.533\n",
      "Ep 1 (Step 000160): Train loss 5.382, Val loss 5.499\n",
      "Ep 1 (Step 000170): Train loss 5.354, Val loss 5.461\n",
      "Ep 1 (Step 000180): Train loss 5.380, Val loss 5.434\n",
      "Ep 1 (Step 000190): Train loss 5.336, Val loss 5.425\n",
      "Ep 1 (Step 000200): Train loss 5.321, Val loss 5.420\n",
      "Ep 1 (Step 000210): Train loss 5.275, Val loss 5.392\n",
      "Ep 1 (Step 000220): Train loss 5.307, Val loss 5.380\n",
      "Ep 1 (Step 000230): Train loss 5.243, Val loss 5.372\n",
      "Ep 1 (Step 000240): Train loss 5.253, Val loss 5.359\n",
      "Ep 1 (Step 000250): Train loss 5.293, Val loss 5.343\n",
      "Ep 1 (Step 000260): Train loss 5.210, Val loss 5.309\n",
      "Ep 1 (Step 000270): Train loss 5.144, Val loss 5.306\n",
      "Ep 1 (Step 000280): Train loss 5.218, Val loss 5.283\n",
      "Ep 1 (Step 000290): Train loss 5.065, Val loss 5.262\n",
      "Ep 1 (Step 000300): Train loss 5.128, Val loss 5.247\n",
      "Ep 1 (Step 000310): Train loss 5.081, Val loss 5.247\n",
      "Ep 1 (Step 000320): Train loss 5.133, Val loss 5.231\n",
      "Ep 1 (Step 000330): Train loss 5.065, Val loss 5.235\n",
      "Ep 1 (Step 000340): Train loss 4.980, Val loss 5.224\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2241\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.910, Val loss 8.882\n",
      "Ep 1 (Step 000010): Train loss 6.896, Val loss 6.852\n",
      "Ep 1 (Step 000020): Train loss 6.431, Val loss 6.461\n",
      "Ep 1 (Step 000030): Train loss 6.423, Val loss 6.449\n",
      "Ep 1 (Step 000040): Train loss 6.288, Val loss 6.337\n",
      "Ep 1 (Step 000050): Train loss 6.230, Val loss 6.216\n",
      "Ep 1 (Step 000060): Train loss 6.080, Val loss 6.058\n",
      "Ep 1 (Step 000070): Train loss 5.896, Val loss 5.949\n",
      "Ep 1 (Step 000080): Train loss 5.814, Val loss 5.881\n",
      "Ep 1 (Step 000090): Train loss 5.642, Val loss 5.801\n",
      "Ep 1 (Step 000100): Train loss 5.677, Val loss 5.741\n",
      "Ep 1 (Step 000110): Train loss 5.574, Val loss 5.696\n",
      "Ep 1 (Step 000120): Train loss 5.572, Val loss 5.662\n",
      "Ep 1 (Step 000130): Train loss 5.582, Val loss 5.612\n",
      "Ep 1 (Step 000140): Train loss 5.445, Val loss 5.571\n",
      "Ep 1 (Step 000150): Train loss 5.459, Val loss 5.513\n",
      "Ep 1 (Step 000160): Train loss 5.346, Val loss 5.485\n",
      "Ep 1 (Step 000170): Train loss 5.395, Val loss 5.484\n",
      "Ep 1 (Step 000180): Train loss 5.351, Val loss 5.476\n",
      "Ep 1 (Step 000190): Train loss 5.349, Val loss 5.446\n",
      "Ep 1 (Step 000200): Train loss 5.278, Val loss 5.421\n",
      "Ep 1 (Step 000210): Train loss 5.312, Val loss 5.400\n",
      "Ep 1 (Step 000220): Train loss 5.310, Val loss 5.391\n",
      "Ep 1 (Step 000230): Train loss 5.271, Val loss 5.364\n",
      "Ep 1 (Step 000240): Train loss 5.156, Val loss 5.344\n",
      "Ep 1 (Step 000250): Train loss 5.170, Val loss 5.334\n",
      "Ep 1 (Step 000260): Train loss 5.216, Val loss 5.326\n",
      "Ep 1 (Step 000270): Train loss 5.164, Val loss 5.305\n",
      "Ep 1 (Step 000280): Train loss 5.143, Val loss 5.282\n",
      "Ep 1 (Step 000290): Train loss 5.079, Val loss 5.270\n",
      "Ep 1 (Step 000300): Train loss 5.115, Val loss 5.271\n",
      "Ep 1 (Step 000310): Train loss 5.070, Val loss 5.270\n",
      "Ep 1 (Step 000320): Train loss 5.071, Val loss 5.258\n",
      "Ep 1 (Step 000330): Train loss 5.197, Val loss 5.237\n",
      "Ep 1 (Step 000340): Train loss 5.070, Val loss 5.239\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2390\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.869, Val loss 8.876\n",
      "Ep 1 (Step 000010): Train loss 6.948, Val loss 6.884\n",
      "Ep 1 (Step 000020): Train loss 6.524, Val loss 6.495\n",
      "Ep 1 (Step 000030): Train loss 6.438, Val loss 6.461\n",
      "Ep 1 (Step 000040): Train loss 6.322, Val loss 6.348\n",
      "Ep 1 (Step 000050): Train loss 6.187, Val loss 6.187\n",
      "Ep 1 (Step 000060): Train loss 6.025, Val loss 6.057\n",
      "Ep 1 (Step 000070): Train loss 5.917, Val loss 5.956\n",
      "Ep 1 (Step 000080): Train loss 5.880, Val loss 5.882\n",
      "Ep 1 (Step 000090): Train loss 5.757, Val loss 5.807\n",
      "Ep 1 (Step 000100): Train loss 5.664, Val loss 5.725\n",
      "Ep 1 (Step 000110): Train loss 5.507, Val loss 5.678\n",
      "Ep 1 (Step 000120): Train loss 5.433, Val loss 5.634\n",
      "Ep 1 (Step 000130): Train loss 5.533, Val loss 5.591\n",
      "Ep 1 (Step 000140): Train loss 5.506, Val loss 5.558\n",
      "Ep 1 (Step 000150): Train loss 5.375, Val loss 5.534\n",
      "Ep 1 (Step 000160): Train loss 5.479, Val loss 5.492\n",
      "Ep 1 (Step 000170): Train loss 5.409, Val loss 5.469\n",
      "Ep 1 (Step 000180): Train loss 5.394, Val loss 5.455\n",
      "Ep 1 (Step 000190): Train loss 5.342, Val loss 5.431\n",
      "Ep 1 (Step 000200): Train loss 5.177, Val loss 5.397\n",
      "Ep 1 (Step 000210): Train loss 5.361, Val loss 5.385\n",
      "Ep 1 (Step 000220): Train loss 5.254, Val loss 5.370\n",
      "Ep 1 (Step 000230): Train loss 5.245, Val loss 5.356\n",
      "Ep 1 (Step 000240): Train loss 5.195, Val loss 5.337\n",
      "Ep 1 (Step 000250): Train loss 5.138, Val loss 5.345\n",
      "Ep 1 (Step 000260): Train loss 5.249, Val loss 5.333\n",
      "Ep 1 (Step 000270): Train loss 5.183, Val loss 5.304\n",
      "Ep 1 (Step 000280): Train loss 5.111, Val loss 5.282\n",
      "Ep 1 (Step 000290): Train loss 5.079, Val loss 5.277\n",
      "Ep 1 (Step 000300): Train loss 5.151, Val loss 5.268\n",
      "Ep 1 (Step 000310): Train loss 5.142, Val loss 5.262\n",
      "Ep 1 (Step 000320): Train loss 5.204, Val loss 5.262\n",
      "Ep 1 (Step 000330): Train loss 5.056, Val loss 5.252\n",
      "Ep 1 (Step 000340): Train loss 5.032, Val loss 5.243\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2428\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.948, Val loss 8.930\n",
      "Ep 1 (Step 000010): Train loss 6.814, Val loss 6.823\n",
      "Ep 1 (Step 000020): Train loss 6.516, Val loss 6.460\n",
      "Ep 1 (Step 000030): Train loss 6.429, Val loss 6.441\n",
      "Ep 1 (Step 000040): Train loss 6.388, Val loss 6.335\n",
      "Ep 1 (Step 000050): Train loss 6.138, Val loss 6.178\n",
      "Ep 1 (Step 000060): Train loss 5.996, Val loss 6.032\n",
      "Ep 1 (Step 000070): Train loss 5.866, Val loss 5.930\n",
      "Ep 1 (Step 000080): Train loss 5.804, Val loss 5.841\n",
      "Ep 1 (Step 000090): Train loss 5.657, Val loss 5.782\n",
      "Ep 1 (Step 000100): Train loss 5.609, Val loss 5.719\n",
      "Ep 1 (Step 000110): Train loss 5.604, Val loss 5.646\n",
      "Ep 1 (Step 000120): Train loss 5.551, Val loss 5.595\n",
      "Ep 1 (Step 000130): Train loss 5.556, Val loss 5.561\n",
      "Ep 1 (Step 000140): Train loss 5.482, Val loss 5.543\n",
      "Ep 1 (Step 000150): Train loss 5.415, Val loss 5.500\n",
      "Ep 1 (Step 000160): Train loss 5.419, Val loss 5.476\n",
      "Ep 1 (Step 000170): Train loss 5.392, Val loss 5.450\n",
      "Ep 1 (Step 000180): Train loss 5.366, Val loss 5.417\n",
      "Ep 1 (Step 000190): Train loss 5.298, Val loss 5.383\n",
      "Ep 1 (Step 000200): Train loss 5.261, Val loss 5.364\n",
      "Ep 1 (Step 000210): Train loss 5.192, Val loss 5.367\n",
      "Ep 1 (Step 000220): Train loss 5.222, Val loss 5.351\n",
      "Ep 1 (Step 000230): Train loss 5.171, Val loss 5.317\n",
      "Ep 1 (Step 000240): Train loss 5.274, Val loss 5.313\n",
      "Ep 1 (Step 000250): Train loss 5.202, Val loss 5.308\n",
      "Ep 1 (Step 000260): Train loss 5.147, Val loss 5.293\n",
      "Ep 1 (Step 000270): Train loss 5.067, Val loss 5.290\n",
      "Ep 1 (Step 000280): Train loss 5.126, Val loss 5.265\n",
      "Ep 1 (Step 000290): Train loss 5.034, Val loss 5.269\n",
      "Ep 1 (Step 000300): Train loss 5.031, Val loss 5.254\n",
      "Ep 1 (Step 000310): Train loss 5.127, Val loss 5.253\n",
      "Ep 1 (Step 000320): Train loss 5.021, Val loss 5.227\n",
      "Ep 1 (Step 000330): Train loss 5.051, Val loss 5.220\n",
      "Ep 1 (Step 000340): Train loss 5.085, Val loss 5.200\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2004\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.882, Val loss 8.873\n",
      "Ep 1 (Step 000010): Train loss 6.835, Val loss 6.836\n",
      "Ep 1 (Step 000020): Train loss 6.504, Val loss 6.474\n",
      "Ep 1 (Step 000030): Train loss 6.425, Val loss 6.459\n",
      "Ep 1 (Step 000040): Train loss 6.333, Val loss 6.316\n",
      "Ep 1 (Step 000050): Train loss 6.172, Val loss 6.163\n",
      "Ep 1 (Step 000060): Train loss 6.054, Val loss 6.091\n",
      "Ep 1 (Step 000070): Train loss 5.909, Val loss 5.947\n",
      "Ep 1 (Step 000080): Train loss 5.786, Val loss 5.856\n",
      "Ep 1 (Step 000090): Train loss 5.791, Val loss 5.789\n",
      "Ep 1 (Step 000100): Train loss 5.722, Val loss 5.715\n",
      "Ep 1 (Step 000110): Train loss 5.623, Val loss 5.655\n",
      "Ep 1 (Step 000120): Train loss 5.540, Val loss 5.640\n",
      "Ep 1 (Step 000130): Train loss 5.516, Val loss 5.571\n",
      "Ep 1 (Step 000140): Train loss 5.382, Val loss 5.537\n",
      "Ep 1 (Step 000150): Train loss 5.492, Val loss 5.504\n",
      "Ep 1 (Step 000160): Train loss 5.405, Val loss 5.481\n",
      "Ep 1 (Step 000170): Train loss 5.396, Val loss 5.439\n",
      "Ep 1 (Step 000180): Train loss 5.324, Val loss 5.423\n",
      "Ep 1 (Step 000190): Train loss 5.225, Val loss 5.396\n",
      "Ep 1 (Step 000200): Train loss 5.268, Val loss 5.366\n",
      "Ep 1 (Step 000210): Train loss 5.309, Val loss 5.345\n",
      "Ep 1 (Step 000220): Train loss 5.239, Val loss 5.337\n",
      "Ep 1 (Step 000230): Train loss 5.232, Val loss 5.321\n",
      "Ep 1 (Step 000240): Train loss 5.148, Val loss 5.299\n",
      "Ep 1 (Step 000250): Train loss 5.149, Val loss 5.294\n",
      "Ep 1 (Step 000260): Train loss 5.109, Val loss 5.284\n",
      "Ep 1 (Step 000270): Train loss 5.159, Val loss 5.270\n",
      "Ep 1 (Step 000280): Train loss 5.252, Val loss 5.256\n",
      "Ep 1 (Step 000290): Train loss 5.092, Val loss 5.253\n",
      "Ep 1 (Step 000300): Train loss 5.075, Val loss 5.243\n",
      "Ep 1 (Step 000310): Train loss 5.025, Val loss 5.232\n",
      "Ep 1 (Step 000320): Train loss 5.040, Val loss 5.221\n",
      "Ep 1 (Step 000330): Train loss 4.977, Val loss 5.206\n",
      "Ep 1 (Step 000340): Train loss 5.042, Val loss 5.196\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1965\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.926, Val loss 8.907\n",
      "Ep 1 (Step 000010): Train loss 6.892, Val loss 6.821\n",
      "Ep 1 (Step 000020): Train loss 6.466, Val loss 6.441\n",
      "Ep 1 (Step 000030): Train loss 6.456, Val loss 6.430\n",
      "Ep 1 (Step 000040): Train loss 6.340, Val loss 6.332\n",
      "Ep 1 (Step 000050): Train loss 6.164, Val loss 6.164\n",
      "Ep 1 (Step 000060): Train loss 6.071, Val loss 6.043\n",
      "Ep 1 (Step 000070): Train loss 5.942, Val loss 5.940\n",
      "Ep 1 (Step 000080): Train loss 5.793, Val loss 5.837\n",
      "Ep 1 (Step 000090): Train loss 5.676, Val loss 5.794\n",
      "Ep 1 (Step 000100): Train loss 5.659, Val loss 5.724\n",
      "Ep 1 (Step 000110): Train loss 5.608, Val loss 5.687\n",
      "Ep 1 (Step 000120): Train loss 5.517, Val loss 5.622\n",
      "Ep 1 (Step 000130): Train loss 5.415, Val loss 5.586\n",
      "Ep 1 (Step 000140): Train loss 5.472, Val loss 5.553\n",
      "Ep 1 (Step 000150): Train loss 5.397, Val loss 5.512\n",
      "Ep 1 (Step 000160): Train loss 5.423, Val loss 5.477\n",
      "Ep 1 (Step 000170): Train loss 5.353, Val loss 5.451\n",
      "Ep 1 (Step 000180): Train loss 5.273, Val loss 5.430\n",
      "Ep 1 (Step 000190): Train loss 5.283, Val loss 5.405\n",
      "Ep 1 (Step 000200): Train loss 5.308, Val loss 5.389\n",
      "Ep 1 (Step 000210): Train loss 5.175, Val loss 5.359\n",
      "Ep 1 (Step 000220): Train loss 5.239, Val loss 5.357\n",
      "Ep 1 (Step 000230): Train loss 5.229, Val loss 5.329\n",
      "Ep 1 (Step 000240): Train loss 5.259, Val loss 5.326\n",
      "Ep 1 (Step 000250): Train loss 5.165, Val loss 5.296\n",
      "Ep 1 (Step 000260): Train loss 5.148, Val loss 5.288\n",
      "Ep 1 (Step 000270): Train loss 5.137, Val loss 5.276\n",
      "Ep 1 (Step 000280): Train loss 5.093, Val loss 5.264\n",
      "Ep 1 (Step 000290): Train loss 5.055, Val loss 5.270\n",
      "Ep 1 (Step 000300): Train loss 5.092, Val loss 5.245\n",
      "Ep 1 (Step 000310): Train loss 5.058, Val loss 5.246\n",
      "Ep 1 (Step 000320): Train loss 5.030, Val loss 5.216\n",
      "Ep 1 (Step 000330): Train loss 5.016, Val loss 5.206\n",
      "Ep 1 (Step 000340): Train loss 5.097, Val loss 5.191\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1912\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.072, Val loss 9.058\n",
      "Ep 1 (Step 000010): Train loss 7.473, Val loss 7.392\n",
      "Ep 1 (Step 000020): Train loss 6.866, Val loss 6.761\n",
      "Ep 1 (Step 000030): Train loss 6.521, Val loss 6.459\n",
      "Ep 1 (Step 000040): Train loss 6.425, Val loss 6.390\n",
      "Ep 1 (Step 000050): Train loss 6.347, Val loss 6.355\n",
      "Ep 1 (Step 000060): Train loss 6.339, Val loss 6.299\n",
      "Ep 1 (Step 000070): Train loss 6.170, Val loss 6.189\n",
      "Ep 1 (Step 000080): Train loss 6.124, Val loss 6.096\n",
      "Ep 1 (Step 000090): Train loss 5.995, Val loss 6.040\n",
      "Ep 1 (Step 000100): Train loss 5.944, Val loss 5.974\n",
      "Ep 1 (Step 000110): Train loss 5.845, Val loss 5.931\n",
      "Ep 1 (Step 000120): Train loss 5.800, Val loss 5.867\n",
      "Ep 1 (Step 000130): Train loss 5.813, Val loss 5.829\n",
      "Ep 1 (Step 000140): Train loss 5.696, Val loss 5.790\n",
      "Ep 1 (Step 000150): Train loss 5.653, Val loss 5.753\n",
      "Ep 1 (Step 000160): Train loss 5.587, Val loss 5.711\n",
      "Ep 1 (Step 000170): Train loss 5.619, Val loss 5.684\n",
      "Ep 1 (Step 000180): Train loss 5.557, Val loss 5.655\n",
      "Ep 1 (Step 000190): Train loss 5.575, Val loss 5.623\n",
      "Ep 1 (Step 000200): Train loss 5.509, Val loss 5.612\n",
      "Ep 1 (Step 000210): Train loss 5.479, Val loss 5.564\n",
      "Ep 1 (Step 000220): Train loss 5.465, Val loss 5.563\n",
      "Ep 1 (Step 000230): Train loss 5.426, Val loss 5.534\n",
      "Ep 1 (Step 000240): Train loss 5.455, Val loss 5.510\n",
      "Ep 1 (Step 000250): Train loss 5.348, Val loss 5.501\n",
      "Ep 1 (Step 000260): Train loss 5.380, Val loss 5.479\n",
      "Ep 1 (Step 000270): Train loss 5.452, Val loss 5.458\n",
      "Ep 1 (Step 000280): Train loss 5.413, Val loss 5.450\n",
      "Ep 1 (Step 000290): Train loss 5.400, Val loss 5.423\n",
      "Ep 1 (Step 000300): Train loss 5.253, Val loss 5.415\n",
      "Ep 1 (Step 000310): Train loss 5.310, Val loss 5.402\n",
      "Ep 1 (Step 000320): Train loss 5.236, Val loss 5.398\n",
      "Ep 1 (Step 000330): Train loss 5.282, Val loss 5.382\n",
      "Ep 1 (Step 000340): Train loss 5.319, Val loss 5.367\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3670\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.095, Val loss 9.068\n",
      "Ep 1 (Step 000010): Train loss 7.492, Val loss 7.538\n",
      "Ep 1 (Step 000020): Train loss 6.875, Val loss 6.869\n",
      "Ep 1 (Step 000030): Train loss 6.550, Val loss 6.516\n",
      "Ep 1 (Step 000040): Train loss 6.431, Val loss 6.417\n",
      "Ep 1 (Step 000050): Train loss 6.359, Val loss 6.372\n",
      "Ep 1 (Step 000060): Train loss 6.286, Val loss 6.306\n",
      "Ep 1 (Step 000070): Train loss 6.258, Val loss 6.226\n",
      "Ep 1 (Step 000080): Train loss 6.155, Val loss 6.123\n",
      "Ep 1 (Step 000090): Train loss 6.033, Val loss 6.045\n",
      "Ep 1 (Step 000100): Train loss 5.881, Val loss 5.987\n",
      "Ep 1 (Step 000110): Train loss 5.900, Val loss 5.934\n",
      "Ep 1 (Step 000120): Train loss 5.848, Val loss 5.888\n",
      "Ep 1 (Step 000130): Train loss 5.763, Val loss 5.841\n",
      "Ep 1 (Step 000140): Train loss 5.706, Val loss 5.795\n",
      "Ep 1 (Step 000150): Train loss 5.755, Val loss 5.758\n",
      "Ep 1 (Step 000160): Train loss 5.660, Val loss 5.735\n",
      "Ep 1 (Step 000170): Train loss 5.611, Val loss 5.704\n",
      "Ep 1 (Step 000180): Train loss 5.523, Val loss 5.664\n",
      "Ep 1 (Step 000190): Train loss 5.607, Val loss 5.644\n",
      "Ep 1 (Step 000200): Train loss 5.559, Val loss 5.618\n",
      "Ep 1 (Step 000210): Train loss 5.506, Val loss 5.586\n",
      "Ep 1 (Step 000220): Train loss 5.534, Val loss 5.566\n",
      "Ep 1 (Step 000230): Train loss 5.419, Val loss 5.556\n",
      "Ep 1 (Step 000240): Train loss 5.460, Val loss 5.535\n",
      "Ep 1 (Step 000250): Train loss 5.402, Val loss 5.522\n",
      "Ep 1 (Step 000260): Train loss 5.402, Val loss 5.500\n",
      "Ep 1 (Step 000270): Train loss 5.316, Val loss 5.474\n",
      "Ep 1 (Step 000280): Train loss 5.363, Val loss 5.448\n",
      "Ep 1 (Step 000290): Train loss 5.348, Val loss 5.439\n",
      "Ep 1 (Step 000300): Train loss 5.246, Val loss 5.440\n",
      "Ep 1 (Step 000310): Train loss 5.331, Val loss 5.427\n",
      "Ep 1 (Step 000320): Train loss 5.273, Val loss 5.413\n",
      "Ep 1 (Step 000330): Train loss 5.295, Val loss 5.408\n",
      "Ep 1 (Step 000340): Train loss 5.293, Val loss 5.399\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3994\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.059, Val loss 9.054\n",
      "Ep 1 (Step 000010): Train loss 7.455, Val loss 7.471\n",
      "Ep 1 (Step 000020): Train loss 6.791, Val loss 6.820\n",
      "Ep 1 (Step 000030): Train loss 6.458, Val loss 6.499\n",
      "Ep 1 (Step 000040): Train loss 6.409, Val loss 6.403\n",
      "Ep 1 (Step 000050): Train loss 6.344, Val loss 6.361\n",
      "Ep 1 (Step 000060): Train loss 6.280, Val loss 6.292\n",
      "Ep 1 (Step 000070): Train loss 6.154, Val loss 6.165\n",
      "Ep 1 (Step 000080): Train loss 6.134, Val loss 6.069\n",
      "Ep 1 (Step 000090): Train loss 6.013, Val loss 6.014\n",
      "Ep 1 (Step 000100): Train loss 5.994, Val loss 5.952\n",
      "Ep 1 (Step 000110): Train loss 5.920, Val loss 5.886\n",
      "Ep 1 (Step 000120): Train loss 5.777, Val loss 5.840\n",
      "Ep 1 (Step 000130): Train loss 5.832, Val loss 5.804\n",
      "Ep 1 (Step 000140): Train loss 5.718, Val loss 5.781\n",
      "Ep 1 (Step 000150): Train loss 5.653, Val loss 5.729\n",
      "Ep 1 (Step 000160): Train loss 5.614, Val loss 5.693\n",
      "Ep 1 (Step 000170): Train loss 5.614, Val loss 5.664\n",
      "Ep 1 (Step 000180): Train loss 5.576, Val loss 5.638\n",
      "Ep 1 (Step 000190): Train loss 5.564, Val loss 5.598\n",
      "Ep 1 (Step 000200): Train loss 5.498, Val loss 5.599\n",
      "Ep 1 (Step 000210): Train loss 5.498, Val loss 5.567\n",
      "Ep 1 (Step 000220): Train loss 5.477, Val loss 5.539\n",
      "Ep 1 (Step 000230): Train loss 5.478, Val loss 5.521\n",
      "Ep 1 (Step 000240): Train loss 5.378, Val loss 5.500\n",
      "Ep 1 (Step 000250): Train loss 5.445, Val loss 5.497\n",
      "Ep 1 (Step 000260): Train loss 5.314, Val loss 5.489\n",
      "Ep 1 (Step 000270): Train loss 5.307, Val loss 5.460\n",
      "Ep 1 (Step 000280): Train loss 5.319, Val loss 5.451\n",
      "Ep 1 (Step 000290): Train loss 5.274, Val loss 5.437\n",
      "Ep 1 (Step 000300): Train loss 5.339, Val loss 5.427\n",
      "Ep 1 (Step 000310): Train loss 5.245, Val loss 5.407\n",
      "Ep 1 (Step 000320): Train loss 5.308, Val loss 5.391\n",
      "Ep 1 (Step 000330): Train loss 5.238, Val loss 5.395\n",
      "Ep 1 (Step 000340): Train loss 5.251, Val loss 5.382\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3821\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.113, Val loss 9.082\n",
      "Ep 1 (Step 000010): Train loss 7.530, Val loss 7.462\n",
      "Ep 1 (Step 000020): Train loss 6.847, Val loss 6.809\n",
      "Ep 1 (Step 000030): Train loss 6.516, Val loss 6.485\n",
      "Ep 1 (Step 000040): Train loss 6.360, Val loss 6.392\n",
      "Ep 1 (Step 000050): Train loss 6.368, Val loss 6.371\n",
      "Ep 1 (Step 000060): Train loss 6.330, Val loss 6.331\n",
      "Ep 1 (Step 000070): Train loss 6.238, Val loss 6.233\n",
      "Ep 1 (Step 000080): Train loss 6.119, Val loss 6.117\n",
      "Ep 1 (Step 000090): Train loss 6.030, Val loss 6.030\n",
      "Ep 1 (Step 000100): Train loss 5.953, Val loss 5.962\n",
      "Ep 1 (Step 000110): Train loss 5.868, Val loss 5.897\n",
      "Ep 1 (Step 000120): Train loss 5.903, Val loss 5.859\n",
      "Ep 1 (Step 000130): Train loss 5.732, Val loss 5.812\n",
      "Ep 1 (Step 000140): Train loss 5.763, Val loss 5.766\n",
      "Ep 1 (Step 000150): Train loss 5.709, Val loss 5.728\n",
      "Ep 1 (Step 000160): Train loss 5.617, Val loss 5.680\n",
      "Ep 1 (Step 000170): Train loss 5.701, Val loss 5.649\n",
      "Ep 1 (Step 000180): Train loss 5.630, Val loss 5.632\n",
      "Ep 1 (Step 000190): Train loss 5.478, Val loss 5.602\n",
      "Ep 1 (Step 000200): Train loss 5.550, Val loss 5.574\n",
      "Ep 1 (Step 000210): Train loss 5.546, Val loss 5.546\n",
      "Ep 1 (Step 000220): Train loss 5.435, Val loss 5.536\n",
      "Ep 1 (Step 000230): Train loss 5.477, Val loss 5.513\n",
      "Ep 1 (Step 000240): Train loss 5.406, Val loss 5.486\n",
      "Ep 1 (Step 000250): Train loss 5.376, Val loss 5.466\n",
      "Ep 1 (Step 000260): Train loss 5.315, Val loss 5.458\n",
      "Ep 1 (Step 000270): Train loss 5.392, Val loss 5.431\n",
      "Ep 1 (Step 000280): Train loss 5.307, Val loss 5.430\n",
      "Ep 1 (Step 000290): Train loss 5.377, Val loss 5.415\n",
      "Ep 1 (Step 000300): Train loss 5.378, Val loss 5.389\n",
      "Ep 1 (Step 000310): Train loss 5.213, Val loss 5.394\n",
      "Ep 1 (Step 000320): Train loss 5.240, Val loss 5.372\n",
      "Ep 1 (Step 000330): Train loss 5.289, Val loss 5.372\n",
      "Ep 1 (Step 000340): Train loss 5.251, Val loss 5.371\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3711\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.074, Val loss 9.061\n",
      "Ep 1 (Step 000010): Train loss 7.453, Val loss 7.418\n",
      "Ep 1 (Step 000020): Train loss 6.806, Val loss 6.770\n",
      "Ep 1 (Step 000030): Train loss 6.475, Val loss 6.473\n",
      "Ep 1 (Step 000040): Train loss 6.422, Val loss 6.402\n",
      "Ep 1 (Step 000050): Train loss 6.369, Val loss 6.383\n",
      "Ep 1 (Step 000060): Train loss 6.338, Val loss 6.331\n",
      "Ep 1 (Step 000070): Train loss 6.205, Val loss 6.220\n",
      "Ep 1 (Step 000080): Train loss 6.128, Val loss 6.131\n",
      "Ep 1 (Step 000090): Train loss 6.027, Val loss 6.051\n",
      "Ep 1 (Step 000100): Train loss 5.975, Val loss 5.988\n",
      "Ep 1 (Step 000110): Train loss 5.927, Val loss 5.911\n",
      "Ep 1 (Step 000120): Train loss 5.857, Val loss 5.876\n",
      "Ep 1 (Step 000130): Train loss 5.766, Val loss 5.826\n",
      "Ep 1 (Step 000140): Train loss 5.707, Val loss 5.773\n",
      "Ep 1 (Step 000150): Train loss 5.706, Val loss 5.731\n",
      "Ep 1 (Step 000160): Train loss 5.746, Val loss 5.714\n",
      "Ep 1 (Step 000170): Train loss 5.567, Val loss 5.667\n",
      "Ep 1 (Step 000180): Train loss 5.619, Val loss 5.647\n",
      "Ep 1 (Step 000190): Train loss 5.568, Val loss 5.620\n",
      "Ep 1 (Step 000200): Train loss 5.491, Val loss 5.585\n",
      "Ep 1 (Step 000210): Train loss 5.501, Val loss 5.549\n",
      "Ep 1 (Step 000220): Train loss 5.517, Val loss 5.549\n",
      "Ep 1 (Step 000230): Train loss 5.431, Val loss 5.524\n",
      "Ep 1 (Step 000240): Train loss 5.462, Val loss 5.492\n",
      "Ep 1 (Step 000250): Train loss 5.386, Val loss 5.477\n",
      "Ep 1 (Step 000260): Train loss 5.342, Val loss 5.464\n",
      "Ep 1 (Step 000270): Train loss 5.346, Val loss 5.441\n",
      "Ep 1 (Step 000280): Train loss 5.334, Val loss 5.429\n",
      "Ep 1 (Step 000290): Train loss 5.275, Val loss 5.410\n",
      "Ep 1 (Step 000300): Train loss 5.363, Val loss 5.401\n",
      "Ep 1 (Step 000310): Train loss 5.278, Val loss 5.383\n",
      "Ep 1 (Step 000320): Train loss 5.204, Val loss 5.378\n",
      "Ep 1 (Step 000330): Train loss 5.228, Val loss 5.359\n",
      "Ep 1 (Step 000340): Train loss 5.272, Val loss 5.365\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3646\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.090, Val loss 9.081\n",
      "Ep 1 (Step 000010): Train loss 7.439, Val loss 7.414\n",
      "Ep 1 (Step 000020): Train loss 6.804, Val loss 6.776\n",
      "Ep 1 (Step 000030): Train loss 6.503, Val loss 6.482\n",
      "Ep 1 (Step 000040): Train loss 6.440, Val loss 6.394\n",
      "Ep 1 (Step 000050): Train loss 6.297, Val loss 6.367\n",
      "Ep 1 (Step 000060): Train loss 6.385, Val loss 6.335\n",
      "Ep 1 (Step 000070): Train loss 6.297, Val loss 6.253\n",
      "Ep 1 (Step 000080): Train loss 6.112, Val loss 6.151\n",
      "Ep 1 (Step 000090): Train loss 6.038, Val loss 6.070\n",
      "Ep 1 (Step 000100): Train loss 5.968, Val loss 5.999\n",
      "Ep 1 (Step 000110): Train loss 5.949, Val loss 5.955\n",
      "Ep 1 (Step 000120): Train loss 5.816, Val loss 5.893\n",
      "Ep 1 (Step 000130): Train loss 5.810, Val loss 5.844\n",
      "Ep 1 (Step 000140): Train loss 5.758, Val loss 5.808\n",
      "Ep 1 (Step 000150): Train loss 5.708, Val loss 5.756\n",
      "Ep 1 (Step 000160): Train loss 5.658, Val loss 5.717\n",
      "Ep 1 (Step 000170): Train loss 5.546, Val loss 5.681\n",
      "Ep 1 (Step 000180): Train loss 5.514, Val loss 5.651\n",
      "Ep 1 (Step 000190): Train loss 5.604, Val loss 5.619\n",
      "Ep 1 (Step 000200): Train loss 5.552, Val loss 5.596\n",
      "Ep 1 (Step 000210): Train loss 5.481, Val loss 5.575\n",
      "Ep 1 (Step 000220): Train loss 5.459, Val loss 5.544\n",
      "Ep 1 (Step 000230): Train loss 5.533, Val loss 5.543\n",
      "Ep 1 (Step 000240): Train loss 5.290, Val loss 5.513\n",
      "Ep 1 (Step 000250): Train loss 5.444, Val loss 5.503\n",
      "Ep 1 (Step 000260): Train loss 5.299, Val loss 5.479\n",
      "Ep 1 (Step 000270): Train loss 5.310, Val loss 5.469\n",
      "Ep 1 (Step 000280): Train loss 5.415, Val loss 5.454\n",
      "Ep 1 (Step 000290): Train loss 5.305, Val loss 5.437\n",
      "Ep 1 (Step 000300): Train loss 5.366, Val loss 5.431\n",
      "Ep 1 (Step 000310): Train loss 5.349, Val loss 5.423\n",
      "Ep 1 (Step 000320): Train loss 5.323, Val loss 5.404\n",
      "Ep 1 (Step 000330): Train loss 5.194, Val loss 5.380\n",
      "Ep 1 (Step 000340): Train loss 5.297, Val loss 5.370\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3705\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.777, Val loss 8.786\n",
      "Ep 1 (Step 000010): Train loss 6.911, Val loss 6.876\n",
      "Ep 1 (Step 000020): Train loss 6.486, Val loss 6.475\n",
      "Ep 1 (Step 000030): Train loss 6.466, Val loss 6.487\n",
      "Ep 1 (Step 000040): Train loss 6.374, Val loss 6.375\n",
      "Ep 1 (Step 000050): Train loss 6.295, Val loss 6.227\n",
      "Ep 1 (Step 000060): Train loss 6.076, Val loss 6.084\n",
      "Ep 1 (Step 000070): Train loss 6.029, Val loss 5.992\n",
      "Ep 1 (Step 000080): Train loss 5.875, Val loss 5.905\n",
      "Ep 1 (Step 000090): Train loss 5.702, Val loss 5.809\n",
      "Ep 1 (Step 000100): Train loss 5.630, Val loss 5.747\n",
      "Ep 1 (Step 000110): Train loss 5.748, Val loss 5.702\n",
      "Ep 1 (Step 000120): Train loss 5.675, Val loss 5.680\n",
      "Ep 1 (Step 000130): Train loss 5.596, Val loss 5.634\n",
      "Ep 1 (Step 000140): Train loss 5.508, Val loss 5.611\n",
      "Ep 1 (Step 000150): Train loss 5.507, Val loss 5.571\n",
      "Ep 1 (Step 000160): Train loss 5.427, Val loss 5.539\n",
      "Ep 1 (Step 000170): Train loss 5.472, Val loss 5.512\n",
      "Ep 1 (Step 000180): Train loss 5.403, Val loss 5.486\n",
      "Ep 1 (Step 000190): Train loss 5.364, Val loss 5.470\n",
      "Ep 1 (Step 000200): Train loss 5.338, Val loss 5.472\n",
      "Ep 1 (Step 000210): Train loss 5.331, Val loss 5.430\n",
      "Ep 1 (Step 000220): Train loss 5.272, Val loss 5.427\n",
      "Ep 1 (Step 000230): Train loss 5.291, Val loss 5.408\n",
      "Ep 1 (Step 000240): Train loss 5.324, Val loss 5.407\n",
      "Ep 1 (Step 000250): Train loss 5.285, Val loss 5.362\n",
      "Ep 1 (Step 000260): Train loss 5.306, Val loss 5.375\n",
      "Ep 1 (Step 000270): Train loss 5.194, Val loss 5.338\n",
      "Ep 1 (Step 000280): Train loss 5.194, Val loss 5.332\n",
      "Ep 1 (Step 000290): Train loss 5.179, Val loss 5.309\n",
      "Ep 1 (Step 000300): Train loss 5.181, Val loss 5.315\n",
      "Ep 1 (Step 000310): Train loss 5.194, Val loss 5.305\n",
      "Ep 1 (Step 000320): Train loss 5.183, Val loss 5.306\n",
      "Ep 1 (Step 000330): Train loss 5.126, Val loss 5.268\n",
      "Ep 1 (Step 000340): Train loss 5.115, Val loss 5.269\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2694\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.813, Val loss 8.810\n",
      "Ep 1 (Step 000010): Train loss 6.887, Val loss 6.861\n",
      "Ep 1 (Step 000020): Train loss 6.568, Val loss 6.491\n",
      "Ep 1 (Step 000030): Train loss 6.436, Val loss 6.477\n",
      "Ep 1 (Step 000040): Train loss 6.340, Val loss 6.387\n",
      "Ep 1 (Step 000050): Train loss 6.179, Val loss 6.207\n",
      "Ep 1 (Step 000060): Train loss 6.084, Val loss 6.082\n",
      "Ep 1 (Step 000070): Train loss 5.889, Val loss 5.983\n",
      "Ep 1 (Step 000080): Train loss 5.844, Val loss 5.932\n",
      "Ep 1 (Step 000090): Train loss 5.779, Val loss 5.820\n",
      "Ep 1 (Step 000100): Train loss 5.739, Val loss 5.764\n",
      "Ep 1 (Step 000110): Train loss 5.674, Val loss 5.708\n",
      "Ep 1 (Step 000120): Train loss 5.546, Val loss 5.673\n",
      "Ep 1 (Step 000130): Train loss 5.483, Val loss 5.634\n",
      "Ep 1 (Step 000140): Train loss 5.478, Val loss 5.574\n",
      "Ep 1 (Step 000150): Train loss 5.503, Val loss 5.571\n",
      "Ep 1 (Step 000160): Train loss 5.427, Val loss 5.560\n",
      "Ep 1 (Step 000170): Train loss 5.375, Val loss 5.525\n",
      "Ep 1 (Step 000180): Train loss 5.402, Val loss 5.497\n",
      "Ep 1 (Step 000190): Train loss 5.402, Val loss 5.466\n",
      "Ep 1 (Step 000200): Train loss 5.301, Val loss 5.441\n",
      "Ep 1 (Step 000210): Train loss 5.336, Val loss 5.425\n",
      "Ep 1 (Step 000220): Train loss 5.281, Val loss 5.428\n",
      "Ep 1 (Step 000230): Train loss 5.317, Val loss 5.400\n",
      "Ep 1 (Step 000240): Train loss 5.288, Val loss 5.410\n",
      "Ep 1 (Step 000250): Train loss 5.210, Val loss 5.378\n",
      "Ep 1 (Step 000260): Train loss 5.232, Val loss 5.362\n",
      "Ep 1 (Step 000270): Train loss 5.284, Val loss 5.358\n",
      "Ep 1 (Step 000280): Train loss 5.297, Val loss 5.331\n",
      "Ep 1 (Step 000290): Train loss 5.280, Val loss 5.329\n",
      "Ep 1 (Step 000300): Train loss 5.249, Val loss 5.310\n",
      "Ep 1 (Step 000310): Train loss 5.151, Val loss 5.305\n",
      "Ep 1 (Step 000320): Train loss 5.137, Val loss 5.314\n",
      "Ep 1 (Step 000330): Train loss 5.164, Val loss 5.305\n",
      "Ep 1 (Step 000340): Train loss 5.185, Val loss 5.269\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2688\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.831, Val loss 8.828\n",
      "Ep 1 (Step 000010): Train loss 6.894, Val loss 6.872\n",
      "Ep 1 (Step 000020): Train loss 6.558, Val loss 6.489\n",
      "Ep 1 (Step 000030): Train loss 6.458, Val loss 6.470\n",
      "Ep 1 (Step 000040): Train loss 6.361, Val loss 6.367\n",
      "Ep 1 (Step 000050): Train loss 6.211, Val loss 6.213\n",
      "Ep 1 (Step 000060): Train loss 6.124, Val loss 6.095\n",
      "Ep 1 (Step 000070): Train loss 5.876, Val loss 5.995\n",
      "Ep 1 (Step 000080): Train loss 5.853, Val loss 5.900\n",
      "Ep 1 (Step 000090): Train loss 5.837, Val loss 5.823\n",
      "Ep 1 (Step 000100): Train loss 5.634, Val loss 5.772\n",
      "Ep 1 (Step 000110): Train loss 5.609, Val loss 5.708\n",
      "Ep 1 (Step 000120): Train loss 5.610, Val loss 5.679\n",
      "Ep 1 (Step 000130): Train loss 5.529, Val loss 5.630\n",
      "Ep 1 (Step 000140): Train loss 5.541, Val loss 5.596\n",
      "Ep 1 (Step 000150): Train loss 5.448, Val loss 5.586\n",
      "Ep 1 (Step 000160): Train loss 5.490, Val loss 5.526\n",
      "Ep 1 (Step 000170): Train loss 5.423, Val loss 5.506\n",
      "Ep 1 (Step 000180): Train loss 5.396, Val loss 5.485\n",
      "Ep 1 (Step 000190): Train loss 5.349, Val loss 5.466\n",
      "Ep 1 (Step 000200): Train loss 5.286, Val loss 5.446\n",
      "Ep 1 (Step 000210): Train loss 5.374, Val loss 5.434\n",
      "Ep 1 (Step 000220): Train loss 5.254, Val loss 5.421\n",
      "Ep 1 (Step 000230): Train loss 5.277, Val loss 5.400\n",
      "Ep 1 (Step 000240): Train loss 5.245, Val loss 5.381\n",
      "Ep 1 (Step 000250): Train loss 5.145, Val loss 5.373\n",
      "Ep 1 (Step 000260): Train loss 5.255, Val loss 5.363\n",
      "Ep 1 (Step 000270): Train loss 5.261, Val loss 5.343\n",
      "Ep 1 (Step 000280): Train loss 5.297, Val loss 5.345\n",
      "Ep 1 (Step 000290): Train loss 5.228, Val loss 5.310\n",
      "Ep 1 (Step 000300): Train loss 5.116, Val loss 5.316\n",
      "Ep 1 (Step 000310): Train loss 5.237, Val loss 5.312\n",
      "Ep 1 (Step 000320): Train loss 5.197, Val loss 5.304\n",
      "Ep 1 (Step 000330): Train loss 5.213, Val loss 5.284\n",
      "Ep 1 (Step 000340): Train loss 5.184, Val loss 5.271\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2707\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.907, Val loss 8.876\n",
      "Ep 1 (Step 000010): Train loss 6.924, Val loss 6.883\n",
      "Ep 1 (Step 000020): Train loss 6.499, Val loss 6.475\n",
      "Ep 1 (Step 000030): Train loss 6.457, Val loss 6.453\n",
      "Ep 1 (Step 000040): Train loss 6.373, Val loss 6.361\n",
      "Ep 1 (Step 000050): Train loss 6.232, Val loss 6.252\n",
      "Ep 1 (Step 000060): Train loss 6.032, Val loss 6.078\n",
      "Ep 1 (Step 000070): Train loss 5.943, Val loss 5.972\n",
      "Ep 1 (Step 000080): Train loss 5.853, Val loss 5.895\n",
      "Ep 1 (Step 000090): Train loss 5.822, Val loss 5.834\n",
      "Ep 1 (Step 000100): Train loss 5.692, Val loss 5.744\n",
      "Ep 1 (Step 000110): Train loss 5.645, Val loss 5.693\n",
      "Ep 1 (Step 000120): Train loss 5.631, Val loss 5.647\n",
      "Ep 1 (Step 000130): Train loss 5.445, Val loss 5.591\n",
      "Ep 1 (Step 000140): Train loss 5.496, Val loss 5.541\n",
      "Ep 1 (Step 000150): Train loss 5.339, Val loss 5.512\n",
      "Ep 1 (Step 000160): Train loss 5.364, Val loss 5.486\n",
      "Ep 1 (Step 000170): Train loss 5.457, Val loss 5.477\n",
      "Ep 1 (Step 000180): Train loss 5.351, Val loss 5.442\n",
      "Ep 1 (Step 000190): Train loss 5.405, Val loss 5.423\n",
      "Ep 1 (Step 000200): Train loss 5.344, Val loss 5.416\n",
      "Ep 1 (Step 000210): Train loss 5.255, Val loss 5.412\n",
      "Ep 1 (Step 000220): Train loss 5.354, Val loss 5.385\n",
      "Ep 1 (Step 000230): Train loss 5.180, Val loss 5.387\n",
      "Ep 1 (Step 000240): Train loss 5.149, Val loss 5.353\n",
      "Ep 1 (Step 000250): Train loss 5.293, Val loss 5.351\n",
      "Ep 1 (Step 000260): Train loss 5.216, Val loss 5.345\n",
      "Ep 1 (Step 000270): Train loss 5.173, Val loss 5.335\n",
      "Ep 1 (Step 000280): Train loss 5.246, Val loss 5.308\n",
      "Ep 1 (Step 000290): Train loss 5.159, Val loss 5.313\n",
      "Ep 1 (Step 000300): Train loss 5.227, Val loss 5.286\n",
      "Ep 1 (Step 000310): Train loss 5.213, Val loss 5.271\n",
      "Ep 1 (Step 000320): Train loss 5.128, Val loss 5.245\n",
      "Ep 1 (Step 000330): Train loss 5.086, Val loss 5.249\n",
      "Ep 1 (Step 000340): Train loss 5.048, Val loss 5.219\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2190\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.808, Val loss 8.772\n",
      "Ep 1 (Step 000010): Train loss 6.886, Val loss 6.802\n",
      "Ep 1 (Step 000020): Train loss 6.441, Val loss 6.464\n",
      "Ep 1 (Step 000030): Train loss 6.441, Val loss 6.473\n",
      "Ep 1 (Step 000040): Train loss 6.425, Val loss 6.414\n",
      "Ep 1 (Step 000050): Train loss 6.205, Val loss 6.282\n",
      "Ep 1 (Step 000060): Train loss 6.126, Val loss 6.134\n",
      "Ep 1 (Step 000070): Train loss 5.952, Val loss 6.004\n",
      "Ep 1 (Step 000080): Train loss 5.936, Val loss 5.920\n",
      "Ep 1 (Step 000090): Train loss 5.810, Val loss 5.842\n",
      "Ep 1 (Step 000100): Train loss 5.702, Val loss 5.756\n",
      "Ep 1 (Step 000110): Train loss 5.672, Val loss 5.710\n",
      "Ep 1 (Step 000120): Train loss 5.635, Val loss 5.660\n",
      "Ep 1 (Step 000130): Train loss 5.506, Val loss 5.627\n",
      "Ep 1 (Step 000140): Train loss 5.532, Val loss 5.589\n",
      "Ep 1 (Step 000150): Train loss 5.561, Val loss 5.535\n",
      "Ep 1 (Step 000160): Train loss 5.414, Val loss 5.538\n",
      "Ep 1 (Step 000170): Train loss 5.343, Val loss 5.506\n",
      "Ep 1 (Step 000180): Train loss 5.375, Val loss 5.488\n",
      "Ep 1 (Step 000190): Train loss 5.360, Val loss 5.457\n",
      "Ep 1 (Step 000200): Train loss 5.316, Val loss 5.442\n",
      "Ep 1 (Step 000210): Train loss 5.405, Val loss 5.426\n",
      "Ep 1 (Step 000220): Train loss 5.337, Val loss 5.416\n",
      "Ep 1 (Step 000230): Train loss 5.412, Val loss 5.395\n",
      "Ep 1 (Step 000240): Train loss 5.275, Val loss 5.382\n",
      "Ep 1 (Step 000250): Train loss 5.132, Val loss 5.369\n",
      "Ep 1 (Step 000260): Train loss 5.216, Val loss 5.333\n",
      "Ep 1 (Step 000270): Train loss 5.237, Val loss 5.320\n",
      "Ep 1 (Step 000280): Train loss 5.087, Val loss 5.302\n",
      "Ep 1 (Step 000290): Train loss 5.245, Val loss 5.290\n",
      "Ep 1 (Step 000300): Train loss 5.101, Val loss 5.267\n",
      "Ep 1 (Step 000310): Train loss 5.146, Val loss 5.270\n",
      "Ep 1 (Step 000320): Train loss 5.104, Val loss 5.250\n",
      "Ep 1 (Step 000330): Train loss 5.072, Val loss 5.242\n",
      "Ep 1 (Step 000340): Train loss 5.101, Val loss 5.238\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2382\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.817, Val loss 8.811\n",
      "Ep 1 (Step 000010): Train loss 6.872, Val loss 6.874\n",
      "Ep 1 (Step 000020): Train loss 6.515, Val loss 6.473\n",
      "Ep 1 (Step 000030): Train loss 6.516, Val loss 6.459\n",
      "Ep 1 (Step 000040): Train loss 6.401, Val loss 6.381\n",
      "Ep 1 (Step 000050): Train loss 6.249, Val loss 6.241\n",
      "Ep 1 (Step 000060): Train loss 6.122, Val loss 6.122\n",
      "Ep 1 (Step 000070): Train loss 5.931, Val loss 5.986\n",
      "Ep 1 (Step 000080): Train loss 5.892, Val loss 5.920\n",
      "Ep 1 (Step 000090): Train loss 5.743, Val loss 5.828\n",
      "Ep 1 (Step 000100): Train loss 5.684, Val loss 5.758\n",
      "Ep 1 (Step 000110): Train loss 5.679, Val loss 5.686\n",
      "Ep 1 (Step 000120): Train loss 5.598, Val loss 5.658\n",
      "Ep 1 (Step 000130): Train loss 5.591, Val loss 5.615\n",
      "Ep 1 (Step 000140): Train loss 5.452, Val loss 5.576\n",
      "Ep 1 (Step 000150): Train loss 5.444, Val loss 5.546\n",
      "Ep 1 (Step 000160): Train loss 5.449, Val loss 5.545\n",
      "Ep 1 (Step 000170): Train loss 5.380, Val loss 5.499\n",
      "Ep 1 (Step 000180): Train loss 5.371, Val loss 5.473\n",
      "Ep 1 (Step 000190): Train loss 5.327, Val loss 5.440\n",
      "Ep 1 (Step 000200): Train loss 5.355, Val loss 5.426\n",
      "Ep 1 (Step 000210): Train loss 5.314, Val loss 5.401\n",
      "Ep 1 (Step 000220): Train loss 5.310, Val loss 5.380\n",
      "Ep 1 (Step 000230): Train loss 5.324, Val loss 5.375\n",
      "Ep 1 (Step 000240): Train loss 5.261, Val loss 5.389\n",
      "Ep 1 (Step 000250): Train loss 5.265, Val loss 5.360\n",
      "Ep 1 (Step 000260): Train loss 5.169, Val loss 5.333\n",
      "Ep 1 (Step 000270): Train loss 5.243, Val loss 5.318\n",
      "Ep 1 (Step 000280): Train loss 5.173, Val loss 5.301\n",
      "Ep 1 (Step 000290): Train loss 5.190, Val loss 5.287\n",
      "Ep 1 (Step 000300): Train loss 5.114, Val loss 5.285\n",
      "Ep 1 (Step 000310): Train loss 5.202, Val loss 5.293\n",
      "Ep 1 (Step 000320): Train loss 5.145, Val loss 5.275\n",
      "Ep 1 (Step 000330): Train loss 5.007, Val loss 5.260\n",
      "Ep 1 (Step 000340): Train loss 5.011, Val loss 5.271\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2715\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.053, Val loss 9.032\n",
      "Ep 1 (Step 000010): Train loss 7.461, Val loss 7.438\n",
      "Ep 1 (Step 000020): Train loss 6.813, Val loss 6.795\n",
      "Ep 1 (Step 000030): Train loss 6.505, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.439, Val loss 6.406\n",
      "Ep 1 (Step 000050): Train loss 6.360, Val loss 6.375\n",
      "Ep 1 (Step 000060): Train loss 6.320, Val loss 6.303\n",
      "Ep 1 (Step 000070): Train loss 6.219, Val loss 6.197\n",
      "Ep 1 (Step 000080): Train loss 6.162, Val loss 6.110\n",
      "Ep 1 (Step 000090): Train loss 5.997, Val loss 6.037\n",
      "Ep 1 (Step 000100): Train loss 5.944, Val loss 5.972\n",
      "Ep 1 (Step 000110): Train loss 5.839, Val loss 5.927\n",
      "Ep 1 (Step 000120): Train loss 5.882, Val loss 5.871\n",
      "Ep 1 (Step 000130): Train loss 5.830, Val loss 5.828\n",
      "Ep 1 (Step 000140): Train loss 5.832, Val loss 5.794\n",
      "Ep 1 (Step 000150): Train loss 5.646, Val loss 5.755\n",
      "Ep 1 (Step 000160): Train loss 5.683, Val loss 5.729\n",
      "Ep 1 (Step 000170): Train loss 5.601, Val loss 5.686\n",
      "Ep 1 (Step 000180): Train loss 5.662, Val loss 5.654\n",
      "Ep 1 (Step 000190): Train loss 5.513, Val loss 5.625\n",
      "Ep 1 (Step 000200): Train loss 5.488, Val loss 5.604\n",
      "Ep 1 (Step 000210): Train loss 5.502, Val loss 5.578\n",
      "Ep 1 (Step 000220): Train loss 5.504, Val loss 5.555\n",
      "Ep 1 (Step 000230): Train loss 5.400, Val loss 5.535\n",
      "Ep 1 (Step 000240): Train loss 5.414, Val loss 5.503\n",
      "Ep 1 (Step 000250): Train loss 5.428, Val loss 5.502\n",
      "Ep 1 (Step 000260): Train loss 5.421, Val loss 5.480\n",
      "Ep 1 (Step 000270): Train loss 5.323, Val loss 5.474\n",
      "Ep 1 (Step 000280): Train loss 5.395, Val loss 5.465\n",
      "Ep 1 (Step 000290): Train loss 5.293, Val loss 5.438\n",
      "Ep 1 (Step 000300): Train loss 5.381, Val loss 5.434\n",
      "Ep 1 (Step 000310): Train loss 5.332, Val loss 5.420\n",
      "Ep 1 (Step 000320): Train loss 5.336, Val loss 5.414\n",
      "Ep 1 (Step 000330): Train loss 5.244, Val loss 5.391\n",
      "Ep 1 (Step 000340): Train loss 5.245, Val loss 5.384\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3843\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.096, Val loss 9.078\n",
      "Ep 1 (Step 000010): Train loss 7.506, Val loss 7.471\n",
      "Ep 1 (Step 000020): Train loss 6.813, Val loss 6.804\n",
      "Ep 1 (Step 000030): Train loss 6.527, Val loss 6.487\n",
      "Ep 1 (Step 000040): Train loss 6.389, Val loss 6.416\n",
      "Ep 1 (Step 000050): Train loss 6.392, Val loss 6.381\n",
      "Ep 1 (Step 000060): Train loss 6.324, Val loss 6.345\n",
      "Ep 1 (Step 000070): Train loss 6.205, Val loss 6.244\n",
      "Ep 1 (Step 000080): Train loss 6.064, Val loss 6.138\n",
      "Ep 1 (Step 000090): Train loss 5.996, Val loss 6.051\n",
      "Ep 1 (Step 000100): Train loss 5.898, Val loss 5.972\n",
      "Ep 1 (Step 000110): Train loss 5.899, Val loss 5.923\n",
      "Ep 1 (Step 000120): Train loss 5.782, Val loss 5.884\n",
      "Ep 1 (Step 000130): Train loss 5.758, Val loss 5.850\n",
      "Ep 1 (Step 000140): Train loss 5.738, Val loss 5.798\n",
      "Ep 1 (Step 000150): Train loss 5.719, Val loss 5.759\n",
      "Ep 1 (Step 000160): Train loss 5.616, Val loss 5.710\n",
      "Ep 1 (Step 000170): Train loss 5.681, Val loss 5.703\n",
      "Ep 1 (Step 000180): Train loss 5.501, Val loss 5.647\n",
      "Ep 1 (Step 000190): Train loss 5.515, Val loss 5.624\n",
      "Ep 1 (Step 000200): Train loss 5.598, Val loss 5.607\n",
      "Ep 1 (Step 000210): Train loss 5.508, Val loss 5.588\n",
      "Ep 1 (Step 000220): Train loss 5.537, Val loss 5.560\n",
      "Ep 1 (Step 000230): Train loss 5.379, Val loss 5.537\n",
      "Ep 1 (Step 000240): Train loss 5.392, Val loss 5.529\n",
      "Ep 1 (Step 000250): Train loss 5.511, Val loss 5.504\n",
      "Ep 1 (Step 000260): Train loss 5.406, Val loss 5.481\n",
      "Ep 1 (Step 000270): Train loss 5.385, Val loss 5.470\n",
      "Ep 1 (Step 000280): Train loss 5.347, Val loss 5.461\n",
      "Ep 1 (Step 000290): Train loss 5.312, Val loss 5.435\n",
      "Ep 1 (Step 000300): Train loss 5.322, Val loss 5.425\n",
      "Ep 1 (Step 000310): Train loss 5.290, Val loss 5.413\n",
      "Ep 1 (Step 000320): Train loss 5.312, Val loss 5.410\n",
      "Ep 1 (Step 000330): Train loss 5.221, Val loss 5.390\n",
      "Ep 1 (Step 000340): Train loss 5.280, Val loss 5.392\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3924\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.096, Val loss 9.098\n",
      "Ep 1 (Step 000010): Train loss 7.446, Val loss 7.474\n",
      "Ep 1 (Step 000020): Train loss 6.833, Val loss 6.819\n",
      "Ep 1 (Step 000030): Train loss 6.484, Val loss 6.496\n",
      "Ep 1 (Step 000040): Train loss 6.412, Val loss 6.409\n",
      "Ep 1 (Step 000050): Train loss 6.403, Val loss 6.372\n",
      "Ep 1 (Step 000060): Train loss 6.293, Val loss 6.332\n",
      "Ep 1 (Step 000070): Train loss 6.177, Val loss 6.238\n",
      "Ep 1 (Step 000080): Train loss 6.139, Val loss 6.126\n",
      "Ep 1 (Step 000090): Train loss 5.956, Val loss 6.041\n",
      "Ep 1 (Step 000100): Train loss 6.027, Val loss 5.985\n",
      "Ep 1 (Step 000110): Train loss 5.993, Val loss 5.940\n",
      "Ep 1 (Step 000120): Train loss 5.816, Val loss 5.870\n",
      "Ep 1 (Step 000130): Train loss 5.761, Val loss 5.836\n",
      "Ep 1 (Step 000140): Train loss 5.728, Val loss 5.793\n",
      "Ep 1 (Step 000150): Train loss 5.718, Val loss 5.768\n",
      "Ep 1 (Step 000160): Train loss 5.636, Val loss 5.726\n",
      "Ep 1 (Step 000170): Train loss 5.658, Val loss 5.697\n",
      "Ep 1 (Step 000180): Train loss 5.589, Val loss 5.672\n",
      "Ep 1 (Step 000190): Train loss 5.566, Val loss 5.645\n",
      "Ep 1 (Step 000200): Train loss 5.571, Val loss 5.612\n",
      "Ep 1 (Step 000210): Train loss 5.551, Val loss 5.594\n",
      "Ep 1 (Step 000220): Train loss 5.468, Val loss 5.569\n",
      "Ep 1 (Step 000230): Train loss 5.395, Val loss 5.557\n",
      "Ep 1 (Step 000240): Train loss 5.478, Val loss 5.541\n",
      "Ep 1 (Step 000250): Train loss 5.348, Val loss 5.518\n",
      "Ep 1 (Step 000260): Train loss 5.412, Val loss 5.492\n",
      "Ep 1 (Step 000270): Train loss 5.389, Val loss 5.473\n",
      "Ep 1 (Step 000280): Train loss 5.336, Val loss 5.466\n",
      "Ep 1 (Step 000290): Train loss 5.315, Val loss 5.459\n",
      "Ep 1 (Step 000300): Train loss 5.355, Val loss 5.438\n",
      "Ep 1 (Step 000310): Train loss 5.270, Val loss 5.441\n",
      "Ep 1 (Step 000320): Train loss 5.339, Val loss 5.422\n",
      "Ep 1 (Step 000330): Train loss 5.269, Val loss 5.404\n",
      "Ep 1 (Step 000340): Train loss 5.293, Val loss 5.400\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.4003\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.050, Val loss 9.055\n",
      "Ep 1 (Step 000010): Train loss 7.531, Val loss 7.467\n",
      "Ep 1 (Step 000020): Train loss 6.849, Val loss 6.790\n",
      "Ep 1 (Step 000030): Train loss 6.389, Val loss 6.475\n",
      "Ep 1 (Step 000040): Train loss 6.349, Val loss 6.398\n",
      "Ep 1 (Step 000050): Train loss 6.357, Val loss 6.372\n",
      "Ep 1 (Step 000060): Train loss 6.336, Val loss 6.307\n",
      "Ep 1 (Step 000070): Train loss 6.250, Val loss 6.227\n",
      "Ep 1 (Step 000080): Train loss 6.090, Val loss 6.116\n",
      "Ep 1 (Step 000090): Train loss 5.987, Val loss 6.036\n",
      "Ep 1 (Step 000100): Train loss 5.930, Val loss 5.978\n",
      "Ep 1 (Step 000110): Train loss 5.862, Val loss 5.912\n",
      "Ep 1 (Step 000120): Train loss 5.834, Val loss 5.858\n",
      "Ep 1 (Step 000130): Train loss 5.822, Val loss 5.808\n",
      "Ep 1 (Step 000140): Train loss 5.755, Val loss 5.763\n",
      "Ep 1 (Step 000150): Train loss 5.711, Val loss 5.725\n",
      "Ep 1 (Step 000160): Train loss 5.632, Val loss 5.704\n",
      "Ep 1 (Step 000170): Train loss 5.636, Val loss 5.671\n",
      "Ep 1 (Step 000180): Train loss 5.589, Val loss 5.646\n",
      "Ep 1 (Step 000190): Train loss 5.554, Val loss 5.611\n",
      "Ep 1 (Step 000200): Train loss 5.521, Val loss 5.591\n",
      "Ep 1 (Step 000210): Train loss 5.580, Val loss 5.617\n",
      "Ep 1 (Step 000220): Train loss 5.478, Val loss 5.557\n",
      "Ep 1 (Step 000230): Train loss 5.451, Val loss 5.535\n",
      "Ep 1 (Step 000240): Train loss 5.433, Val loss 5.504\n",
      "Ep 1 (Step 000250): Train loss 5.474, Val loss 5.490\n",
      "Ep 1 (Step 000260): Train loss 5.460, Val loss 5.465\n",
      "Ep 1 (Step 000270): Train loss 5.343, Val loss 5.448\n",
      "Ep 1 (Step 000280): Train loss 5.410, Val loss 5.422\n",
      "Ep 1 (Step 000290): Train loss 5.295, Val loss 5.411\n",
      "Ep 1 (Step 000300): Train loss 5.335, Val loss 5.407\n",
      "Ep 1 (Step 000310): Train loss 5.329, Val loss 5.399\n",
      "Ep 1 (Step 000320): Train loss 5.315, Val loss 5.381\n",
      "Ep 1 (Step 000330): Train loss 5.241, Val loss 5.366\n",
      "Ep 1 (Step 000340): Train loss 5.227, Val loss 5.368\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3677\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.101, Val loss 9.104\n",
      "Ep 1 (Step 000010): Train loss 7.578, Val loss 7.550\n",
      "Ep 1 (Step 000020): Train loss 6.874, Val loss 6.863\n",
      "Ep 1 (Step 000030): Train loss 6.536, Val loss 6.498\n",
      "Ep 1 (Step 000040): Train loss 6.434, Val loss 6.419\n",
      "Ep 1 (Step 000050): Train loss 6.356, Val loss 6.374\n",
      "Ep 1 (Step 000060): Train loss 6.247, Val loss 6.312\n",
      "Ep 1 (Step 000070): Train loss 6.220, Val loss 6.219\n",
      "Ep 1 (Step 000080): Train loss 6.051, Val loss 6.119\n",
      "Ep 1 (Step 000090): Train loss 6.021, Val loss 6.057\n",
      "Ep 1 (Step 000100): Train loss 5.943, Val loss 5.985\n",
      "Ep 1 (Step 000110): Train loss 5.892, Val loss 5.929\n",
      "Ep 1 (Step 000120): Train loss 5.870, Val loss 5.875\n",
      "Ep 1 (Step 000130): Train loss 5.816, Val loss 5.832\n",
      "Ep 1 (Step 000140): Train loss 5.737, Val loss 5.785\n",
      "Ep 1 (Step 000150): Train loss 5.672, Val loss 5.749\n",
      "Ep 1 (Step 000160): Train loss 5.752, Val loss 5.708\n",
      "Ep 1 (Step 000170): Train loss 5.563, Val loss 5.666\n",
      "Ep 1 (Step 000180): Train loss 5.544, Val loss 5.633\n",
      "Ep 1 (Step 000190): Train loss 5.505, Val loss 5.611\n",
      "Ep 1 (Step 000200): Train loss 5.519, Val loss 5.598\n",
      "Ep 1 (Step 000210): Train loss 5.470, Val loss 5.564\n",
      "Ep 1 (Step 000220): Train loss 5.528, Val loss 5.533\n",
      "Ep 1 (Step 000230): Train loss 5.471, Val loss 5.518\n",
      "Ep 1 (Step 000240): Train loss 5.358, Val loss 5.501\n",
      "Ep 1 (Step 000250): Train loss 5.378, Val loss 5.494\n",
      "Ep 1 (Step 000260): Train loss 5.380, Val loss 5.471\n",
      "Ep 1 (Step 000270): Train loss 5.403, Val loss 5.460\n",
      "Ep 1 (Step 000280): Train loss 5.388, Val loss 5.434\n",
      "Ep 1 (Step 000290): Train loss 5.303, Val loss 5.434\n",
      "Ep 1 (Step 000300): Train loss 5.305, Val loss 5.419\n",
      "Ep 1 (Step 000310): Train loss 5.322, Val loss 5.405\n",
      "Ep 1 (Step 000320): Train loss 5.221, Val loss 5.389\n",
      "Ep 1 (Step 000330): Train loss 5.231, Val loss 5.378\n",
      "Ep 1 (Step 000340): Train loss 5.211, Val loss 5.365\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3653\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.100, Val loss 9.074\n",
      "Ep 1 (Step 000010): Train loss 7.473, Val loss 7.427\n",
      "Ep 1 (Step 000020): Train loss 6.819, Val loss 6.794\n",
      "Ep 1 (Step 000030): Train loss 6.488, Val loss 6.485\n",
      "Ep 1 (Step 000040): Train loss 6.383, Val loss 6.409\n",
      "Ep 1 (Step 000050): Train loss 6.344, Val loss 6.363\n",
      "Ep 1 (Step 000060): Train loss 6.306, Val loss 6.311\n",
      "Ep 1 (Step 000070): Train loss 6.252, Val loss 6.224\n",
      "Ep 1 (Step 000080): Train loss 6.100, Val loss 6.129\n",
      "Ep 1 (Step 000090): Train loss 6.030, Val loss 6.046\n",
      "Ep 1 (Step 000100): Train loss 5.937, Val loss 5.975\n",
      "Ep 1 (Step 000110): Train loss 5.896, Val loss 5.934\n",
      "Ep 1 (Step 000120): Train loss 5.792, Val loss 5.857\n",
      "Ep 1 (Step 000130): Train loss 5.823, Val loss 5.817\n",
      "Ep 1 (Step 000140): Train loss 5.744, Val loss 5.781\n",
      "Ep 1 (Step 000150): Train loss 5.643, Val loss 5.737\n",
      "Ep 1 (Step 000160): Train loss 5.629, Val loss 5.700\n",
      "Ep 1 (Step 000170): Train loss 5.576, Val loss 5.668\n",
      "Ep 1 (Step 000180): Train loss 5.597, Val loss 5.634\n",
      "Ep 1 (Step 000190): Train loss 5.576, Val loss 5.615\n",
      "Ep 1 (Step 000200): Train loss 5.529, Val loss 5.579\n",
      "Ep 1 (Step 000210): Train loss 5.495, Val loss 5.570\n",
      "Ep 1 (Step 000220): Train loss 5.555, Val loss 5.542\n",
      "Ep 1 (Step 000230): Train loss 5.429, Val loss 5.542\n",
      "Ep 1 (Step 000240): Train loss 5.458, Val loss 5.507\n",
      "Ep 1 (Step 000250): Train loss 5.363, Val loss 5.487\n",
      "Ep 1 (Step 000260): Train loss 5.437, Val loss 5.473\n",
      "Ep 1 (Step 000270): Train loss 5.385, Val loss 5.448\n",
      "Ep 1 (Step 000280): Train loss 5.316, Val loss 5.426\n",
      "Ep 1 (Step 000290): Train loss 5.355, Val loss 5.422\n",
      "Ep 1 (Step 000300): Train loss 5.266, Val loss 5.414\n",
      "Ep 1 (Step 000310): Train loss 5.387, Val loss 5.415\n",
      "Ep 1 (Step 000320): Train loss 5.267, Val loss 5.404\n",
      "Ep 1 (Step 000330): Train loss 5.305, Val loss 5.377\n",
      "Ep 1 (Step 000340): Train loss 5.233, Val loss 5.364\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3638\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.803, Val loss 8.792\n",
      "Ep 1 (Step 000010): Train loss 6.929, Val loss 6.865\n",
      "Ep 1 (Step 000020): Train loss 6.496, Val loss 6.434\n",
      "Ep 1 (Step 000030): Train loss 6.418, Val loss 6.412\n",
      "Ep 1 (Step 000040): Train loss 6.304, Val loss 6.314\n",
      "Ep 1 (Step 000050): Train loss 6.178, Val loss 6.180\n",
      "Ep 1 (Step 000060): Train loss 6.070, Val loss 6.085\n",
      "Ep 1 (Step 000070): Train loss 5.955, Val loss 5.955\n",
      "Ep 1 (Step 000080): Train loss 5.851, Val loss 5.869\n",
      "Ep 1 (Step 000090): Train loss 5.818, Val loss 5.802\n",
      "Ep 1 (Step 000100): Train loss 5.714, Val loss 5.745\n",
      "Ep 1 (Step 000110): Train loss 5.615, Val loss 5.708\n",
      "Ep 1 (Step 000120): Train loss 5.687, Val loss 5.644\n",
      "Ep 1 (Step 000130): Train loss 5.601, Val loss 5.616\n",
      "Ep 1 (Step 000140): Train loss 5.546, Val loss 5.594\n",
      "Ep 1 (Step 000150): Train loss 5.452, Val loss 5.558\n",
      "Ep 1 (Step 000160): Train loss 5.450, Val loss 5.518\n",
      "Ep 1 (Step 000170): Train loss 5.440, Val loss 5.507\n",
      "Ep 1 (Step 000180): Train loss 5.380, Val loss 5.474\n",
      "Ep 1 (Step 000190): Train loss 5.354, Val loss 5.463\n",
      "Ep 1 (Step 000200): Train loss 5.383, Val loss 5.441\n",
      "Ep 1 (Step 000210): Train loss 5.267, Val loss 5.445\n",
      "Ep 1 (Step 000220): Train loss 5.310, Val loss 5.424\n",
      "Ep 1 (Step 000230): Train loss 5.291, Val loss 5.414\n",
      "Ep 1 (Step 000240): Train loss 5.282, Val loss 5.407\n",
      "Ep 1 (Step 000250): Train loss 5.373, Val loss 5.393\n",
      "Ep 1 (Step 000260): Train loss 5.289, Val loss 5.373\n",
      "Ep 1 (Step 000270): Train loss 5.185, Val loss 5.357\n",
      "Ep 1 (Step 000280): Train loss 5.200, Val loss 5.334\n",
      "Ep 1 (Step 000290): Train loss 5.165, Val loss 5.316\n",
      "Ep 1 (Step 000300): Train loss 5.150, Val loss 5.326\n",
      "Ep 1 (Step 000310): Train loss 5.181, Val loss 5.324\n",
      "Ep 1 (Step 000320): Train loss 5.310, Val loss 5.303\n",
      "Ep 1 (Step 000330): Train loss 5.199, Val loss 5.291\n",
      "Ep 1 (Step 000340): Train loss 5.092, Val loss 5.282\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2819\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.814, Val loss 8.774\n",
      "Ep 1 (Step 000010): Train loss 6.908, Val loss 6.842\n",
      "Ep 1 (Step 000020): Train loss 6.526, Val loss 6.459\n",
      "Ep 1 (Step 000030): Train loss 6.405, Val loss 6.449\n",
      "Ep 1 (Step 000040): Train loss 6.274, Val loss 6.333\n",
      "Ep 1 (Step 000050): Train loss 6.193, Val loss 6.192\n",
      "Ep 1 (Step 000060): Train loss 6.038, Val loss 6.066\n",
      "Ep 1 (Step 000070): Train loss 5.899, Val loss 5.987\n",
      "Ep 1 (Step 000080): Train loss 5.843, Val loss 5.878\n",
      "Ep 1 (Step 000090): Train loss 5.778, Val loss 5.797\n",
      "Ep 1 (Step 000100): Train loss 5.735, Val loss 5.759\n",
      "Ep 1 (Step 000110): Train loss 5.652, Val loss 5.718\n",
      "Ep 1 (Step 000120): Train loss 5.666, Val loss 5.660\n",
      "Ep 1 (Step 000130): Train loss 5.474, Val loss 5.622\n",
      "Ep 1 (Step 000140): Train loss 5.581, Val loss 5.584\n",
      "Ep 1 (Step 000150): Train loss 5.457, Val loss 5.543\n",
      "Ep 1 (Step 000160): Train loss 5.497, Val loss 5.527\n",
      "Ep 1 (Step 000170): Train loss 5.469, Val loss 5.509\n",
      "Ep 1 (Step 000180): Train loss 5.338, Val loss 5.483\n",
      "Ep 1 (Step 000190): Train loss 5.431, Val loss 5.465\n",
      "Ep 1 (Step 000200): Train loss 5.340, Val loss 5.458\n",
      "Ep 1 (Step 000210): Train loss 5.349, Val loss 5.444\n",
      "Ep 1 (Step 000220): Train loss 5.261, Val loss 5.426\n",
      "Ep 1 (Step 000230): Train loss 5.298, Val loss 5.424\n",
      "Ep 1 (Step 000240): Train loss 5.372, Val loss 5.430\n",
      "Ep 1 (Step 000250): Train loss 5.290, Val loss 5.375\n",
      "Ep 1 (Step 000260): Train loss 5.305, Val loss 5.375\n",
      "Ep 1 (Step 000270): Train loss 5.192, Val loss 5.359\n",
      "Ep 1 (Step 000280): Train loss 5.218, Val loss 5.343\n",
      "Ep 1 (Step 000290): Train loss 5.215, Val loss 5.332\n",
      "Ep 1 (Step 000300): Train loss 5.139, Val loss 5.326\n",
      "Ep 1 (Step 000310): Train loss 5.175, Val loss 5.310\n",
      "Ep 1 (Step 000320): Train loss 5.095, Val loss 5.316\n",
      "Ep 1 (Step 000330): Train loss 5.149, Val loss 5.285\n",
      "Ep 1 (Step 000340): Train loss 5.118, Val loss 5.273\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2729\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.767, Val loss 8.745\n",
      "Ep 1 (Step 000010): Train loss 6.913, Val loss 6.836\n",
      "Ep 1 (Step 000020): Train loss 6.473, Val loss 6.461\n",
      "Ep 1 (Step 000030): Train loss 6.379, Val loss 6.445\n",
      "Ep 1 (Step 000040): Train loss 6.362, Val loss 6.354\n",
      "Ep 1 (Step 000050): Train loss 6.247, Val loss 6.190\n",
      "Ep 1 (Step 000060): Train loss 6.083, Val loss 6.065\n",
      "Ep 1 (Step 000070): Train loss 5.922, Val loss 5.991\n",
      "Ep 1 (Step 000080): Train loss 5.821, Val loss 5.917\n",
      "Ep 1 (Step 000090): Train loss 5.822, Val loss 5.828\n",
      "Ep 1 (Step 000100): Train loss 5.693, Val loss 5.763\n",
      "Ep 1 (Step 000110): Train loss 5.675, Val loss 5.707\n",
      "Ep 1 (Step 000120): Train loss 5.531, Val loss 5.664\n",
      "Ep 1 (Step 000130): Train loss 5.633, Val loss 5.642\n",
      "Ep 1 (Step 000140): Train loss 5.504, Val loss 5.557\n",
      "Ep 1 (Step 000150): Train loss 5.477, Val loss 5.549\n",
      "Ep 1 (Step 000160): Train loss 5.508, Val loss 5.530\n",
      "Ep 1 (Step 000170): Train loss 5.389, Val loss 5.493\n",
      "Ep 1 (Step 000180): Train loss 5.409, Val loss 5.498\n",
      "Ep 1 (Step 000190): Train loss 5.413, Val loss 5.473\n",
      "Ep 1 (Step 000200): Train loss 5.371, Val loss 5.447\n",
      "Ep 1 (Step 000210): Train loss 5.313, Val loss 5.424\n",
      "Ep 1 (Step 000220): Train loss 5.237, Val loss 5.403\n",
      "Ep 1 (Step 000230): Train loss 5.278, Val loss 5.387\n",
      "Ep 1 (Step 000240): Train loss 5.284, Val loss 5.375\n",
      "Ep 1 (Step 000250): Train loss 5.188, Val loss 5.349\n",
      "Ep 1 (Step 000260): Train loss 5.299, Val loss 5.340\n",
      "Ep 1 (Step 000270): Train loss 5.274, Val loss 5.326\n",
      "Ep 1 (Step 000280): Train loss 5.212, Val loss 5.336\n",
      "Ep 1 (Step 000290): Train loss 5.265, Val loss 5.315\n",
      "Ep 1 (Step 000300): Train loss 5.221, Val loss 5.304\n",
      "Ep 1 (Step 000310): Train loss 5.164, Val loss 5.309\n",
      "Ep 1 (Step 000320): Train loss 5.246, Val loss 5.292\n",
      "Ep 1 (Step 000330): Train loss 5.090, Val loss 5.291\n",
      "Ep 1 (Step 000340): Train loss 5.148, Val loss 5.276\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2758\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.824, Val loss 8.822\n",
      "Ep 1 (Step 000010): Train loss 6.920, Val loss 6.864\n",
      "Ep 1 (Step 000020): Train loss 6.567, Val loss 6.446\n",
      "Ep 1 (Step 000030): Train loss 6.441, Val loss 6.444\n",
      "Ep 1 (Step 000040): Train loss 6.372, Val loss 6.327\n",
      "Ep 1 (Step 000050): Train loss 6.216, Val loss 6.183\n",
      "Ep 1 (Step 000060): Train loss 5.978, Val loss 6.038\n",
      "Ep 1 (Step 000070): Train loss 5.879, Val loss 5.947\n",
      "Ep 1 (Step 000080): Train loss 5.913, Val loss 5.871\n",
      "Ep 1 (Step 000090): Train loss 5.839, Val loss 5.785\n",
      "Ep 1 (Step 000100): Train loss 5.689, Val loss 5.731\n",
      "Ep 1 (Step 000110): Train loss 5.674, Val loss 5.693\n",
      "Ep 1 (Step 000120): Train loss 5.530, Val loss 5.650\n",
      "Ep 1 (Step 000130): Train loss 5.599, Val loss 5.625\n",
      "Ep 1 (Step 000140): Train loss 5.513, Val loss 5.580\n",
      "Ep 1 (Step 000150): Train loss 5.453, Val loss 5.544\n",
      "Ep 1 (Step 000160): Train loss 5.403, Val loss 5.512\n",
      "Ep 1 (Step 000170): Train loss 5.429, Val loss 5.492\n",
      "Ep 1 (Step 000180): Train loss 5.316, Val loss 5.459\n",
      "Ep 1 (Step 000190): Train loss 5.300, Val loss 5.451\n",
      "Ep 1 (Step 000200): Train loss 5.335, Val loss 5.418\n",
      "Ep 1 (Step 000210): Train loss 5.283, Val loss 5.407\n",
      "Ep 1 (Step 000220): Train loss 5.194, Val loss 5.401\n",
      "Ep 1 (Step 000230): Train loss 5.288, Val loss 5.390\n",
      "Ep 1 (Step 000240): Train loss 5.236, Val loss 5.360\n",
      "Ep 1 (Step 000250): Train loss 5.133, Val loss 5.347\n",
      "Ep 1 (Step 000260): Train loss 5.214, Val loss 5.333\n",
      "Ep 1 (Step 000270): Train loss 5.207, Val loss 5.314\n",
      "Ep 1 (Step 000280): Train loss 5.232, Val loss 5.313\n",
      "Ep 1 (Step 000290): Train loss 5.145, Val loss 5.297\n",
      "Ep 1 (Step 000300): Train loss 5.147, Val loss 5.286\n",
      "Ep 1 (Step 000310): Train loss 5.125, Val loss 5.275\n",
      "Ep 1 (Step 000320): Train loss 5.108, Val loss 5.275\n",
      "Ep 1 (Step 000330): Train loss 5.128, Val loss 5.262\n",
      "Ep 1 (Step 000340): Train loss 5.169, Val loss 5.260\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2599\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.808, Val loss 8.794\n",
      "Ep 1 (Step 000010): Train loss 6.793, Val loss 6.844\n",
      "Ep 1 (Step 000020): Train loss 6.497, Val loss 6.497\n",
      "Ep 1 (Step 000030): Train loss 6.494, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.379, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.198, Val loss 6.246\n",
      "Ep 1 (Step 000060): Train loss 6.052, Val loss 6.080\n",
      "Ep 1 (Step 000070): Train loss 5.937, Val loss 5.980\n",
      "Ep 1 (Step 000080): Train loss 5.781, Val loss 5.881\n",
      "Ep 1 (Step 000090): Train loss 5.865, Val loss 5.812\n",
      "Ep 1 (Step 000100): Train loss 5.739, Val loss 5.737\n",
      "Ep 1 (Step 000110): Train loss 5.639, Val loss 5.693\n",
      "Ep 1 (Step 000120): Train loss 5.665, Val loss 5.644\n",
      "Ep 1 (Step 000130): Train loss 5.566, Val loss 5.605\n",
      "Ep 1 (Step 000140): Train loss 5.523, Val loss 5.573\n",
      "Ep 1 (Step 000150): Train loss 5.490, Val loss 5.527\n",
      "Ep 1 (Step 000160): Train loss 5.538, Val loss 5.504\n",
      "Ep 1 (Step 000170): Train loss 5.393, Val loss 5.475\n",
      "Ep 1 (Step 000180): Train loss 5.338, Val loss 5.450\n",
      "Ep 1 (Step 000190): Train loss 5.394, Val loss 5.431\n",
      "Ep 1 (Step 000200): Train loss 5.275, Val loss 5.413\n",
      "Ep 1 (Step 000210): Train loss 5.264, Val loss 5.396\n",
      "Ep 1 (Step 000220): Train loss 5.289, Val loss 5.379\n",
      "Ep 1 (Step 000230): Train loss 5.272, Val loss 5.351\n",
      "Ep 1 (Step 000240): Train loss 5.304, Val loss 5.332\n",
      "Ep 1 (Step 000250): Train loss 5.285, Val loss 5.327\n",
      "Ep 1 (Step 000260): Train loss 5.215, Val loss 5.328\n",
      "Ep 1 (Step 000270): Train loss 5.096, Val loss 5.307\n",
      "Ep 1 (Step 000280): Train loss 5.235, Val loss 5.292\n",
      "Ep 1 (Step 000290): Train loss 5.213, Val loss 5.283\n",
      "Ep 1 (Step 000300): Train loss 5.151, Val loss 5.287\n",
      "Ep 1 (Step 000310): Train loss 5.181, Val loss 5.261\n",
      "Ep 1 (Step 000320): Train loss 5.106, Val loss 5.253\n",
      "Ep 1 (Step 000330): Train loss 5.092, Val loss 5.256\n",
      "Ep 1 (Step 000340): Train loss 5.062, Val loss 5.255\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2553\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.780, Val loss 8.769\n",
      "Ep 1 (Step 000010): Train loss 6.904, Val loss 6.870\n",
      "Ep 1 (Step 000020): Train loss 6.535, Val loss 6.500\n",
      "Ep 1 (Step 000030): Train loss 6.489, Val loss 6.451\n",
      "Ep 1 (Step 000040): Train loss 6.400, Val loss 6.382\n",
      "Ep 1 (Step 000050): Train loss 6.244, Val loss 6.230\n",
      "Ep 1 (Step 000060): Train loss 6.081, Val loss 6.095\n",
      "Ep 1 (Step 000070): Train loss 5.943, Val loss 5.986\n",
      "Ep 1 (Step 000080): Train loss 5.875, Val loss 5.897\n",
      "Ep 1 (Step 000090): Train loss 5.823, Val loss 5.834\n",
      "Ep 1 (Step 000100): Train loss 5.752, Val loss 5.756\n",
      "Ep 1 (Step 000110): Train loss 5.735, Val loss 5.698\n",
      "Ep 1 (Step 000120): Train loss 5.589, Val loss 5.654\n",
      "Ep 1 (Step 000130): Train loss 5.601, Val loss 5.615\n",
      "Ep 1 (Step 000140): Train loss 5.525, Val loss 5.598\n",
      "Ep 1 (Step 000150): Train loss 5.437, Val loss 5.556\n",
      "Ep 1 (Step 000160): Train loss 5.398, Val loss 5.511\n",
      "Ep 1 (Step 000170): Train loss 5.403, Val loss 5.498\n",
      "Ep 1 (Step 000180): Train loss 5.414, Val loss 5.476\n",
      "Ep 1 (Step 000190): Train loss 5.329, Val loss 5.456\n",
      "Ep 1 (Step 000200): Train loss 5.361, Val loss 5.420\n",
      "Ep 1 (Step 000210): Train loss 5.367, Val loss 5.409\n",
      "Ep 1 (Step 000220): Train loss 5.290, Val loss 5.395\n",
      "Ep 1 (Step 000230): Train loss 5.179, Val loss 5.367\n",
      "Ep 1 (Step 000240): Train loss 5.313, Val loss 5.369\n",
      "Ep 1 (Step 000250): Train loss 5.330, Val loss 5.340\n",
      "Ep 1 (Step 000260): Train loss 5.197, Val loss 5.324\n",
      "Ep 1 (Step 000270): Train loss 5.236, Val loss 5.311\n",
      "Ep 1 (Step 000280): Train loss 5.147, Val loss 5.307\n",
      "Ep 1 (Step 000290): Train loss 5.195, Val loss 5.284\n",
      "Ep 1 (Step 000300): Train loss 5.163, Val loss 5.278\n",
      "Ep 1 (Step 000310): Train loss 5.117, Val loss 5.284\n",
      "Ep 1 (Step 000320): Train loss 5.150, Val loss 5.270\n",
      "Ep 1 (Step 000330): Train loss 5.087, Val loss 5.253\n",
      "Ep 1 (Step 000340): Train loss 5.149, Val loss 5.251\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2505\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.016, Val loss 9.020\n",
      "Ep 1 (Step 000010): Train loss 7.396, Val loss 7.444\n",
      "Ep 1 (Step 000020): Train loss 6.836, Val loss 6.783\n",
      "Ep 1 (Step 000030): Train loss 6.512, Val loss 6.465\n",
      "Ep 1 (Step 000040): Train loss 6.415, Val loss 6.379\n",
      "Ep 1 (Step 000050): Train loss 6.319, Val loss 6.318\n",
      "Ep 1 (Step 000060): Train loss 6.261, Val loss 6.223\n",
      "Ep 1 (Step 000070): Train loss 6.120, Val loss 6.131\n",
      "Ep 1 (Step 000080): Train loss 6.035, Val loss 6.038\n",
      "Ep 1 (Step 000090): Train loss 5.938, Val loss 5.961\n",
      "Ep 1 (Step 000100): Train loss 5.853, Val loss 5.892\n",
      "Ep 1 (Step 000110): Train loss 5.788, Val loss 5.857\n",
      "Ep 1 (Step 000120): Train loss 5.806, Val loss 5.798\n",
      "Ep 1 (Step 000130): Train loss 5.739, Val loss 5.753\n",
      "Ep 1 (Step 000140): Train loss 5.611, Val loss 5.716\n",
      "Ep 1 (Step 000150): Train loss 5.611, Val loss 5.685\n",
      "Ep 1 (Step 000160): Train loss 5.524, Val loss 5.655\n",
      "Ep 1 (Step 000170): Train loss 5.633, Val loss 5.634\n",
      "Ep 1 (Step 000180): Train loss 5.554, Val loss 5.608\n",
      "Ep 1 (Step 000190): Train loss 5.453, Val loss 5.591\n",
      "Ep 1 (Step 000200): Train loss 5.508, Val loss 5.562\n",
      "Ep 1 (Step 000210): Train loss 5.392, Val loss 5.531\n",
      "Ep 1 (Step 000220): Train loss 5.500, Val loss 5.506\n",
      "Ep 1 (Step 000230): Train loss 5.414, Val loss 5.473\n",
      "Ep 1 (Step 000240): Train loss 5.351, Val loss 5.453\n",
      "Ep 1 (Step 000250): Train loss 5.424, Val loss 5.437\n",
      "Ep 1 (Step 000260): Train loss 5.268, Val loss 5.419\n",
      "Ep 1 (Step 000270): Train loss 5.252, Val loss 5.407\n",
      "Ep 1 (Step 000280): Train loss 5.278, Val loss 5.399\n",
      "Ep 1 (Step 000290): Train loss 5.316, Val loss 5.387\n",
      "Ep 1 (Step 000300): Train loss 5.271, Val loss 5.373\n",
      "Ep 1 (Step 000310): Train loss 5.294, Val loss 5.357\n",
      "Ep 1 (Step 000320): Train loss 5.269, Val loss 5.348\n",
      "Ep 1 (Step 000330): Train loss 5.194, Val loss 5.340\n",
      "Ep 1 (Step 000340): Train loss 5.197, Val loss 5.332\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3321\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.089, Val loss 9.078\n",
      "Ep 1 (Step 000010): Train loss 7.527, Val loss 7.519\n",
      "Ep 1 (Step 000020): Train loss 6.827, Val loss 6.832\n",
      "Ep 1 (Step 000030): Train loss 6.505, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.408, Val loss 6.386\n",
      "Ep 1 (Step 000050): Train loss 6.345, Val loss 6.323\n",
      "Ep 1 (Step 000060): Train loss 6.121, Val loss 6.200\n",
      "Ep 1 (Step 000070): Train loss 6.077, Val loss 6.089\n",
      "Ep 1 (Step 000080): Train loss 6.017, Val loss 6.023\n",
      "Ep 1 (Step 000090): Train loss 5.934, Val loss 5.971\n",
      "Ep 1 (Step 000100): Train loss 5.813, Val loss 5.907\n",
      "Ep 1 (Step 000110): Train loss 5.697, Val loss 5.864\n",
      "Ep 1 (Step 000120): Train loss 5.688, Val loss 5.812\n",
      "Ep 1 (Step 000130): Train loss 5.710, Val loss 5.758\n",
      "Ep 1 (Step 000140): Train loss 5.645, Val loss 5.716\n",
      "Ep 1 (Step 000150): Train loss 5.589, Val loss 5.686\n",
      "Ep 1 (Step 000160): Train loss 5.658, Val loss 5.647\n",
      "Ep 1 (Step 000170): Train loss 5.502, Val loss 5.629\n",
      "Ep 1 (Step 000180): Train loss 5.525, Val loss 5.589\n",
      "Ep 1 (Step 000190): Train loss 5.408, Val loss 5.560\n",
      "Ep 1 (Step 000200): Train loss 5.516, Val loss 5.551\n",
      "Ep 1 (Step 000210): Train loss 5.367, Val loss 5.527\n",
      "Ep 1 (Step 000220): Train loss 5.447, Val loss 5.504\n",
      "Ep 1 (Step 000230): Train loss 5.370, Val loss 5.475\n",
      "Ep 1 (Step 000240): Train loss 5.377, Val loss 5.456\n",
      "Ep 1 (Step 000250): Train loss 5.270, Val loss 5.443\n",
      "Ep 1 (Step 000260): Train loss 5.321, Val loss 5.431\n",
      "Ep 1 (Step 000270): Train loss 5.209, Val loss 5.413\n",
      "Ep 1 (Step 000280): Train loss 5.348, Val loss 5.405\n",
      "Ep 1 (Step 000290): Train loss 5.261, Val loss 5.376\n",
      "Ep 1 (Step 000300): Train loss 5.273, Val loss 5.376\n",
      "Ep 1 (Step 000310): Train loss 5.249, Val loss 5.356\n",
      "Ep 1 (Step 000320): Train loss 5.197, Val loss 5.347\n",
      "Ep 1 (Step 000330): Train loss 5.253, Val loss 5.331\n",
      "Ep 1 (Step 000340): Train loss 5.165, Val loss 5.329\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3291\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.063, Val loss 9.036\n",
      "Ep 1 (Step 000010): Train loss 7.387, Val loss 7.392\n",
      "Ep 1 (Step 000020): Train loss 6.760, Val loss 6.733\n",
      "Ep 1 (Step 000030): Train loss 6.520, Val loss 6.449\n",
      "Ep 1 (Step 000040): Train loss 6.378, Val loss 6.387\n",
      "Ep 1 (Step 000050): Train loss 6.344, Val loss 6.333\n",
      "Ep 1 (Step 000060): Train loss 6.256, Val loss 6.212\n",
      "Ep 1 (Step 000070): Train loss 6.125, Val loss 6.101\n",
      "Ep 1 (Step 000080): Train loss 6.089, Val loss 6.029\n",
      "Ep 1 (Step 000090): Train loss 5.872, Val loss 5.958\n",
      "Ep 1 (Step 000100): Train loss 5.838, Val loss 5.874\n",
      "Ep 1 (Step 000110): Train loss 5.803, Val loss 5.854\n",
      "Ep 1 (Step 000120): Train loss 5.808, Val loss 5.787\n",
      "Ep 1 (Step 000130): Train loss 5.736, Val loss 5.760\n",
      "Ep 1 (Step 000140): Train loss 5.617, Val loss 5.721\n",
      "Ep 1 (Step 000150): Train loss 5.627, Val loss 5.679\n",
      "Ep 1 (Step 000160): Train loss 5.629, Val loss 5.647\n",
      "Ep 1 (Step 000170): Train loss 5.514, Val loss 5.613\n",
      "Ep 1 (Step 000180): Train loss 5.462, Val loss 5.592\n",
      "Ep 1 (Step 000190): Train loss 5.535, Val loss 5.562\n",
      "Ep 1 (Step 000200): Train loss 5.436, Val loss 5.545\n",
      "Ep 1 (Step 000210): Train loss 5.433, Val loss 5.513\n",
      "Ep 1 (Step 000220): Train loss 5.339, Val loss 5.500\n",
      "Ep 1 (Step 000230): Train loss 5.303, Val loss 5.470\n",
      "Ep 1 (Step 000240): Train loss 5.359, Val loss 5.460\n",
      "Ep 1 (Step 000250): Train loss 5.350, Val loss 5.439\n",
      "Ep 1 (Step 000260): Train loss 5.333, Val loss 5.418\n",
      "Ep 1 (Step 000270): Train loss 5.299, Val loss 5.406\n",
      "Ep 1 (Step 000280): Train loss 5.254, Val loss 5.388\n",
      "Ep 1 (Step 000290): Train loss 5.293, Val loss 5.372\n",
      "Ep 1 (Step 000300): Train loss 5.253, Val loss 5.360\n",
      "Ep 1 (Step 000310): Train loss 5.226, Val loss 5.339\n",
      "Ep 1 (Step 000320): Train loss 5.217, Val loss 5.338\n",
      "Ep 1 (Step 000330): Train loss 5.169, Val loss 5.323\n",
      "Ep 1 (Step 000340): Train loss 5.153, Val loss 5.336\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3357\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.056, Val loss 9.046\n",
      "Ep 1 (Step 000010): Train loss 7.449, Val loss 7.403\n",
      "Ep 1 (Step 000020): Train loss 6.819, Val loss 6.755\n",
      "Ep 1 (Step 000030): Train loss 6.491, Val loss 6.456\n",
      "Ep 1 (Step 000040): Train loss 6.402, Val loss 6.379\n",
      "Ep 1 (Step 000050): Train loss 6.336, Val loss 6.333\n",
      "Ep 1 (Step 000060): Train loss 6.284, Val loss 6.244\n",
      "Ep 1 (Step 000070): Train loss 6.134, Val loss 6.136\n",
      "Ep 1 (Step 000080): Train loss 6.037, Val loss 6.040\n",
      "Ep 1 (Step 000090): Train loss 6.007, Val loss 5.988\n",
      "Ep 1 (Step 000100): Train loss 5.886, Val loss 5.911\n",
      "Ep 1 (Step 000110): Train loss 5.800, Val loss 5.845\n",
      "Ep 1 (Step 000120): Train loss 5.787, Val loss 5.799\n",
      "Ep 1 (Step 000130): Train loss 5.677, Val loss 5.766\n",
      "Ep 1 (Step 000140): Train loss 5.742, Val loss 5.729\n",
      "Ep 1 (Step 000150): Train loss 5.664, Val loss 5.695\n",
      "Ep 1 (Step 000160): Train loss 5.596, Val loss 5.651\n",
      "Ep 1 (Step 000170): Train loss 5.554, Val loss 5.619\n",
      "Ep 1 (Step 000180): Train loss 5.463, Val loss 5.590\n",
      "Ep 1 (Step 000190): Train loss 5.472, Val loss 5.556\n",
      "Ep 1 (Step 000200): Train loss 5.463, Val loss 5.539\n",
      "Ep 1 (Step 000210): Train loss 5.340, Val loss 5.523\n",
      "Ep 1 (Step 000220): Train loss 5.431, Val loss 5.511\n",
      "Ep 1 (Step 000230): Train loss 5.396, Val loss 5.481\n",
      "Ep 1 (Step 000240): Train loss 5.340, Val loss 5.465\n",
      "Ep 1 (Step 000250): Train loss 5.380, Val loss 5.438\n",
      "Ep 1 (Step 000260): Train loss 5.249, Val loss 5.416\n",
      "Ep 1 (Step 000270): Train loss 5.362, Val loss 5.397\n",
      "Ep 1 (Step 000280): Train loss 5.295, Val loss 5.385\n",
      "Ep 1 (Step 000290): Train loss 5.249, Val loss 5.373\n",
      "Ep 1 (Step 000300): Train loss 5.222, Val loss 5.345\n",
      "Ep 1 (Step 000310): Train loss 5.205, Val loss 5.330\n",
      "Ep 1 (Step 000320): Train loss 5.273, Val loss 5.319\n",
      "Ep 1 (Step 000330): Train loss 5.168, Val loss 5.331\n",
      "Ep 1 (Step 000340): Train loss 5.265, Val loss 5.320\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3198\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.055, Val loss 9.036\n",
      "Ep 1 (Step 000010): Train loss 7.479, Val loss 7.423\n",
      "Ep 1 (Step 000020): Train loss 6.785, Val loss 6.772\n",
      "Ep 1 (Step 000030): Train loss 6.496, Val loss 6.465\n",
      "Ep 1 (Step 000040): Train loss 6.428, Val loss 6.386\n",
      "Ep 1 (Step 000050): Train loss 6.343, Val loss 6.343\n",
      "Ep 1 (Step 000060): Train loss 6.258, Val loss 6.274\n",
      "Ep 1 (Step 000070): Train loss 6.084, Val loss 6.147\n",
      "Ep 1 (Step 000080): Train loss 5.989, Val loss 6.050\n",
      "Ep 1 (Step 000090): Train loss 5.955, Val loss 5.959\n",
      "Ep 1 (Step 000100): Train loss 5.820, Val loss 5.906\n",
      "Ep 1 (Step 000110): Train loss 5.757, Val loss 5.839\n",
      "Ep 1 (Step 000120): Train loss 5.742, Val loss 5.796\n",
      "Ep 1 (Step 000130): Train loss 5.693, Val loss 5.750\n",
      "Ep 1 (Step 000140): Train loss 5.674, Val loss 5.713\n",
      "Ep 1 (Step 000150): Train loss 5.640, Val loss 5.666\n",
      "Ep 1 (Step 000160): Train loss 5.536, Val loss 5.632\n",
      "Ep 1 (Step 000170): Train loss 5.546, Val loss 5.590\n",
      "Ep 1 (Step 000180): Train loss 5.529, Val loss 5.573\n",
      "Ep 1 (Step 000190): Train loss 5.523, Val loss 5.555\n",
      "Ep 1 (Step 000200): Train loss 5.507, Val loss 5.525\n",
      "Ep 1 (Step 000210): Train loss 5.358, Val loss 5.493\n",
      "Ep 1 (Step 000220): Train loss 5.390, Val loss 5.474\n",
      "Ep 1 (Step 000230): Train loss 5.406, Val loss 5.463\n",
      "Ep 1 (Step 000240): Train loss 5.318, Val loss 5.440\n",
      "Ep 1 (Step 000250): Train loss 5.279, Val loss 5.430\n",
      "Ep 1 (Step 000260): Train loss 5.340, Val loss 5.406\n",
      "Ep 1 (Step 000270): Train loss 5.261, Val loss 5.397\n",
      "Ep 1 (Step 000280): Train loss 5.242, Val loss 5.379\n",
      "Ep 1 (Step 000290): Train loss 5.335, Val loss 5.365\n",
      "Ep 1 (Step 000300): Train loss 5.235, Val loss 5.344\n",
      "Ep 1 (Step 000310): Train loss 5.189, Val loss 5.335\n",
      "Ep 1 (Step 000320): Train loss 5.233, Val loss 5.318\n",
      "Ep 1 (Step 000330): Train loss 5.181, Val loss 5.309\n",
      "Ep 1 (Step 000340): Train loss 5.235, Val loss 5.306\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3057\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.046, Val loss 9.050\n",
      "Ep 1 (Step 000010): Train loss 7.462, Val loss 7.489\n",
      "Ep 1 (Step 000020): Train loss 6.816, Val loss 6.807\n",
      "Ep 1 (Step 000030): Train loss 6.456, Val loss 6.484\n",
      "Ep 1 (Step 000040): Train loss 6.395, Val loss 6.382\n",
      "Ep 1 (Step 000050): Train loss 6.395, Val loss 6.333\n",
      "Ep 1 (Step 000060): Train loss 6.248, Val loss 6.258\n",
      "Ep 1 (Step 000070): Train loss 6.119, Val loss 6.146\n",
      "Ep 1 (Step 000080): Train loss 6.022, Val loss 6.052\n",
      "Ep 1 (Step 000090): Train loss 5.979, Val loss 5.965\n",
      "Ep 1 (Step 000100): Train loss 5.892, Val loss 5.907\n",
      "Ep 1 (Step 000110): Train loss 5.806, Val loss 5.846\n",
      "Ep 1 (Step 000120): Train loss 5.831, Val loss 5.800\n",
      "Ep 1 (Step 000130): Train loss 5.678, Val loss 5.765\n",
      "Ep 1 (Step 000140): Train loss 5.698, Val loss 5.711\n",
      "Ep 1 (Step 000150): Train loss 5.583, Val loss 5.668\n",
      "Ep 1 (Step 000160): Train loss 5.599, Val loss 5.634\n",
      "Ep 1 (Step 000170): Train loss 5.538, Val loss 5.620\n",
      "Ep 1 (Step 000180): Train loss 5.508, Val loss 5.580\n",
      "Ep 1 (Step 000190): Train loss 5.414, Val loss 5.540\n",
      "Ep 1 (Step 000200): Train loss 5.466, Val loss 5.521\n",
      "Ep 1 (Step 000210): Train loss 5.457, Val loss 5.500\n",
      "Ep 1 (Step 000220): Train loss 5.391, Val loss 5.477\n",
      "Ep 1 (Step 000230): Train loss 5.393, Val loss 5.455\n",
      "Ep 1 (Step 000240): Train loss 5.376, Val loss 5.442\n",
      "Ep 1 (Step 000250): Train loss 5.238, Val loss 5.429\n",
      "Ep 1 (Step 000260): Train loss 5.390, Val loss 5.398\n",
      "Ep 1 (Step 000270): Train loss 5.211, Val loss 5.388\n",
      "Ep 1 (Step 000280): Train loss 5.220, Val loss 5.365\n",
      "Ep 1 (Step 000290): Train loss 5.292, Val loss 5.351\n",
      "Ep 1 (Step 000300): Train loss 5.221, Val loss 5.353\n",
      "Ep 1 (Step 000310): Train loss 5.226, Val loss 5.335\n",
      "Ep 1 (Step 000320): Train loss 5.244, Val loss 5.320\n",
      "Ep 1 (Step 000330): Train loss 5.209, Val loss 5.313\n",
      "Ep 1 (Step 000340): Train loss 5.150, Val loss 5.301\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3014\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.821, Val loss 8.807\n",
      "Ep 1 (Step 000010): Train loss 6.938, Val loss 6.876\n",
      "Ep 1 (Step 000020): Train loss 6.451, Val loss 6.452\n",
      "Ep 1 (Step 000030): Train loss 6.388, Val loss 6.414\n",
      "Ep 1 (Step 000040): Train loss 6.306, Val loss 6.305\n",
      "Ep 1 (Step 000050): Train loss 6.119, Val loss 6.138\n",
      "Ep 1 (Step 000060): Train loss 5.983, Val loss 6.029\n",
      "Ep 1 (Step 000070): Train loss 5.870, Val loss 5.926\n",
      "Ep 1 (Step 000080): Train loss 5.789, Val loss 5.829\n",
      "Ep 1 (Step 000090): Train loss 5.668, Val loss 5.763\n",
      "Ep 1 (Step 000100): Train loss 5.618, Val loss 5.710\n",
      "Ep 1 (Step 000110): Train loss 5.609, Val loss 5.673\n",
      "Ep 1 (Step 000120): Train loss 5.493, Val loss 5.628\n",
      "Ep 1 (Step 000130): Train loss 5.530, Val loss 5.584\n",
      "Ep 1 (Step 000140): Train loss 5.499, Val loss 5.528\n",
      "Ep 1 (Step 000150): Train loss 5.375, Val loss 5.506\n",
      "Ep 1 (Step 000160): Train loss 5.341, Val loss 5.480\n",
      "Ep 1 (Step 000170): Train loss 5.453, Val loss 5.457\n",
      "Ep 1 (Step 000180): Train loss 5.329, Val loss 5.437\n",
      "Ep 1 (Step 000190): Train loss 5.415, Val loss 5.431\n",
      "Ep 1 (Step 000200): Train loss 5.362, Val loss 5.426\n",
      "Ep 1 (Step 000210): Train loss 5.302, Val loss 5.381\n",
      "Ep 1 (Step 000220): Train loss 5.234, Val loss 5.370\n",
      "Ep 1 (Step 000230): Train loss 5.231, Val loss 5.347\n",
      "Ep 1 (Step 000240): Train loss 5.213, Val loss 5.370\n",
      "Ep 1 (Step 000250): Train loss 5.228, Val loss 5.322\n",
      "Ep 1 (Step 000260): Train loss 5.252, Val loss 5.315\n",
      "Ep 1 (Step 000270): Train loss 5.241, Val loss 5.306\n",
      "Ep 1 (Step 000280): Train loss 5.148, Val loss 5.288\n",
      "Ep 1 (Step 000290): Train loss 5.104, Val loss 5.267\n",
      "Ep 1 (Step 000300): Train loss 5.078, Val loss 5.263\n",
      "Ep 1 (Step 000310): Train loss 5.290, Val loss 5.256\n",
      "Ep 1 (Step 000320): Train loss 5.167, Val loss 5.262\n",
      "Ep 1 (Step 000330): Train loss 5.073, Val loss 5.233\n",
      "Ep 1 (Step 000340): Train loss 5.094, Val loss 5.225\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2246\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.796, Val loss 8.802\n",
      "Ep 1 (Step 000010): Train loss 6.906, Val loss 6.869\n",
      "Ep 1 (Step 000020): Train loss 6.518, Val loss 6.456\n",
      "Ep 1 (Step 000030): Train loss 6.437, Val loss 6.431\n",
      "Ep 1 (Step 000040): Train loss 6.261, Val loss 6.278\n",
      "Ep 1 (Step 000050): Train loss 6.157, Val loss 6.118\n",
      "Ep 1 (Step 000060): Train loss 5.952, Val loss 5.994\n",
      "Ep 1 (Step 000070): Train loss 5.818, Val loss 5.896\n",
      "Ep 1 (Step 000080): Train loss 5.796, Val loss 5.814\n",
      "Ep 1 (Step 000090): Train loss 5.736, Val loss 5.762\n",
      "Ep 1 (Step 000100): Train loss 5.654, Val loss 5.694\n",
      "Ep 1 (Step 000110): Train loss 5.561, Val loss 5.631\n",
      "Ep 1 (Step 000120): Train loss 5.507, Val loss 5.609\n",
      "Ep 1 (Step 000130): Train loss 5.510, Val loss 5.535\n",
      "Ep 1 (Step 000140): Train loss 5.445, Val loss 5.502\n",
      "Ep 1 (Step 000150): Train loss 5.353, Val loss 5.486\n",
      "Ep 1 (Step 000160): Train loss 5.298, Val loss 5.465\n",
      "Ep 1 (Step 000170): Train loss 5.329, Val loss 5.423\n",
      "Ep 1 (Step 000180): Train loss 5.285, Val loss 5.430\n",
      "Ep 1 (Step 000190): Train loss 5.378, Val loss 5.394\n",
      "Ep 1 (Step 000200): Train loss 5.284, Val loss 5.378\n",
      "Ep 1 (Step 000210): Train loss 5.332, Val loss 5.386\n",
      "Ep 1 (Step 000220): Train loss 5.193, Val loss 5.376\n",
      "Ep 1 (Step 000230): Train loss 5.214, Val loss 5.357\n",
      "Ep 1 (Step 000240): Train loss 5.223, Val loss 5.332\n",
      "Ep 1 (Step 000250): Train loss 5.183, Val loss 5.315\n",
      "Ep 1 (Step 000260): Train loss 5.109, Val loss 5.314\n",
      "Ep 1 (Step 000270): Train loss 5.122, Val loss 5.298\n",
      "Ep 1 (Step 000280): Train loss 5.134, Val loss 5.315\n",
      "Ep 1 (Step 000290): Train loss 5.114, Val loss 5.272\n",
      "Ep 1 (Step 000300): Train loss 5.058, Val loss 5.257\n",
      "Ep 1 (Step 000310): Train loss 5.156, Val loss 5.249\n",
      "Ep 1 (Step 000320): Train loss 5.102, Val loss 5.239\n",
      "Ep 1 (Step 000330): Train loss 5.048, Val loss 5.241\n",
      "Ep 1 (Step 000340): Train loss 5.088, Val loss 5.226\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2263\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.812, Val loss 8.778\n",
      "Ep 1 (Step 000010): Train loss 6.858, Val loss 6.837\n",
      "Ep 1 (Step 000020): Train loss 6.466, Val loss 6.464\n",
      "Ep 1 (Step 000030): Train loss 6.460, Val loss 6.425\n",
      "Ep 1 (Step 000040): Train loss 6.254, Val loss 6.259\n",
      "Ep 1 (Step 000050): Train loss 6.076, Val loss 6.096\n",
      "Ep 1 (Step 000060): Train loss 6.030, Val loss 5.970\n",
      "Ep 1 (Step 000070): Train loss 5.985, Val loss 5.925\n",
      "Ep 1 (Step 000080): Train loss 5.835, Val loss 5.798\n",
      "Ep 1 (Step 000090): Train loss 5.656, Val loss 5.753\n",
      "Ep 1 (Step 000100): Train loss 5.615, Val loss 5.708\n",
      "Ep 1 (Step 000110): Train loss 5.565, Val loss 5.643\n",
      "Ep 1 (Step 000120): Train loss 5.506, Val loss 5.588\n",
      "Ep 1 (Step 000130): Train loss 5.435, Val loss 5.567\n",
      "Ep 1 (Step 000140): Train loss 5.474, Val loss 5.535\n",
      "Ep 1 (Step 000150): Train loss 5.432, Val loss 5.505\n",
      "Ep 1 (Step 000160): Train loss 5.434, Val loss 5.477\n",
      "Ep 1 (Step 000170): Train loss 5.368, Val loss 5.475\n",
      "Ep 1 (Step 000180): Train loss 5.293, Val loss 5.440\n",
      "Ep 1 (Step 000190): Train loss 5.277, Val loss 5.402\n",
      "Ep 1 (Step 000200): Train loss 5.292, Val loss 5.397\n",
      "Ep 1 (Step 000210): Train loss 5.291, Val loss 5.406\n",
      "Ep 1 (Step 000220): Train loss 5.216, Val loss 5.374\n",
      "Ep 1 (Step 000230): Train loss 5.293, Val loss 5.348\n",
      "Ep 1 (Step 000240): Train loss 5.252, Val loss 5.370\n",
      "Ep 1 (Step 000250): Train loss 5.264, Val loss 5.339\n",
      "Ep 1 (Step 000260): Train loss 5.201, Val loss 5.321\n",
      "Ep 1 (Step 000270): Train loss 5.072, Val loss 5.309\n",
      "Ep 1 (Step 000280): Train loss 5.236, Val loss 5.281\n",
      "Ep 1 (Step 000290): Train loss 5.159, Val loss 5.271\n",
      "Ep 1 (Step 000300): Train loss 5.112, Val loss 5.249\n",
      "Ep 1 (Step 000310): Train loss 5.063, Val loss 5.242\n",
      "Ep 1 (Step 000320): Train loss 5.114, Val loss 5.233\n",
      "Ep 1 (Step 000330): Train loss 5.086, Val loss 5.226\n",
      "Ep 1 (Step 000340): Train loss 5.053, Val loss 5.219\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2188\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.784, Val loss 8.738\n",
      "Ep 1 (Step 000010): Train loss 6.868, Val loss 6.819\n",
      "Ep 1 (Step 000020): Train loss 6.456, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.434, Val loss 6.435\n",
      "Ep 1 (Step 000040): Train loss 6.295, Val loss 6.267\n",
      "Ep 1 (Step 000050): Train loss 6.139, Val loss 6.126\n",
      "Ep 1 (Step 000060): Train loss 5.922, Val loss 5.997\n",
      "Ep 1 (Step 000070): Train loss 5.863, Val loss 5.873\n",
      "Ep 1 (Step 000080): Train loss 5.707, Val loss 5.807\n",
      "Ep 1 (Step 000090): Train loss 5.721, Val loss 5.749\n",
      "Ep 1 (Step 000100): Train loss 5.611, Val loss 5.695\n",
      "Ep 1 (Step 000110): Train loss 5.525, Val loss 5.647\n",
      "Ep 1 (Step 000120): Train loss 5.516, Val loss 5.601\n",
      "Ep 1 (Step 000130): Train loss 5.504, Val loss 5.550\n",
      "Ep 1 (Step 000140): Train loss 5.400, Val loss 5.515\n",
      "Ep 1 (Step 000150): Train loss 5.431, Val loss 5.491\n",
      "Ep 1 (Step 000160): Train loss 5.322, Val loss 5.452\n",
      "Ep 1 (Step 000170): Train loss 5.285, Val loss 5.452\n",
      "Ep 1 (Step 000180): Train loss 5.278, Val loss 5.407\n",
      "Ep 1 (Step 000190): Train loss 5.280, Val loss 5.373\n",
      "Ep 1 (Step 000200): Train loss 5.361, Val loss 5.359\n",
      "Ep 1 (Step 000210): Train loss 5.157, Val loss 5.349\n",
      "Ep 1 (Step 000220): Train loss 5.247, Val loss 5.339\n",
      "Ep 1 (Step 000230): Train loss 5.138, Val loss 5.320\n",
      "Ep 1 (Step 000240): Train loss 5.211, Val loss 5.304\n",
      "Ep 1 (Step 000250): Train loss 5.106, Val loss 5.281\n",
      "Ep 1 (Step 000260): Train loss 5.137, Val loss 5.260\n",
      "Ep 1 (Step 000270): Train loss 5.218, Val loss 5.266\n",
      "Ep 1 (Step 000280): Train loss 5.024, Val loss 5.251\n",
      "Ep 1 (Step 000290): Train loss 5.130, Val loss 5.244\n",
      "Ep 1 (Step 000300): Train loss 5.099, Val loss 5.242\n",
      "Ep 1 (Step 000310): Train loss 5.073, Val loss 5.214\n",
      "Ep 1 (Step 000320): Train loss 5.042, Val loss 5.217\n",
      "Ep 1 (Step 000330): Train loss 5.081, Val loss 5.212\n",
      "Ep 1 (Step 000340): Train loss 4.994, Val loss 5.186\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1858\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.852, Val loss 8.825\n",
      "Ep 1 (Step 000010): Train loss 6.883, Val loss 6.847\n",
      "Ep 1 (Step 000020): Train loss 6.462, Val loss 6.459\n",
      "Ep 1 (Step 000030): Train loss 6.458, Val loss 6.443\n",
      "Ep 1 (Step 000040): Train loss 6.302, Val loss 6.303\n",
      "Ep 1 (Step 000050): Train loss 6.109, Val loss 6.116\n",
      "Ep 1 (Step 000060): Train loss 5.965, Val loss 6.018\n",
      "Ep 1 (Step 000070): Train loss 5.836, Val loss 5.892\n",
      "Ep 1 (Step 000080): Train loss 5.776, Val loss 5.810\n",
      "Ep 1 (Step 000090): Train loss 5.679, Val loss 5.726\n",
      "Ep 1 (Step 000100): Train loss 5.586, Val loss 5.665\n",
      "Ep 1 (Step 000110): Train loss 5.589, Val loss 5.608\n",
      "Ep 1 (Step 000120): Train loss 5.505, Val loss 5.573\n",
      "Ep 1 (Step 000130): Train loss 5.493, Val loss 5.532\n",
      "Ep 1 (Step 000140): Train loss 5.452, Val loss 5.490\n",
      "Ep 1 (Step 000150): Train loss 5.372, Val loss 5.461\n",
      "Ep 1 (Step 000160): Train loss 5.318, Val loss 5.431\n",
      "Ep 1 (Step 000170): Train loss 5.358, Val loss 5.424\n",
      "Ep 1 (Step 000180): Train loss 5.286, Val loss 5.414\n",
      "Ep 1 (Step 000190): Train loss 5.229, Val loss 5.375\n",
      "Ep 1 (Step 000200): Train loss 5.328, Val loss 5.354\n",
      "Ep 1 (Step 000210): Train loss 5.189, Val loss 5.344\n",
      "Ep 1 (Step 000220): Train loss 5.168, Val loss 5.321\n",
      "Ep 1 (Step 000230): Train loss 5.262, Val loss 5.308\n",
      "Ep 1 (Step 000240): Train loss 5.219, Val loss 5.309\n",
      "Ep 1 (Step 000250): Train loss 5.235, Val loss 5.299\n",
      "Ep 1 (Step 000260): Train loss 5.156, Val loss 5.271\n",
      "Ep 1 (Step 000270): Train loss 5.151, Val loss 5.260\n",
      "Ep 1 (Step 000280): Train loss 5.154, Val loss 5.241\n",
      "Ep 1 (Step 000290): Train loss 5.108, Val loss 5.226\n",
      "Ep 1 (Step 000300): Train loss 5.089, Val loss 5.230\n",
      "Ep 1 (Step 000310): Train loss 5.051, Val loss 5.216\n",
      "Ep 1 (Step 000320): Train loss 5.014, Val loss 5.205\n",
      "Ep 1 (Step 000330): Train loss 5.036, Val loss 5.192\n",
      "Ep 1 (Step 000340): Train loss 4.953, Val loss 5.193\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1932\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.810, Val loss 8.792\n",
      "Ep 1 (Step 000010): Train loss 6.836, Val loss 6.816\n",
      "Ep 1 (Step 000020): Train loss 6.457, Val loss 6.453\n",
      "Ep 1 (Step 000030): Train loss 6.398, Val loss 6.416\n",
      "Ep 1 (Step 000040): Train loss 6.248, Val loss 6.280\n",
      "Ep 1 (Step 000050): Train loss 6.043, Val loss 6.098\n",
      "Ep 1 (Step 000060): Train loss 5.940, Val loss 5.988\n",
      "Ep 1 (Step 000070): Train loss 5.831, Val loss 5.889\n",
      "Ep 1 (Step 000080): Train loss 5.732, Val loss 5.825\n",
      "Ep 1 (Step 000090): Train loss 5.720, Val loss 5.763\n",
      "Ep 1 (Step 000100): Train loss 5.629, Val loss 5.701\n",
      "Ep 1 (Step 000110): Train loss 5.602, Val loss 5.649\n",
      "Ep 1 (Step 000120): Train loss 5.519, Val loss 5.599\n",
      "Ep 1 (Step 000130): Train loss 5.439, Val loss 5.557\n",
      "Ep 1 (Step 000140): Train loss 5.455, Val loss 5.502\n",
      "Ep 1 (Step 000150): Train loss 5.438, Val loss 5.497\n",
      "Ep 1 (Step 000160): Train loss 5.426, Val loss 5.458\n",
      "Ep 1 (Step 000170): Train loss 5.335, Val loss 5.456\n",
      "Ep 1 (Step 000180): Train loss 5.342, Val loss 5.434\n",
      "Ep 1 (Step 000190): Train loss 5.331, Val loss 5.402\n",
      "Ep 1 (Step 000200): Train loss 5.287, Val loss 5.377\n",
      "Ep 1 (Step 000210): Train loss 5.288, Val loss 5.352\n",
      "Ep 1 (Step 000220): Train loss 5.153, Val loss 5.350\n",
      "Ep 1 (Step 000230): Train loss 5.253, Val loss 5.336\n",
      "Ep 1 (Step 000240): Train loss 5.250, Val loss 5.330\n",
      "Ep 1 (Step 000250): Train loss 5.150, Val loss 5.296\n",
      "Ep 1 (Step 000260): Train loss 5.193, Val loss 5.281\n",
      "Ep 1 (Step 000270): Train loss 5.114, Val loss 5.262\n",
      "Ep 1 (Step 000280): Train loss 5.267, Val loss 5.259\n",
      "Ep 1 (Step 000290): Train loss 5.054, Val loss 5.239\n",
      "Ep 1 (Step 000300): Train loss 5.095, Val loss 5.242\n",
      "Ep 1 (Step 000310): Train loss 5.130, Val loss 5.232\n",
      "Ep 1 (Step 000320): Train loss 5.002, Val loss 5.224\n",
      "Ep 1 (Step 000330): Train loss 5.080, Val loss 5.195\n",
      "Ep 1 (Step 000340): Train loss 4.944, Val loss 5.199\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1985\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.053, Val loss 9.040\n",
      "Ep 1 (Step 000010): Train loss 7.445, Val loss 7.427\n",
      "Ep 1 (Step 000020): Train loss 6.801, Val loss 6.771\n",
      "Ep 1 (Step 000030): Train loss 6.507, Val loss 6.460\n",
      "Ep 1 (Step 000040): Train loss 6.371, Val loss 6.393\n",
      "Ep 1 (Step 000050): Train loss 6.324, Val loss 6.336\n",
      "Ep 1 (Step 000060): Train loss 6.179, Val loss 6.198\n",
      "Ep 1 (Step 000070): Train loss 6.013, Val loss 6.086\n",
      "Ep 1 (Step 000080): Train loss 5.921, Val loss 6.016\n",
      "Ep 1 (Step 000090): Train loss 5.973, Val loss 5.964\n",
      "Ep 1 (Step 000100): Train loss 5.866, Val loss 5.911\n",
      "Ep 1 (Step 000110): Train loss 5.777, Val loss 5.835\n",
      "Ep 1 (Step 000120): Train loss 5.805, Val loss 5.782\n",
      "Ep 1 (Step 000130): Train loss 5.613, Val loss 5.739\n",
      "Ep 1 (Step 000140): Train loss 5.649, Val loss 5.695\n",
      "Ep 1 (Step 000150): Train loss 5.594, Val loss 5.651\n",
      "Ep 1 (Step 000160): Train loss 5.610, Val loss 5.620\n",
      "Ep 1 (Step 000170): Train loss 5.510, Val loss 5.613\n",
      "Ep 1 (Step 000180): Train loss 5.586, Val loss 5.576\n",
      "Ep 1 (Step 000190): Train loss 5.515, Val loss 5.560\n",
      "Ep 1 (Step 000200): Train loss 5.416, Val loss 5.525\n",
      "Ep 1 (Step 000210): Train loss 5.401, Val loss 5.509\n",
      "Ep 1 (Step 000220): Train loss 5.462, Val loss 5.501\n",
      "Ep 1 (Step 000230): Train loss 5.324, Val loss 5.487\n",
      "Ep 1 (Step 000240): Train loss 5.386, Val loss 5.463\n",
      "Ep 1 (Step 000250): Train loss 5.271, Val loss 5.450\n",
      "Ep 1 (Step 000260): Train loss 5.316, Val loss 5.426\n",
      "Ep 1 (Step 000270): Train loss 5.284, Val loss 5.409\n",
      "Ep 1 (Step 000280): Train loss 5.322, Val loss 5.396\n",
      "Ep 1 (Step 000290): Train loss 5.273, Val loss 5.388\n",
      "Ep 1 (Step 000300): Train loss 5.315, Val loss 5.373\n",
      "Ep 1 (Step 000310): Train loss 5.197, Val loss 5.377\n",
      "Ep 1 (Step 000320): Train loss 5.206, Val loss 5.352\n",
      "Ep 1 (Step 000330): Train loss 5.210, Val loss 5.328\n",
      "Ep 1 (Step 000340): Train loss 5.213, Val loss 5.338\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3376\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.077, Val loss 9.084\n",
      "Ep 1 (Step 000010): Train loss 7.448, Val loss 7.464\n",
      "Ep 1 (Step 000020): Train loss 6.811, Val loss 6.816\n",
      "Ep 1 (Step 000030): Train loss 6.464, Val loss 6.493\n",
      "Ep 1 (Step 000040): Train loss 6.372, Val loss 6.376\n",
      "Ep 1 (Step 000050): Train loss 6.300, Val loss 6.315\n",
      "Ep 1 (Step 000060): Train loss 6.216, Val loss 6.214\n",
      "Ep 1 (Step 000070): Train loss 6.083, Val loss 6.106\n",
      "Ep 1 (Step 000080): Train loss 5.973, Val loss 6.036\n",
      "Ep 1 (Step 000090): Train loss 5.931, Val loss 5.969\n",
      "Ep 1 (Step 000100): Train loss 5.837, Val loss 5.900\n",
      "Ep 1 (Step 000110): Train loss 5.868, Val loss 5.853\n",
      "Ep 1 (Step 000120): Train loss 5.744, Val loss 5.802\n",
      "Ep 1 (Step 000130): Train loss 5.660, Val loss 5.766\n",
      "Ep 1 (Step 000140): Train loss 5.571, Val loss 5.734\n",
      "Ep 1 (Step 000150): Train loss 5.603, Val loss 5.685\n",
      "Ep 1 (Step 000160): Train loss 5.581, Val loss 5.648\n",
      "Ep 1 (Step 000170): Train loss 5.559, Val loss 5.616\n",
      "Ep 1 (Step 000180): Train loss 5.564, Val loss 5.583\n",
      "Ep 1 (Step 000190): Train loss 5.442, Val loss 5.575\n",
      "Ep 1 (Step 000200): Train loss 5.463, Val loss 5.545\n",
      "Ep 1 (Step 000210): Train loss 5.480, Val loss 5.521\n",
      "Ep 1 (Step 000220): Train loss 5.410, Val loss 5.525\n",
      "Ep 1 (Step 000230): Train loss 5.293, Val loss 5.500\n",
      "Ep 1 (Step 000240): Train loss 5.347, Val loss 5.492\n",
      "Ep 1 (Step 000250): Train loss 5.312, Val loss 5.466\n",
      "Ep 1 (Step 000260): Train loss 5.353, Val loss 5.443\n",
      "Ep 1 (Step 000270): Train loss 5.304, Val loss 5.427\n",
      "Ep 1 (Step 000280): Train loss 5.335, Val loss 5.414\n",
      "Ep 1 (Step 000290): Train loss 5.297, Val loss 5.402\n",
      "Ep 1 (Step 000300): Train loss 5.227, Val loss 5.378\n",
      "Ep 1 (Step 000310): Train loss 5.275, Val loss 5.371\n",
      "Ep 1 (Step 000320): Train loss 5.255, Val loss 5.356\n",
      "Ep 1 (Step 000330): Train loss 5.230, Val loss 5.326\n",
      "Ep 1 (Step 000340): Train loss 5.204, Val loss 5.321\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3210\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.026, Val loss 9.040\n",
      "Ep 1 (Step 000010): Train loss 7.385, Val loss 7.400\n",
      "Ep 1 (Step 000020): Train loss 6.817, Val loss 6.762\n",
      "Ep 1 (Step 000030): Train loss 6.493, Val loss 6.470\n",
      "Ep 1 (Step 000040): Train loss 6.342, Val loss 6.379\n",
      "Ep 1 (Step 000050): Train loss 6.309, Val loss 6.341\n",
      "Ep 1 (Step 000060): Train loss 6.284, Val loss 6.264\n",
      "Ep 1 (Step 000070): Train loss 6.121, Val loss 6.121\n",
      "Ep 1 (Step 000080): Train loss 6.014, Val loss 6.050\n",
      "Ep 1 (Step 000090): Train loss 5.902, Val loss 5.965\n",
      "Ep 1 (Step 000100): Train loss 5.861, Val loss 5.916\n",
      "Ep 1 (Step 000110): Train loss 5.803, Val loss 5.853\n",
      "Ep 1 (Step 000120): Train loss 5.794, Val loss 5.808\n",
      "Ep 1 (Step 000130): Train loss 5.692, Val loss 5.767\n",
      "Ep 1 (Step 000140): Train loss 5.689, Val loss 5.723\n",
      "Ep 1 (Step 000150): Train loss 5.592, Val loss 5.671\n",
      "Ep 1 (Step 000160): Train loss 5.586, Val loss 5.639\n",
      "Ep 1 (Step 000170): Train loss 5.524, Val loss 5.603\n",
      "Ep 1 (Step 000180): Train loss 5.586, Val loss 5.570\n",
      "Ep 1 (Step 000190): Train loss 5.499, Val loss 5.561\n",
      "Ep 1 (Step 000200): Train loss 5.449, Val loss 5.540\n",
      "Ep 1 (Step 000210): Train loss 5.372, Val loss 5.514\n",
      "Ep 1 (Step 000220): Train loss 5.346, Val loss 5.495\n",
      "Ep 1 (Step 000230): Train loss 5.455, Val loss 5.490\n",
      "Ep 1 (Step 000240): Train loss 5.311, Val loss 5.480\n",
      "Ep 1 (Step 000250): Train loss 5.326, Val loss 5.462\n",
      "Ep 1 (Step 000260): Train loss 5.360, Val loss 5.435\n",
      "Ep 1 (Step 000270): Train loss 5.278, Val loss 5.424\n",
      "Ep 1 (Step 000280): Train loss 5.358, Val loss 5.408\n",
      "Ep 1 (Step 000290): Train loss 5.244, Val loss 5.397\n",
      "Ep 1 (Step 000300): Train loss 5.204, Val loss 5.388\n",
      "Ep 1 (Step 000310): Train loss 5.182, Val loss 5.368\n",
      "Ep 1 (Step 000320): Train loss 5.234, Val loss 5.372\n",
      "Ep 1 (Step 000330): Train loss 5.187, Val loss 5.344\n",
      "Ep 1 (Step 000340): Train loss 5.174, Val loss 5.320\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3198\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.052, Val loss 9.038\n",
      "Ep 1 (Step 000010): Train loss 7.430, Val loss 7.416\n",
      "Ep 1 (Step 000020): Train loss 6.777, Val loss 6.788\n",
      "Ep 1 (Step 000030): Train loss 6.490, Val loss 6.487\n",
      "Ep 1 (Step 000040): Train loss 6.416, Val loss 6.394\n",
      "Ep 1 (Step 000050): Train loss 6.349, Val loss 6.349\n",
      "Ep 1 (Step 000060): Train loss 6.309, Val loss 6.268\n",
      "Ep 1 (Step 000070): Train loss 6.164, Val loss 6.138\n",
      "Ep 1 (Step 000080): Train loss 6.040, Val loss 6.053\n",
      "Ep 1 (Step 000090): Train loss 5.988, Val loss 5.983\n",
      "Ep 1 (Step 000100): Train loss 5.873, Val loss 5.907\n",
      "Ep 1 (Step 000110): Train loss 5.746, Val loss 5.852\n",
      "Ep 1 (Step 000120): Train loss 5.771, Val loss 5.799\n",
      "Ep 1 (Step 000130): Train loss 5.779, Val loss 5.760\n",
      "Ep 1 (Step 000140): Train loss 5.585, Val loss 5.731\n",
      "Ep 1 (Step 000150): Train loss 5.618, Val loss 5.680\n",
      "Ep 1 (Step 000160): Train loss 5.583, Val loss 5.655\n",
      "Ep 1 (Step 000170): Train loss 5.556, Val loss 5.611\n",
      "Ep 1 (Step 000180): Train loss 5.456, Val loss 5.583\n",
      "Ep 1 (Step 000190): Train loss 5.464, Val loss 5.561\n",
      "Ep 1 (Step 000200): Train loss 5.435, Val loss 5.531\n",
      "Ep 1 (Step 000210): Train loss 5.371, Val loss 5.513\n",
      "Ep 1 (Step 000220): Train loss 5.393, Val loss 5.485\n",
      "Ep 1 (Step 000230): Train loss 5.313, Val loss 5.467\n",
      "Ep 1 (Step 000240): Train loss 5.325, Val loss 5.459\n",
      "Ep 1 (Step 000250): Train loss 5.280, Val loss 5.445\n",
      "Ep 1 (Step 000260): Train loss 5.315, Val loss 5.408\n",
      "Ep 1 (Step 000270): Train loss 5.272, Val loss 5.392\n",
      "Ep 1 (Step 000280): Train loss 5.302, Val loss 5.368\n",
      "Ep 1 (Step 000290): Train loss 5.250, Val loss 5.365\n",
      "Ep 1 (Step 000300): Train loss 5.207, Val loss 5.364\n",
      "Ep 1 (Step 000310): Train loss 5.212, Val loss 5.335\n",
      "Ep 1 (Step 000320): Train loss 5.184, Val loss 5.316\n",
      "Ep 1 (Step 000330): Train loss 5.143, Val loss 5.306\n",
      "Ep 1 (Step 000340): Train loss 5.166, Val loss 5.304\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3042\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.041, Val loss 9.036\n",
      "Ep 1 (Step 000010): Train loss 7.409, Val loss 7.362\n",
      "Ep 1 (Step 000020): Train loss 6.773, Val loss 6.741\n",
      "Ep 1 (Step 000030): Train loss 6.493, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.365, Val loss 6.388\n",
      "Ep 1 (Step 000050): Train loss 6.375, Val loss 6.341\n",
      "Ep 1 (Step 000060): Train loss 6.217, Val loss 6.262\n",
      "Ep 1 (Step 000070): Train loss 6.026, Val loss 6.126\n",
      "Ep 1 (Step 000080): Train loss 5.985, Val loss 6.059\n",
      "Ep 1 (Step 000090): Train loss 6.017, Val loss 5.980\n",
      "Ep 1 (Step 000100): Train loss 5.870, Val loss 5.914\n",
      "Ep 1 (Step 000110): Train loss 5.838, Val loss 5.858\n",
      "Ep 1 (Step 000120): Train loss 5.733, Val loss 5.815\n",
      "Ep 1 (Step 000130): Train loss 5.627, Val loss 5.761\n",
      "Ep 1 (Step 000140): Train loss 5.659, Val loss 5.717\n",
      "Ep 1 (Step 000150): Train loss 5.600, Val loss 5.679\n",
      "Ep 1 (Step 000160): Train loss 5.560, Val loss 5.645\n",
      "Ep 1 (Step 000170): Train loss 5.505, Val loss 5.602\n",
      "Ep 1 (Step 000180): Train loss 5.481, Val loss 5.594\n",
      "Ep 1 (Step 000190): Train loss 5.423, Val loss 5.576\n",
      "Ep 1 (Step 000200): Train loss 5.512, Val loss 5.541\n",
      "Ep 1 (Step 000210): Train loss 5.371, Val loss 5.527\n",
      "Ep 1 (Step 000220): Train loss 5.396, Val loss 5.503\n",
      "Ep 1 (Step 000230): Train loss 5.361, Val loss 5.475\n",
      "Ep 1 (Step 000240): Train loss 5.427, Val loss 5.466\n",
      "Ep 1 (Step 000250): Train loss 5.277, Val loss 5.438\n",
      "Ep 1 (Step 000260): Train loss 5.258, Val loss 5.422\n",
      "Ep 1 (Step 000270): Train loss 5.288, Val loss 5.410\n",
      "Ep 1 (Step 000280): Train loss 5.264, Val loss 5.394\n",
      "Ep 1 (Step 000290): Train loss 5.239, Val loss 5.391\n",
      "Ep 1 (Step 000300): Train loss 5.260, Val loss 5.386\n",
      "Ep 1 (Step 000310): Train loss 5.256, Val loss 5.355\n",
      "Ep 1 (Step 000320): Train loss 5.223, Val loss 5.348\n",
      "Ep 1 (Step 000330): Train loss 5.088, Val loss 5.326\n",
      "Ep 1 (Step 000340): Train loss 5.260, Val loss 5.330\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3301\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.059, Val loss 9.058\n",
      "Ep 1 (Step 000010): Train loss 7.498, Val loss 7.455\n",
      "Ep 1 (Step 000020): Train loss 6.826, Val loss 6.782\n",
      "Ep 1 (Step 000030): Train loss 6.465, Val loss 6.471\n",
      "Ep 1 (Step 000040): Train loss 6.427, Val loss 6.394\n",
      "Ep 1 (Step 000050): Train loss 6.414, Val loss 6.350\n",
      "Ep 1 (Step 000060): Train loss 6.290, Val loss 6.273\n",
      "Ep 1 (Step 000070): Train loss 6.171, Val loss 6.129\n",
      "Ep 1 (Step 000080): Train loss 6.012, Val loss 6.058\n",
      "Ep 1 (Step 000090): Train loss 5.916, Val loss 5.975\n",
      "Ep 1 (Step 000100): Train loss 5.859, Val loss 5.903\n",
      "Ep 1 (Step 000110): Train loss 5.868, Val loss 5.859\n",
      "Ep 1 (Step 000120): Train loss 5.748, Val loss 5.800\n",
      "Ep 1 (Step 000130): Train loss 5.661, Val loss 5.750\n",
      "Ep 1 (Step 000140): Train loss 5.670, Val loss 5.715\n",
      "Ep 1 (Step 000150): Train loss 5.612, Val loss 5.685\n",
      "Ep 1 (Step 000160): Train loss 5.550, Val loss 5.649\n",
      "Ep 1 (Step 000170): Train loss 5.535, Val loss 5.614\n",
      "Ep 1 (Step 000180): Train loss 5.573, Val loss 5.594\n",
      "Ep 1 (Step 000190): Train loss 5.594, Val loss 5.559\n",
      "Ep 1 (Step 000200): Train loss 5.463, Val loss 5.547\n",
      "Ep 1 (Step 000210): Train loss 5.424, Val loss 5.523\n",
      "Ep 1 (Step 000220): Train loss 5.361, Val loss 5.498\n",
      "Ep 1 (Step 000230): Train loss 5.383, Val loss 5.463\n",
      "Ep 1 (Step 000240): Train loss 5.323, Val loss 5.445\n",
      "Ep 1 (Step 000250): Train loss 5.366, Val loss 5.432\n",
      "Ep 1 (Step 000260): Train loss 5.270, Val loss 5.412\n",
      "Ep 1 (Step 000270): Train loss 5.337, Val loss 5.407\n",
      "Ep 1 (Step 000280): Train loss 5.309, Val loss 5.405\n",
      "Ep 1 (Step 000290): Train loss 5.256, Val loss 5.385\n",
      "Ep 1 (Step 000300): Train loss 5.183, Val loss 5.370\n",
      "Ep 1 (Step 000310): Train loss 5.177, Val loss 5.351\n",
      "Ep 1 (Step 000320): Train loss 5.210, Val loss 5.348\n",
      "Ep 1 (Step 000330): Train loss 5.157, Val loss 5.323\n",
      "Ep 1 (Step 000340): Train loss 5.114, Val loss 5.316\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3156\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.789, Val loss 8.752\n",
      "Ep 1 (Step 000010): Train loss 6.816, Val loss 6.778\n",
      "Ep 1 (Step 000020): Train loss 6.472, Val loss 6.479\n",
      "Ep 1 (Step 000030): Train loss 6.476, Val loss 6.447\n",
      "Ep 1 (Step 000040): Train loss 6.381, Val loss 6.309\n",
      "Ep 1 (Step 000050): Train loss 6.124, Val loss 6.137\n",
      "Ep 1 (Step 000060): Train loss 6.003, Val loss 6.002\n",
      "Ep 1 (Step 000070): Train loss 5.890, Val loss 5.951\n",
      "Ep 1 (Step 000080): Train loss 5.765, Val loss 5.848\n",
      "Ep 1 (Step 000090): Train loss 5.707, Val loss 5.748\n",
      "Ep 1 (Step 000100): Train loss 5.629, Val loss 5.704\n",
      "Ep 1 (Step 000110): Train loss 5.623, Val loss 5.651\n",
      "Ep 1 (Step 000120): Train loss 5.570, Val loss 5.600\n",
      "Ep 1 (Step 000130): Train loss 5.439, Val loss 5.563\n",
      "Ep 1 (Step 000140): Train loss 5.458, Val loss 5.546\n",
      "Ep 1 (Step 000150): Train loss 5.474, Val loss 5.502\n",
      "Ep 1 (Step 000160): Train loss 5.367, Val loss 5.483\n",
      "Ep 1 (Step 000170): Train loss 5.386, Val loss 5.458\n",
      "Ep 1 (Step 000180): Train loss 5.268, Val loss 5.434\n",
      "Ep 1 (Step 000190): Train loss 5.324, Val loss 5.429\n",
      "Ep 1 (Step 000200): Train loss 5.368, Val loss 5.405\n",
      "Ep 1 (Step 000210): Train loss 5.277, Val loss 5.378\n",
      "Ep 1 (Step 000220): Train loss 5.284, Val loss 5.377\n",
      "Ep 1 (Step 000230): Train loss 5.269, Val loss 5.345\n",
      "Ep 1 (Step 000240): Train loss 5.250, Val loss 5.334\n",
      "Ep 1 (Step 000250): Train loss 5.164, Val loss 5.322\n",
      "Ep 1 (Step 000260): Train loss 5.233, Val loss 5.303\n",
      "Ep 1 (Step 000270): Train loss 5.235, Val loss 5.262\n",
      "Ep 1 (Step 000280): Train loss 5.179, Val loss 5.254\n",
      "Ep 1 (Step 000290): Train loss 5.173, Val loss 5.278\n",
      "Ep 1 (Step 000300): Train loss 5.185, Val loss 5.269\n",
      "Ep 1 (Step 000310): Train loss 5.148, Val loss 5.278\n",
      "Ep 1 (Step 000320): Train loss 5.130, Val loss 5.247\n",
      "Ep 1 (Step 000330): Train loss 5.131, Val loss 5.233\n",
      "Ep 1 (Step 000340): Train loss 5.049, Val loss 5.240\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2397\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.751, Val loss 8.760\n",
      "Ep 1 (Step 000010): Train loss 6.876, Val loss 6.861\n",
      "Ep 1 (Step 000020): Train loss 6.495, Val loss 6.454\n",
      "Ep 1 (Step 000030): Train loss 6.400, Val loss 6.399\n",
      "Ep 1 (Step 000040): Train loss 6.267, Val loss 6.250\n",
      "Ep 1 (Step 000050): Train loss 6.105, Val loss 6.100\n",
      "Ep 1 (Step 000060): Train loss 5.964, Val loss 5.973\n",
      "Ep 1 (Step 000070): Train loss 5.920, Val loss 5.887\n",
      "Ep 1 (Step 000080): Train loss 5.799, Val loss 5.785\n",
      "Ep 1 (Step 000090): Train loss 5.731, Val loss 5.725\n",
      "Ep 1 (Step 000100): Train loss 5.685, Val loss 5.667\n",
      "Ep 1 (Step 000110): Train loss 5.578, Val loss 5.605\n",
      "Ep 1 (Step 000120): Train loss 5.534, Val loss 5.555\n",
      "Ep 1 (Step 000130): Train loss 5.569, Val loss 5.541\n",
      "Ep 1 (Step 000140): Train loss 5.465, Val loss 5.510\n",
      "Ep 1 (Step 000150): Train loss 5.349, Val loss 5.477\n",
      "Ep 1 (Step 000160): Train loss 5.320, Val loss 5.470\n",
      "Ep 1 (Step 000170): Train loss 5.331, Val loss 5.463\n",
      "Ep 1 (Step 000180): Train loss 5.301, Val loss 5.409\n",
      "Ep 1 (Step 000190): Train loss 5.301, Val loss 5.397\n",
      "Ep 1 (Step 000200): Train loss 5.281, Val loss 5.397\n",
      "Ep 1 (Step 000210): Train loss 5.265, Val loss 5.394\n",
      "Ep 1 (Step 000220): Train loss 5.201, Val loss 5.363\n",
      "Ep 1 (Step 000230): Train loss 5.359, Val loss 5.340\n",
      "Ep 1 (Step 000240): Train loss 5.229, Val loss 5.344\n",
      "Ep 1 (Step 000250): Train loss 5.272, Val loss 5.320\n",
      "Ep 1 (Step 000260): Train loss 5.125, Val loss 5.300\n",
      "Ep 1 (Step 000270): Train loss 5.097, Val loss 5.317\n",
      "Ep 1 (Step 000280): Train loss 5.220, Val loss 5.280\n",
      "Ep 1 (Step 000290): Train loss 5.145, Val loss 5.298\n",
      "Ep 1 (Step 000300): Train loss 5.111, Val loss 5.273\n",
      "Ep 1 (Step 000310): Train loss 5.211, Val loss 5.265\n",
      "Ep 1 (Step 000320): Train loss 5.061, Val loss 5.258\n",
      "Ep 1 (Step 000330): Train loss 5.040, Val loss 5.242\n",
      "Ep 1 (Step 000340): Train loss 5.109, Val loss 5.238\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2384\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.829, Val loss 8.794\n",
      "Ep 1 (Step 000010): Train loss 6.848, Val loss 6.832\n",
      "Ep 1 (Step 000020): Train loss 6.453, Val loss 6.455\n",
      "Ep 1 (Step 000030): Train loss 6.385, Val loss 6.415\n",
      "Ep 1 (Step 000040): Train loss 6.218, Val loss 6.250\n",
      "Ep 1 (Step 000050): Train loss 6.108, Val loss 6.100\n",
      "Ep 1 (Step 000060): Train loss 6.020, Val loss 5.978\n",
      "Ep 1 (Step 000070): Train loss 5.836, Val loss 5.873\n",
      "Ep 1 (Step 000080): Train loss 5.799, Val loss 5.792\n",
      "Ep 1 (Step 000090): Train loss 5.694, Val loss 5.732\n",
      "Ep 1 (Step 000100): Train loss 5.587, Val loss 5.671\n",
      "Ep 1 (Step 000110): Train loss 5.587, Val loss 5.616\n",
      "Ep 1 (Step 000120): Train loss 5.514, Val loss 5.584\n",
      "Ep 1 (Step 000130): Train loss 5.435, Val loss 5.554\n",
      "Ep 1 (Step 000140): Train loss 5.510, Val loss 5.521\n",
      "Ep 1 (Step 000150): Train loss 5.406, Val loss 5.483\n",
      "Ep 1 (Step 000160): Train loss 5.444, Val loss 5.471\n",
      "Ep 1 (Step 000170): Train loss 5.275, Val loss 5.445\n",
      "Ep 1 (Step 000180): Train loss 5.264, Val loss 5.426\n",
      "Ep 1 (Step 000190): Train loss 5.319, Val loss 5.403\n",
      "Ep 1 (Step 000200): Train loss 5.260, Val loss 5.377\n",
      "Ep 1 (Step 000210): Train loss 5.312, Val loss 5.366\n",
      "Ep 1 (Step 000220): Train loss 5.331, Val loss 5.361\n",
      "Ep 1 (Step 000230): Train loss 5.258, Val loss 5.367\n",
      "Ep 1 (Step 000240): Train loss 5.171, Val loss 5.341\n",
      "Ep 1 (Step 000250): Train loss 5.181, Val loss 5.326\n",
      "Ep 1 (Step 000260): Train loss 5.093, Val loss 5.311\n",
      "Ep 1 (Step 000270): Train loss 5.178, Val loss 5.289\n",
      "Ep 1 (Step 000280): Train loss 5.134, Val loss 5.298\n",
      "Ep 1 (Step 000290): Train loss 5.108, Val loss 5.288\n",
      "Ep 1 (Step 000300): Train loss 5.107, Val loss 5.270\n",
      "Ep 1 (Step 000310): Train loss 5.100, Val loss 5.259\n",
      "Ep 1 (Step 000320): Train loss 5.007, Val loss 5.237\n",
      "Ep 1 (Step 000330): Train loss 5.052, Val loss 5.218\n",
      "Ep 1 (Step 000340): Train loss 5.005, Val loss 5.231\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2313\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.772, Val loss 8.735\n",
      "Ep 1 (Step 000010): Train loss 6.833, Val loss 6.843\n",
      "Ep 1 (Step 000020): Train loss 6.513, Val loss 6.470\n",
      "Ep 1 (Step 000030): Train loss 6.390, Val loss 6.451\n",
      "Ep 1 (Step 000040): Train loss 6.267, Val loss 6.308\n",
      "Ep 1 (Step 000050): Train loss 6.194, Val loss 6.156\n",
      "Ep 1 (Step 000060): Train loss 6.005, Val loss 6.003\n",
      "Ep 1 (Step 000070): Train loss 5.855, Val loss 5.894\n",
      "Ep 1 (Step 000080): Train loss 5.748, Val loss 5.823\n",
      "Ep 1 (Step 000090): Train loss 5.742, Val loss 5.760\n",
      "Ep 1 (Step 000100): Train loss 5.570, Val loss 5.672\n",
      "Ep 1 (Step 000110): Train loss 5.528, Val loss 5.620\n",
      "Ep 1 (Step 000120): Train loss 5.600, Val loss 5.578\n",
      "Ep 1 (Step 000130): Train loss 5.537, Val loss 5.563\n",
      "Ep 1 (Step 000140): Train loss 5.441, Val loss 5.515\n",
      "Ep 1 (Step 000150): Train loss 5.417, Val loss 5.482\n",
      "Ep 1 (Step 000160): Train loss 5.332, Val loss 5.467\n",
      "Ep 1 (Step 000170): Train loss 5.322, Val loss 5.444\n",
      "Ep 1 (Step 000180): Train loss 5.336, Val loss 5.437\n",
      "Ep 1 (Step 000190): Train loss 5.314, Val loss 5.412\n",
      "Ep 1 (Step 000200): Train loss 5.240, Val loss 5.389\n",
      "Ep 1 (Step 000210): Train loss 5.262, Val loss 5.376\n",
      "Ep 1 (Step 000220): Train loss 5.242, Val loss 5.358\n",
      "Ep 1 (Step 000230): Train loss 5.260, Val loss 5.329\n",
      "Ep 1 (Step 000240): Train loss 5.269, Val loss 5.325\n",
      "Ep 1 (Step 000250): Train loss 5.179, Val loss 5.290\n",
      "Ep 1 (Step 000260): Train loss 5.156, Val loss 5.295\n",
      "Ep 1 (Step 000270): Train loss 5.122, Val loss 5.287\n",
      "Ep 1 (Step 000280): Train loss 5.135, Val loss 5.276\n",
      "Ep 1 (Step 000290): Train loss 5.110, Val loss 5.247\n",
      "Ep 1 (Step 000300): Train loss 5.038, Val loss 5.228\n",
      "Ep 1 (Step 000310): Train loss 5.072, Val loss 5.224\n",
      "Ep 1 (Step 000320): Train loss 5.090, Val loss 5.214\n",
      "Ep 1 (Step 000330): Train loss 5.068, Val loss 5.213\n",
      "Ep 1 (Step 000340): Train loss 5.048, Val loss 5.184\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1844\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.861, Val loss 8.861\n",
      "Ep 1 (Step 000010): Train loss 6.852, Val loss 6.873\n",
      "Ep 1 (Step 000020): Train loss 6.467, Val loss 6.469\n",
      "Ep 1 (Step 000030): Train loss 6.402, Val loss 6.408\n",
      "Ep 1 (Step 000040): Train loss 6.248, Val loss 6.232\n",
      "Ep 1 (Step 000050): Train loss 6.164, Val loss 6.082\n",
      "Ep 1 (Step 000060): Train loss 5.938, Val loss 5.968\n",
      "Ep 1 (Step 000070): Train loss 5.890, Val loss 5.878\n",
      "Ep 1 (Step 000080): Train loss 5.733, Val loss 5.839\n",
      "Ep 1 (Step 000090): Train loss 5.651, Val loss 5.769\n",
      "Ep 1 (Step 000100): Train loss 5.685, Val loss 5.722\n",
      "Ep 1 (Step 000110): Train loss 5.637, Val loss 5.650\n",
      "Ep 1 (Step 000120): Train loss 5.584, Val loss 5.598\n",
      "Ep 1 (Step 000130): Train loss 5.486, Val loss 5.549\n",
      "Ep 1 (Step 000140): Train loss 5.406, Val loss 5.515\n",
      "Ep 1 (Step 000150): Train loss 5.433, Val loss 5.505\n",
      "Ep 1 (Step 000160): Train loss 5.383, Val loss 5.475\n",
      "Ep 1 (Step 000170): Train loss 5.362, Val loss 5.434\n",
      "Ep 1 (Step 000180): Train loss 5.374, Val loss 5.427\n",
      "Ep 1 (Step 000190): Train loss 5.324, Val loss 5.387\n",
      "Ep 1 (Step 000200): Train loss 5.376, Val loss 5.384\n",
      "Ep 1 (Step 000210): Train loss 5.221, Val loss 5.341\n",
      "Ep 1 (Step 000220): Train loss 5.201, Val loss 5.320\n",
      "Ep 1 (Step 000230): Train loss 5.201, Val loss 5.307\n",
      "Ep 1 (Step 000240): Train loss 5.156, Val loss 5.297\n",
      "Ep 1 (Step 000250): Train loss 5.178, Val loss 5.296\n",
      "Ep 1 (Step 000260): Train loss 5.158, Val loss 5.285\n",
      "Ep 1 (Step 000270): Train loss 5.150, Val loss 5.289\n",
      "Ep 1 (Step 000280): Train loss 5.195, Val loss 5.262\n",
      "Ep 1 (Step 000290): Train loss 5.110, Val loss 5.265\n",
      "Ep 1 (Step 000300): Train loss 5.133, Val loss 5.247\n",
      "Ep 1 (Step 000310): Train loss 5.101, Val loss 5.246\n",
      "Ep 1 (Step 000320): Train loss 5.089, Val loss 5.218\n",
      "Ep 1 (Step 000330): Train loss 4.984, Val loss 5.210\n",
      "Ep 1 (Step 000340): Train loss 5.107, Val loss 5.213\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2131\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.828, Val loss 8.777\n",
      "Ep 1 (Step 000010): Train loss 6.867, Val loss 6.863\n",
      "Ep 1 (Step 000020): Train loss 6.547, Val loss 6.474\n",
      "Ep 1 (Step 000030): Train loss 6.430, Val loss 6.443\n",
      "Ep 1 (Step 000040): Train loss 6.264, Val loss 6.289\n",
      "Ep 1 (Step 000050): Train loss 6.154, Val loss 6.124\n",
      "Ep 1 (Step 000060): Train loss 5.972, Val loss 5.993\n",
      "Ep 1 (Step 000070): Train loss 5.899, Val loss 5.883\n",
      "Ep 1 (Step 000080): Train loss 5.757, Val loss 5.820\n",
      "Ep 1 (Step 000090): Train loss 5.730, Val loss 5.738\n",
      "Ep 1 (Step 000100): Train loss 5.616, Val loss 5.661\n",
      "Ep 1 (Step 000110): Train loss 5.567, Val loss 5.637\n",
      "Ep 1 (Step 000120): Train loss 5.588, Val loss 5.617\n",
      "Ep 1 (Step 000130): Train loss 5.551, Val loss 5.559\n",
      "Ep 1 (Step 000140): Train loss 5.477, Val loss 5.530\n",
      "Ep 1 (Step 000150): Train loss 5.420, Val loss 5.518\n",
      "Ep 1 (Step 000160): Train loss 5.331, Val loss 5.488\n",
      "Ep 1 (Step 000170): Train loss 5.354, Val loss 5.439\n",
      "Ep 1 (Step 000180): Train loss 5.355, Val loss 5.409\n",
      "Ep 1 (Step 000190): Train loss 5.293, Val loss 5.405\n",
      "Ep 1 (Step 000200): Train loss 5.262, Val loss 5.374\n",
      "Ep 1 (Step 000210): Train loss 5.183, Val loss 5.362\n",
      "Ep 1 (Step 000220): Train loss 5.235, Val loss 5.342\n",
      "Ep 1 (Step 000230): Train loss 5.205, Val loss 5.346\n",
      "Ep 1 (Step 000240): Train loss 5.222, Val loss 5.305\n",
      "Ep 1 (Step 000250): Train loss 5.163, Val loss 5.303\n",
      "Ep 1 (Step 000260): Train loss 5.148, Val loss 5.278\n",
      "Ep 1 (Step 000270): Train loss 5.166, Val loss 5.271\n",
      "Ep 1 (Step 000280): Train loss 5.135, Val loss 5.268\n",
      "Ep 1 (Step 000290): Train loss 5.119, Val loss 5.252\n",
      "Ep 1 (Step 000300): Train loss 5.063, Val loss 5.233\n",
      "Ep 1 (Step 000310): Train loss 5.107, Val loss 5.223\n",
      "Ep 1 (Step 000320): Train loss 5.046, Val loss 5.209\n",
      "Ep 1 (Step 000330): Train loss 5.042, Val loss 5.200\n",
      "Ep 1 (Step 000340): Train loss 5.032, Val loss 5.202\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2018\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.947, Val loss 8.925\n",
      "Ep 1 (Step 000010): Train loss 7.435, Val loss 7.416\n",
      "Ep 1 (Step 000020): Train loss 6.775, Val loss 6.780\n",
      "Ep 1 (Step 000030): Train loss 6.513, Val loss 6.465\n",
      "Ep 1 (Step 000040): Train loss 6.404, Val loss 6.368\n",
      "Ep 1 (Step 000050): Train loss 6.332, Val loss 6.318\n",
      "Ep 1 (Step 000060): Train loss 6.202, Val loss 6.227\n",
      "Ep 1 (Step 000070): Train loss 6.102, Val loss 6.112\n",
      "Ep 1 (Step 000080): Train loss 5.984, Val loss 6.010\n",
      "Ep 1 (Step 000090): Train loss 5.885, Val loss 5.943\n",
      "Ep 1 (Step 000100): Train loss 5.890, Val loss 5.913\n",
      "Ep 1 (Step 000110): Train loss 5.838, Val loss 5.853\n",
      "Ep 1 (Step 000120): Train loss 5.742, Val loss 5.810\n",
      "Ep 1 (Step 000130): Train loss 5.722, Val loss 5.766\n",
      "Ep 1 (Step 000140): Train loss 5.671, Val loss 5.718\n",
      "Ep 1 (Step 000150): Train loss 5.562, Val loss 5.688\n",
      "Ep 1 (Step 000160): Train loss 5.581, Val loss 5.666\n",
      "Ep 1 (Step 000170): Train loss 5.610, Val loss 5.633\n",
      "Ep 1 (Step 000180): Train loss 5.555, Val loss 5.613\n",
      "Ep 1 (Step 000190): Train loss 5.538, Val loss 5.580\n",
      "Ep 1 (Step 000200): Train loss 5.402, Val loss 5.552\n",
      "Ep 1 (Step 000210): Train loss 5.502, Val loss 5.536\n",
      "Ep 1 (Step 000220): Train loss 5.401, Val loss 5.515\n",
      "Ep 1 (Step 000230): Train loss 5.372, Val loss 5.499\n",
      "Ep 1 (Step 000240): Train loss 5.371, Val loss 5.467\n",
      "Ep 1 (Step 000250): Train loss 5.345, Val loss 5.483\n",
      "Ep 1 (Step 000260): Train loss 5.427, Val loss 5.448\n",
      "Ep 1 (Step 000270): Train loss 5.294, Val loss 5.429\n",
      "Ep 1 (Step 000280): Train loss 5.335, Val loss 5.418\n",
      "Ep 1 (Step 000290): Train loss 5.358, Val loss 5.408\n",
      "Ep 1 (Step 000300): Train loss 5.251, Val loss 5.399\n",
      "Ep 1 (Step 000310): Train loss 5.264, Val loss 5.398\n",
      "Ep 1 (Step 000320): Train loss 5.247, Val loss 5.365\n",
      "Ep 1 (Step 000330): Train loss 5.286, Val loss 5.357\n",
      "Ep 1 (Step 000340): Train loss 5.320, Val loss 5.362\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3621\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.960, Val loss 8.943\n",
      "Ep 1 (Step 000010): Train loss 7.456, Val loss 7.442\n",
      "Ep 1 (Step 000020): Train loss 6.833, Val loss 6.802\n",
      "Ep 1 (Step 000030): Train loss 6.492, Val loss 6.468\n",
      "Ep 1 (Step 000040): Train loss 6.427, Val loss 6.364\n",
      "Ep 1 (Step 000050): Train loss 6.362, Val loss 6.335\n",
      "Ep 1 (Step 000060): Train loss 6.287, Val loss 6.241\n",
      "Ep 1 (Step 000070): Train loss 6.067, Val loss 6.129\n",
      "Ep 1 (Step 000080): Train loss 6.071, Val loss 6.043\n",
      "Ep 1 (Step 000090): Train loss 5.928, Val loss 5.974\n",
      "Ep 1 (Step 000100): Train loss 5.781, Val loss 5.916\n",
      "Ep 1 (Step 000110): Train loss 5.807, Val loss 5.880\n",
      "Ep 1 (Step 000120): Train loss 5.722, Val loss 5.831\n",
      "Ep 1 (Step 000130): Train loss 5.710, Val loss 5.778\n",
      "Ep 1 (Step 000140): Train loss 5.689, Val loss 5.739\n",
      "Ep 1 (Step 000150): Train loss 5.596, Val loss 5.685\n",
      "Ep 1 (Step 000160): Train loss 5.591, Val loss 5.661\n",
      "Ep 1 (Step 000170): Train loss 5.559, Val loss 5.631\n",
      "Ep 1 (Step 000180): Train loss 5.627, Val loss 5.610\n",
      "Ep 1 (Step 000190): Train loss 5.514, Val loss 5.587\n",
      "Ep 1 (Step 000200): Train loss 5.499, Val loss 5.564\n",
      "Ep 1 (Step 000210): Train loss 5.469, Val loss 5.542\n",
      "Ep 1 (Step 000220): Train loss 5.472, Val loss 5.530\n",
      "Ep 1 (Step 000230): Train loss 5.471, Val loss 5.498\n",
      "Ep 1 (Step 000240): Train loss 5.377, Val loss 5.489\n",
      "Ep 1 (Step 000250): Train loss 5.487, Val loss 5.480\n",
      "Ep 1 (Step 000260): Train loss 5.346, Val loss 5.450\n",
      "Ep 1 (Step 000270): Train loss 5.271, Val loss 5.450\n",
      "Ep 1 (Step 000280): Train loss 5.286, Val loss 5.445\n",
      "Ep 1 (Step 000290): Train loss 5.287, Val loss 5.419\n",
      "Ep 1 (Step 000300): Train loss 5.334, Val loss 5.414\n",
      "Ep 1 (Step 000310): Train loss 5.298, Val loss 5.389\n",
      "Ep 1 (Step 000320): Train loss 5.222, Val loss 5.383\n",
      "Ep 1 (Step 000330): Train loss 5.291, Val loss 5.366\n",
      "Ep 1 (Step 000340): Train loss 5.223, Val loss 5.357\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3566\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.864, Val loss 8.839\n",
      "Ep 1 (Step 000010): Train loss 7.420, Val loss 7.384\n",
      "Ep 1 (Step 000020): Train loss 6.776, Val loss 6.743\n",
      "Ep 1 (Step 000030): Train loss 6.469, Val loss 6.445\n",
      "Ep 1 (Step 000040): Train loss 6.340, Val loss 6.364\n",
      "Ep 1 (Step 000050): Train loss 6.306, Val loss 6.307\n",
      "Ep 1 (Step 000060): Train loss 6.191, Val loss 6.212\n",
      "Ep 1 (Step 000070): Train loss 6.125, Val loss 6.093\n",
      "Ep 1 (Step 000080): Train loss 6.034, Val loss 6.013\n",
      "Ep 1 (Step 000090): Train loss 5.944, Val loss 5.956\n",
      "Ep 1 (Step 000100): Train loss 5.853, Val loss 5.892\n",
      "Ep 1 (Step 000110): Train loss 5.891, Val loss 5.846\n",
      "Ep 1 (Step 000120): Train loss 5.766, Val loss 5.799\n",
      "Ep 1 (Step 000130): Train loss 5.782, Val loss 5.771\n",
      "Ep 1 (Step 000140): Train loss 5.642, Val loss 5.724\n",
      "Ep 1 (Step 000150): Train loss 5.587, Val loss 5.693\n",
      "Ep 1 (Step 000160): Train loss 5.558, Val loss 5.648\n",
      "Ep 1 (Step 000170): Train loss 5.602, Val loss 5.633\n",
      "Ep 1 (Step 000180): Train loss 5.535, Val loss 5.604\n",
      "Ep 1 (Step 000190): Train loss 5.485, Val loss 5.561\n",
      "Ep 1 (Step 000200): Train loss 5.495, Val loss 5.540\n",
      "Ep 1 (Step 000210): Train loss 5.482, Val loss 5.521\n",
      "Ep 1 (Step 000220): Train loss 5.439, Val loss 5.498\n",
      "Ep 1 (Step 000230): Train loss 5.393, Val loss 5.489\n",
      "Ep 1 (Step 000240): Train loss 5.315, Val loss 5.459\n",
      "Ep 1 (Step 000250): Train loss 5.399, Val loss 5.459\n",
      "Ep 1 (Step 000260): Train loss 5.348, Val loss 5.437\n",
      "Ep 1 (Step 000270): Train loss 5.336, Val loss 5.416\n",
      "Ep 1 (Step 000280): Train loss 5.306, Val loss 5.397\n",
      "Ep 1 (Step 000290): Train loss 5.374, Val loss 5.393\n",
      "Ep 1 (Step 000300): Train loss 5.303, Val loss 5.397\n",
      "Ep 1 (Step 000310): Train loss 5.222, Val loss 5.387\n",
      "Ep 1 (Step 000320): Train loss 5.284, Val loss 5.379\n",
      "Ep 1 (Step 000330): Train loss 5.242, Val loss 5.354\n",
      "Ep 1 (Step 000340): Train loss 5.226, Val loss 5.353\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3530\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.893, Val loss 8.863\n",
      "Ep 1 (Step 000010): Train loss 7.397, Val loss 7.365\n",
      "Ep 1 (Step 000020): Train loss 6.778, Val loss 6.726\n",
      "Ep 1 (Step 000030): Train loss 6.552, Val loss 6.448\n",
      "Ep 1 (Step 000040): Train loss 6.357, Val loss 6.362\n",
      "Ep 1 (Step 000050): Train loss 6.300, Val loss 6.312\n",
      "Ep 1 (Step 000060): Train loss 6.237, Val loss 6.249\n",
      "Ep 1 (Step 000070): Train loss 6.091, Val loss 6.142\n",
      "Ep 1 (Step 000080): Train loss 6.016, Val loss 6.059\n",
      "Ep 1 (Step 000090): Train loss 5.942, Val loss 6.001\n",
      "Ep 1 (Step 000100): Train loss 5.851, Val loss 5.930\n",
      "Ep 1 (Step 000110): Train loss 5.839, Val loss 5.863\n",
      "Ep 1 (Step 000120): Train loss 5.787, Val loss 5.813\n",
      "Ep 1 (Step 000130): Train loss 5.755, Val loss 5.761\n",
      "Ep 1 (Step 000140): Train loss 5.631, Val loss 5.743\n",
      "Ep 1 (Step 000150): Train loss 5.739, Val loss 5.702\n",
      "Ep 1 (Step 000160): Train loss 5.682, Val loss 5.691\n",
      "Ep 1 (Step 000170): Train loss 5.648, Val loss 5.635\n",
      "Ep 1 (Step 000180): Train loss 5.540, Val loss 5.602\n",
      "Ep 1 (Step 000190): Train loss 5.549, Val loss 5.574\n",
      "Ep 1 (Step 000200): Train loss 5.476, Val loss 5.550\n",
      "Ep 1 (Step 000210): Train loss 5.454, Val loss 5.548\n",
      "Ep 1 (Step 000220): Train loss 5.449, Val loss 5.506\n",
      "Ep 1 (Step 000230): Train loss 5.432, Val loss 5.486\n",
      "Ep 1 (Step 000240): Train loss 5.416, Val loss 5.477\n",
      "Ep 1 (Step 000250): Train loss 5.399, Val loss 5.464\n",
      "Ep 1 (Step 000260): Train loss 5.311, Val loss 5.444\n",
      "Ep 1 (Step 000270): Train loss 5.395, Val loss 5.438\n",
      "Ep 1 (Step 000280): Train loss 5.420, Val loss 5.418\n",
      "Ep 1 (Step 000290): Train loss 5.251, Val loss 5.408\n",
      "Ep 1 (Step 000300): Train loss 5.305, Val loss 5.398\n",
      "Ep 1 (Step 000310): Train loss 5.244, Val loss 5.391\n",
      "Ep 1 (Step 000320): Train loss 5.319, Val loss 5.378\n",
      "Ep 1 (Step 000330): Train loss 5.339, Val loss 5.356\n",
      "Ep 1 (Step 000340): Train loss 5.288, Val loss 5.347\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3466\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.912, Val loss 8.876\n",
      "Ep 1 (Step 000010): Train loss 7.429, Val loss 7.430\n",
      "Ep 1 (Step 000020): Train loss 6.775, Val loss 6.777\n",
      "Ep 1 (Step 000030): Train loss 6.483, Val loss 6.479\n",
      "Ep 1 (Step 000040): Train loss 6.340, Val loss 6.371\n",
      "Ep 1 (Step 000050): Train loss 6.263, Val loss 6.310\n",
      "Ep 1 (Step 000060): Train loss 6.167, Val loss 6.217\n",
      "Ep 1 (Step 000070): Train loss 6.136, Val loss 6.084\n",
      "Ep 1 (Step 000080): Train loss 5.966, Val loss 6.004\n",
      "Ep 1 (Step 000090): Train loss 5.952, Val loss 5.934\n",
      "Ep 1 (Step 000100): Train loss 5.828, Val loss 5.877\n",
      "Ep 1 (Step 000110): Train loss 5.808, Val loss 5.859\n",
      "Ep 1 (Step 000120): Train loss 5.794, Val loss 5.789\n",
      "Ep 1 (Step 000130): Train loss 5.762, Val loss 5.748\n",
      "Ep 1 (Step 000140): Train loss 5.583, Val loss 5.706\n",
      "Ep 1 (Step 000150): Train loss 5.645, Val loss 5.683\n",
      "Ep 1 (Step 000160): Train loss 5.569, Val loss 5.641\n",
      "Ep 1 (Step 000170): Train loss 5.602, Val loss 5.609\n",
      "Ep 1 (Step 000180): Train loss 5.539, Val loss 5.590\n",
      "Ep 1 (Step 000190): Train loss 5.548, Val loss 5.552\n",
      "Ep 1 (Step 000200): Train loss 5.476, Val loss 5.540\n",
      "Ep 1 (Step 000210): Train loss 5.388, Val loss 5.527\n",
      "Ep 1 (Step 000220): Train loss 5.354, Val loss 5.498\n",
      "Ep 1 (Step 000230): Train loss 5.317, Val loss 5.492\n",
      "Ep 1 (Step 000240): Train loss 5.373, Val loss 5.460\n",
      "Ep 1 (Step 000250): Train loss 5.411, Val loss 5.449\n",
      "Ep 1 (Step 000260): Train loss 5.396, Val loss 5.425\n",
      "Ep 1 (Step 000270): Train loss 5.334, Val loss 5.408\n",
      "Ep 1 (Step 000280): Train loss 5.311, Val loss 5.394\n",
      "Ep 1 (Step 000290): Train loss 5.240, Val loss 5.385\n",
      "Ep 1 (Step 000300): Train loss 5.319, Val loss 5.388\n",
      "Ep 1 (Step 000310): Train loss 5.343, Val loss 5.373\n",
      "Ep 1 (Step 000320): Train loss 5.262, Val loss 5.358\n",
      "Ep 1 (Step 000330): Train loss 5.193, Val loss 5.345\n",
      "Ep 1 (Step 000340): Train loss 5.204, Val loss 5.343\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3427\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.930, Val loss 8.921\n",
      "Ep 1 (Step 000010): Train loss 7.392, Val loss 7.424\n",
      "Ep 1 (Step 000020): Train loss 6.792, Val loss 6.783\n",
      "Ep 1 (Step 000030): Train loss 6.470, Val loss 6.491\n",
      "Ep 1 (Step 000040): Train loss 6.430, Val loss 6.398\n",
      "Ep 1 (Step 000050): Train loss 6.383, Val loss 6.348\n",
      "Ep 1 (Step 000060): Train loss 6.291, Val loss 6.275\n",
      "Ep 1 (Step 000070): Train loss 6.201, Val loss 6.166\n",
      "Ep 1 (Step 000080): Train loss 6.046, Val loss 6.040\n",
      "Ep 1 (Step 000090): Train loss 5.978, Val loss 5.992\n",
      "Ep 1 (Step 000100): Train loss 5.880, Val loss 5.927\n",
      "Ep 1 (Step 000110): Train loss 5.859, Val loss 5.879\n",
      "Ep 1 (Step 000120): Train loss 5.782, Val loss 5.806\n",
      "Ep 1 (Step 000130): Train loss 5.772, Val loss 5.771\n",
      "Ep 1 (Step 000140): Train loss 5.712, Val loss 5.756\n",
      "Ep 1 (Step 000150): Train loss 5.650, Val loss 5.709\n",
      "Ep 1 (Step 000160): Train loss 5.663, Val loss 5.658\n",
      "Ep 1 (Step 000170): Train loss 5.576, Val loss 5.639\n",
      "Ep 1 (Step 000180): Train loss 5.517, Val loss 5.587\n",
      "Ep 1 (Step 000190): Train loss 5.465, Val loss 5.570\n",
      "Ep 1 (Step 000200): Train loss 5.495, Val loss 5.552\n",
      "Ep 1 (Step 000210): Train loss 5.445, Val loss 5.519\n",
      "Ep 1 (Step 000220): Train loss 5.554, Val loss 5.497\n",
      "Ep 1 (Step 000230): Train loss 5.384, Val loss 5.494\n",
      "Ep 1 (Step 000240): Train loss 5.446, Val loss 5.470\n",
      "Ep 1 (Step 000250): Train loss 5.342, Val loss 5.462\n",
      "Ep 1 (Step 000260): Train loss 5.386, Val loss 5.444\n",
      "Ep 1 (Step 000270): Train loss 5.305, Val loss 5.426\n",
      "Ep 1 (Step 000280): Train loss 5.349, Val loss 5.420\n",
      "Ep 1 (Step 000290): Train loss 5.328, Val loss 5.402\n",
      "Ep 1 (Step 000300): Train loss 5.323, Val loss 5.385\n",
      "Ep 1 (Step 000310): Train loss 5.250, Val loss 5.387\n",
      "Ep 1 (Step 000320): Train loss 5.300, Val loss 5.375\n",
      "Ep 1 (Step 000330): Train loss 5.240, Val loss 5.357\n",
      "Ep 1 (Step 000340): Train loss 5.222, Val loss 5.341\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3408\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.657, Val loss 8.647\n",
      "Ep 1 (Step 000010): Train loss 6.864, Val loss 6.808\n",
      "Ep 1 (Step 000020): Train loss 6.512, Val loss 6.459\n",
      "Ep 1 (Step 000030): Train loss 6.460, Val loss 6.444\n",
      "Ep 1 (Step 000040): Train loss 6.316, Val loss 6.333\n",
      "Ep 1 (Step 000050): Train loss 6.152, Val loss 6.166\n",
      "Ep 1 (Step 000060): Train loss 6.090, Val loss 6.077\n",
      "Ep 1 (Step 000070): Train loss 5.903, Val loss 5.961\n",
      "Ep 1 (Step 000080): Train loss 5.900, Val loss 5.901\n",
      "Ep 1 (Step 000090): Train loss 5.794, Val loss 5.820\n",
      "Ep 1 (Step 000100): Train loss 5.695, Val loss 5.765\n",
      "Ep 1 (Step 000110): Train loss 5.659, Val loss 5.699\n",
      "Ep 1 (Step 000120): Train loss 5.665, Val loss 5.657\n",
      "Ep 1 (Step 000130): Train loss 5.545, Val loss 5.631\n",
      "Ep 1 (Step 000140): Train loss 5.550, Val loss 5.608\n",
      "Ep 1 (Step 000150): Train loss 5.442, Val loss 5.563\n",
      "Ep 1 (Step 000160): Train loss 5.468, Val loss 5.535\n",
      "Ep 1 (Step 000170): Train loss 5.345, Val loss 5.515\n",
      "Ep 1 (Step 000180): Train loss 5.415, Val loss 5.489\n",
      "Ep 1 (Step 000190): Train loss 5.332, Val loss 5.481\n",
      "Ep 1 (Step 000200): Train loss 5.310, Val loss 5.462\n",
      "Ep 1 (Step 000210): Train loss 5.372, Val loss 5.445\n",
      "Ep 1 (Step 000220): Train loss 5.372, Val loss 5.424\n",
      "Ep 1 (Step 000230): Train loss 5.414, Val loss 5.420\n",
      "Ep 1 (Step 000240): Train loss 5.286, Val loss 5.413\n",
      "Ep 1 (Step 000250): Train loss 5.280, Val loss 5.385\n",
      "Ep 1 (Step 000260): Train loss 5.312, Val loss 5.378\n",
      "Ep 1 (Step 000270): Train loss 5.263, Val loss 5.369\n",
      "Ep 1 (Step 000280): Train loss 5.225, Val loss 5.361\n",
      "Ep 1 (Step 000290): Train loss 5.136, Val loss 5.331\n",
      "Ep 1 (Step 000300): Train loss 5.198, Val loss 5.308\n",
      "Ep 1 (Step 000310): Train loss 5.122, Val loss 5.293\n",
      "Ep 1 (Step 000320): Train loss 5.132, Val loss 5.278\n",
      "Ep 1 (Step 000330): Train loss 5.156, Val loss 5.280\n",
      "Ep 1 (Step 000340): Train loss 5.068, Val loss 5.279\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2790\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.633, Val loss 8.588\n",
      "Ep 1 (Step 000010): Train loss 6.811, Val loss 6.815\n",
      "Ep 1 (Step 000020): Train loss 6.471, Val loss 6.462\n",
      "Ep 1 (Step 000030): Train loss 6.468, Val loss 6.440\n",
      "Ep 1 (Step 000040): Train loss 6.363, Val loss 6.324\n",
      "Ep 1 (Step 000050): Train loss 6.124, Val loss 6.183\n",
      "Ep 1 (Step 000060): Train loss 6.050, Val loss 6.092\n",
      "Ep 1 (Step 000070): Train loss 5.998, Val loss 5.986\n",
      "Ep 1 (Step 000080): Train loss 5.793, Val loss 5.897\n",
      "Ep 1 (Step 000090): Train loss 5.883, Val loss 5.830\n",
      "Ep 1 (Step 000100): Train loss 5.662, Val loss 5.764\n",
      "Ep 1 (Step 000110): Train loss 5.681, Val loss 5.703\n",
      "Ep 1 (Step 000120): Train loss 5.540, Val loss 5.656\n",
      "Ep 1 (Step 000130): Train loss 5.596, Val loss 5.618\n",
      "Ep 1 (Step 000140): Train loss 5.502, Val loss 5.601\n",
      "Ep 1 (Step 000150): Train loss 5.501, Val loss 5.574\n",
      "Ep 1 (Step 000160): Train loss 5.436, Val loss 5.541\n",
      "Ep 1 (Step 000170): Train loss 5.388, Val loss 5.535\n",
      "Ep 1 (Step 000180): Train loss 5.502, Val loss 5.514\n",
      "Ep 1 (Step 000190): Train loss 5.362, Val loss 5.472\n",
      "Ep 1 (Step 000200): Train loss 5.341, Val loss 5.451\n",
      "Ep 1 (Step 000210): Train loss 5.391, Val loss 5.436\n",
      "Ep 1 (Step 000220): Train loss 5.341, Val loss 5.423\n",
      "Ep 1 (Step 000230): Train loss 5.352, Val loss 5.399\n",
      "Ep 1 (Step 000240): Train loss 5.265, Val loss 5.401\n",
      "Ep 1 (Step 000250): Train loss 5.285, Val loss 5.379\n",
      "Ep 1 (Step 000260): Train loss 5.324, Val loss 5.359\n",
      "Ep 1 (Step 000270): Train loss 5.251, Val loss 5.337\n",
      "Ep 1 (Step 000280): Train loss 5.250, Val loss 5.343\n",
      "Ep 1 (Step 000290): Train loss 5.228, Val loss 5.310\n",
      "Ep 1 (Step 000300): Train loss 5.215, Val loss 5.308\n",
      "Ep 1 (Step 000310): Train loss 5.156, Val loss 5.295\n",
      "Ep 1 (Step 000320): Train loss 5.178, Val loss 5.288\n",
      "Ep 1 (Step 000330): Train loss 5.171, Val loss 5.276\n",
      "Ep 1 (Step 000340): Train loss 5.121, Val loss 5.274\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2740\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.679, Val loss 8.657\n",
      "Ep 1 (Step 000010): Train loss 6.840, Val loss 6.845\n",
      "Ep 1 (Step 000020): Train loss 6.491, Val loss 6.471\n",
      "Ep 1 (Step 000030): Train loss 6.382, Val loss 6.417\n",
      "Ep 1 (Step 000040): Train loss 6.227, Val loss 6.279\n",
      "Ep 1 (Step 000050): Train loss 6.138, Val loss 6.177\n",
      "Ep 1 (Step 000060): Train loss 6.038, Val loss 6.056\n",
      "Ep 1 (Step 000070): Train loss 5.804, Val loss 5.946\n",
      "Ep 1 (Step 000080): Train loss 5.819, Val loss 5.867\n",
      "Ep 1 (Step 000090): Train loss 5.837, Val loss 5.811\n",
      "Ep 1 (Step 000100): Train loss 5.681, Val loss 5.750\n",
      "Ep 1 (Step 000110): Train loss 5.606, Val loss 5.697\n",
      "Ep 1 (Step 000120): Train loss 5.603, Val loss 5.651\n",
      "Ep 1 (Step 000130): Train loss 5.542, Val loss 5.612\n",
      "Ep 1 (Step 000140): Train loss 5.500, Val loss 5.593\n",
      "Ep 1 (Step 000150): Train loss 5.495, Val loss 5.567\n",
      "Ep 1 (Step 000160): Train loss 5.433, Val loss 5.502\n",
      "Ep 1 (Step 000170): Train loss 5.342, Val loss 5.493\n",
      "Ep 1 (Step 000180): Train loss 5.353, Val loss 5.474\n",
      "Ep 1 (Step 000190): Train loss 5.402, Val loss 5.475\n",
      "Ep 1 (Step 000200): Train loss 5.362, Val loss 5.444\n",
      "Ep 1 (Step 000210): Train loss 5.307, Val loss 5.431\n",
      "Ep 1 (Step 000220): Train loss 5.349, Val loss 5.399\n",
      "Ep 1 (Step 000230): Train loss 5.352, Val loss 5.380\n",
      "Ep 1 (Step 000240): Train loss 5.219, Val loss 5.356\n",
      "Ep 1 (Step 000250): Train loss 5.249, Val loss 5.362\n",
      "Ep 1 (Step 000260): Train loss 5.223, Val loss 5.354\n",
      "Ep 1 (Step 000270): Train loss 5.162, Val loss 5.336\n",
      "Ep 1 (Step 000280): Train loss 5.181, Val loss 5.325\n",
      "Ep 1 (Step 000290): Train loss 5.155, Val loss 5.324\n",
      "Ep 1 (Step 000300): Train loss 5.242, Val loss 5.323\n",
      "Ep 1 (Step 000310): Train loss 5.206, Val loss 5.296\n",
      "Ep 1 (Step 000320): Train loss 5.146, Val loss 5.299\n",
      "Ep 1 (Step 000330): Train loss 5.125, Val loss 5.291\n",
      "Ep 1 (Step 000340): Train loss 5.089, Val loss 5.287\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2867\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.684, Val loss 8.678\n",
      "Ep 1 (Step 000010): Train loss 6.899, Val loss 6.857\n",
      "Ep 1 (Step 000020): Train loss 6.494, Val loss 6.478\n",
      "Ep 1 (Step 000030): Train loss 6.457, Val loss 6.475\n",
      "Ep 1 (Step 000040): Train loss 6.299, Val loss 6.348\n",
      "Ep 1 (Step 000050): Train loss 6.123, Val loss 6.199\n",
      "Ep 1 (Step 000060): Train loss 6.013, Val loss 6.067\n",
      "Ep 1 (Step 000070): Train loss 6.000, Val loss 5.988\n",
      "Ep 1 (Step 000080): Train loss 5.918, Val loss 5.901\n",
      "Ep 1 (Step 000090): Train loss 5.850, Val loss 5.854\n",
      "Ep 1 (Step 000100): Train loss 5.649, Val loss 5.750\n",
      "Ep 1 (Step 000110): Train loss 5.596, Val loss 5.710\n",
      "Ep 1 (Step 000120): Train loss 5.577, Val loss 5.648\n",
      "Ep 1 (Step 000130): Train loss 5.544, Val loss 5.632\n",
      "Ep 1 (Step 000140): Train loss 5.582, Val loss 5.574\n",
      "Ep 1 (Step 000150): Train loss 5.477, Val loss 5.544\n",
      "Ep 1 (Step 000160): Train loss 5.470, Val loss 5.505\n",
      "Ep 1 (Step 000170): Train loss 5.386, Val loss 5.488\n",
      "Ep 1 (Step 000180): Train loss 5.376, Val loss 5.446\n",
      "Ep 1 (Step 000190): Train loss 5.403, Val loss 5.437\n",
      "Ep 1 (Step 000200): Train loss 5.357, Val loss 5.415\n",
      "Ep 1 (Step 000210): Train loss 5.314, Val loss 5.415\n",
      "Ep 1 (Step 000220): Train loss 5.246, Val loss 5.413\n",
      "Ep 1 (Step 000230): Train loss 5.386, Val loss 5.377\n",
      "Ep 1 (Step 000240): Train loss 5.231, Val loss 5.362\n",
      "Ep 1 (Step 000250): Train loss 5.231, Val loss 5.343\n",
      "Ep 1 (Step 000260): Train loss 5.236, Val loss 5.322\n",
      "Ep 1 (Step 000270): Train loss 5.201, Val loss 5.320\n",
      "Ep 1 (Step 000280): Train loss 5.202, Val loss 5.307\n",
      "Ep 1 (Step 000290): Train loss 5.211, Val loss 5.293\n",
      "Ep 1 (Step 000300): Train loss 5.204, Val loss 5.281\n",
      "Ep 1 (Step 000310): Train loss 5.103, Val loss 5.281\n",
      "Ep 1 (Step 000320): Train loss 5.211, Val loss 5.270\n",
      "Ep 1 (Step 000330): Train loss 5.108, Val loss 5.239\n",
      "Ep 1 (Step 000340): Train loss 5.128, Val loss 5.258\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2578\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.660, Val loss 8.635\n",
      "Ep 1 (Step 000010): Train loss 6.909, Val loss 6.872\n",
      "Ep 1 (Step 000020): Train loss 6.440, Val loss 6.471\n",
      "Ep 1 (Step 000030): Train loss 6.460, Val loss 6.454\n",
      "Ep 1 (Step 000040): Train loss 6.319, Val loss 6.357\n",
      "Ep 1 (Step 000050): Train loss 6.247, Val loss 6.227\n",
      "Ep 1 (Step 000060): Train loss 6.092, Val loss 6.070\n",
      "Ep 1 (Step 000070): Train loss 6.043, Val loss 5.972\n",
      "Ep 1 (Step 000080): Train loss 5.894, Val loss 5.898\n",
      "Ep 1 (Step 000090): Train loss 5.795, Val loss 5.818\n",
      "Ep 1 (Step 000100): Train loss 5.718, Val loss 5.770\n",
      "Ep 1 (Step 000110): Train loss 5.649, Val loss 5.700\n",
      "Ep 1 (Step 000120): Train loss 5.586, Val loss 5.648\n",
      "Ep 1 (Step 000130): Train loss 5.607, Val loss 5.593\n",
      "Ep 1 (Step 000140): Train loss 5.575, Val loss 5.565\n",
      "Ep 1 (Step 000150): Train loss 5.553, Val loss 5.544\n",
      "Ep 1 (Step 000160): Train loss 5.402, Val loss 5.526\n",
      "Ep 1 (Step 000170): Train loss 5.445, Val loss 5.502\n",
      "Ep 1 (Step 000180): Train loss 5.365, Val loss 5.464\n",
      "Ep 1 (Step 000190): Train loss 5.339, Val loss 5.431\n",
      "Ep 1 (Step 000200): Train loss 5.289, Val loss 5.409\n",
      "Ep 1 (Step 000210): Train loss 5.356, Val loss 5.397\n",
      "Ep 1 (Step 000220): Train loss 5.312, Val loss 5.399\n",
      "Ep 1 (Step 000230): Train loss 5.341, Val loss 5.374\n",
      "Ep 1 (Step 000240): Train loss 5.200, Val loss 5.359\n",
      "Ep 1 (Step 000250): Train loss 5.267, Val loss 5.357\n",
      "Ep 1 (Step 000260): Train loss 5.134, Val loss 5.352\n",
      "Ep 1 (Step 000270): Train loss 5.289, Val loss 5.328\n",
      "Ep 1 (Step 000280): Train loss 5.142, Val loss 5.295\n",
      "Ep 1 (Step 000290): Train loss 5.158, Val loss 5.289\n",
      "Ep 1 (Step 000300): Train loss 5.050, Val loss 5.290\n",
      "Ep 1 (Step 000310): Train loss 5.220, Val loss 5.294\n",
      "Ep 1 (Step 000320): Train loss 5.155, Val loss 5.262\n",
      "Ep 1 (Step 000330): Train loss 5.163, Val loss 5.274\n",
      "Ep 1 (Step 000340): Train loss 5.190, Val loss 5.252\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2519\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.673, Val loss 8.641\n",
      "Ep 1 (Step 000010): Train loss 6.885, Val loss 6.831\n",
      "Ep 1 (Step 000020): Train loss 6.498, Val loss 6.452\n",
      "Ep 1 (Step 000030): Train loss 6.481, Val loss 6.425\n",
      "Ep 1 (Step 000040): Train loss 6.336, Val loss 6.329\n",
      "Ep 1 (Step 000050): Train loss 6.168, Val loss 6.170\n",
      "Ep 1 (Step 000060): Train loss 5.999, Val loss 6.042\n",
      "Ep 1 (Step 000070): Train loss 5.945, Val loss 5.928\n",
      "Ep 1 (Step 000080): Train loss 5.847, Val loss 5.859\n",
      "Ep 1 (Step 000090): Train loss 5.751, Val loss 5.774\n",
      "Ep 1 (Step 000100): Train loss 5.708, Val loss 5.708\n",
      "Ep 1 (Step 000110): Train loss 5.678, Val loss 5.675\n",
      "Ep 1 (Step 000120): Train loss 5.541, Val loss 5.638\n",
      "Ep 1 (Step 000130): Train loss 5.543, Val loss 5.604\n",
      "Ep 1 (Step 000140): Train loss 5.555, Val loss 5.552\n",
      "Ep 1 (Step 000150): Train loss 5.460, Val loss 5.524\n",
      "Ep 1 (Step 000160): Train loss 5.373, Val loss 5.515\n",
      "Ep 1 (Step 000170): Train loss 5.520, Val loss 5.479\n",
      "Ep 1 (Step 000180): Train loss 5.395, Val loss 5.460\n",
      "Ep 1 (Step 000190): Train loss 5.402, Val loss 5.428\n",
      "Ep 1 (Step 000200): Train loss 5.325, Val loss 5.415\n",
      "Ep 1 (Step 000210): Train loss 5.380, Val loss 5.416\n",
      "Ep 1 (Step 000220): Train loss 5.347, Val loss 5.398\n",
      "Ep 1 (Step 000230): Train loss 5.262, Val loss 5.379\n",
      "Ep 1 (Step 000240): Train loss 5.235, Val loss 5.361\n",
      "Ep 1 (Step 000250): Train loss 5.216, Val loss 5.332\n",
      "Ep 1 (Step 000260): Train loss 5.211, Val loss 5.354\n",
      "Ep 1 (Step 000270): Train loss 5.235, Val loss 5.328\n",
      "Ep 1 (Step 000280): Train loss 5.221, Val loss 5.326\n",
      "Ep 1 (Step 000290): Train loss 5.154, Val loss 5.323\n",
      "Ep 1 (Step 000300): Train loss 5.232, Val loss 5.313\n",
      "Ep 1 (Step 000310): Train loss 5.065, Val loss 5.308\n",
      "Ep 1 (Step 000320): Train loss 5.116, Val loss 5.295\n",
      "Ep 1 (Step 000330): Train loss 5.143, Val loss 5.276\n",
      "Ep 1 (Step 000340): Train loss 5.151, Val loss 5.262\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2618\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.939, Val loss 8.937\n",
      "Ep 1 (Step 000010): Train loss 7.464, Val loss 7.431\n",
      "Ep 1 (Step 000020): Train loss 6.833, Val loss 6.800\n",
      "Ep 1 (Step 000030): Train loss 6.495, Val loss 6.477\n",
      "Ep 1 (Step 000040): Train loss 6.375, Val loss 6.370\n",
      "Ep 1 (Step 000050): Train loss 6.353, Val loss 6.323\n",
      "Ep 1 (Step 000060): Train loss 6.240, Val loss 6.227\n",
      "Ep 1 (Step 000070): Train loss 6.110, Val loss 6.124\n",
      "Ep 1 (Step 000080): Train loss 5.990, Val loss 6.028\n",
      "Ep 1 (Step 000090): Train loss 5.929, Val loss 5.957\n",
      "Ep 1 (Step 000100): Train loss 5.872, Val loss 5.898\n",
      "Ep 1 (Step 000110): Train loss 5.798, Val loss 5.864\n",
      "Ep 1 (Step 000120): Train loss 5.768, Val loss 5.797\n",
      "Ep 1 (Step 000130): Train loss 5.696, Val loss 5.763\n",
      "Ep 1 (Step 000140): Train loss 5.680, Val loss 5.730\n",
      "Ep 1 (Step 000150): Train loss 5.586, Val loss 5.685\n",
      "Ep 1 (Step 000160): Train loss 5.588, Val loss 5.662\n",
      "Ep 1 (Step 000170): Train loss 5.617, Val loss 5.628\n",
      "Ep 1 (Step 000180): Train loss 5.481, Val loss 5.605\n",
      "Ep 1 (Step 000190): Train loss 5.461, Val loss 5.582\n",
      "Ep 1 (Step 000200): Train loss 5.511, Val loss 5.554\n",
      "Ep 1 (Step 000210): Train loss 5.468, Val loss 5.528\n",
      "Ep 1 (Step 000220): Train loss 5.466, Val loss 5.524\n",
      "Ep 1 (Step 000230): Train loss 5.437, Val loss 5.512\n",
      "Ep 1 (Step 000240): Train loss 5.453, Val loss 5.484\n",
      "Ep 1 (Step 000250): Train loss 5.359, Val loss 5.476\n",
      "Ep 1 (Step 000260): Train loss 5.386, Val loss 5.456\n",
      "Ep 1 (Step 000270): Train loss 5.447, Val loss 5.448\n",
      "Ep 1 (Step 000280): Train loss 5.281, Val loss 5.429\n",
      "Ep 1 (Step 000290): Train loss 5.307, Val loss 5.408\n",
      "Ep 1 (Step 000300): Train loss 5.264, Val loss 5.416\n",
      "Ep 1 (Step 000310): Train loss 5.338, Val loss 5.404\n",
      "Ep 1 (Step 000320): Train loss 5.271, Val loss 5.384\n",
      "Ep 1 (Step 000330): Train loss 5.275, Val loss 5.382\n",
      "Ep 1 (Step 000340): Train loss 5.254, Val loss 5.355\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3550\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.906, Val loss 8.890\n",
      "Ep 1 (Step 000010): Train loss 7.448, Val loss 7.383\n",
      "Ep 1 (Step 000020): Train loss 6.790, Val loss 6.739\n",
      "Ep 1 (Step 000030): Train loss 6.506, Val loss 6.449\n",
      "Ep 1 (Step 000040): Train loss 6.382, Val loss 6.373\n",
      "Ep 1 (Step 000050): Train loss 6.377, Val loss 6.353\n",
      "Ep 1 (Step 000060): Train loss 6.252, Val loss 6.271\n",
      "Ep 1 (Step 000070): Train loss 6.135, Val loss 6.159\n",
      "Ep 1 (Step 000080): Train loss 6.065, Val loss 6.071\n",
      "Ep 1 (Step 000090): Train loss 5.926, Val loss 5.980\n",
      "Ep 1 (Step 000100): Train loss 5.916, Val loss 5.914\n",
      "Ep 1 (Step 000110): Train loss 5.869, Val loss 5.878\n",
      "Ep 1 (Step 000120): Train loss 5.764, Val loss 5.818\n",
      "Ep 1 (Step 000130): Train loss 5.747, Val loss 5.783\n",
      "Ep 1 (Step 000140): Train loss 5.727, Val loss 5.733\n",
      "Ep 1 (Step 000150): Train loss 5.704, Val loss 5.714\n",
      "Ep 1 (Step 000160): Train loss 5.563, Val loss 5.672\n",
      "Ep 1 (Step 000170): Train loss 5.609, Val loss 5.664\n",
      "Ep 1 (Step 000180): Train loss 5.481, Val loss 5.612\n",
      "Ep 1 (Step 000190): Train loss 5.552, Val loss 5.598\n",
      "Ep 1 (Step 000200): Train loss 5.557, Val loss 5.576\n",
      "Ep 1 (Step 000210): Train loss 5.476, Val loss 5.555\n",
      "Ep 1 (Step 000220): Train loss 5.328, Val loss 5.531\n",
      "Ep 1 (Step 000230): Train loss 5.463, Val loss 5.526\n",
      "Ep 1 (Step 000240): Train loss 5.457, Val loss 5.492\n",
      "Ep 1 (Step 000250): Train loss 5.389, Val loss 5.465\n",
      "Ep 1 (Step 000260): Train loss 5.372, Val loss 5.467\n",
      "Ep 1 (Step 000270): Train loss 5.321, Val loss 5.441\n",
      "Ep 1 (Step 000280): Train loss 5.308, Val loss 5.450\n",
      "Ep 1 (Step 000290): Train loss 5.240, Val loss 5.430\n",
      "Ep 1 (Step 000300): Train loss 5.348, Val loss 5.419\n",
      "Ep 1 (Step 000310): Train loss 5.276, Val loss 5.402\n",
      "Ep 1 (Step 000320): Train loss 5.152, Val loss 5.388\n",
      "Ep 1 (Step 000330): Train loss 5.272, Val loss 5.386\n",
      "Ep 1 (Step 000340): Train loss 5.246, Val loss 5.372\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3724\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.902, Val loss 8.861\n",
      "Ep 1 (Step 000010): Train loss 7.381, Val loss 7.363\n",
      "Ep 1 (Step 000020): Train loss 6.802, Val loss 6.732\n",
      "Ep 1 (Step 000030): Train loss 6.460, Val loss 6.449\n",
      "Ep 1 (Step 000040): Train loss 6.405, Val loss 6.360\n",
      "Ep 1 (Step 000050): Train loss 6.375, Val loss 6.299\n",
      "Ep 1 (Step 000060): Train loss 6.188, Val loss 6.208\n",
      "Ep 1 (Step 000070): Train loss 6.140, Val loss 6.111\n",
      "Ep 1 (Step 000080): Train loss 6.023, Val loss 6.021\n",
      "Ep 1 (Step 000090): Train loss 5.897, Val loss 5.975\n",
      "Ep 1 (Step 000100): Train loss 5.874, Val loss 5.900\n",
      "Ep 1 (Step 000110): Train loss 5.812, Val loss 5.851\n",
      "Ep 1 (Step 000120): Train loss 5.772, Val loss 5.814\n",
      "Ep 1 (Step 000130): Train loss 5.689, Val loss 5.771\n",
      "Ep 1 (Step 000140): Train loss 5.655, Val loss 5.736\n",
      "Ep 1 (Step 000150): Train loss 5.587, Val loss 5.698\n",
      "Ep 1 (Step 000160): Train loss 5.631, Val loss 5.666\n",
      "Ep 1 (Step 000170): Train loss 5.593, Val loss 5.628\n",
      "Ep 1 (Step 000180): Train loss 5.549, Val loss 5.590\n",
      "Ep 1 (Step 000190): Train loss 5.520, Val loss 5.573\n",
      "Ep 1 (Step 000200): Train loss 5.575, Val loss 5.546\n",
      "Ep 1 (Step 000210): Train loss 5.428, Val loss 5.516\n",
      "Ep 1 (Step 000220): Train loss 5.368, Val loss 5.506\n",
      "Ep 1 (Step 000230): Train loss 5.394, Val loss 5.490\n",
      "Ep 1 (Step 000240): Train loss 5.335, Val loss 5.476\n",
      "Ep 1 (Step 000250): Train loss 5.427, Val loss 5.465\n",
      "Ep 1 (Step 000260): Train loss 5.356, Val loss 5.449\n",
      "Ep 1 (Step 000270): Train loss 5.359, Val loss 5.443\n",
      "Ep 1 (Step 000280): Train loss 5.330, Val loss 5.425\n",
      "Ep 1 (Step 000290): Train loss 5.364, Val loss 5.409\n",
      "Ep 1 (Step 000300): Train loss 5.222, Val loss 5.396\n",
      "Ep 1 (Step 000310): Train loss 5.268, Val loss 5.392\n",
      "Ep 1 (Step 000320): Train loss 5.309, Val loss 5.396\n",
      "Ep 1 (Step 000330): Train loss 5.171, Val loss 5.367\n",
      "Ep 1 (Step 000340): Train loss 5.211, Val loss 5.362\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3623\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.958, Val loss 8.932\n",
      "Ep 1 (Step 000010): Train loss 7.451, Val loss 7.422\n",
      "Ep 1 (Step 000020): Train loss 6.785, Val loss 6.769\n",
      "Ep 1 (Step 000030): Train loss 6.496, Val loss 6.456\n",
      "Ep 1 (Step 000040): Train loss 6.405, Val loss 6.365\n",
      "Ep 1 (Step 000050): Train loss 6.333, Val loss 6.334\n",
      "Ep 1 (Step 000060): Train loss 6.200, Val loss 6.233\n",
      "Ep 1 (Step 000070): Train loss 6.143, Val loss 6.134\n",
      "Ep 1 (Step 000080): Train loss 5.988, Val loss 6.037\n",
      "Ep 1 (Step 000090): Train loss 5.994, Val loss 5.966\n",
      "Ep 1 (Step 000100): Train loss 5.900, Val loss 5.891\n",
      "Ep 1 (Step 000110): Train loss 5.818, Val loss 5.848\n",
      "Ep 1 (Step 000120): Train loss 5.768, Val loss 5.785\n",
      "Ep 1 (Step 000130): Train loss 5.718, Val loss 5.748\n",
      "Ep 1 (Step 000140): Train loss 5.659, Val loss 5.718\n",
      "Ep 1 (Step 000150): Train loss 5.608, Val loss 5.682\n",
      "Ep 1 (Step 000160): Train loss 5.608, Val loss 5.655\n",
      "Ep 1 (Step 000170): Train loss 5.640, Val loss 5.635\n",
      "Ep 1 (Step 000180): Train loss 5.496, Val loss 5.591\n",
      "Ep 1 (Step 000190): Train loss 5.488, Val loss 5.576\n",
      "Ep 1 (Step 000200): Train loss 5.526, Val loss 5.556\n",
      "Ep 1 (Step 000210): Train loss 5.439, Val loss 5.531\n",
      "Ep 1 (Step 000220): Train loss 5.470, Val loss 5.501\n",
      "Ep 1 (Step 000230): Train loss 5.452, Val loss 5.494\n",
      "Ep 1 (Step 000240): Train loss 5.385, Val loss 5.484\n",
      "Ep 1 (Step 000250): Train loss 5.346, Val loss 5.459\n",
      "Ep 1 (Step 000260): Train loss 5.330, Val loss 5.448\n",
      "Ep 1 (Step 000270): Train loss 5.358, Val loss 5.440\n",
      "Ep 1 (Step 000280): Train loss 5.390, Val loss 5.420\n",
      "Ep 1 (Step 000290): Train loss 5.309, Val loss 5.404\n",
      "Ep 1 (Step 000300): Train loss 5.378, Val loss 5.394\n",
      "Ep 1 (Step 000310): Train loss 5.260, Val loss 5.378\n",
      "Ep 1 (Step 000320): Train loss 5.178, Val loss 5.371\n",
      "Ep 1 (Step 000330): Train loss 5.311, Val loss 5.352\n",
      "Ep 1 (Step 000340): Train loss 5.260, Val loss 5.340\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3398\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.901, Val loss 8.912\n",
      "Ep 1 (Step 000010): Train loss 7.429, Val loss 7.447\n",
      "Ep 1 (Step 000020): Train loss 6.835, Val loss 6.799\n",
      "Ep 1 (Step 000030): Train loss 6.459, Val loss 6.476\n",
      "Ep 1 (Step 000040): Train loss 6.411, Val loss 6.389\n",
      "Ep 1 (Step 000050): Train loss 6.355, Val loss 6.353\n",
      "Ep 1 (Step 000060): Train loss 6.258, Val loss 6.254\n",
      "Ep 1 (Step 000070): Train loss 6.087, Val loss 6.134\n",
      "Ep 1 (Step 000080): Train loss 6.046, Val loss 6.047\n",
      "Ep 1 (Step 000090): Train loss 5.985, Val loss 5.966\n",
      "Ep 1 (Step 000100): Train loss 5.945, Val loss 5.910\n",
      "Ep 1 (Step 000110): Train loss 5.773, Val loss 5.859\n",
      "Ep 1 (Step 000120): Train loss 5.751, Val loss 5.820\n",
      "Ep 1 (Step 000130): Train loss 5.691, Val loss 5.774\n",
      "Ep 1 (Step 000140): Train loss 5.696, Val loss 5.732\n",
      "Ep 1 (Step 000150): Train loss 5.635, Val loss 5.699\n",
      "Ep 1 (Step 000160): Train loss 5.621, Val loss 5.670\n",
      "Ep 1 (Step 000170): Train loss 5.516, Val loss 5.635\n",
      "Ep 1 (Step 000180): Train loss 5.494, Val loss 5.597\n",
      "Ep 1 (Step 000190): Train loss 5.514, Val loss 5.574\n",
      "Ep 1 (Step 000200): Train loss 5.486, Val loss 5.549\n",
      "Ep 1 (Step 000210): Train loss 5.544, Val loss 5.533\n",
      "Ep 1 (Step 000220): Train loss 5.404, Val loss 5.506\n",
      "Ep 1 (Step 000230): Train loss 5.391, Val loss 5.493\n",
      "Ep 1 (Step 000240): Train loss 5.365, Val loss 5.472\n",
      "Ep 1 (Step 000250): Train loss 5.394, Val loss 5.469\n",
      "Ep 1 (Step 000260): Train loss 5.438, Val loss 5.443\n",
      "Ep 1 (Step 000270): Train loss 5.308, Val loss 5.423\n",
      "Ep 1 (Step 000280): Train loss 5.262, Val loss 5.413\n",
      "Ep 1 (Step 000290): Train loss 5.309, Val loss 5.408\n",
      "Ep 1 (Step 000300): Train loss 5.250, Val loss 5.382\n",
      "Ep 1 (Step 000310): Train loss 5.289, Val loss 5.370\n",
      "Ep 1 (Step 000320): Train loss 5.232, Val loss 5.354\n",
      "Ep 1 (Step 000330): Train loss 5.208, Val loss 5.352\n",
      "Ep 1 (Step 000340): Train loss 5.273, Val loss 5.324\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3236\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.915, Val loss 8.909\n",
      "Ep 1 (Step 000010): Train loss 7.464, Val loss 7.434\n",
      "Ep 1 (Step 000020): Train loss 6.772, Val loss 6.784\n",
      "Ep 1 (Step 000030): Train loss 6.462, Val loss 6.466\n",
      "Ep 1 (Step 000040): Train loss 6.423, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.355, Val loss 6.324\n",
      "Ep 1 (Step 000060): Train loss 6.222, Val loss 6.237\n",
      "Ep 1 (Step 000070): Train loss 6.065, Val loss 6.134\n",
      "Ep 1 (Step 000080): Train loss 6.041, Val loss 6.037\n",
      "Ep 1 (Step 000090): Train loss 5.977, Val loss 5.967\n",
      "Ep 1 (Step 000100): Train loss 5.914, Val loss 5.928\n",
      "Ep 1 (Step 000110): Train loss 5.866, Val loss 5.837\n",
      "Ep 1 (Step 000120): Train loss 5.767, Val loss 5.796\n",
      "Ep 1 (Step 000130): Train loss 5.824, Val loss 5.777\n",
      "Ep 1 (Step 000140): Train loss 5.703, Val loss 5.707\n",
      "Ep 1 (Step 000150): Train loss 5.661, Val loss 5.683\n",
      "Ep 1 (Step 000160): Train loss 5.574, Val loss 5.674\n",
      "Ep 1 (Step 000170): Train loss 5.554, Val loss 5.613\n",
      "Ep 1 (Step 000180): Train loss 5.546, Val loss 5.600\n",
      "Ep 1 (Step 000190): Train loss 5.457, Val loss 5.574\n",
      "Ep 1 (Step 000200): Train loss 5.439, Val loss 5.548\n",
      "Ep 1 (Step 000210): Train loss 5.378, Val loss 5.529\n",
      "Ep 1 (Step 000220): Train loss 5.466, Val loss 5.506\n",
      "Ep 1 (Step 000230): Train loss 5.459, Val loss 5.488\n",
      "Ep 1 (Step 000240): Train loss 5.365, Val loss 5.487\n",
      "Ep 1 (Step 000250): Train loss 5.358, Val loss 5.471\n",
      "Ep 1 (Step 000260): Train loss 5.405, Val loss 5.439\n",
      "Ep 1 (Step 000270): Train loss 5.375, Val loss 5.423\n",
      "Ep 1 (Step 000280): Train loss 5.440, Val loss 5.397\n",
      "Ep 1 (Step 000290): Train loss 5.312, Val loss 5.395\n",
      "Ep 1 (Step 000300): Train loss 5.266, Val loss 5.369\n",
      "Ep 1 (Step 000310): Train loss 5.231, Val loss 5.359\n",
      "Ep 1 (Step 000320): Train loss 5.323, Val loss 5.341\n",
      "Ep 1 (Step 000330): Train loss 5.217, Val loss 5.340\n",
      "Ep 1 (Step 000340): Train loss 5.289, Val loss 5.337\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3370\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.560, Val loss 8.555\n",
      "Ep 1 (Step 000010): Train loss 6.819, Val loss 6.799\n",
      "Ep 1 (Step 000020): Train loss 6.524, Val loss 6.464\n",
      "Ep 1 (Step 000030): Train loss 6.443, Val loss 6.443\n",
      "Ep 1 (Step 000040): Train loss 6.334, Val loss 6.303\n",
      "Ep 1 (Step 000050): Train loss 6.156, Val loss 6.125\n",
      "Ep 1 (Step 000060): Train loss 5.945, Val loss 5.997\n",
      "Ep 1 (Step 000070): Train loss 5.887, Val loss 5.907\n",
      "Ep 1 (Step 000080): Train loss 5.728, Val loss 5.814\n",
      "Ep 1 (Step 000090): Train loss 5.748, Val loss 5.767\n",
      "Ep 1 (Step 000100): Train loss 5.675, Val loss 5.702\n",
      "Ep 1 (Step 000110): Train loss 5.610, Val loss 5.665\n",
      "Ep 1 (Step 000120): Train loss 5.524, Val loss 5.634\n",
      "Ep 1 (Step 000130): Train loss 5.596, Val loss 5.595\n",
      "Ep 1 (Step 000140): Train loss 5.510, Val loss 5.579\n",
      "Ep 1 (Step 000150): Train loss 5.431, Val loss 5.527\n",
      "Ep 1 (Step 000160): Train loss 5.398, Val loss 5.502\n",
      "Ep 1 (Step 000170): Train loss 5.419, Val loss 5.503\n",
      "Ep 1 (Step 000180): Train loss 5.370, Val loss 5.471\n",
      "Ep 1 (Step 000190): Train loss 5.289, Val loss 5.449\n",
      "Ep 1 (Step 000200): Train loss 5.339, Val loss 5.430\n",
      "Ep 1 (Step 000210): Train loss 5.399, Val loss 5.438\n",
      "Ep 1 (Step 000220): Train loss 5.235, Val loss 5.398\n",
      "Ep 1 (Step 000230): Train loss 5.355, Val loss 5.398\n",
      "Ep 1 (Step 000240): Train loss 5.232, Val loss 5.371\n",
      "Ep 1 (Step 000250): Train loss 5.262, Val loss 5.371\n",
      "Ep 1 (Step 000260): Train loss 5.220, Val loss 5.368\n",
      "Ep 1 (Step 000270): Train loss 5.228, Val loss 5.381\n",
      "Ep 1 (Step 000280): Train loss 5.152, Val loss 5.333\n",
      "Ep 1 (Step 000290): Train loss 5.164, Val loss 5.326\n",
      "Ep 1 (Step 000300): Train loss 5.246, Val loss 5.313\n",
      "Ep 1 (Step 000310): Train loss 5.125, Val loss 5.288\n",
      "Ep 1 (Step 000320): Train loss 5.135, Val loss 5.279\n",
      "Ep 1 (Step 000330): Train loss 5.208, Val loss 5.279\n",
      "Ep 1 (Step 000340): Train loss 5.160, Val loss 5.265\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2646\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.574, Val loss 8.568\n",
      "Ep 1 (Step 000010): Train loss 6.867, Val loss 6.845\n",
      "Ep 1 (Step 000020): Train loss 6.497, Val loss 6.490\n",
      "Ep 1 (Step 000030): Train loss 6.458, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.341, Val loss 6.313\n",
      "Ep 1 (Step 000050): Train loss 6.121, Val loss 6.219\n",
      "Ep 1 (Step 000060): Train loss 6.033, Val loss 6.043\n",
      "Ep 1 (Step 000070): Train loss 5.933, Val loss 5.963\n",
      "Ep 1 (Step 000080): Train loss 5.837, Val loss 5.850\n",
      "Ep 1 (Step 000090): Train loss 5.752, Val loss 5.805\n",
      "Ep 1 (Step 000100): Train loss 5.775, Val loss 5.723\n",
      "Ep 1 (Step 000110): Train loss 5.690, Val loss 5.697\n",
      "Ep 1 (Step 000120): Train loss 5.649, Val loss 5.663\n",
      "Ep 1 (Step 000130): Train loss 5.574, Val loss 5.608\n",
      "Ep 1 (Step 000140): Train loss 5.398, Val loss 5.576\n",
      "Ep 1 (Step 000150): Train loss 5.426, Val loss 5.544\n",
      "Ep 1 (Step 000160): Train loss 5.550, Val loss 5.514\n",
      "Ep 1 (Step 000170): Train loss 5.498, Val loss 5.479\n",
      "Ep 1 (Step 000180): Train loss 5.352, Val loss 5.460\n",
      "Ep 1 (Step 000190): Train loss 5.361, Val loss 5.427\n",
      "Ep 1 (Step 000200): Train loss 5.345, Val loss 5.428\n",
      "Ep 1 (Step 000210): Train loss 5.318, Val loss 5.417\n",
      "Ep 1 (Step 000220): Train loss 5.279, Val loss 5.389\n",
      "Ep 1 (Step 000230): Train loss 5.302, Val loss 5.386\n",
      "Ep 1 (Step 000240): Train loss 5.226, Val loss 5.380\n",
      "Ep 1 (Step 000250): Train loss 5.189, Val loss 5.347\n",
      "Ep 1 (Step 000260): Train loss 5.206, Val loss 5.337\n",
      "Ep 1 (Step 000270): Train loss 5.273, Val loss 5.340\n",
      "Ep 1 (Step 000280): Train loss 5.194, Val loss 5.325\n",
      "Ep 1 (Step 000290): Train loss 5.212, Val loss 5.323\n",
      "Ep 1 (Step 000300): Train loss 5.298, Val loss 5.319\n",
      "Ep 1 (Step 000310): Train loss 5.218, Val loss 5.286\n",
      "Ep 1 (Step 000320): Train loss 5.141, Val loss 5.289\n",
      "Ep 1 (Step 000330): Train loss 5.148, Val loss 5.294\n",
      "Ep 1 (Step 000340): Train loss 5.098, Val loss 5.287\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2872\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.677, Val loss 8.637\n",
      "Ep 1 (Step 000010): Train loss 6.856, Val loss 6.828\n",
      "Ep 1 (Step 000020): Train loss 6.481, Val loss 6.468\n",
      "Ep 1 (Step 000030): Train loss 6.400, Val loss 6.438\n",
      "Ep 1 (Step 000040): Train loss 6.355, Val loss 6.313\n",
      "Ep 1 (Step 000050): Train loss 6.111, Val loss 6.174\n",
      "Ep 1 (Step 000060): Train loss 5.992, Val loss 6.048\n",
      "Ep 1 (Step 000070): Train loss 5.921, Val loss 5.963\n",
      "Ep 1 (Step 000080): Train loss 5.858, Val loss 5.863\n",
      "Ep 1 (Step 000090): Train loss 5.840, Val loss 5.767\n",
      "Ep 1 (Step 000100): Train loss 5.675, Val loss 5.714\n",
      "Ep 1 (Step 000110): Train loss 5.595, Val loss 5.696\n",
      "Ep 1 (Step 000120): Train loss 5.588, Val loss 5.620\n",
      "Ep 1 (Step 000130): Train loss 5.425, Val loss 5.584\n",
      "Ep 1 (Step 000140): Train loss 5.452, Val loss 5.571\n",
      "Ep 1 (Step 000150): Train loss 5.488, Val loss 5.540\n",
      "Ep 1 (Step 000160): Train loss 5.480, Val loss 5.506\n",
      "Ep 1 (Step 000170): Train loss 5.435, Val loss 5.488\n",
      "Ep 1 (Step 000180): Train loss 5.305, Val loss 5.477\n",
      "Ep 1 (Step 000190): Train loss 5.387, Val loss 5.466\n",
      "Ep 1 (Step 000200): Train loss 5.289, Val loss 5.431\n",
      "Ep 1 (Step 000210): Train loss 5.395, Val loss 5.402\n",
      "Ep 1 (Step 000220): Train loss 5.229, Val loss 5.385\n",
      "Ep 1 (Step 000230): Train loss 5.265, Val loss 5.396\n",
      "Ep 1 (Step 000240): Train loss 5.211, Val loss 5.374\n",
      "Ep 1 (Step 000250): Train loss 5.235, Val loss 5.348\n",
      "Ep 1 (Step 000260): Train loss 5.273, Val loss 5.337\n",
      "Ep 1 (Step 000270): Train loss 5.175, Val loss 5.334\n",
      "Ep 1 (Step 000280): Train loss 5.211, Val loss 5.332\n",
      "Ep 1 (Step 000290): Train loss 5.220, Val loss 5.315\n",
      "Ep 1 (Step 000300): Train loss 5.154, Val loss 5.293\n",
      "Ep 1 (Step 000310): Train loss 5.169, Val loss 5.271\n",
      "Ep 1 (Step 000320): Train loss 5.150, Val loss 5.285\n",
      "Ep 1 (Step 000330): Train loss 5.136, Val loss 5.266\n",
      "Ep 1 (Step 000340): Train loss 5.123, Val loss 5.282\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2819\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.577, Val loss 8.558\n",
      "Ep 1 (Step 000010): Train loss 6.841, Val loss 6.757\n",
      "Ep 1 (Step 000020): Train loss 6.465, Val loss 6.467\n",
      "Ep 1 (Step 000030): Train loss 6.458, Val loss 6.464\n",
      "Ep 1 (Step 000040): Train loss 6.348, Val loss 6.346\n",
      "Ep 1 (Step 000050): Train loss 6.229, Val loss 6.215\n",
      "Ep 1 (Step 000060): Train loss 6.021, Val loss 6.085\n",
      "Ep 1 (Step 000070): Train loss 5.975, Val loss 5.993\n",
      "Ep 1 (Step 000080): Train loss 5.903, Val loss 5.894\n",
      "Ep 1 (Step 000090): Train loss 5.813, Val loss 5.821\n",
      "Ep 1 (Step 000100): Train loss 5.733, Val loss 5.757\n",
      "Ep 1 (Step 000110): Train loss 5.582, Val loss 5.713\n",
      "Ep 1 (Step 000120): Train loss 5.618, Val loss 5.667\n",
      "Ep 1 (Step 000130): Train loss 5.655, Val loss 5.643\n",
      "Ep 1 (Step 000140): Train loss 5.522, Val loss 5.598\n",
      "Ep 1 (Step 000150): Train loss 5.501, Val loss 5.554\n",
      "Ep 1 (Step 000160): Train loss 5.516, Val loss 5.530\n",
      "Ep 1 (Step 000170): Train loss 5.364, Val loss 5.493\n",
      "Ep 1 (Step 000180): Train loss 5.379, Val loss 5.485\n",
      "Ep 1 (Step 000190): Train loss 5.401, Val loss 5.462\n",
      "Ep 1 (Step 000200): Train loss 5.385, Val loss 5.460\n",
      "Ep 1 (Step 000210): Train loss 5.232, Val loss 5.422\n",
      "Ep 1 (Step 000220): Train loss 5.382, Val loss 5.413\n",
      "Ep 1 (Step 000230): Train loss 5.270, Val loss 5.385\n",
      "Ep 1 (Step 000240): Train loss 5.207, Val loss 5.376\n",
      "Ep 1 (Step 000250): Train loss 5.272, Val loss 5.345\n",
      "Ep 1 (Step 000260): Train loss 5.230, Val loss 5.324\n",
      "Ep 1 (Step 000270): Train loss 5.206, Val loss 5.323\n",
      "Ep 1 (Step 000280): Train loss 5.153, Val loss 5.303\n",
      "Ep 1 (Step 000290): Train loss 5.128, Val loss 5.309\n",
      "Ep 1 (Step 000300): Train loss 5.200, Val loss 5.303\n",
      "Ep 1 (Step 000310): Train loss 5.188, Val loss 5.286\n",
      "Ep 1 (Step 000320): Train loss 5.241, Val loss 5.280\n",
      "Ep 1 (Step 000330): Train loss 5.139, Val loss 5.266\n",
      "Ep 1 (Step 000340): Train loss 5.095, Val loss 5.228\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2283\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.713, Val loss 8.683\n",
      "Ep 1 (Step 000010): Train loss 6.979, Val loss 6.874\n",
      "Ep 1 (Step 000020): Train loss 6.550, Val loss 6.455\n",
      "Ep 1 (Step 000030): Train loss 6.468, Val loss 6.447\n",
      "Ep 1 (Step 000040): Train loss 6.312, Val loss 6.293\n",
      "Ep 1 (Step 000050): Train loss 6.148, Val loss 6.170\n",
      "Ep 1 (Step 000060): Train loss 5.960, Val loss 6.016\n",
      "Ep 1 (Step 000070): Train loss 5.876, Val loss 5.921\n",
      "Ep 1 (Step 000080): Train loss 5.810, Val loss 5.841\n",
      "Ep 1 (Step 000090): Train loss 5.761, Val loss 5.795\n",
      "Ep 1 (Step 000100): Train loss 5.700, Val loss 5.768\n",
      "Ep 1 (Step 000110): Train loss 5.601, Val loss 5.680\n",
      "Ep 1 (Step 000120): Train loss 5.546, Val loss 5.631\n",
      "Ep 1 (Step 000130): Train loss 5.611, Val loss 5.614\n",
      "Ep 1 (Step 000140): Train loss 5.506, Val loss 5.575\n",
      "Ep 1 (Step 000150): Train loss 5.519, Val loss 5.564\n",
      "Ep 1 (Step 000160): Train loss 5.463, Val loss 5.520\n",
      "Ep 1 (Step 000170): Train loss 5.319, Val loss 5.496\n",
      "Ep 1 (Step 000180): Train loss 5.427, Val loss 5.460\n",
      "Ep 1 (Step 000190): Train loss 5.412, Val loss 5.448\n",
      "Ep 1 (Step 000200): Train loss 5.352, Val loss 5.440\n",
      "Ep 1 (Step 000210): Train loss 5.289, Val loss 5.414\n",
      "Ep 1 (Step 000220): Train loss 5.294, Val loss 5.391\n",
      "Ep 1 (Step 000230): Train loss 5.292, Val loss 5.398\n",
      "Ep 1 (Step 000240): Train loss 5.282, Val loss 5.356\n",
      "Ep 1 (Step 000250): Train loss 5.184, Val loss 5.335\n",
      "Ep 1 (Step 000260): Train loss 5.256, Val loss 5.332\n",
      "Ep 1 (Step 000270): Train loss 5.173, Val loss 5.301\n",
      "Ep 1 (Step 000280): Train loss 5.290, Val loss 5.294\n",
      "Ep 1 (Step 000290): Train loss 5.187, Val loss 5.279\n",
      "Ep 1 (Step 000300): Train loss 5.202, Val loss 5.303\n",
      "Ep 1 (Step 000310): Train loss 5.103, Val loss 5.265\n",
      "Ep 1 (Step 000320): Train loss 5.151, Val loss 5.240\n",
      "Ep 1 (Step 000330): Train loss 5.145, Val loss 5.260\n",
      "Ep 1 (Step 000340): Train loss 5.123, Val loss 5.252\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2520\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.611, Val loss 8.595\n",
      "Ep 1 (Step 000010): Train loss 6.857, Val loss 6.846\n",
      "Ep 1 (Step 000020): Train loss 6.538, Val loss 6.475\n",
      "Ep 1 (Step 000030): Train loss 6.455, Val loss 6.429\n",
      "Ep 1 (Step 000040): Train loss 6.312, Val loss 6.277\n",
      "Ep 1 (Step 000050): Train loss 6.162, Val loss 6.152\n",
      "Ep 1 (Step 000060): Train loss 6.004, Val loss 6.015\n",
      "Ep 1 (Step 000070): Train loss 5.914, Val loss 5.888\n",
      "Ep 1 (Step 000080): Train loss 5.817, Val loss 5.815\n",
      "Ep 1 (Step 000090): Train loss 5.671, Val loss 5.763\n",
      "Ep 1 (Step 000100): Train loss 5.643, Val loss 5.726\n",
      "Ep 1 (Step 000110): Train loss 5.567, Val loss 5.665\n",
      "Ep 1 (Step 000120): Train loss 5.549, Val loss 5.598\n",
      "Ep 1 (Step 000130): Train loss 5.479, Val loss 5.596\n",
      "Ep 1 (Step 000140): Train loss 5.472, Val loss 5.552\n",
      "Ep 1 (Step 000150): Train loss 5.419, Val loss 5.537\n",
      "Ep 1 (Step 000160): Train loss 5.443, Val loss 5.516\n",
      "Ep 1 (Step 000170): Train loss 5.392, Val loss 5.498\n",
      "Ep 1 (Step 000180): Train loss 5.356, Val loss 5.461\n",
      "Ep 1 (Step 000190): Train loss 5.342, Val loss 5.426\n",
      "Ep 1 (Step 000200): Train loss 5.275, Val loss 5.411\n",
      "Ep 1 (Step 000210): Train loss 5.366, Val loss 5.397\n",
      "Ep 1 (Step 000220): Train loss 5.284, Val loss 5.400\n",
      "Ep 1 (Step 000230): Train loss 5.275, Val loss 5.387\n",
      "Ep 1 (Step 000240): Train loss 5.266, Val loss 5.362\n",
      "Ep 1 (Step 000250): Train loss 5.306, Val loss 5.352\n",
      "Ep 1 (Step 000260): Train loss 5.347, Val loss 5.337\n",
      "Ep 1 (Step 000270): Train loss 5.235, Val loss 5.314\n",
      "Ep 1 (Step 000280): Train loss 5.179, Val loss 5.296\n",
      "Ep 1 (Step 000290): Train loss 5.131, Val loss 5.295\n",
      "Ep 1 (Step 000300): Train loss 5.202, Val loss 5.288\n",
      "Ep 1 (Step 000310): Train loss 5.089, Val loss 5.258\n",
      "Ep 1 (Step 000320): Train loss 5.043, Val loss 5.263\n",
      "Ep 1 (Step 000330): Train loss 5.124, Val loss 5.276\n",
      "Ep 1 (Step 000340): Train loss 5.135, Val loss 5.253\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2529\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.935, Val loss 8.934\n",
      "Ep 1 (Step 000010): Train loss 7.465, Val loss 7.457\n",
      "Ep 1 (Step 000020): Train loss 6.795, Val loss 6.793\n",
      "Ep 1 (Step 000030): Train loss 6.458, Val loss 6.460\n",
      "Ep 1 (Step 000040): Train loss 6.378, Val loss 6.345\n",
      "Ep 1 (Step 000050): Train loss 6.249, Val loss 6.232\n",
      "Ep 1 (Step 000060): Train loss 6.169, Val loss 6.113\n",
      "Ep 1 (Step 000070): Train loss 6.040, Val loss 6.043\n",
      "Ep 1 (Step 000080): Train loss 5.941, Val loss 5.954\n",
      "Ep 1 (Step 000090): Train loss 5.922, Val loss 5.890\n",
      "Ep 1 (Step 000100): Train loss 5.803, Val loss 5.830\n",
      "Ep 1 (Step 000110): Train loss 5.795, Val loss 5.785\n",
      "Ep 1 (Step 000120): Train loss 5.678, Val loss 5.743\n",
      "Ep 1 (Step 000130): Train loss 5.692, Val loss 5.725\n",
      "Ep 1 (Step 000140): Train loss 5.650, Val loss 5.677\n",
      "Ep 1 (Step 000150): Train loss 5.567, Val loss 5.648\n",
      "Ep 1 (Step 000160): Train loss 5.521, Val loss 5.600\n",
      "Ep 1 (Step 000170): Train loss 5.473, Val loss 5.577\n",
      "Ep 1 (Step 000180): Train loss 5.409, Val loss 5.555\n",
      "Ep 1 (Step 000190): Train loss 5.454, Val loss 5.527\n",
      "Ep 1 (Step 000200): Train loss 5.272, Val loss 5.510\n",
      "Ep 1 (Step 000210): Train loss 5.322, Val loss 5.490\n",
      "Ep 1 (Step 000220): Train loss 5.404, Val loss 5.475\n",
      "Ep 1 (Step 000230): Train loss 5.404, Val loss 5.440\n",
      "Ep 1 (Step 000240): Train loss 5.408, Val loss 5.432\n",
      "Ep 1 (Step 000250): Train loss 5.286, Val loss 5.424\n",
      "Ep 1 (Step 000260): Train loss 5.311, Val loss 5.401\n",
      "Ep 1 (Step 000270): Train loss 5.256, Val loss 5.382\n",
      "Ep 1 (Step 000280): Train loss 5.287, Val loss 5.369\n",
      "Ep 1 (Step 000290): Train loss 5.244, Val loss 5.357\n",
      "Ep 1 (Step 000300): Train loss 5.225, Val loss 5.354\n",
      "Ep 1 (Step 000310): Train loss 5.156, Val loss 5.329\n",
      "Ep 1 (Step 000320): Train loss 5.205, Val loss 5.328\n",
      "Ep 1 (Step 000330): Train loss 5.228, Val loss 5.314\n",
      "Ep 1 (Step 000340): Train loss 5.070, Val loss 5.307\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3074\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.937, Val loss 8.916\n",
      "Ep 1 (Step 000010): Train loss 7.469, Val loss 7.446\n",
      "Ep 1 (Step 000020): Train loss 6.769, Val loss 6.787\n",
      "Ep 1 (Step 000030): Train loss 6.461, Val loss 6.472\n",
      "Ep 1 (Step 000040): Train loss 6.362, Val loss 6.380\n",
      "Ep 1 (Step 000050): Train loss 6.317, Val loss 6.303\n",
      "Ep 1 (Step 000060): Train loss 6.145, Val loss 6.177\n",
      "Ep 1 (Step 000070): Train loss 6.114, Val loss 6.089\n",
      "Ep 1 (Step 000080): Train loss 6.029, Val loss 6.004\n",
      "Ep 1 (Step 000090): Train loss 5.922, Val loss 5.928\n",
      "Ep 1 (Step 000100): Train loss 5.876, Val loss 5.877\n",
      "Ep 1 (Step 000110): Train loss 5.718, Val loss 5.814\n",
      "Ep 1 (Step 000120): Train loss 5.668, Val loss 5.767\n",
      "Ep 1 (Step 000130): Train loss 5.714, Val loss 5.720\n",
      "Ep 1 (Step 000140): Train loss 5.592, Val loss 5.685\n",
      "Ep 1 (Step 000150): Train loss 5.670, Val loss 5.656\n",
      "Ep 1 (Step 000160): Train loss 5.589, Val loss 5.613\n",
      "Ep 1 (Step 000170): Train loss 5.391, Val loss 5.581\n",
      "Ep 1 (Step 000180): Train loss 5.428, Val loss 5.551\n",
      "Ep 1 (Step 000190): Train loss 5.415, Val loss 5.525\n",
      "Ep 1 (Step 000200): Train loss 5.433, Val loss 5.496\n",
      "Ep 1 (Step 000210): Train loss 5.380, Val loss 5.483\n",
      "Ep 1 (Step 000220): Train loss 5.284, Val loss 5.472\n",
      "Ep 1 (Step 000230): Train loss 5.298, Val loss 5.443\n",
      "Ep 1 (Step 000240): Train loss 5.424, Val loss 5.434\n",
      "Ep 1 (Step 000250): Train loss 5.305, Val loss 5.412\n",
      "Ep 1 (Step 000260): Train loss 5.252, Val loss 5.413\n",
      "Ep 1 (Step 000270): Train loss 5.294, Val loss 5.405\n",
      "Ep 1 (Step 000280): Train loss 5.206, Val loss 5.364\n",
      "Ep 1 (Step 000290): Train loss 5.142, Val loss 5.360\n",
      "Ep 1 (Step 000300): Train loss 5.319, Val loss 5.345\n",
      "Ep 1 (Step 000310): Train loss 5.275, Val loss 5.331\n",
      "Ep 1 (Step 000320): Train loss 5.182, Val loss 5.321\n",
      "Ep 1 (Step 000330): Train loss 5.328, Val loss 5.334\n",
      "Ep 1 (Step 000340): Train loss 5.270, Val loss 5.320\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3201\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.893, Val loss 8.896\n",
      "Ep 1 (Step 000010): Train loss 7.416, Val loss 7.402\n",
      "Ep 1 (Step 000020): Train loss 6.758, Val loss 6.761\n",
      "Ep 1 (Step 000030): Train loss 6.418, Val loss 6.448\n",
      "Ep 1 (Step 000040): Train loss 6.332, Val loss 6.341\n",
      "Ep 1 (Step 000050): Train loss 6.211, Val loss 6.257\n",
      "Ep 1 (Step 000060): Train loss 6.087, Val loss 6.120\n",
      "Ep 1 (Step 000070): Train loss 6.037, Val loss 6.017\n",
      "Ep 1 (Step 000080): Train loss 5.944, Val loss 5.973\n",
      "Ep 1 (Step 000090): Train loss 5.841, Val loss 5.894\n",
      "Ep 1 (Step 000100): Train loss 5.817, Val loss 5.822\n",
      "Ep 1 (Step 000110): Train loss 5.704, Val loss 5.786\n",
      "Ep 1 (Step 000120): Train loss 5.700, Val loss 5.741\n",
      "Ep 1 (Step 000130): Train loss 5.603, Val loss 5.700\n",
      "Ep 1 (Step 000140): Train loss 5.630, Val loss 5.661\n",
      "Ep 1 (Step 000150): Train loss 5.557, Val loss 5.629\n",
      "Ep 1 (Step 000160): Train loss 5.420, Val loss 5.601\n",
      "Ep 1 (Step 000170): Train loss 5.500, Val loss 5.579\n",
      "Ep 1 (Step 000180): Train loss 5.425, Val loss 5.548\n",
      "Ep 1 (Step 000190): Train loss 5.479, Val loss 5.501\n",
      "Ep 1 (Step 000200): Train loss 5.443, Val loss 5.502\n",
      "Ep 1 (Step 000210): Train loss 5.395, Val loss 5.467\n",
      "Ep 1 (Step 000220): Train loss 5.369, Val loss 5.473\n",
      "Ep 1 (Step 000230): Train loss 5.374, Val loss 5.444\n",
      "Ep 1 (Step 000240): Train loss 5.361, Val loss 5.431\n",
      "Ep 1 (Step 000250): Train loss 5.365, Val loss 5.409\n",
      "Ep 1 (Step 000260): Train loss 5.257, Val loss 5.389\n",
      "Ep 1 (Step 000270): Train loss 5.247, Val loss 5.391\n",
      "Ep 1 (Step 000280): Train loss 5.242, Val loss 5.379\n",
      "Ep 1 (Step 000290): Train loss 5.237, Val loss 5.366\n",
      "Ep 1 (Step 000300): Train loss 5.188, Val loss 5.363\n",
      "Ep 1 (Step 000310): Train loss 5.293, Val loss 5.335\n",
      "Ep 1 (Step 000320): Train loss 5.226, Val loss 5.320\n",
      "Ep 1 (Step 000330): Train loss 5.239, Val loss 5.323\n",
      "Ep 1 (Step 000340): Train loss 5.157, Val loss 5.299\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2988\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.915, Val loss 8.891\n",
      "Ep 1 (Step 000010): Train loss 7.488, Val loss 7.418\n",
      "Ep 1 (Step 000020): Train loss 6.829, Val loss 6.763\n",
      "Ep 1 (Step 000030): Train loss 6.473, Val loss 6.454\n",
      "Ep 1 (Step 000040): Train loss 6.398, Val loss 6.376\n",
      "Ep 1 (Step 000050): Train loss 6.295, Val loss 6.274\n",
      "Ep 1 (Step 000060): Train loss 6.129, Val loss 6.183\n",
      "Ep 1 (Step 000070): Train loss 6.075, Val loss 6.047\n",
      "Ep 1 (Step 000080): Train loss 5.918, Val loss 5.981\n",
      "Ep 1 (Step 000090): Train loss 5.868, Val loss 5.903\n",
      "Ep 1 (Step 000100): Train loss 5.939, Val loss 5.854\n",
      "Ep 1 (Step 000110): Train loss 5.818, Val loss 5.800\n",
      "Ep 1 (Step 000120): Train loss 5.716, Val loss 5.738\n",
      "Ep 1 (Step 000130): Train loss 5.723, Val loss 5.689\n",
      "Ep 1 (Step 000140): Train loss 5.563, Val loss 5.666\n",
      "Ep 1 (Step 000150): Train loss 5.575, Val loss 5.615\n",
      "Ep 1 (Step 000160): Train loss 5.557, Val loss 5.585\n",
      "Ep 1 (Step 000170): Train loss 5.538, Val loss 5.570\n",
      "Ep 1 (Step 000180): Train loss 5.479, Val loss 5.542\n",
      "Ep 1 (Step 000190): Train loss 5.415, Val loss 5.519\n",
      "Ep 1 (Step 000200): Train loss 5.481, Val loss 5.488\n",
      "Ep 1 (Step 000210): Train loss 5.435, Val loss 5.484\n",
      "Ep 1 (Step 000220): Train loss 5.385, Val loss 5.456\n",
      "Ep 1 (Step 000230): Train loss 5.331, Val loss 5.426\n",
      "Ep 1 (Step 000240): Train loss 5.324, Val loss 5.398\n",
      "Ep 1 (Step 000250): Train loss 5.372, Val loss 5.370\n",
      "Ep 1 (Step 000260): Train loss 5.251, Val loss 5.364\n",
      "Ep 1 (Step 000270): Train loss 5.158, Val loss 5.365\n",
      "Ep 1 (Step 000280): Train loss 5.225, Val loss 5.359\n",
      "Ep 1 (Step 000290): Train loss 5.282, Val loss 5.349\n",
      "Ep 1 (Step 000300): Train loss 5.231, Val loss 5.342\n",
      "Ep 1 (Step 000310): Train loss 5.228, Val loss 5.338\n",
      "Ep 1 (Step 000320): Train loss 5.272, Val loss 5.312\n",
      "Ep 1 (Step 000330): Train loss 5.120, Val loss 5.283\n",
      "Ep 1 (Step 000340): Train loss 5.235, Val loss 5.279\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2789\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.938, Val loss 8.913\n",
      "Ep 1 (Step 000010): Train loss 7.388, Val loss 7.366\n",
      "Ep 1 (Step 000020): Train loss 6.730, Val loss 6.739\n",
      "Ep 1 (Step 000030): Train loss 6.466, Val loss 6.446\n",
      "Ep 1 (Step 000040): Train loss 6.337, Val loss 6.344\n",
      "Ep 1 (Step 000050): Train loss 6.290, Val loss 6.269\n",
      "Ep 1 (Step 000060): Train loss 6.114, Val loss 6.119\n",
      "Ep 1 (Step 000070): Train loss 6.005, Val loss 6.033\n",
      "Ep 1 (Step 000080): Train loss 5.847, Val loss 5.967\n",
      "Ep 1 (Step 000090): Train loss 5.843, Val loss 5.914\n",
      "Ep 1 (Step 000100): Train loss 5.852, Val loss 5.851\n",
      "Ep 1 (Step 000110): Train loss 5.754, Val loss 5.784\n",
      "Ep 1 (Step 000120): Train loss 5.667, Val loss 5.728\n",
      "Ep 1 (Step 000130): Train loss 5.637, Val loss 5.686\n",
      "Ep 1 (Step 000140): Train loss 5.538, Val loss 5.657\n",
      "Ep 1 (Step 000150): Train loss 5.636, Val loss 5.606\n",
      "Ep 1 (Step 000160): Train loss 5.543, Val loss 5.579\n",
      "Ep 1 (Step 000170): Train loss 5.494, Val loss 5.559\n",
      "Ep 1 (Step 000180): Train loss 5.423, Val loss 5.538\n",
      "Ep 1 (Step 000190): Train loss 5.421, Val loss 5.513\n",
      "Ep 1 (Step 000200): Train loss 5.386, Val loss 5.495\n",
      "Ep 1 (Step 000210): Train loss 5.362, Val loss 5.473\n",
      "Ep 1 (Step 000220): Train loss 5.336, Val loss 5.458\n",
      "Ep 1 (Step 000230): Train loss 5.344, Val loss 5.428\n",
      "Ep 1 (Step 000240): Train loss 5.202, Val loss 5.419\n",
      "Ep 1 (Step 000250): Train loss 5.281, Val loss 5.414\n",
      "Ep 1 (Step 000260): Train loss 5.325, Val loss 5.393\n",
      "Ep 1 (Step 000270): Train loss 5.240, Val loss 5.393\n",
      "Ep 1 (Step 000280): Train loss 5.201, Val loss 5.359\n",
      "Ep 1 (Step 000290): Train loss 5.267, Val loss 5.347\n",
      "Ep 1 (Step 000300): Train loss 5.263, Val loss 5.326\n",
      "Ep 1 (Step 000310): Train loss 5.179, Val loss 5.322\n",
      "Ep 1 (Step 000320): Train loss 5.139, Val loss 5.310\n",
      "Ep 1 (Step 000330): Train loss 5.204, Val loss 5.289\n",
      "Ep 1 (Step 000340): Train loss 5.153, Val loss 5.280\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2798\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.928, Val loss 8.904\n",
      "Ep 1 (Step 000010): Train loss 7.443, Val loss 7.428\n",
      "Ep 1 (Step 000020): Train loss 6.805, Val loss 6.788\n",
      "Ep 1 (Step 000030): Train loss 6.425, Val loss 6.454\n",
      "Ep 1 (Step 000040): Train loss 6.371, Val loss 6.372\n",
      "Ep 1 (Step 000050): Train loss 6.204, Val loss 6.278\n",
      "Ep 1 (Step 000060): Train loss 6.202, Val loss 6.133\n",
      "Ep 1 (Step 000070): Train loss 6.096, Val loss 6.034\n",
      "Ep 1 (Step 000080): Train loss 5.919, Val loss 5.952\n",
      "Ep 1 (Step 000090): Train loss 5.854, Val loss 5.879\n",
      "Ep 1 (Step 000100): Train loss 5.792, Val loss 5.824\n",
      "Ep 1 (Step 000110): Train loss 5.753, Val loss 5.776\n",
      "Ep 1 (Step 000120): Train loss 5.655, Val loss 5.727\n",
      "Ep 1 (Step 000130): Train loss 5.612, Val loss 5.676\n",
      "Ep 1 (Step 000140): Train loss 5.592, Val loss 5.641\n",
      "Ep 1 (Step 000150): Train loss 5.558, Val loss 5.632\n",
      "Ep 1 (Step 000160): Train loss 5.554, Val loss 5.591\n",
      "Ep 1 (Step 000170): Train loss 5.382, Val loss 5.556\n",
      "Ep 1 (Step 000180): Train loss 5.404, Val loss 5.520\n",
      "Ep 1 (Step 000190): Train loss 5.386, Val loss 5.503\n",
      "Ep 1 (Step 000200): Train loss 5.412, Val loss 5.496\n",
      "Ep 1 (Step 000210): Train loss 5.301, Val loss 5.461\n",
      "Ep 1 (Step 000220): Train loss 5.356, Val loss 5.442\n",
      "Ep 1 (Step 000230): Train loss 5.336, Val loss 5.433\n",
      "Ep 1 (Step 000240): Train loss 5.349, Val loss 5.411\n",
      "Ep 1 (Step 000250): Train loss 5.329, Val loss 5.407\n",
      "Ep 1 (Step 000260): Train loss 5.149, Val loss 5.388\n",
      "Ep 1 (Step 000270): Train loss 5.301, Val loss 5.383\n",
      "Ep 1 (Step 000280): Train loss 5.178, Val loss 5.370\n",
      "Ep 1 (Step 000290): Train loss 5.228, Val loss 5.346\n",
      "Ep 1 (Step 000300): Train loss 5.249, Val loss 5.338\n",
      "Ep 1 (Step 000310): Train loss 5.139, Val loss 5.306\n",
      "Ep 1 (Step 000320): Train loss 5.168, Val loss 5.312\n",
      "Ep 1 (Step 000330): Train loss 5.196, Val loss 5.285\n",
      "Ep 1 (Step 000340): Train loss 5.184, Val loss 5.287\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2867\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.600, Val loss 8.609\n",
      "Ep 1 (Step 000010): Train loss 6.812, Val loss 6.803\n",
      "Ep 1 (Step 000020): Train loss 6.436, Val loss 6.442\n",
      "Ep 1 (Step 000030): Train loss 6.392, Val loss 6.347\n",
      "Ep 1 (Step 000040): Train loss 6.227, Val loss 6.202\n",
      "Ep 1 (Step 000050): Train loss 6.058, Val loss 6.035\n",
      "Ep 1 (Step 000060): Train loss 5.922, Val loss 5.937\n",
      "Ep 1 (Step 000070): Train loss 5.840, Val loss 5.850\n",
      "Ep 1 (Step 000080): Train loss 5.689, Val loss 5.802\n",
      "Ep 1 (Step 000090): Train loss 5.703, Val loss 5.757\n",
      "Ep 1 (Step 000100): Train loss 5.581, Val loss 5.665\n",
      "Ep 1 (Step 000110): Train loss 5.525, Val loss 5.618\n",
      "Ep 1 (Step 000120): Train loss 5.522, Val loss 5.597\n",
      "Ep 1 (Step 000130): Train loss 5.508, Val loss 5.545\n",
      "Ep 1 (Step 000140): Train loss 5.486, Val loss 5.517\n",
      "Ep 1 (Step 000150): Train loss 5.432, Val loss 5.511\n",
      "Ep 1 (Step 000160): Train loss 5.376, Val loss 5.484\n",
      "Ep 1 (Step 000170): Train loss 5.360, Val loss 5.457\n",
      "Ep 1 (Step 000180): Train loss 5.321, Val loss 5.425\n",
      "Ep 1 (Step 000190): Train loss 5.367, Val loss 5.402\n",
      "Ep 1 (Step 000200): Train loss 5.365, Val loss 5.376\n",
      "Ep 1 (Step 000210): Train loss 5.241, Val loss 5.382\n",
      "Ep 1 (Step 000220): Train loss 5.239, Val loss 5.370\n",
      "Ep 1 (Step 000230): Train loss 5.329, Val loss 5.349\n",
      "Ep 1 (Step 000240): Train loss 5.272, Val loss 5.334\n",
      "Ep 1 (Step 000250): Train loss 5.162, Val loss 5.319\n",
      "Ep 1 (Step 000260): Train loss 5.241, Val loss 5.313\n",
      "Ep 1 (Step 000270): Train loss 5.162, Val loss 5.294\n",
      "Ep 1 (Step 000280): Train loss 5.152, Val loss 5.292\n",
      "Ep 1 (Step 000290): Train loss 5.201, Val loss 5.272\n",
      "Ep 1 (Step 000300): Train loss 5.213, Val loss 5.270\n",
      "Ep 1 (Step 000310): Train loss 5.198, Val loss 5.260\n",
      "Ep 1 (Step 000320): Train loss 5.087, Val loss 5.252\n",
      "Ep 1 (Step 000330): Train loss 5.127, Val loss 5.219\n",
      "Ep 1 (Step 000340): Train loss 5.071, Val loss 5.234\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2343\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.664, Val loss 8.654\n",
      "Ep 1 (Step 000010): Train loss 6.847, Val loss 6.868\n",
      "Ep 1 (Step 000020): Train loss 6.471, Val loss 6.496\n",
      "Ep 1 (Step 000030): Train loss 6.406, Val loss 6.430\n",
      "Ep 1 (Step 000040): Train loss 6.314, Val loss 6.273\n",
      "Ep 1 (Step 000050): Train loss 6.071, Val loss 6.124\n",
      "Ep 1 (Step 000060): Train loss 5.996, Val loss 5.997\n",
      "Ep 1 (Step 000070): Train loss 5.812, Val loss 5.877\n",
      "Ep 1 (Step 000080): Train loss 5.758, Val loss 5.814\n",
      "Ep 1 (Step 000090): Train loss 5.744, Val loss 5.764\n",
      "Ep 1 (Step 000100): Train loss 5.553, Val loss 5.699\n",
      "Ep 1 (Step 000110): Train loss 5.512, Val loss 5.638\n",
      "Ep 1 (Step 000120): Train loss 5.525, Val loss 5.638\n",
      "Ep 1 (Step 000130): Train loss 5.469, Val loss 5.551\n",
      "Ep 1 (Step 000140): Train loss 5.445, Val loss 5.546\n",
      "Ep 1 (Step 000150): Train loss 5.426, Val loss 5.516\n",
      "Ep 1 (Step 000160): Train loss 5.358, Val loss 5.503\n",
      "Ep 1 (Step 000170): Train loss 5.379, Val loss 5.458\n",
      "Ep 1 (Step 000180): Train loss 5.316, Val loss 5.435\n",
      "Ep 1 (Step 000190): Train loss 5.263, Val loss 5.419\n",
      "Ep 1 (Step 000200): Train loss 5.211, Val loss 5.390\n",
      "Ep 1 (Step 000210): Train loss 5.171, Val loss 5.381\n",
      "Ep 1 (Step 000220): Train loss 5.303, Val loss 5.375\n",
      "Ep 1 (Step 000230): Train loss 5.271, Val loss 5.353\n",
      "Ep 1 (Step 000240): Train loss 5.270, Val loss 5.347\n",
      "Ep 1 (Step 000250): Train loss 5.189, Val loss 5.327\n",
      "Ep 1 (Step 000260): Train loss 5.234, Val loss 5.305\n",
      "Ep 1 (Step 000270): Train loss 5.164, Val loss 5.287\n",
      "Ep 1 (Step 000280): Train loss 5.183, Val loss 5.280\n",
      "Ep 1 (Step 000290): Train loss 5.209, Val loss 5.268\n",
      "Ep 1 (Step 000300): Train loss 5.083, Val loss 5.236\n",
      "Ep 1 (Step 000310): Train loss 5.133, Val loss 5.258\n",
      "Ep 1 (Step 000320): Train loss 5.053, Val loss 5.260\n",
      "Ep 1 (Step 000330): Train loss 5.051, Val loss 5.246\n",
      "Ep 1 (Step 000340): Train loss 5.143, Val loss 5.226\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2264\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.640, Val loss 8.606\n",
      "Ep 1 (Step 000010): Train loss 6.804, Val loss 6.808\n",
      "Ep 1 (Step 000020): Train loss 6.507, Val loss 6.481\n",
      "Ep 1 (Step 000030): Train loss 6.405, Val loss 6.393\n",
      "Ep 1 (Step 000040): Train loss 6.310, Val loss 6.272\n",
      "Ep 1 (Step 000050): Train loss 6.183, Val loss 6.130\n",
      "Ep 1 (Step 000060): Train loss 5.941, Val loss 5.999\n",
      "Ep 1 (Step 000070): Train loss 5.884, Val loss 5.910\n",
      "Ep 1 (Step 000080): Train loss 5.741, Val loss 5.826\n",
      "Ep 1 (Step 000090): Train loss 5.715, Val loss 5.779\n",
      "Ep 1 (Step 000100): Train loss 5.712, Val loss 5.713\n",
      "Ep 1 (Step 000110): Train loss 5.599, Val loss 5.663\n",
      "Ep 1 (Step 000120): Train loss 5.464, Val loss 5.631\n",
      "Ep 1 (Step 000130): Train loss 5.572, Val loss 5.591\n",
      "Ep 1 (Step 000140): Train loss 5.519, Val loss 5.563\n",
      "Ep 1 (Step 000150): Train loss 5.416, Val loss 5.552\n",
      "Ep 1 (Step 000160): Train loss 5.412, Val loss 5.512\n",
      "Ep 1 (Step 000170): Train loss 5.338, Val loss 5.478\n",
      "Ep 1 (Step 000180): Train loss 5.312, Val loss 5.458\n",
      "Ep 1 (Step 000190): Train loss 5.317, Val loss 5.429\n",
      "Ep 1 (Step 000200): Train loss 5.355, Val loss 5.416\n",
      "Ep 1 (Step 000210): Train loss 5.240, Val loss 5.382\n",
      "Ep 1 (Step 000220): Train loss 5.249, Val loss 5.372\n",
      "Ep 1 (Step 000230): Train loss 5.317, Val loss 5.374\n",
      "Ep 1 (Step 000240): Train loss 5.245, Val loss 5.368\n",
      "Ep 1 (Step 000250): Train loss 5.331, Val loss 5.361\n",
      "Ep 1 (Step 000260): Train loss 5.267, Val loss 5.321\n",
      "Ep 1 (Step 000270): Train loss 5.215, Val loss 5.315\n",
      "Ep 1 (Step 000280): Train loss 5.221, Val loss 5.306\n",
      "Ep 1 (Step 000290): Train loss 5.205, Val loss 5.303\n",
      "Ep 1 (Step 000300): Train loss 5.125, Val loss 5.296\n",
      "Ep 1 (Step 000310): Train loss 5.167, Val loss 5.275\n",
      "Ep 1 (Step 000320): Train loss 5.144, Val loss 5.261\n",
      "Ep 1 (Step 000330): Train loss 5.046, Val loss 5.237\n",
      "Ep 1 (Step 000340): Train loss 5.101, Val loss 5.228\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2281\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.568, Val loss 8.528\n",
      "Ep 1 (Step 000010): Train loss 6.838, Val loss 6.794\n",
      "Ep 1 (Step 000020): Train loss 6.430, Val loss 6.418\n",
      "Ep 1 (Step 000030): Train loss 6.436, Val loss 6.371\n",
      "Ep 1 (Step 000040): Train loss 6.286, Val loss 6.196\n",
      "Ep 1 (Step 000050): Train loss 6.054, Val loss 6.057\n",
      "Ep 1 (Step 000060): Train loss 5.920, Val loss 5.933\n",
      "Ep 1 (Step 000070): Train loss 5.834, Val loss 5.857\n",
      "Ep 1 (Step 000080): Train loss 5.719, Val loss 5.778\n",
      "Ep 1 (Step 000090): Train loss 5.728, Val loss 5.702\n",
      "Ep 1 (Step 000100): Train loss 5.648, Val loss 5.644\n",
      "Ep 1 (Step 000110): Train loss 5.593, Val loss 5.601\n",
      "Ep 1 (Step 000120): Train loss 5.514, Val loss 5.580\n",
      "Ep 1 (Step 000130): Train loss 5.488, Val loss 5.520\n",
      "Ep 1 (Step 000140): Train loss 5.530, Val loss 5.501\n",
      "Ep 1 (Step 000150): Train loss 5.374, Val loss 5.478\n",
      "Ep 1 (Step 000160): Train loss 5.375, Val loss 5.443\n",
      "Ep 1 (Step 000170): Train loss 5.481, Val loss 5.430\n",
      "Ep 1 (Step 000180): Train loss 5.305, Val loss 5.422\n",
      "Ep 1 (Step 000190): Train loss 5.288, Val loss 5.406\n",
      "Ep 1 (Step 000200): Train loss 5.309, Val loss 5.391\n",
      "Ep 1 (Step 000210): Train loss 5.198, Val loss 5.380\n",
      "Ep 1 (Step 000220): Train loss 5.200, Val loss 5.348\n",
      "Ep 1 (Step 000230): Train loss 5.203, Val loss 5.344\n",
      "Ep 1 (Step 000240): Train loss 5.196, Val loss 5.337\n",
      "Ep 1 (Step 000250): Train loss 5.243, Val loss 5.323\n",
      "Ep 1 (Step 000260): Train loss 5.171, Val loss 5.296\n",
      "Ep 1 (Step 000270): Train loss 5.101, Val loss 5.289\n",
      "Ep 1 (Step 000280): Train loss 5.085, Val loss 5.278\n",
      "Ep 1 (Step 000290): Train loss 5.168, Val loss 5.270\n",
      "Ep 1 (Step 000300): Train loss 5.051, Val loss 5.255\n",
      "Ep 1 (Step 000310): Train loss 5.160, Val loss 5.251\n",
      "Ep 1 (Step 000320): Train loss 5.121, Val loss 5.222\n",
      "Ep 1 (Step 000330): Train loss 5.121, Val loss 5.232\n",
      "Ep 1 (Step 000340): Train loss 5.055, Val loss 5.224\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2236\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.654, Val loss 8.596\n",
      "Ep 1 (Step 000010): Train loss 6.904, Val loss 6.818\n",
      "Ep 1 (Step 000020): Train loss 6.378, Val loss 6.424\n",
      "Ep 1 (Step 000030): Train loss 6.338, Val loss 6.365\n",
      "Ep 1 (Step 000040): Train loss 6.165, Val loss 6.184\n",
      "Ep 1 (Step 000050): Train loss 6.059, Val loss 6.047\n",
      "Ep 1 (Step 000060): Train loss 5.927, Val loss 5.930\n",
      "Ep 1 (Step 000070): Train loss 5.881, Val loss 5.828\n",
      "Ep 1 (Step 000080): Train loss 5.730, Val loss 5.768\n",
      "Ep 1 (Step 000090): Train loss 5.669, Val loss 5.719\n",
      "Ep 1 (Step 000100): Train loss 5.591, Val loss 5.654\n",
      "Ep 1 (Step 000110): Train loss 5.550, Val loss 5.608\n",
      "Ep 1 (Step 000120): Train loss 5.430, Val loss 5.569\n",
      "Ep 1 (Step 000130): Train loss 5.556, Val loss 5.539\n",
      "Ep 1 (Step 000140): Train loss 5.441, Val loss 5.493\n",
      "Ep 1 (Step 000150): Train loss 5.396, Val loss 5.464\n",
      "Ep 1 (Step 000160): Train loss 5.327, Val loss 5.456\n",
      "Ep 1 (Step 000170): Train loss 5.344, Val loss 5.422\n",
      "Ep 1 (Step 000180): Train loss 5.329, Val loss 5.407\n",
      "Ep 1 (Step 000190): Train loss 5.261, Val loss 5.379\n",
      "Ep 1 (Step 000200): Train loss 5.328, Val loss 5.363\n",
      "Ep 1 (Step 000210): Train loss 5.284, Val loss 5.352\n",
      "Ep 1 (Step 000220): Train loss 5.240, Val loss 5.343\n",
      "Ep 1 (Step 000230): Train loss 5.313, Val loss 5.313\n",
      "Ep 1 (Step 000240): Train loss 5.174, Val loss 5.309\n",
      "Ep 1 (Step 000250): Train loss 5.171, Val loss 5.315\n",
      "Ep 1 (Step 000260): Train loss 5.234, Val loss 5.297\n",
      "Ep 1 (Step 000270): Train loss 5.142, Val loss 5.264\n",
      "Ep 1 (Step 000280): Train loss 5.160, Val loss 5.257\n",
      "Ep 1 (Step 000290): Train loss 5.080, Val loss 5.244\n",
      "Ep 1 (Step 000300): Train loss 5.182, Val loss 5.237\n",
      "Ep 1 (Step 000310): Train loss 5.148, Val loss 5.240\n",
      "Ep 1 (Step 000320): Train loss 5.010, Val loss 5.220\n",
      "Ep 1 (Step 000330): Train loss 5.127, Val loss 5.200\n",
      "Ep 1 (Step 000340): Train loss 5.078, Val loss 5.209\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2087\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.641, Val loss 8.607\n",
      "Ep 1 (Step 000010): Train loss 6.857, Val loss 6.833\n",
      "Ep 1 (Step 000020): Train loss 6.446, Val loss 6.410\n",
      "Ep 1 (Step 000030): Train loss 6.411, Val loss 6.366\n",
      "Ep 1 (Step 000040): Train loss 6.174, Val loss 6.204\n",
      "Ep 1 (Step 000050): Train loss 6.141, Val loss 6.088\n",
      "Ep 1 (Step 000060): Train loss 5.916, Val loss 5.981\n",
      "Ep 1 (Step 000070): Train loss 5.916, Val loss 5.890\n",
      "Ep 1 (Step 000080): Train loss 5.813, Val loss 5.815\n",
      "Ep 1 (Step 000090): Train loss 5.750, Val loss 5.766\n",
      "Ep 1 (Step 000100): Train loss 5.655, Val loss 5.723\n",
      "Ep 1 (Step 000110): Train loss 5.557, Val loss 5.669\n",
      "Ep 1 (Step 000120): Train loss 5.523, Val loss 5.614\n",
      "Ep 1 (Step 000130): Train loss 5.531, Val loss 5.571\n",
      "Ep 1 (Step 000140): Train loss 5.513, Val loss 5.538\n",
      "Ep 1 (Step 000150): Train loss 5.498, Val loss 5.508\n",
      "Ep 1 (Step 000160): Train loss 5.379, Val loss 5.492\n",
      "Ep 1 (Step 000170): Train loss 5.323, Val loss 5.460\n",
      "Ep 1 (Step 000180): Train loss 5.380, Val loss 5.421\n",
      "Ep 1 (Step 000190): Train loss 5.310, Val loss 5.389\n",
      "Ep 1 (Step 000200): Train loss 5.283, Val loss 5.385\n",
      "Ep 1 (Step 000210): Train loss 5.296, Val loss 5.368\n",
      "Ep 1 (Step 000220): Train loss 5.207, Val loss 5.344\n",
      "Ep 1 (Step 000230): Train loss 5.257, Val loss 5.341\n",
      "Ep 1 (Step 000240): Train loss 5.245, Val loss 5.326\n",
      "Ep 1 (Step 000250): Train loss 5.212, Val loss 5.318\n",
      "Ep 1 (Step 000260): Train loss 5.165, Val loss 5.310\n",
      "Ep 1 (Step 000270): Train loss 5.164, Val loss 5.284\n",
      "Ep 1 (Step 000280): Train loss 5.160, Val loss 5.269\n",
      "Ep 1 (Step 000290): Train loss 5.090, Val loss 5.253\n",
      "Ep 1 (Step 000300): Train loss 5.188, Val loss 5.246\n",
      "Ep 1 (Step 000310): Train loss 5.031, Val loss 5.237\n",
      "Ep 1 (Step 000320): Train loss 5.101, Val loss 5.237\n",
      "Ep 1 (Step 000330): Train loss 5.071, Val loss 5.211\n",
      "Ep 1 (Step 000340): Train loss 4.994, Val loss 5.190\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1898\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.882, Val loss 8.863\n",
      "Ep 1 (Step 000010): Train loss 7.491, Val loss 7.444\n",
      "Ep 1 (Step 000020): Train loss 6.767, Val loss 6.785\n",
      "Ep 1 (Step 000030): Train loss 6.471, Val loss 6.451\n",
      "Ep 1 (Step 000040): Train loss 6.374, Val loss 6.358\n",
      "Ep 1 (Step 000050): Train loss 6.285, Val loss 6.265\n",
      "Ep 1 (Step 000060): Train loss 6.111, Val loss 6.148\n",
      "Ep 1 (Step 000070): Train loss 6.029, Val loss 6.069\n",
      "Ep 1 (Step 000080): Train loss 5.979, Val loss 5.981\n",
      "Ep 1 (Step 000090): Train loss 5.887, Val loss 5.912\n",
      "Ep 1 (Step 000100): Train loss 5.804, Val loss 5.859\n",
      "Ep 1 (Step 000110): Train loss 5.706, Val loss 5.796\n",
      "Ep 1 (Step 000120): Train loss 5.679, Val loss 5.754\n",
      "Ep 1 (Step 000130): Train loss 5.648, Val loss 5.710\n",
      "Ep 1 (Step 000140): Train loss 5.598, Val loss 5.672\n",
      "Ep 1 (Step 000150): Train loss 5.485, Val loss 5.629\n",
      "Ep 1 (Step 000160): Train loss 5.530, Val loss 5.590\n",
      "Ep 1 (Step 000170): Train loss 5.495, Val loss 5.551\n",
      "Ep 1 (Step 000180): Train loss 5.477, Val loss 5.536\n",
      "Ep 1 (Step 000190): Train loss 5.490, Val loss 5.527\n",
      "Ep 1 (Step 000200): Train loss 5.425, Val loss 5.494\n",
      "Ep 1 (Step 000210): Train loss 5.370, Val loss 5.467\n",
      "Ep 1 (Step 000220): Train loss 5.455, Val loss 5.454\n",
      "Ep 1 (Step 000230): Train loss 5.350, Val loss 5.435\n",
      "Ep 1 (Step 000240): Train loss 5.300, Val loss 5.440\n",
      "Ep 1 (Step 000250): Train loss 5.364, Val loss 5.420\n",
      "Ep 1 (Step 000260): Train loss 5.267, Val loss 5.401\n",
      "Ep 1 (Step 000270): Train loss 5.297, Val loss 5.377\n",
      "Ep 1 (Step 000280): Train loss 5.241, Val loss 5.383\n",
      "Ep 1 (Step 000290): Train loss 5.251, Val loss 5.365\n",
      "Ep 1 (Step 000300): Train loss 5.189, Val loss 5.352\n",
      "Ep 1 (Step 000310): Train loss 5.211, Val loss 5.333\n",
      "Ep 1 (Step 000320): Train loss 5.210, Val loss 5.337\n",
      "Ep 1 (Step 000330): Train loss 5.243, Val loss 5.314\n",
      "Ep 1 (Step 000340): Train loss 5.185, Val loss 5.296\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2961\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.914, Val loss 8.898\n",
      "Ep 1 (Step 000010): Train loss 7.478, Val loss 7.425\n",
      "Ep 1 (Step 000020): Train loss 6.808, Val loss 6.771\n",
      "Ep 1 (Step 000030): Train loss 6.439, Val loss 6.454\n",
      "Ep 1 (Step 000040): Train loss 6.348, Val loss 6.347\n",
      "Ep 1 (Step 000050): Train loss 6.292, Val loss 6.231\n",
      "Ep 1 (Step 000060): Train loss 6.105, Val loss 6.126\n",
      "Ep 1 (Step 000070): Train loss 6.023, Val loss 6.028\n",
      "Ep 1 (Step 000080): Train loss 5.898, Val loss 5.941\n",
      "Ep 1 (Step 000090): Train loss 5.880, Val loss 5.899\n",
      "Ep 1 (Step 000100): Train loss 5.870, Val loss 5.841\n",
      "Ep 1 (Step 000110): Train loss 5.747, Val loss 5.779\n",
      "Ep 1 (Step 000120): Train loss 5.682, Val loss 5.731\n",
      "Ep 1 (Step 000130): Train loss 5.581, Val loss 5.698\n",
      "Ep 1 (Step 000140): Train loss 5.579, Val loss 5.683\n",
      "Ep 1 (Step 000150): Train loss 5.561, Val loss 5.638\n",
      "Ep 1 (Step 000160): Train loss 5.462, Val loss 5.608\n",
      "Ep 1 (Step 000170): Train loss 5.497, Val loss 5.572\n",
      "Ep 1 (Step 000180): Train loss 5.495, Val loss 5.549\n",
      "Ep 1 (Step 000190): Train loss 5.353, Val loss 5.516\n",
      "Ep 1 (Step 000200): Train loss 5.419, Val loss 5.484\n",
      "Ep 1 (Step 000210): Train loss 5.363, Val loss 5.486\n",
      "Ep 1 (Step 000220): Train loss 5.410, Val loss 5.451\n",
      "Ep 1 (Step 000230): Train loss 5.347, Val loss 5.439\n",
      "Ep 1 (Step 000240): Train loss 5.331, Val loss 5.415\n",
      "Ep 1 (Step 000250): Train loss 5.311, Val loss 5.401\n",
      "Ep 1 (Step 000260): Train loss 5.288, Val loss 5.409\n",
      "Ep 1 (Step 000270): Train loss 5.233, Val loss 5.377\n",
      "Ep 1 (Step 000280): Train loss 5.202, Val loss 5.351\n",
      "Ep 1 (Step 000290): Train loss 5.188, Val loss 5.347\n",
      "Ep 1 (Step 000300): Train loss 5.279, Val loss 5.333\n",
      "Ep 1 (Step 000310): Train loss 5.258, Val loss 5.319\n",
      "Ep 1 (Step 000320): Train loss 5.198, Val loss 5.303\n",
      "Ep 1 (Step 000330): Train loss 5.120, Val loss 5.296\n",
      "Ep 1 (Step 000340): Train loss 5.179, Val loss 5.292\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2917\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.946, Val loss 8.953\n",
      "Ep 1 (Step 000010): Train loss 7.449, Val loss 7.434\n",
      "Ep 1 (Step 000020): Train loss 6.867, Val loss 6.797\n",
      "Ep 1 (Step 000030): Train loss 6.535, Val loss 6.473\n",
      "Ep 1 (Step 000040): Train loss 6.330, Val loss 6.359\n",
      "Ep 1 (Step 000050): Train loss 6.314, Val loss 6.270\n",
      "Ep 1 (Step 000060): Train loss 6.130, Val loss 6.164\n",
      "Ep 1 (Step 000070): Train loss 6.103, Val loss 6.080\n",
      "Ep 1 (Step 000080): Train loss 5.895, Val loss 5.981\n",
      "Ep 1 (Step 000090): Train loss 5.814, Val loss 5.907\n",
      "Ep 1 (Step 000100): Train loss 5.858, Val loss 5.871\n",
      "Ep 1 (Step 000110): Train loss 5.715, Val loss 5.798\n",
      "Ep 1 (Step 000120): Train loss 5.732, Val loss 5.743\n",
      "Ep 1 (Step 000130): Train loss 5.630, Val loss 5.698\n",
      "Ep 1 (Step 000140): Train loss 5.678, Val loss 5.668\n",
      "Ep 1 (Step 000150): Train loss 5.548, Val loss 5.630\n",
      "Ep 1 (Step 000160): Train loss 5.525, Val loss 5.610\n",
      "Ep 1 (Step 000170): Train loss 5.530, Val loss 5.568\n",
      "Ep 1 (Step 000180): Train loss 5.437, Val loss 5.551\n",
      "Ep 1 (Step 000190): Train loss 5.431, Val loss 5.529\n",
      "Ep 1 (Step 000200): Train loss 5.401, Val loss 5.506\n",
      "Ep 1 (Step 000210): Train loss 5.371, Val loss 5.484\n",
      "Ep 1 (Step 000220): Train loss 5.335, Val loss 5.474\n",
      "Ep 1 (Step 000230): Train loss 5.350, Val loss 5.453\n",
      "Ep 1 (Step 000240): Train loss 5.364, Val loss 5.445\n",
      "Ep 1 (Step 000250): Train loss 5.336, Val loss 5.425\n",
      "Ep 1 (Step 000260): Train loss 5.370, Val loss 5.415\n",
      "Ep 1 (Step 000270): Train loss 5.246, Val loss 5.403\n",
      "Ep 1 (Step 000280): Train loss 5.384, Val loss 5.395\n",
      "Ep 1 (Step 000290): Train loss 5.231, Val loss 5.377\n",
      "Ep 1 (Step 000300): Train loss 5.214, Val loss 5.350\n",
      "Ep 1 (Step 000310): Train loss 5.264, Val loss 5.335\n",
      "Ep 1 (Step 000320): Train loss 5.226, Val loss 5.322\n",
      "Ep 1 (Step 000330): Train loss 5.120, Val loss 5.329\n",
      "Ep 1 (Step 000340): Train loss 5.138, Val loss 5.324\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3241\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.869, Val loss 8.808\n",
      "Ep 1 (Step 000010): Train loss 7.389, Val loss 7.347\n",
      "Ep 1 (Step 000020): Train loss 6.756, Val loss 6.719\n",
      "Ep 1 (Step 000030): Train loss 6.430, Val loss 6.428\n",
      "Ep 1 (Step 000040): Train loss 6.348, Val loss 6.340\n",
      "Ep 1 (Step 000050): Train loss 6.312, Val loss 6.250\n",
      "Ep 1 (Step 000060): Train loss 6.101, Val loss 6.118\n",
      "Ep 1 (Step 000070): Train loss 6.051, Val loss 6.038\n",
      "Ep 1 (Step 000080): Train loss 5.902, Val loss 5.955\n",
      "Ep 1 (Step 000090): Train loss 5.940, Val loss 5.908\n",
      "Ep 1 (Step 000100): Train loss 5.832, Val loss 5.846\n",
      "Ep 1 (Step 000110): Train loss 5.650, Val loss 5.773\n",
      "Ep 1 (Step 000120): Train loss 5.626, Val loss 5.731\n",
      "Ep 1 (Step 000130): Train loss 5.606, Val loss 5.681\n",
      "Ep 1 (Step 000140): Train loss 5.564, Val loss 5.652\n",
      "Ep 1 (Step 000150): Train loss 5.587, Val loss 5.622\n",
      "Ep 1 (Step 000160): Train loss 5.516, Val loss 5.584\n",
      "Ep 1 (Step 000170): Train loss 5.428, Val loss 5.552\n",
      "Ep 1 (Step 000180): Train loss 5.423, Val loss 5.528\n",
      "Ep 1 (Step 000190): Train loss 5.311, Val loss 5.512\n",
      "Ep 1 (Step 000200): Train loss 5.484, Val loss 5.500\n",
      "Ep 1 (Step 000210): Train loss 5.323, Val loss 5.464\n",
      "Ep 1 (Step 000220): Train loss 5.358, Val loss 5.450\n",
      "Ep 1 (Step 000230): Train loss 5.358, Val loss 5.434\n",
      "Ep 1 (Step 000240): Train loss 5.234, Val loss 5.407\n",
      "Ep 1 (Step 000250): Train loss 5.304, Val loss 5.390\n",
      "Ep 1 (Step 000260): Train loss 5.286, Val loss 5.379\n",
      "Ep 1 (Step 000270): Train loss 5.247, Val loss 5.367\n",
      "Ep 1 (Step 000280): Train loss 5.289, Val loss 5.368\n",
      "Ep 1 (Step 000290): Train loss 5.157, Val loss 5.334\n",
      "Ep 1 (Step 000300): Train loss 5.197, Val loss 5.325\n",
      "Ep 1 (Step 000310): Train loss 5.191, Val loss 5.311\n",
      "Ep 1 (Step 000320): Train loss 5.161, Val loss 5.297\n",
      "Ep 1 (Step 000330): Train loss 5.203, Val loss 5.292\n",
      "Ep 1 (Step 000340): Train loss 5.130, Val loss 5.284\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2839\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.900, Val loss 8.873\n",
      "Ep 1 (Step 000010): Train loss 7.413, Val loss 7.342\n",
      "Ep 1 (Step 000020): Train loss 6.758, Val loss 6.729\n",
      "Ep 1 (Step 000030): Train loss 6.505, Val loss 6.459\n",
      "Ep 1 (Step 000040): Train loss 6.416, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.260, Val loss 6.289\n",
      "Ep 1 (Step 000060): Train loss 6.193, Val loss 6.176\n",
      "Ep 1 (Step 000070): Train loss 6.114, Val loss 6.080\n",
      "Ep 1 (Step 000080): Train loss 6.008, Val loss 5.993\n",
      "Ep 1 (Step 000090): Train loss 5.916, Val loss 5.914\n",
      "Ep 1 (Step 000100): Train loss 5.790, Val loss 5.852\n",
      "Ep 1 (Step 000110): Train loss 5.814, Val loss 5.819\n",
      "Ep 1 (Step 000120): Train loss 5.643, Val loss 5.756\n",
      "Ep 1 (Step 000130): Train loss 5.648, Val loss 5.713\n",
      "Ep 1 (Step 000140): Train loss 5.590, Val loss 5.678\n",
      "Ep 1 (Step 000150): Train loss 5.511, Val loss 5.634\n",
      "Ep 1 (Step 000160): Train loss 5.538, Val loss 5.615\n",
      "Ep 1 (Step 000170): Train loss 5.540, Val loss 5.577\n",
      "Ep 1 (Step 000180): Train loss 5.523, Val loss 5.544\n",
      "Ep 1 (Step 000190): Train loss 5.422, Val loss 5.525\n",
      "Ep 1 (Step 000200): Train loss 5.460, Val loss 5.518\n",
      "Ep 1 (Step 000210): Train loss 5.452, Val loss 5.480\n",
      "Ep 1 (Step 000220): Train loss 5.396, Val loss 5.461\n",
      "Ep 1 (Step 000230): Train loss 5.376, Val loss 5.435\n",
      "Ep 1 (Step 000240): Train loss 5.304, Val loss 5.418\n",
      "Ep 1 (Step 000250): Train loss 5.343, Val loss 5.411\n",
      "Ep 1 (Step 000260): Train loss 5.213, Val loss 5.393\n",
      "Ep 1 (Step 000270): Train loss 5.266, Val loss 5.375\n",
      "Ep 1 (Step 000280): Train loss 5.322, Val loss 5.367\n",
      "Ep 1 (Step 000290): Train loss 5.242, Val loss 5.351\n",
      "Ep 1 (Step 000300): Train loss 5.227, Val loss 5.338\n",
      "Ep 1 (Step 000310): Train loss 5.243, Val loss 5.320\n",
      "Ep 1 (Step 000320): Train loss 5.277, Val loss 5.305\n",
      "Ep 1 (Step 000330): Train loss 5.189, Val loss 5.293\n",
      "Ep 1 (Step 000340): Train loss 5.211, Val loss 5.285\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2847\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.888, Val loss 8.855\n",
      "Ep 1 (Step 000010): Train loss 7.371, Val loss 7.357\n",
      "Ep 1 (Step 000020): Train loss 6.743, Val loss 6.734\n",
      "Ep 1 (Step 000030): Train loss 6.514, Val loss 6.444\n",
      "Ep 1 (Step 000040): Train loss 6.301, Val loss 6.359\n",
      "Ep 1 (Step 000050): Train loss 6.253, Val loss 6.271\n",
      "Ep 1 (Step 000060): Train loss 6.111, Val loss 6.147\n",
      "Ep 1 (Step 000070): Train loss 6.058, Val loss 6.049\n",
      "Ep 1 (Step 000080): Train loss 5.972, Val loss 5.973\n",
      "Ep 1 (Step 000090): Train loss 5.932, Val loss 5.899\n",
      "Ep 1 (Step 000100): Train loss 5.850, Val loss 5.843\n",
      "Ep 1 (Step 000110): Train loss 5.688, Val loss 5.799\n",
      "Ep 1 (Step 000120): Train loss 5.654, Val loss 5.743\n",
      "Ep 1 (Step 000130): Train loss 5.661, Val loss 5.700\n",
      "Ep 1 (Step 000140): Train loss 5.665, Val loss 5.659\n",
      "Ep 1 (Step 000150): Train loss 5.614, Val loss 5.630\n",
      "Ep 1 (Step 000160): Train loss 5.485, Val loss 5.592\n",
      "Ep 1 (Step 000170): Train loss 5.499, Val loss 5.566\n",
      "Ep 1 (Step 000180): Train loss 5.467, Val loss 5.539\n",
      "Ep 1 (Step 000190): Train loss 5.405, Val loss 5.506\n",
      "Ep 1 (Step 000200): Train loss 5.405, Val loss 5.493\n",
      "Ep 1 (Step 000210): Train loss 5.439, Val loss 5.468\n",
      "Ep 1 (Step 000220): Train loss 5.279, Val loss 5.443\n",
      "Ep 1 (Step 000230): Train loss 5.382, Val loss 5.421\n",
      "Ep 1 (Step 000240): Train loss 5.440, Val loss 5.412\n",
      "Ep 1 (Step 000250): Train loss 5.321, Val loss 5.397\n",
      "Ep 1 (Step 000260): Train loss 5.251, Val loss 5.377\n",
      "Ep 1 (Step 000270): Train loss 5.307, Val loss 5.366\n",
      "Ep 1 (Step 000280): Train loss 5.277, Val loss 5.350\n",
      "Ep 1 (Step 000290): Train loss 5.360, Val loss 5.332\n",
      "Ep 1 (Step 000300): Train loss 5.320, Val loss 5.311\n",
      "Ep 1 (Step 000310): Train loss 5.165, Val loss 5.298\n",
      "Ep 1 (Step 000320): Train loss 5.175, Val loss 5.297\n",
      "Ep 1 (Step 000330): Train loss 5.109, Val loss 5.295\n",
      "Ep 1 (Step 000340): Train loss 5.161, Val loss 5.291\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2912\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.614, Val loss 8.589\n",
      "Ep 1 (Step 000010): Train loss 6.843, Val loss 6.812\n",
      "Ep 1 (Step 000020): Train loss 6.454, Val loss 6.453\n",
      "Ep 1 (Step 000030): Train loss 6.367, Val loss 6.407\n",
      "Ep 1 (Step 000040): Train loss 6.220, Val loss 6.227\n",
      "Ep 1 (Step 000050): Train loss 6.088, Val loss 6.116\n",
      "Ep 1 (Step 000060): Train loss 5.952, Val loss 5.989\n",
      "Ep 1 (Step 000070): Train loss 5.898, Val loss 5.898\n",
      "Ep 1 (Step 000080): Train loss 5.671, Val loss 5.817\n",
      "Ep 1 (Step 000090): Train loss 5.696, Val loss 5.750\n",
      "Ep 1 (Step 000100): Train loss 5.637, Val loss 5.720\n",
      "Ep 1 (Step 000110): Train loss 5.625, Val loss 5.648\n",
      "Ep 1 (Step 000120): Train loss 5.599, Val loss 5.597\n",
      "Ep 1 (Step 000130): Train loss 5.548, Val loss 5.612\n",
      "Ep 1 (Step 000140): Train loss 5.529, Val loss 5.541\n",
      "Ep 1 (Step 000150): Train loss 5.426, Val loss 5.520\n",
      "Ep 1 (Step 000160): Train loss 5.431, Val loss 5.482\n",
      "Ep 1 (Step 000170): Train loss 5.417, Val loss 5.463\n",
      "Ep 1 (Step 000180): Train loss 5.277, Val loss 5.453\n",
      "Ep 1 (Step 000190): Train loss 5.340, Val loss 5.434\n",
      "Ep 1 (Step 000200): Train loss 5.332, Val loss 5.402\n",
      "Ep 1 (Step 000210): Train loss 5.228, Val loss 5.382\n",
      "Ep 1 (Step 000220): Train loss 5.259, Val loss 5.359\n",
      "Ep 1 (Step 000230): Train loss 5.183, Val loss 5.359\n",
      "Ep 1 (Step 000240): Train loss 5.172, Val loss 5.346\n",
      "Ep 1 (Step 000250): Train loss 5.234, Val loss 5.313\n",
      "Ep 1 (Step 000260): Train loss 5.174, Val loss 5.322\n",
      "Ep 1 (Step 000270): Train loss 5.175, Val loss 5.307\n",
      "Ep 1 (Step 000280): Train loss 5.184, Val loss 5.273\n",
      "Ep 1 (Step 000290): Train loss 5.241, Val loss 5.272\n",
      "Ep 1 (Step 000300): Train loss 5.182, Val loss 5.264\n",
      "Ep 1 (Step 000310): Train loss 5.190, Val loss 5.247\n",
      "Ep 1 (Step 000320): Train loss 5.034, Val loss 5.252\n",
      "Ep 1 (Step 000330): Train loss 5.079, Val loss 5.232\n",
      "Ep 1 (Step 000340): Train loss 5.082, Val loss 5.229\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2286\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.634, Val loss 8.587\n",
      "Ep 1 (Step 000010): Train loss 6.855, Val loss 6.800\n",
      "Ep 1 (Step 000020): Train loss 6.475, Val loss 6.441\n",
      "Ep 1 (Step 000030): Train loss 6.440, Val loss 6.403\n",
      "Ep 1 (Step 000040): Train loss 6.325, Val loss 6.272\n",
      "Ep 1 (Step 000050): Train loss 6.053, Val loss 6.105\n",
      "Ep 1 (Step 000060): Train loss 5.965, Val loss 5.992\n",
      "Ep 1 (Step 000070): Train loss 5.785, Val loss 5.888\n",
      "Ep 1 (Step 000080): Train loss 5.742, Val loss 5.802\n",
      "Ep 1 (Step 000090): Train loss 5.648, Val loss 5.726\n",
      "Ep 1 (Step 000100): Train loss 5.645, Val loss 5.690\n",
      "Ep 1 (Step 000110): Train loss 5.552, Val loss 5.648\n",
      "Ep 1 (Step 000120): Train loss 5.558, Val loss 5.601\n",
      "Ep 1 (Step 000130): Train loss 5.420, Val loss 5.566\n",
      "Ep 1 (Step 000140): Train loss 5.354, Val loss 5.525\n",
      "Ep 1 (Step 000150): Train loss 5.404, Val loss 5.525\n",
      "Ep 1 (Step 000160): Train loss 5.450, Val loss 5.505\n",
      "Ep 1 (Step 000170): Train loss 5.397, Val loss 5.477\n",
      "Ep 1 (Step 000180): Train loss 5.430, Val loss 5.445\n",
      "Ep 1 (Step 000190): Train loss 5.326, Val loss 5.445\n",
      "Ep 1 (Step 000200): Train loss 5.391, Val loss 5.420\n",
      "Ep 1 (Step 000210): Train loss 5.267, Val loss 5.387\n",
      "Ep 1 (Step 000220): Train loss 5.134, Val loss 5.385\n",
      "Ep 1 (Step 000230): Train loss 5.226, Val loss 5.377\n",
      "Ep 1 (Step 000240): Train loss 5.319, Val loss 5.358\n",
      "Ep 1 (Step 000250): Train loss 5.152, Val loss 5.326\n",
      "Ep 1 (Step 000260): Train loss 5.164, Val loss 5.313\n",
      "Ep 1 (Step 000270): Train loss 5.190, Val loss 5.303\n",
      "Ep 1 (Step 000280): Train loss 5.146, Val loss 5.308\n",
      "Ep 1 (Step 000290): Train loss 5.176, Val loss 5.286\n",
      "Ep 1 (Step 000300): Train loss 5.159, Val loss 5.261\n",
      "Ep 1 (Step 000310): Train loss 5.164, Val loss 5.248\n",
      "Ep 1 (Step 000320): Train loss 5.026, Val loss 5.240\n",
      "Ep 1 (Step 000330): Train loss 5.084, Val loss 5.239\n",
      "Ep 1 (Step 000340): Train loss 5.095, Val loss 5.213\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2133\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.663, Val loss 8.599\n",
      "Ep 1 (Step 000010): Train loss 6.899, Val loss 6.840\n",
      "Ep 1 (Step 000020): Train loss 6.458, Val loss 6.511\n",
      "Ep 1 (Step 000030): Train loss 6.399, Val loss 6.449\n",
      "Ep 1 (Step 000040): Train loss 6.191, Val loss 6.267\n",
      "Ep 1 (Step 000050): Train loss 6.044, Val loss 6.125\n",
      "Ep 1 (Step 000060): Train loss 5.983, Val loss 6.010\n",
      "Ep 1 (Step 000070): Train loss 5.882, Val loss 5.924\n",
      "Ep 1 (Step 000080): Train loss 5.885, Val loss 5.837\n",
      "Ep 1 (Step 000090): Train loss 5.809, Val loss 5.824\n",
      "Ep 1 (Step 000100): Train loss 5.667, Val loss 5.730\n",
      "Ep 1 (Step 000110): Train loss 5.568, Val loss 5.659\n",
      "Ep 1 (Step 000120): Train loss 5.487, Val loss 5.617\n",
      "Ep 1 (Step 000130): Train loss 5.549, Val loss 5.581\n",
      "Ep 1 (Step 000140): Train loss 5.451, Val loss 5.553\n",
      "Ep 1 (Step 000150): Train loss 5.434, Val loss 5.529\n",
      "Ep 1 (Step 000160): Train loss 5.444, Val loss 5.502\n",
      "Ep 1 (Step 000170): Train loss 5.401, Val loss 5.478\n",
      "Ep 1 (Step 000180): Train loss 5.342, Val loss 5.448\n",
      "Ep 1 (Step 000190): Train loss 5.326, Val loss 5.426\n",
      "Ep 1 (Step 000200): Train loss 5.299, Val loss 5.414\n",
      "Ep 1 (Step 000210): Train loss 5.288, Val loss 5.406\n",
      "Ep 1 (Step 000220): Train loss 5.223, Val loss 5.370\n",
      "Ep 1 (Step 000230): Train loss 5.298, Val loss 5.357\n",
      "Ep 1 (Step 000240): Train loss 5.243, Val loss 5.326\n",
      "Ep 1 (Step 000250): Train loss 5.194, Val loss 5.309\n",
      "Ep 1 (Step 000260): Train loss 5.235, Val loss 5.308\n",
      "Ep 1 (Step 000270): Train loss 5.186, Val loss 5.297\n",
      "Ep 1 (Step 000280): Train loss 5.150, Val loss 5.269\n",
      "Ep 1 (Step 000290): Train loss 5.171, Val loss 5.281\n",
      "Ep 1 (Step 000300): Train loss 5.135, Val loss 5.247\n",
      "Ep 1 (Step 000310): Train loss 5.180, Val loss 5.266\n",
      "Ep 1 (Step 000320): Train loss 5.115, Val loss 5.254\n",
      "Ep 1 (Step 000330): Train loss 5.037, Val loss 5.233\n",
      "Ep 1 (Step 000340): Train loss 5.123, Val loss 5.210\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2099\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.578, Val loss 8.561\n",
      "Ep 1 (Step 000010): Train loss 6.795, Val loss 6.797\n",
      "Ep 1 (Step 000020): Train loss 6.503, Val loss 6.450\n",
      "Ep 1 (Step 000030): Train loss 6.410, Val loss 6.388\n",
      "Ep 1 (Step 000040): Train loss 6.179, Val loss 6.220\n",
      "Ep 1 (Step 000050): Train loss 6.025, Val loss 6.061\n",
      "Ep 1 (Step 000060): Train loss 5.863, Val loss 5.947\n",
      "Ep 1 (Step 000070): Train loss 5.801, Val loss 5.877\n",
      "Ep 1 (Step 000080): Train loss 5.772, Val loss 5.806\n",
      "Ep 1 (Step 000090): Train loss 5.719, Val loss 5.752\n",
      "Ep 1 (Step 000100): Train loss 5.690, Val loss 5.687\n",
      "Ep 1 (Step 000110): Train loss 5.478, Val loss 5.628\n",
      "Ep 1 (Step 000120): Train loss 5.523, Val loss 5.575\n",
      "Ep 1 (Step 000130): Train loss 5.460, Val loss 5.523\n",
      "Ep 1 (Step 000140): Train loss 5.416, Val loss 5.496\n",
      "Ep 1 (Step 000150): Train loss 5.400, Val loss 5.479\n",
      "Ep 1 (Step 000160): Train loss 5.354, Val loss 5.465\n",
      "Ep 1 (Step 000170): Train loss 5.372, Val loss 5.440\n",
      "Ep 1 (Step 000180): Train loss 5.358, Val loss 5.421\n",
      "Ep 1 (Step 000190): Train loss 5.322, Val loss 5.406\n",
      "Ep 1 (Step 000200): Train loss 5.219, Val loss 5.380\n",
      "Ep 1 (Step 000210): Train loss 5.321, Val loss 5.365\n",
      "Ep 1 (Step 000220): Train loss 5.269, Val loss 5.345\n",
      "Ep 1 (Step 000230): Train loss 5.218, Val loss 5.311\n",
      "Ep 1 (Step 000240): Train loss 5.186, Val loss 5.307\n",
      "Ep 1 (Step 000250): Train loss 5.211, Val loss 5.301\n",
      "Ep 1 (Step 000260): Train loss 5.142, Val loss 5.288\n",
      "Ep 1 (Step 000270): Train loss 5.269, Val loss 5.267\n",
      "Ep 1 (Step 000280): Train loss 5.098, Val loss 5.245\n",
      "Ep 1 (Step 000290): Train loss 5.113, Val loss 5.246\n",
      "Ep 1 (Step 000300): Train loss 5.120, Val loss 5.250\n",
      "Ep 1 (Step 000310): Train loss 5.093, Val loss 5.248\n",
      "Ep 1 (Step 000320): Train loss 5.020, Val loss 5.214\n",
      "Ep 1 (Step 000330): Train loss 5.055, Val loss 5.223\n",
      "Ep 1 (Step 000340): Train loss 5.063, Val loss 5.208\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2081\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.631, Val loss 8.591\n",
      "Ep 1 (Step 000010): Train loss 6.822, Val loss 6.804\n",
      "Ep 1 (Step 000020): Train loss 6.438, Val loss 6.452\n",
      "Ep 1 (Step 000030): Train loss 6.343, Val loss 6.385\n",
      "Ep 1 (Step 000040): Train loss 6.173, Val loss 6.234\n",
      "Ep 1 (Step 000050): Train loss 6.100, Val loss 6.071\n",
      "Ep 1 (Step 000060): Train loss 5.956, Val loss 5.964\n",
      "Ep 1 (Step 000070): Train loss 5.845, Val loss 5.869\n",
      "Ep 1 (Step 000080): Train loss 5.761, Val loss 5.812\n",
      "Ep 1 (Step 000090): Train loss 5.684, Val loss 5.761\n",
      "Ep 1 (Step 000100): Train loss 5.511, Val loss 5.689\n",
      "Ep 1 (Step 000110): Train loss 5.540, Val loss 5.651\n",
      "Ep 1 (Step 000120): Train loss 5.546, Val loss 5.606\n",
      "Ep 1 (Step 000130): Train loss 5.518, Val loss 5.561\n",
      "Ep 1 (Step 000140): Train loss 5.485, Val loss 5.542\n",
      "Ep 1 (Step 000150): Train loss 5.392, Val loss 5.507\n",
      "Ep 1 (Step 000160): Train loss 5.284, Val loss 5.474\n",
      "Ep 1 (Step 000170): Train loss 5.366, Val loss 5.473\n",
      "Ep 1 (Step 000180): Train loss 5.394, Val loss 5.436\n",
      "Ep 1 (Step 000190): Train loss 5.322, Val loss 5.424\n",
      "Ep 1 (Step 000200): Train loss 5.282, Val loss 5.411\n",
      "Ep 1 (Step 000210): Train loss 5.237, Val loss 5.380\n",
      "Ep 1 (Step 000220): Train loss 5.177, Val loss 5.364\n",
      "Ep 1 (Step 000230): Train loss 5.181, Val loss 5.335\n",
      "Ep 1 (Step 000240): Train loss 5.127, Val loss 5.316\n",
      "Ep 1 (Step 000250): Train loss 5.169, Val loss 5.319\n",
      "Ep 1 (Step 000260): Train loss 5.175, Val loss 5.296\n",
      "Ep 1 (Step 000270): Train loss 5.215, Val loss 5.286\n",
      "Ep 1 (Step 000280): Train loss 5.159, Val loss 5.252\n",
      "Ep 1 (Step 000290): Train loss 5.001, Val loss 5.251\n",
      "Ep 1 (Step 000300): Train loss 5.077, Val loss 5.236\n",
      "Ep 1 (Step 000310): Train loss 5.119, Val loss 5.230\n",
      "Ep 1 (Step 000320): Train loss 5.135, Val loss 5.210\n",
      "Ep 1 (Step 000330): Train loss 5.117, Val loss 5.208\n",
      "Ep 1 (Step 000340): Train loss 5.098, Val loss 5.201\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2006\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.642, Val loss 8.614\n",
      "Ep 1 (Step 000010): Train loss 6.863, Val loss 6.845\n",
      "Ep 1 (Step 000020): Train loss 6.447, Val loss 6.442\n",
      "Ep 1 (Step 000030): Train loss 6.408, Val loss 6.409\n",
      "Ep 1 (Step 000040): Train loss 6.241, Val loss 6.231\n",
      "Ep 1 (Step 000050): Train loss 6.075, Val loss 6.075\n",
      "Ep 1 (Step 000060): Train loss 5.927, Val loss 5.957\n",
      "Ep 1 (Step 000070): Train loss 5.848, Val loss 5.885\n",
      "Ep 1 (Step 000080): Train loss 5.724, Val loss 5.803\n",
      "Ep 1 (Step 000090): Train loss 5.709, Val loss 5.733\n",
      "Ep 1 (Step 000100): Train loss 5.643, Val loss 5.669\n",
      "Ep 1 (Step 000110): Train loss 5.613, Val loss 5.646\n",
      "Ep 1 (Step 000120): Train loss 5.536, Val loss 5.601\n",
      "Ep 1 (Step 000130): Train loss 5.508, Val loss 5.573\n",
      "Ep 1 (Step 000140): Train loss 5.434, Val loss 5.525\n",
      "Ep 1 (Step 000150): Train loss 5.420, Val loss 5.493\n",
      "Ep 1 (Step 000160): Train loss 5.412, Val loss 5.473\n",
      "Ep 1 (Step 000170): Train loss 5.296, Val loss 5.428\n",
      "Ep 1 (Step 000180): Train loss 5.372, Val loss 5.420\n",
      "Ep 1 (Step 000190): Train loss 5.340, Val loss 5.406\n",
      "Ep 1 (Step 000200): Train loss 5.308, Val loss 5.391\n",
      "Ep 1 (Step 000210): Train loss 5.270, Val loss 5.362\n",
      "Ep 1 (Step 000220): Train loss 5.277, Val loss 5.352\n",
      "Ep 1 (Step 000230): Train loss 5.239, Val loss 5.322\n",
      "Ep 1 (Step 000240): Train loss 5.221, Val loss 5.323\n",
      "Ep 1 (Step 000250): Train loss 5.235, Val loss 5.313\n",
      "Ep 1 (Step 000260): Train loss 5.128, Val loss 5.305\n",
      "Ep 1 (Step 000270): Train loss 5.203, Val loss 5.282\n",
      "Ep 1 (Step 000280): Train loss 5.140, Val loss 5.253\n",
      "Ep 1 (Step 000290): Train loss 5.095, Val loss 5.241\n",
      "Ep 1 (Step 000300): Train loss 5.094, Val loss 5.249\n",
      "Ep 1 (Step 000310): Train loss 5.135, Val loss 5.233\n",
      "Ep 1 (Step 000320): Train loss 5.017, Val loss 5.211\n",
      "Ep 1 (Step 000330): Train loss 5.018, Val loss 5.209\n",
      "Ep 1 (Step 000340): Train loss 5.028, Val loss 5.223\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2229\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.864, Val loss 8.820\n",
      "Ep 1 (Step 000010): Train loss 7.500, Val loss 7.479\n",
      "Ep 1 (Step 000020): Train loss 6.802, Val loss 6.808\n",
      "Ep 1 (Step 000030): Train loss 6.467, Val loss 6.467\n",
      "Ep 1 (Step 000040): Train loss 6.375, Val loss 6.354\n",
      "Ep 1 (Step 000050): Train loss 6.240, Val loss 6.303\n",
      "Ep 1 (Step 000060): Train loss 6.226, Val loss 6.179\n",
      "Ep 1 (Step 000070): Train loss 6.139, Val loss 6.060\n",
      "Ep 1 (Step 000080): Train loss 5.890, Val loss 5.981\n",
      "Ep 1 (Step 000090): Train loss 5.992, Val loss 5.932\n",
      "Ep 1 (Step 000100): Train loss 5.806, Val loss 5.863\n",
      "Ep 1 (Step 000110): Train loss 5.747, Val loss 5.813\n",
      "Ep 1 (Step 000120): Train loss 5.713, Val loss 5.776\n",
      "Ep 1 (Step 000130): Train loss 5.692, Val loss 5.733\n",
      "Ep 1 (Step 000140): Train loss 5.707, Val loss 5.689\n",
      "Ep 1 (Step 000150): Train loss 5.601, Val loss 5.672\n",
      "Ep 1 (Step 000160): Train loss 5.564, Val loss 5.630\n",
      "Ep 1 (Step 000170): Train loss 5.566, Val loss 5.606\n",
      "Ep 1 (Step 000180): Train loss 5.478, Val loss 5.574\n",
      "Ep 1 (Step 000190): Train loss 5.455, Val loss 5.541\n",
      "Ep 1 (Step 000200): Train loss 5.491, Val loss 5.538\n",
      "Ep 1 (Step 000210): Train loss 5.369, Val loss 5.522\n",
      "Ep 1 (Step 000220): Train loss 5.401, Val loss 5.491\n",
      "Ep 1 (Step 000230): Train loss 5.378, Val loss 5.472\n",
      "Ep 1 (Step 000240): Train loss 5.300, Val loss 5.458\n",
      "Ep 1 (Step 000250): Train loss 5.304, Val loss 5.446\n",
      "Ep 1 (Step 000260): Train loss 5.379, Val loss 5.447\n",
      "Ep 1 (Step 000270): Train loss 5.352, Val loss 5.425\n",
      "Ep 1 (Step 000280): Train loss 5.335, Val loss 5.426\n",
      "Ep 1 (Step 000290): Train loss 5.278, Val loss 5.396\n",
      "Ep 1 (Step 000300): Train loss 5.262, Val loss 5.384\n",
      "Ep 1 (Step 000310): Train loss 5.323, Val loss 5.391\n",
      "Ep 1 (Step 000320): Train loss 5.219, Val loss 5.366\n",
      "Ep 1 (Step 000330): Train loss 5.231, Val loss 5.361\n",
      "Ep 1 (Step 000340): Train loss 5.239, Val loss 5.347\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3475\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.863, Val loss 8.830\n",
      "Ep 1 (Step 000010): Train loss 7.427, Val loss 7.405\n",
      "Ep 1 (Step 000020): Train loss 6.762, Val loss 6.764\n",
      "Ep 1 (Step 000030): Train loss 6.487, Val loss 6.462\n",
      "Ep 1 (Step 000040): Train loss 6.368, Val loss 6.359\n",
      "Ep 1 (Step 000050): Train loss 6.284, Val loss 6.292\n",
      "Ep 1 (Step 000060): Train loss 6.172, Val loss 6.191\n",
      "Ep 1 (Step 000070): Train loss 6.099, Val loss 6.098\n",
      "Ep 1 (Step 000080): Train loss 6.056, Val loss 6.020\n",
      "Ep 1 (Step 000090): Train loss 5.858, Val loss 5.935\n",
      "Ep 1 (Step 000100): Train loss 5.835, Val loss 5.867\n",
      "Ep 1 (Step 000110): Train loss 5.843, Val loss 5.838\n",
      "Ep 1 (Step 000120): Train loss 5.776, Val loss 5.792\n",
      "Ep 1 (Step 000130): Train loss 5.710, Val loss 5.746\n",
      "Ep 1 (Step 000140): Train loss 5.656, Val loss 5.713\n",
      "Ep 1 (Step 000150): Train loss 5.600, Val loss 5.680\n",
      "Ep 1 (Step 000160): Train loss 5.513, Val loss 5.676\n",
      "Ep 1 (Step 000170): Train loss 5.506, Val loss 5.628\n",
      "Ep 1 (Step 000180): Train loss 5.501, Val loss 5.601\n",
      "Ep 1 (Step 000190): Train loss 5.591, Val loss 5.582\n",
      "Ep 1 (Step 000200): Train loss 5.471, Val loss 5.563\n",
      "Ep 1 (Step 000210): Train loss 5.443, Val loss 5.532\n",
      "Ep 1 (Step 000220): Train loss 5.407, Val loss 5.510\n",
      "Ep 1 (Step 000230): Train loss 5.390, Val loss 5.475\n",
      "Ep 1 (Step 000240): Train loss 5.341, Val loss 5.446\n",
      "Ep 1 (Step 000250): Train loss 5.414, Val loss 5.431\n",
      "Ep 1 (Step 000260): Train loss 5.291, Val loss 5.430\n",
      "Ep 1 (Step 000270): Train loss 5.311, Val loss 5.430\n",
      "Ep 1 (Step 000280): Train loss 5.237, Val loss 5.415\n",
      "Ep 1 (Step 000290): Train loss 5.369, Val loss 5.396\n",
      "Ep 1 (Step 000300): Train loss 5.210, Val loss 5.378\n",
      "Ep 1 (Step 000310): Train loss 5.220, Val loss 5.372\n",
      "Ep 1 (Step 000320): Train loss 5.214, Val loss 5.360\n",
      "Ep 1 (Step 000330): Train loss 5.285, Val loss 5.374\n",
      "Ep 1 (Step 000340): Train loss 5.289, Val loss 5.354\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3544\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.725, Val loss 8.686\n",
      "Ep 1 (Step 000010): Train loss 7.336, Val loss 7.366\n",
      "Ep 1 (Step 000020): Train loss 6.705, Val loss 6.746\n",
      "Ep 1 (Step 000030): Train loss 6.442, Val loss 6.464\n",
      "Ep 1 (Step 000040): Train loss 6.405, Val loss 6.384\n",
      "Ep 1 (Step 000050): Train loss 6.314, Val loss 6.291\n",
      "Ep 1 (Step 000060): Train loss 6.175, Val loss 6.182\n",
      "Ep 1 (Step 000070): Train loss 6.117, Val loss 6.148\n",
      "Ep 1 (Step 000080): Train loss 6.038, Val loss 6.004\n",
      "Ep 1 (Step 000090): Train loss 5.938, Val loss 5.940\n",
      "Ep 1 (Step 000100): Train loss 5.826, Val loss 5.882\n",
      "Ep 1 (Step 000110): Train loss 5.746, Val loss 5.827\n",
      "Ep 1 (Step 000120): Train loss 5.779, Val loss 5.801\n",
      "Ep 1 (Step 000130): Train loss 5.650, Val loss 5.748\n",
      "Ep 1 (Step 000140): Train loss 5.636, Val loss 5.702\n",
      "Ep 1 (Step 000150): Train loss 5.658, Val loss 5.679\n",
      "Ep 1 (Step 000160): Train loss 5.618, Val loss 5.653\n",
      "Ep 1 (Step 000170): Train loss 5.533, Val loss 5.623\n",
      "Ep 1 (Step 000180): Train loss 5.508, Val loss 5.610\n",
      "Ep 1 (Step 000190): Train loss 5.525, Val loss 5.551\n",
      "Ep 1 (Step 000200): Train loss 5.536, Val loss 5.528\n",
      "Ep 1 (Step 000210): Train loss 5.489, Val loss 5.508\n",
      "Ep 1 (Step 000220): Train loss 5.414, Val loss 5.487\n",
      "Ep 1 (Step 000230): Train loss 5.461, Val loss 5.472\n",
      "Ep 1 (Step 000240): Train loss 5.343, Val loss 5.463\n",
      "Ep 1 (Step 000250): Train loss 5.375, Val loss 5.454\n",
      "Ep 1 (Step 000260): Train loss 5.340, Val loss 5.429\n",
      "Ep 1 (Step 000270): Train loss 5.251, Val loss 5.434\n",
      "Ep 1 (Step 000280): Train loss 5.240, Val loss 5.412\n",
      "Ep 1 (Step 000290): Train loss 5.269, Val loss 5.410\n",
      "Ep 1 (Step 000300): Train loss 5.225, Val loss 5.388\n",
      "Ep 1 (Step 000310): Train loss 5.292, Val loss 5.376\n",
      "Ep 1 (Step 000320): Train loss 5.244, Val loss 5.372\n",
      "Ep 1 (Step 000330): Train loss 5.237, Val loss 5.356\n",
      "Ep 1 (Step 000340): Train loss 5.178, Val loss 5.362\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3623\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.733, Val loss 8.694\n",
      "Ep 1 (Step 000010): Train loss 7.393, Val loss 7.374\n",
      "Ep 1 (Step 000020): Train loss 6.792, Val loss 6.746\n",
      "Ep 1 (Step 000030): Train loss 6.511, Val loss 6.456\n",
      "Ep 1 (Step 000040): Train loss 6.393, Val loss 6.380\n",
      "Ep 1 (Step 000050): Train loss 6.294, Val loss 6.328\n",
      "Ep 1 (Step 000060): Train loss 6.207, Val loss 6.225\n",
      "Ep 1 (Step 000070): Train loss 6.169, Val loss 6.128\n",
      "Ep 1 (Step 000080): Train loss 6.030, Val loss 6.042\n",
      "Ep 1 (Step 000090): Train loss 5.970, Val loss 5.963\n",
      "Ep 1 (Step 000100): Train loss 5.889, Val loss 5.895\n",
      "Ep 1 (Step 000110): Train loss 5.802, Val loss 5.841\n",
      "Ep 1 (Step 000120): Train loss 5.769, Val loss 5.787\n",
      "Ep 1 (Step 000130): Train loss 5.709, Val loss 5.742\n",
      "Ep 1 (Step 000140): Train loss 5.655, Val loss 5.709\n",
      "Ep 1 (Step 000150): Train loss 5.546, Val loss 5.673\n",
      "Ep 1 (Step 000160): Train loss 5.581, Val loss 5.644\n",
      "Ep 1 (Step 000170): Train loss 5.570, Val loss 5.599\n",
      "Ep 1 (Step 000180): Train loss 5.430, Val loss 5.568\n",
      "Ep 1 (Step 000190): Train loss 5.499, Val loss 5.546\n",
      "Ep 1 (Step 000200): Train loss 5.539, Val loss 5.526\n",
      "Ep 1 (Step 000210): Train loss 5.415, Val loss 5.506\n",
      "Ep 1 (Step 000220): Train loss 5.416, Val loss 5.490\n",
      "Ep 1 (Step 000230): Train loss 5.416, Val loss 5.469\n",
      "Ep 1 (Step 000240): Train loss 5.466, Val loss 5.459\n",
      "Ep 1 (Step 000250): Train loss 5.301, Val loss 5.451\n",
      "Ep 1 (Step 000260): Train loss 5.242, Val loss 5.436\n",
      "Ep 1 (Step 000270): Train loss 5.246, Val loss 5.416\n",
      "Ep 1 (Step 000280): Train loss 5.326, Val loss 5.407\n",
      "Ep 1 (Step 000290): Train loss 5.285, Val loss 5.386\n",
      "Ep 1 (Step 000300): Train loss 5.285, Val loss 5.376\n",
      "Ep 1 (Step 000310): Train loss 5.280, Val loss 5.354\n",
      "Ep 1 (Step 000320): Train loss 5.326, Val loss 5.358\n",
      "Ep 1 (Step 000330): Train loss 5.232, Val loss 5.354\n",
      "Ep 1 (Step 000340): Train loss 5.209, Val loss 5.333\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3333\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.800, Val loss 8.784\n",
      "Ep 1 (Step 000010): Train loss 7.393, Val loss 7.389\n",
      "Ep 1 (Step 000020): Train loss 6.799, Val loss 6.754\n",
      "Ep 1 (Step 000030): Train loss 6.469, Val loss 6.455\n",
      "Ep 1 (Step 000040): Train loss 6.396, Val loss 6.389\n",
      "Ep 1 (Step 000050): Train loss 6.403, Val loss 6.343\n",
      "Ep 1 (Step 000060): Train loss 6.192, Val loss 6.250\n",
      "Ep 1 (Step 000070): Train loss 6.095, Val loss 6.103\n",
      "Ep 1 (Step 000080): Train loss 5.963, Val loss 6.035\n",
      "Ep 1 (Step 000090): Train loss 5.873, Val loss 5.947\n",
      "Ep 1 (Step 000100): Train loss 5.812, Val loss 5.872\n",
      "Ep 1 (Step 000110): Train loss 5.810, Val loss 5.818\n",
      "Ep 1 (Step 000120): Train loss 5.755, Val loss 5.770\n",
      "Ep 1 (Step 000130): Train loss 5.734, Val loss 5.735\n",
      "Ep 1 (Step 000140): Train loss 5.695, Val loss 5.695\n",
      "Ep 1 (Step 000150): Train loss 5.621, Val loss 5.676\n",
      "Ep 1 (Step 000160): Train loss 5.561, Val loss 5.631\n",
      "Ep 1 (Step 000170): Train loss 5.533, Val loss 5.594\n",
      "Ep 1 (Step 000180): Train loss 5.529, Val loss 5.580\n",
      "Ep 1 (Step 000190): Train loss 5.460, Val loss 5.551\n",
      "Ep 1 (Step 000200): Train loss 5.414, Val loss 5.538\n",
      "Ep 1 (Step 000210): Train loss 5.395, Val loss 5.514\n",
      "Ep 1 (Step 000220): Train loss 5.376, Val loss 5.500\n",
      "Ep 1 (Step 000230): Train loss 5.411, Val loss 5.472\n",
      "Ep 1 (Step 000240): Train loss 5.399, Val loss 5.454\n",
      "Ep 1 (Step 000250): Train loss 5.303, Val loss 5.447\n",
      "Ep 1 (Step 000260): Train loss 5.311, Val loss 5.424\n",
      "Ep 1 (Step 000270): Train loss 5.292, Val loss 5.420\n",
      "Ep 1 (Step 000280): Train loss 5.308, Val loss 5.400\n",
      "Ep 1 (Step 000290): Train loss 5.257, Val loss 5.387\n",
      "Ep 1 (Step 000300): Train loss 5.198, Val loss 5.378\n",
      "Ep 1 (Step 000310): Train loss 5.288, Val loss 5.365\n",
      "Ep 1 (Step 000320): Train loss 5.252, Val loss 5.355\n",
      "Ep 1 (Step 000330): Train loss 5.207, Val loss 5.348\n",
      "Ep 1 (Step 000340): Train loss 5.193, Val loss 5.322\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3223\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.818, Val loss 8.804\n",
      "Ep 1 (Step 000010): Train loss 7.404, Val loss 7.373\n",
      "Ep 1 (Step 000020): Train loss 6.725, Val loss 6.732\n",
      "Ep 1 (Step 000030): Train loss 6.500, Val loss 6.444\n",
      "Ep 1 (Step 000040): Train loss 6.376, Val loss 6.349\n",
      "Ep 1 (Step 000050): Train loss 6.274, Val loss 6.275\n",
      "Ep 1 (Step 000060): Train loss 6.143, Val loss 6.154\n",
      "Ep 1 (Step 000070): Train loss 6.090, Val loss 6.092\n",
      "Ep 1 (Step 000080): Train loss 5.992, Val loss 5.977\n",
      "Ep 1 (Step 000090): Train loss 5.894, Val loss 5.912\n",
      "Ep 1 (Step 000100): Train loss 5.783, Val loss 5.855\n",
      "Ep 1 (Step 000110): Train loss 5.720, Val loss 5.807\n",
      "Ep 1 (Step 000120): Train loss 5.719, Val loss 5.778\n",
      "Ep 1 (Step 000130): Train loss 5.616, Val loss 5.736\n",
      "Ep 1 (Step 000140): Train loss 5.629, Val loss 5.694\n",
      "Ep 1 (Step 000150): Train loss 5.607, Val loss 5.647\n",
      "Ep 1 (Step 000160): Train loss 5.528, Val loss 5.632\n",
      "Ep 1 (Step 000170): Train loss 5.634, Val loss 5.611\n",
      "Ep 1 (Step 000180): Train loss 5.504, Val loss 5.570\n",
      "Ep 1 (Step 000190): Train loss 5.469, Val loss 5.543\n",
      "Ep 1 (Step 000200): Train loss 5.438, Val loss 5.532\n",
      "Ep 1 (Step 000210): Train loss 5.329, Val loss 5.500\n",
      "Ep 1 (Step 000220): Train loss 5.395, Val loss 5.469\n",
      "Ep 1 (Step 000230): Train loss 5.459, Val loss 5.462\n",
      "Ep 1 (Step 000240): Train loss 5.305, Val loss 5.448\n",
      "Ep 1 (Step 000250): Train loss 5.259, Val loss 5.418\n",
      "Ep 1 (Step 000260): Train loss 5.313, Val loss 5.406\n",
      "Ep 1 (Step 000270): Train loss 5.332, Val loss 5.391\n",
      "Ep 1 (Step 000280): Train loss 5.261, Val loss 5.375\n",
      "Ep 1 (Step 000290): Train loss 5.221, Val loss 5.369\n",
      "Ep 1 (Step 000300): Train loss 5.223, Val loss 5.356\n",
      "Ep 1 (Step 000310): Train loss 5.246, Val loss 5.349\n",
      "Ep 1 (Step 000320): Train loss 5.209, Val loss 5.329\n",
      "Ep 1 (Step 000330): Train loss 5.225, Val loss 5.324\n",
      "Ep 1 (Step 000340): Train loss 5.234, Val loss 5.325\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3245\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.589, Val loss 8.545\n",
      "Ep 1 (Step 000010): Train loss 6.896, Val loss 6.826\n",
      "Ep 1 (Step 000020): Train loss 6.453, Val loss 6.468\n",
      "Ep 1 (Step 000030): Train loss 6.382, Val loss 6.420\n",
      "Ep 1 (Step 000040): Train loss 6.256, Val loss 6.243\n",
      "Ep 1 (Step 000050): Train loss 6.160, Val loss 6.181\n",
      "Ep 1 (Step 000060): Train loss 6.089, Val loss 6.061\n",
      "Ep 1 (Step 000070): Train loss 5.932, Val loss 5.973\n",
      "Ep 1 (Step 000080): Train loss 5.874, Val loss 5.890\n",
      "Ep 1 (Step 000090): Train loss 5.796, Val loss 5.819\n",
      "Ep 1 (Step 000100): Train loss 5.757, Val loss 5.773\n",
      "Ep 1 (Step 000110): Train loss 5.668, Val loss 5.751\n",
      "Ep 1 (Step 000120): Train loss 5.624, Val loss 5.712\n",
      "Ep 1 (Step 000130): Train loss 5.619, Val loss 5.653\n",
      "Ep 1 (Step 000140): Train loss 5.556, Val loss 5.625\n",
      "Ep 1 (Step 000150): Train loss 5.481, Val loss 5.613\n",
      "Ep 1 (Step 000160): Train loss 5.465, Val loss 5.565\n",
      "Ep 1 (Step 000170): Train loss 5.478, Val loss 5.529\n",
      "Ep 1 (Step 000180): Train loss 5.399, Val loss 5.511\n",
      "Ep 1 (Step 000190): Train loss 5.389, Val loss 5.489\n",
      "Ep 1 (Step 000200): Train loss 5.339, Val loss 5.486\n",
      "Ep 1 (Step 000210): Train loss 5.477, Val loss 5.461\n",
      "Ep 1 (Step 000220): Train loss 5.290, Val loss 5.437\n",
      "Ep 1 (Step 000230): Train loss 5.269, Val loss 5.434\n",
      "Ep 1 (Step 000240): Train loss 5.246, Val loss 5.409\n",
      "Ep 1 (Step 000250): Train loss 5.269, Val loss 5.408\n",
      "Ep 1 (Step 000260): Train loss 5.298, Val loss 5.397\n",
      "Ep 1 (Step 000270): Train loss 5.256, Val loss 5.390\n",
      "Ep 1 (Step 000280): Train loss 5.285, Val loss 5.386\n",
      "Ep 1 (Step 000290): Train loss 5.332, Val loss 5.395\n",
      "Ep 1 (Step 000300): Train loss 5.337, Val loss 5.361\n",
      "Ep 1 (Step 000310): Train loss 5.178, Val loss 5.333\n",
      "Ep 1 (Step 000320): Train loss 5.231, Val loss 5.318\n",
      "Ep 1 (Step 000330): Train loss 5.205, Val loss 5.296\n",
      "Ep 1 (Step 000340): Train loss 5.187, Val loss 5.299\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2993\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.521, Val loss 8.470\n",
      "Ep 1 (Step 000010): Train loss 6.794, Val loss 6.806\n",
      "Ep 1 (Step 000020): Train loss 6.449, Val loss 6.434\n",
      "Ep 1 (Step 000030): Train loss 6.477, Val loss 6.384\n",
      "Ep 1 (Step 000040): Train loss 6.297, Val loss 6.224\n",
      "Ep 1 (Step 000050): Train loss 6.150, Val loss 6.106\n",
      "Ep 1 (Step 000060): Train loss 5.995, Val loss 6.024\n",
      "Ep 1 (Step 000070): Train loss 5.923, Val loss 5.920\n",
      "Ep 1 (Step 000080): Train loss 5.850, Val loss 5.854\n",
      "Ep 1 (Step 000090): Train loss 5.819, Val loss 5.786\n",
      "Ep 1 (Step 000100): Train loss 5.675, Val loss 5.707\n",
      "Ep 1 (Step 000110): Train loss 5.548, Val loss 5.669\n",
      "Ep 1 (Step 000120): Train loss 5.541, Val loss 5.631\n",
      "Ep 1 (Step 000130): Train loss 5.577, Val loss 5.610\n",
      "Ep 1 (Step 000140): Train loss 5.485, Val loss 5.581\n",
      "Ep 1 (Step 000150): Train loss 5.472, Val loss 5.530\n",
      "Ep 1 (Step 000160): Train loss 5.507, Val loss 5.515\n",
      "Ep 1 (Step 000170): Train loss 5.445, Val loss 5.512\n",
      "Ep 1 (Step 000180): Train loss 5.426, Val loss 5.497\n",
      "Ep 1 (Step 000190): Train loss 5.358, Val loss 5.469\n",
      "Ep 1 (Step 000200): Train loss 5.367, Val loss 5.463\n",
      "Ep 1 (Step 000210): Train loss 5.346, Val loss 5.424\n",
      "Ep 1 (Step 000220): Train loss 5.378, Val loss 5.413\n",
      "Ep 1 (Step 000230): Train loss 5.324, Val loss 5.395\n",
      "Ep 1 (Step 000240): Train loss 5.219, Val loss 5.408\n",
      "Ep 1 (Step 000250): Train loss 5.292, Val loss 5.372\n",
      "Ep 1 (Step 000260): Train loss 5.275, Val loss 5.373\n",
      "Ep 1 (Step 000270): Train loss 5.273, Val loss 5.363\n",
      "Ep 1 (Step 000280): Train loss 5.250, Val loss 5.360\n",
      "Ep 1 (Step 000290): Train loss 5.258, Val loss 5.348\n",
      "Ep 1 (Step 000300): Train loss 5.169, Val loss 5.333\n",
      "Ep 1 (Step 000310): Train loss 5.191, Val loss 5.318\n",
      "Ep 1 (Step 000320): Train loss 5.191, Val loss 5.312\n",
      "Ep 1 (Step 000330): Train loss 5.227, Val loss 5.300\n",
      "Ep 1 (Step 000340): Train loss 5.176, Val loss 5.308\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3082\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.566, Val loss 8.540\n",
      "Ep 1 (Step 000010): Train loss 6.797, Val loss 6.813\n",
      "Ep 1 (Step 000020): Train loss 6.509, Val loss 6.460\n",
      "Ep 1 (Step 000030): Train loss 6.458, Val loss 6.467\n",
      "Ep 1 (Step 000040): Train loss 6.361, Val loss 6.350\n",
      "Ep 1 (Step 000050): Train loss 6.195, Val loss 6.228\n",
      "Ep 1 (Step 000060): Train loss 6.060, Val loss 6.100\n",
      "Ep 1 (Step 000070): Train loss 6.007, Val loss 5.982\n",
      "Ep 1 (Step 000080): Train loss 5.856, Val loss 5.919\n",
      "Ep 1 (Step 000090): Train loss 5.824, Val loss 5.830\n",
      "Ep 1 (Step 000100): Train loss 5.695, Val loss 5.766\n",
      "Ep 1 (Step 000110): Train loss 5.633, Val loss 5.733\n",
      "Ep 1 (Step 000120): Train loss 5.611, Val loss 5.677\n",
      "Ep 1 (Step 000130): Train loss 5.582, Val loss 5.647\n",
      "Ep 1 (Step 000140): Train loss 5.501, Val loss 5.611\n",
      "Ep 1 (Step 000150): Train loss 5.500, Val loss 5.572\n",
      "Ep 1 (Step 000160): Train loss 5.481, Val loss 5.549\n",
      "Ep 1 (Step 000170): Train loss 5.517, Val loss 5.523\n",
      "Ep 1 (Step 000180): Train loss 5.446, Val loss 5.502\n",
      "Ep 1 (Step 000190): Train loss 5.432, Val loss 5.483\n",
      "Ep 1 (Step 000200): Train loss 5.370, Val loss 5.471\n",
      "Ep 1 (Step 000210): Train loss 5.348, Val loss 5.466\n",
      "Ep 1 (Step 000220): Train loss 5.343, Val loss 5.423\n",
      "Ep 1 (Step 000230): Train loss 5.334, Val loss 5.421\n",
      "Ep 1 (Step 000240): Train loss 5.246, Val loss 5.403\n",
      "Ep 1 (Step 000250): Train loss 5.303, Val loss 5.405\n",
      "Ep 1 (Step 000260): Train loss 5.194, Val loss 5.396\n",
      "Ep 1 (Step 000270): Train loss 5.283, Val loss 5.381\n",
      "Ep 1 (Step 000280): Train loss 5.233, Val loss 5.363\n",
      "Ep 1 (Step 000290): Train loss 5.302, Val loss 5.359\n",
      "Ep 1 (Step 000300): Train loss 5.247, Val loss 5.342\n",
      "Ep 1 (Step 000310): Train loss 5.178, Val loss 5.318\n",
      "Ep 1 (Step 000320): Train loss 5.154, Val loss 5.318\n",
      "Ep 1 (Step 000330): Train loss 5.149, Val loss 5.301\n",
      "Ep 1 (Step 000340): Train loss 5.179, Val loss 5.301\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3006\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.614, Val loss 8.589\n",
      "Ep 1 (Step 000010): Train loss 6.864, Val loss 6.837\n",
      "Ep 1 (Step 000020): Train loss 6.460, Val loss 6.450\n",
      "Ep 1 (Step 000030): Train loss 6.353, Val loss 6.424\n",
      "Ep 1 (Step 000040): Train loss 6.277, Val loss 6.308\n",
      "Ep 1 (Step 000050): Train loss 6.133, Val loss 6.138\n",
      "Ep 1 (Step 000060): Train loss 5.982, Val loss 6.038\n",
      "Ep 1 (Step 000070): Train loss 5.914, Val loss 5.943\n",
      "Ep 1 (Step 000080): Train loss 5.832, Val loss 5.845\n",
      "Ep 1 (Step 000090): Train loss 5.747, Val loss 5.795\n",
      "Ep 1 (Step 000100): Train loss 5.709, Val loss 5.751\n",
      "Ep 1 (Step 000110): Train loss 5.578, Val loss 5.692\n",
      "Ep 1 (Step 000120): Train loss 5.516, Val loss 5.631\n",
      "Ep 1 (Step 000130): Train loss 5.510, Val loss 5.590\n",
      "Ep 1 (Step 000140): Train loss 5.505, Val loss 5.571\n",
      "Ep 1 (Step 000150): Train loss 5.433, Val loss 5.542\n",
      "Ep 1 (Step 000160): Train loss 5.443, Val loss 5.517\n",
      "Ep 1 (Step 000170): Train loss 5.354, Val loss 5.476\n",
      "Ep 1 (Step 000180): Train loss 5.373, Val loss 5.464\n",
      "Ep 1 (Step 000190): Train loss 5.338, Val loss 5.451\n",
      "Ep 1 (Step 000200): Train loss 5.369, Val loss 5.428\n",
      "Ep 1 (Step 000210): Train loss 5.419, Val loss 5.439\n",
      "Ep 1 (Step 000220): Train loss 5.349, Val loss 5.376\n",
      "Ep 1 (Step 000230): Train loss 5.239, Val loss 5.392\n",
      "Ep 1 (Step 000240): Train loss 5.234, Val loss 5.373\n",
      "Ep 1 (Step 000250): Train loss 5.208, Val loss 5.345\n",
      "Ep 1 (Step 000260): Train loss 5.169, Val loss 5.343\n",
      "Ep 1 (Step 000270): Train loss 5.258, Val loss 5.315\n",
      "Ep 1 (Step 000280): Train loss 5.190, Val loss 5.302\n",
      "Ep 1 (Step 000290): Train loss 5.169, Val loss 5.303\n",
      "Ep 1 (Step 000300): Train loss 5.171, Val loss 5.299\n",
      "Ep 1 (Step 000310): Train loss 5.210, Val loss 5.281\n",
      "Ep 1 (Step 000320): Train loss 5.123, Val loss 5.264\n",
      "Ep 1 (Step 000330): Train loss 5.129, Val loss 5.254\n",
      "Ep 1 (Step 000340): Train loss 5.114, Val loss 5.263\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2633\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.514, Val loss 8.504\n",
      "Ep 1 (Step 000010): Train loss 6.780, Val loss 6.793\n",
      "Ep 1 (Step 000020): Train loss 6.468, Val loss 6.485\n",
      "Ep 1 (Step 000030): Train loss 6.391, Val loss 6.420\n",
      "Ep 1 (Step 000040): Train loss 6.433, Val loss 6.343\n",
      "Ep 1 (Step 000050): Train loss 6.081, Val loss 6.124\n",
      "Ep 1 (Step 000060): Train loss 5.964, Val loss 6.002\n",
      "Ep 1 (Step 000070): Train loss 5.888, Val loss 5.932\n",
      "Ep 1 (Step 000080): Train loss 5.836, Val loss 5.830\n",
      "Ep 1 (Step 000090): Train loss 5.744, Val loss 5.790\n",
      "Ep 1 (Step 000100): Train loss 5.715, Val loss 5.709\n",
      "Ep 1 (Step 000110): Train loss 5.645, Val loss 5.676\n",
      "Ep 1 (Step 000120): Train loss 5.586, Val loss 5.638\n",
      "Ep 1 (Step 000130): Train loss 5.523, Val loss 5.589\n",
      "Ep 1 (Step 000140): Train loss 5.470, Val loss 5.558\n",
      "Ep 1 (Step 000150): Train loss 5.466, Val loss 5.524\n",
      "Ep 1 (Step 000160): Train loss 5.387, Val loss 5.511\n",
      "Ep 1 (Step 000170): Train loss 5.384, Val loss 5.485\n",
      "Ep 1 (Step 000180): Train loss 5.363, Val loss 5.489\n",
      "Ep 1 (Step 000190): Train loss 5.260, Val loss 5.448\n",
      "Ep 1 (Step 000200): Train loss 5.339, Val loss 5.427\n",
      "Ep 1 (Step 000210): Train loss 5.334, Val loss 5.421\n",
      "Ep 1 (Step 000220): Train loss 5.314, Val loss 5.383\n",
      "Ep 1 (Step 000230): Train loss 5.301, Val loss 5.379\n",
      "Ep 1 (Step 000240): Train loss 5.227, Val loss 5.342\n",
      "Ep 1 (Step 000250): Train loss 5.196, Val loss 5.347\n",
      "Ep 1 (Step 000260): Train loss 5.229, Val loss 5.333\n",
      "Ep 1 (Step 000270): Train loss 5.166, Val loss 5.320\n",
      "Ep 1 (Step 000280): Train loss 5.228, Val loss 5.311\n",
      "Ep 1 (Step 000290): Train loss 5.201, Val loss 5.306\n",
      "Ep 1 (Step 000300): Train loss 5.127, Val loss 5.288\n",
      "Ep 1 (Step 000310): Train loss 5.145, Val loss 5.279\n",
      "Ep 1 (Step 000320): Train loss 5.068, Val loss 5.269\n",
      "Ep 1 (Step 000330): Train loss 5.143, Val loss 5.272\n",
      "Ep 1 (Step 000340): Train loss 5.083, Val loss 5.251\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2514\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.579, Val loss 8.551\n",
      "Ep 1 (Step 000010): Train loss 6.858, Val loss 6.861\n",
      "Ep 1 (Step 000020): Train loss 6.486, Val loss 6.469\n",
      "Ep 1 (Step 000030): Train loss 6.424, Val loss 6.411\n",
      "Ep 1 (Step 000040): Train loss 6.283, Val loss 6.296\n",
      "Ep 1 (Step 000050): Train loss 6.193, Val loss 6.167\n",
      "Ep 1 (Step 000060): Train loss 6.083, Val loss 6.062\n",
      "Ep 1 (Step 000070): Train loss 5.864, Val loss 5.957\n",
      "Ep 1 (Step 000080): Train loss 5.917, Val loss 5.898\n",
      "Ep 1 (Step 000090): Train loss 5.789, Val loss 5.806\n",
      "Ep 1 (Step 000100): Train loss 5.739, Val loss 5.765\n",
      "Ep 1 (Step 000110): Train loss 5.632, Val loss 5.703\n",
      "Ep 1 (Step 000120): Train loss 5.636, Val loss 5.662\n",
      "Ep 1 (Step 000130): Train loss 5.579, Val loss 5.625\n",
      "Ep 1 (Step 000140): Train loss 5.520, Val loss 5.605\n",
      "Ep 1 (Step 000150): Train loss 5.481, Val loss 5.579\n",
      "Ep 1 (Step 000160): Train loss 5.480, Val loss 5.567\n",
      "Ep 1 (Step 000170): Train loss 5.426, Val loss 5.549\n",
      "Ep 1 (Step 000180): Train loss 5.427, Val loss 5.532\n",
      "Ep 1 (Step 000190): Train loss 5.385, Val loss 5.512\n",
      "Ep 1 (Step 000200): Train loss 5.392, Val loss 5.505\n",
      "Ep 1 (Step 000210): Train loss 5.402, Val loss 5.469\n",
      "Ep 1 (Step 000220): Train loss 5.357, Val loss 5.442\n",
      "Ep 1 (Step 000230): Train loss 5.313, Val loss 5.411\n",
      "Ep 1 (Step 000240): Train loss 5.309, Val loss 5.400\n",
      "Ep 1 (Step 000250): Train loss 5.303, Val loss 5.389\n",
      "Ep 1 (Step 000260): Train loss 5.335, Val loss 5.385\n",
      "Ep 1 (Step 000270): Train loss 5.307, Val loss 5.381\n",
      "Ep 1 (Step 000280): Train loss 5.252, Val loss 5.353\n",
      "Ep 1 (Step 000290): Train loss 5.245, Val loss 5.359\n",
      "Ep 1 (Step 000300): Train loss 5.187, Val loss 5.322\n",
      "Ep 1 (Step 000310): Train loss 5.203, Val loss 5.324\n",
      "Ep 1 (Step 000320): Train loss 5.156, Val loss 5.307\n",
      "Ep 1 (Step 000330): Train loss 5.153, Val loss 5.319\n",
      "Ep 1 (Step 000340): Train loss 5.147, Val loss 5.300\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3002\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.795, Val loss 8.773\n",
      "Ep 1 (Step 000010): Train loss 7.423, Val loss 7.371\n",
      "Ep 1 (Step 000020): Train loss 6.789, Val loss 6.741\n",
      "Ep 1 (Step 000030): Train loss 6.467, Val loss 6.452\n",
      "Ep 1 (Step 000040): Train loss 6.328, Val loss 6.362\n",
      "Ep 1 (Step 000050): Train loss 6.291, Val loss 6.285\n",
      "Ep 1 (Step 000060): Train loss 6.162, Val loss 6.157\n",
      "Ep 1 (Step 000070): Train loss 6.073, Val loss 6.074\n",
      "Ep 1 (Step 000080): Train loss 6.039, Val loss 5.983\n",
      "Ep 1 (Step 000090): Train loss 5.890, Val loss 5.929\n",
      "Ep 1 (Step 000100): Train loss 5.860, Val loss 5.884\n",
      "Ep 1 (Step 000110): Train loss 5.753, Val loss 5.821\n",
      "Ep 1 (Step 000120): Train loss 5.716, Val loss 5.793\n",
      "Ep 1 (Step 000130): Train loss 5.724, Val loss 5.734\n",
      "Ep 1 (Step 000140): Train loss 5.653, Val loss 5.701\n",
      "Ep 1 (Step 000150): Train loss 5.599, Val loss 5.668\n",
      "Ep 1 (Step 000160): Train loss 5.586, Val loss 5.640\n",
      "Ep 1 (Step 000170): Train loss 5.491, Val loss 5.618\n",
      "Ep 1 (Step 000180): Train loss 5.543, Val loss 5.592\n",
      "Ep 1 (Step 000190): Train loss 5.433, Val loss 5.570\n",
      "Ep 1 (Step 000200): Train loss 5.428, Val loss 5.556\n",
      "Ep 1 (Step 000210): Train loss 5.475, Val loss 5.519\n",
      "Ep 1 (Step 000220): Train loss 5.408, Val loss 5.505\n",
      "Ep 1 (Step 000230): Train loss 5.425, Val loss 5.486\n",
      "Ep 1 (Step 000240): Train loss 5.477, Val loss 5.478\n",
      "Ep 1 (Step 000250): Train loss 5.327, Val loss 5.449\n",
      "Ep 1 (Step 000260): Train loss 5.310, Val loss 5.440\n",
      "Ep 1 (Step 000270): Train loss 5.353, Val loss 5.411\n",
      "Ep 1 (Step 000280): Train loss 5.341, Val loss 5.411\n",
      "Ep 1 (Step 000290): Train loss 5.280, Val loss 5.400\n",
      "Ep 1 (Step 000300): Train loss 5.262, Val loss 5.387\n",
      "Ep 1 (Step 000310): Train loss 5.221, Val loss 5.379\n",
      "Ep 1 (Step 000320): Train loss 5.220, Val loss 5.368\n",
      "Ep 1 (Step 000330): Train loss 5.226, Val loss 5.362\n",
      "Ep 1 (Step 000340): Train loss 5.193, Val loss 5.346\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3460\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.829, Val loss 8.757\n",
      "Ep 1 (Step 000010): Train loss 7.425, Val loss 7.353\n",
      "Ep 1 (Step 000020): Train loss 6.776, Val loss 6.716\n",
      "Ep 1 (Step 000030): Train loss 6.501, Val loss 6.432\n",
      "Ep 1 (Step 000040): Train loss 6.368, Val loss 6.350\n",
      "Ep 1 (Step 000050): Train loss 6.268, Val loss 6.272\n",
      "Ep 1 (Step 000060): Train loss 6.207, Val loss 6.144\n",
      "Ep 1 (Step 000070): Train loss 6.199, Val loss 6.157\n",
      "Ep 1 (Step 000080): Train loss 5.987, Val loss 5.968\n",
      "Ep 1 (Step 000090): Train loss 5.933, Val loss 5.893\n",
      "Ep 1 (Step 000100): Train loss 5.879, Val loss 5.841\n",
      "Ep 1 (Step 000110): Train loss 5.696, Val loss 5.806\n",
      "Ep 1 (Step 000120): Train loss 5.727, Val loss 5.765\n",
      "Ep 1 (Step 000130): Train loss 5.725, Val loss 5.758\n",
      "Ep 1 (Step 000140): Train loss 5.735, Val loss 5.694\n",
      "Ep 1 (Step 000150): Train loss 5.637, Val loss 5.665\n",
      "Ep 1 (Step 000160): Train loss 5.611, Val loss 5.638\n",
      "Ep 1 (Step 000170): Train loss 5.500, Val loss 5.618\n",
      "Ep 1 (Step 000180): Train loss 5.451, Val loss 5.587\n",
      "Ep 1 (Step 000190): Train loss 5.452, Val loss 5.578\n",
      "Ep 1 (Step 000200): Train loss 5.478, Val loss 5.556\n",
      "Ep 1 (Step 000210): Train loss 5.398, Val loss 5.535\n",
      "Ep 1 (Step 000220): Train loss 5.461, Val loss 5.508\n",
      "Ep 1 (Step 000230): Train loss 5.451, Val loss 5.490\n",
      "Ep 1 (Step 000240): Train loss 5.356, Val loss 5.481\n",
      "Ep 1 (Step 000250): Train loss 5.368, Val loss 5.470\n",
      "Ep 1 (Step 000260): Train loss 5.368, Val loss 5.447\n",
      "Ep 1 (Step 000270): Train loss 5.310, Val loss 5.415\n",
      "Ep 1 (Step 000280): Train loss 5.306, Val loss 5.402\n",
      "Ep 1 (Step 000290): Train loss 5.305, Val loss 5.410\n",
      "Ep 1 (Step 000300): Train loss 5.257, Val loss 5.389\n",
      "Ep 1 (Step 000310): Train loss 5.246, Val loss 5.384\n",
      "Ep 1 (Step 000320): Train loss 5.254, Val loss 5.364\n",
      "Ep 1 (Step 000330): Train loss 5.175, Val loss 5.353\n",
      "Ep 1 (Step 000340): Train loss 5.255, Val loss 5.339\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3390\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.827, Val loss 8.804\n",
      "Ep 1 (Step 000010): Train loss 7.404, Val loss 7.405\n",
      "Ep 1 (Step 000020): Train loss 6.758, Val loss 6.756\n",
      "Ep 1 (Step 000030): Train loss 6.454, Val loss 6.453\n",
      "Ep 1 (Step 000040): Train loss 6.404, Val loss 6.369\n",
      "Ep 1 (Step 000050): Train loss 6.253, Val loss 6.301\n",
      "Ep 1 (Step 000060): Train loss 6.141, Val loss 6.229\n",
      "Ep 1 (Step 000070): Train loss 6.091, Val loss 6.114\n",
      "Ep 1 (Step 000080): Train loss 6.017, Val loss 6.026\n",
      "Ep 1 (Step 000090): Train loss 5.970, Val loss 5.959\n",
      "Ep 1 (Step 000100): Train loss 5.800, Val loss 5.894\n",
      "Ep 1 (Step 000110): Train loss 5.766, Val loss 5.846\n",
      "Ep 1 (Step 000120): Train loss 5.812, Val loss 5.796\n",
      "Ep 1 (Step 000130): Train loss 5.692, Val loss 5.747\n",
      "Ep 1 (Step 000140): Train loss 5.605, Val loss 5.699\n",
      "Ep 1 (Step 000150): Train loss 5.560, Val loss 5.675\n",
      "Ep 1 (Step 000160): Train loss 5.566, Val loss 5.637\n",
      "Ep 1 (Step 000170): Train loss 5.498, Val loss 5.608\n",
      "Ep 1 (Step 000180): Train loss 5.565, Val loss 5.594\n",
      "Ep 1 (Step 000190): Train loss 5.462, Val loss 5.573\n",
      "Ep 1 (Step 000200): Train loss 5.410, Val loss 5.542\n",
      "Ep 1 (Step 000210): Train loss 5.463, Val loss 5.525\n",
      "Ep 1 (Step 000220): Train loss 5.372, Val loss 5.514\n",
      "Ep 1 (Step 000230): Train loss 5.445, Val loss 5.484\n",
      "Ep 1 (Step 000240): Train loss 5.357, Val loss 5.488\n",
      "Ep 1 (Step 000250): Train loss 5.384, Val loss 5.473\n",
      "Ep 1 (Step 000260): Train loss 5.327, Val loss 5.447\n",
      "Ep 1 (Step 000270): Train loss 5.248, Val loss 5.429\n",
      "Ep 1 (Step 000280): Train loss 5.279, Val loss 5.413\n",
      "Ep 1 (Step 000290): Train loss 5.229, Val loss 5.403\n",
      "Ep 1 (Step 000300): Train loss 5.267, Val loss 5.385\n",
      "Ep 1 (Step 000310): Train loss 5.251, Val loss 5.381\n",
      "Ep 1 (Step 000320): Train loss 5.233, Val loss 5.370\n",
      "Ep 1 (Step 000330): Train loss 5.286, Val loss 5.350\n",
      "Ep 1 (Step 000340): Train loss 5.324, Val loss 5.346\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3455\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.820, Val loss 8.813\n",
      "Ep 1 (Step 000010): Train loss 7.423, Val loss 7.384\n",
      "Ep 1 (Step 000020): Train loss 6.740, Val loss 6.757\n",
      "Ep 1 (Step 000030): Train loss 6.485, Val loss 6.461\n",
      "Ep 1 (Step 000040): Train loss 6.370, Val loss 6.366\n",
      "Ep 1 (Step 000050): Train loss 6.271, Val loss 6.296\n",
      "Ep 1 (Step 000060): Train loss 6.195, Val loss 6.179\n",
      "Ep 1 (Step 000070): Train loss 6.082, Val loss 6.092\n",
      "Ep 1 (Step 000080): Train loss 5.955, Val loss 6.010\n",
      "Ep 1 (Step 000090): Train loss 5.912, Val loss 5.928\n",
      "Ep 1 (Step 000100): Train loss 5.850, Val loss 5.870\n",
      "Ep 1 (Step 000110): Train loss 5.797, Val loss 5.825\n",
      "Ep 1 (Step 000120): Train loss 5.716, Val loss 5.778\n",
      "Ep 1 (Step 000130): Train loss 5.596, Val loss 5.719\n",
      "Ep 1 (Step 000140): Train loss 5.682, Val loss 5.691\n",
      "Ep 1 (Step 000150): Train loss 5.573, Val loss 5.665\n",
      "Ep 1 (Step 000160): Train loss 5.594, Val loss 5.629\n",
      "Ep 1 (Step 000170): Train loss 5.652, Val loss 5.604\n",
      "Ep 1 (Step 000180): Train loss 5.492, Val loss 5.588\n",
      "Ep 1 (Step 000190): Train loss 5.535, Val loss 5.559\n",
      "Ep 1 (Step 000200): Train loss 5.482, Val loss 5.543\n",
      "Ep 1 (Step 000210): Train loss 5.454, Val loss 5.519\n",
      "Ep 1 (Step 000220): Train loss 5.364, Val loss 5.500\n",
      "Ep 1 (Step 000230): Train loss 5.480, Val loss 5.510\n",
      "Ep 1 (Step 000240): Train loss 5.387, Val loss 5.485\n",
      "Ep 1 (Step 000250): Train loss 5.395, Val loss 5.464\n",
      "Ep 1 (Step 000260): Train loss 5.328, Val loss 5.441\n",
      "Ep 1 (Step 000270): Train loss 5.290, Val loss 5.417\n",
      "Ep 1 (Step 000280): Train loss 5.310, Val loss 5.408\n",
      "Ep 1 (Step 000290): Train loss 5.253, Val loss 5.384\n",
      "Ep 1 (Step 000300): Train loss 5.289, Val loss 5.381\n",
      "Ep 1 (Step 000310): Train loss 5.280, Val loss 5.373\n",
      "Ep 1 (Step 000320): Train loss 5.197, Val loss 5.351\n",
      "Ep 1 (Step 000330): Train loss 5.282, Val loss 5.340\n",
      "Ep 1 (Step 000340): Train loss 5.296, Val loss 5.324\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3244\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.746, Val loss 8.721\n",
      "Ep 1 (Step 000010): Train loss 7.397, Val loss 7.341\n",
      "Ep 1 (Step 000020): Train loss 6.742, Val loss 6.728\n",
      "Ep 1 (Step 000030): Train loss 6.436, Val loss 6.461\n",
      "Ep 1 (Step 000040): Train loss 6.338, Val loss 6.374\n",
      "Ep 1 (Step 000050): Train loss 6.331, Val loss 6.299\n",
      "Ep 1 (Step 000060): Train loss 6.221, Val loss 6.212\n",
      "Ep 1 (Step 000070): Train loss 6.109, Val loss 6.106\n",
      "Ep 1 (Step 000080): Train loss 5.975, Val loss 6.051\n",
      "Ep 1 (Step 000090): Train loss 5.913, Val loss 5.951\n",
      "Ep 1 (Step 000100): Train loss 5.897, Val loss 5.901\n",
      "Ep 1 (Step 000110): Train loss 5.803, Val loss 5.849\n",
      "Ep 1 (Step 000120): Train loss 5.767, Val loss 5.795\n",
      "Ep 1 (Step 000130): Train loss 5.721, Val loss 5.759\n",
      "Ep 1 (Step 000140): Train loss 5.690, Val loss 5.713\n",
      "Ep 1 (Step 000150): Train loss 5.612, Val loss 5.675\n",
      "Ep 1 (Step 000160): Train loss 5.599, Val loss 5.631\n",
      "Ep 1 (Step 000170): Train loss 5.614, Val loss 5.605\n",
      "Ep 1 (Step 000180): Train loss 5.547, Val loss 5.577\n",
      "Ep 1 (Step 000190): Train loss 5.417, Val loss 5.553\n",
      "Ep 1 (Step 000200): Train loss 5.518, Val loss 5.540\n",
      "Ep 1 (Step 000210): Train loss 5.431, Val loss 5.521\n",
      "Ep 1 (Step 000220): Train loss 5.390, Val loss 5.504\n",
      "Ep 1 (Step 000230): Train loss 5.392, Val loss 5.483\n",
      "Ep 1 (Step 000240): Train loss 5.330, Val loss 5.465\n",
      "Ep 1 (Step 000250): Train loss 5.354, Val loss 5.453\n",
      "Ep 1 (Step 000260): Train loss 5.376, Val loss 5.422\n",
      "Ep 1 (Step 000270): Train loss 5.362, Val loss 5.420\n",
      "Ep 1 (Step 000280): Train loss 5.362, Val loss 5.407\n",
      "Ep 1 (Step 000290): Train loss 5.344, Val loss 5.397\n",
      "Ep 1 (Step 000300): Train loss 5.281, Val loss 5.380\n",
      "Ep 1 (Step 000310): Train loss 5.276, Val loss 5.362\n",
      "Ep 1 (Step 000320): Train loss 5.196, Val loss 5.368\n",
      "Ep 1 (Step 000330): Train loss 5.201, Val loss 5.358\n",
      "Ep 1 (Step 000340): Train loss 5.205, Val loss 5.343\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3426\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.799, Val loss 8.778\n",
      "Ep 1 (Step 000010): Train loss 7.441, Val loss 7.422\n",
      "Ep 1 (Step 000020): Train loss 6.779, Val loss 6.781\n",
      "Ep 1 (Step 000030): Train loss 6.498, Val loss 6.464\n",
      "Ep 1 (Step 000040): Train loss 6.351, Val loss 6.387\n",
      "Ep 1 (Step 000050): Train loss 6.276, Val loss 6.299\n",
      "Ep 1 (Step 000060): Train loss 6.141, Val loss 6.185\n",
      "Ep 1 (Step 000070): Train loss 6.088, Val loss 6.087\n",
      "Ep 1 (Step 000080): Train loss 5.966, Val loss 6.022\n",
      "Ep 1 (Step 000090): Train loss 5.916, Val loss 5.931\n",
      "Ep 1 (Step 000100): Train loss 5.892, Val loss 5.868\n",
      "Ep 1 (Step 000110): Train loss 5.804, Val loss 5.808\n",
      "Ep 1 (Step 000120): Train loss 5.727, Val loss 5.765\n",
      "Ep 1 (Step 000130): Train loss 5.666, Val loss 5.732\n",
      "Ep 1 (Step 000140): Train loss 5.660, Val loss 5.685\n",
      "Ep 1 (Step 000150): Train loss 5.665, Val loss 5.661\n",
      "Ep 1 (Step 000160): Train loss 5.612, Val loss 5.641\n",
      "Ep 1 (Step 000170): Train loss 5.556, Val loss 5.619\n",
      "Ep 1 (Step 000180): Train loss 5.538, Val loss 5.581\n",
      "Ep 1 (Step 000190): Train loss 5.464, Val loss 5.569\n",
      "Ep 1 (Step 000200): Train loss 5.441, Val loss 5.527\n",
      "Ep 1 (Step 000210): Train loss 5.409, Val loss 5.512\n",
      "Ep 1 (Step 000220): Train loss 5.413, Val loss 5.489\n",
      "Ep 1 (Step 000230): Train loss 5.344, Val loss 5.468\n",
      "Ep 1 (Step 000240): Train loss 5.462, Val loss 5.460\n",
      "Ep 1 (Step 000250): Train loss 5.398, Val loss 5.454\n",
      "Ep 1 (Step 000260): Train loss 5.317, Val loss 5.451\n",
      "Ep 1 (Step 000270): Train loss 5.339, Val loss 5.431\n",
      "Ep 1 (Step 000280): Train loss 5.365, Val loss 5.412\n",
      "Ep 1 (Step 000290): Train loss 5.273, Val loss 5.393\n",
      "Ep 1 (Step 000300): Train loss 5.237, Val loss 5.370\n",
      "Ep 1 (Step 000310): Train loss 5.215, Val loss 5.366\n",
      "Ep 1 (Step 000320): Train loss 5.228, Val loss 5.352\n",
      "Ep 1 (Step 000330): Train loss 5.255, Val loss 5.353\n",
      "Ep 1 (Step 000340): Train loss 5.225, Val loss 5.331\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3307\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.463, Val loss 8.466\n",
      "Ep 1 (Step 000010): Train loss 6.825, Val loss 6.775\n",
      "Ep 1 (Step 000020): Train loss 6.458, Val loss 6.437\n",
      "Ep 1 (Step 000030): Train loss 6.418, Val loss 6.412\n",
      "Ep 1 (Step 000040): Train loss 6.314, Val loss 6.269\n",
      "Ep 1 (Step 000050): Train loss 6.175, Val loss 6.187\n",
      "Ep 1 (Step 000060): Train loss 6.065, Val loss 6.033\n",
      "Ep 1 (Step 000070): Train loss 5.823, Val loss 5.946\n",
      "Ep 1 (Step 000080): Train loss 5.896, Val loss 5.893\n",
      "Ep 1 (Step 000090): Train loss 5.867, Val loss 5.807\n",
      "Ep 1 (Step 000100): Train loss 5.745, Val loss 5.774\n",
      "Ep 1 (Step 000110): Train loss 5.606, Val loss 5.715\n",
      "Ep 1 (Step 000120): Train loss 5.549, Val loss 5.690\n",
      "Ep 1 (Step 000130): Train loss 5.602, Val loss 5.627\n",
      "Ep 1 (Step 000140): Train loss 5.566, Val loss 5.613\n",
      "Ep 1 (Step 000150): Train loss 5.613, Val loss 5.589\n",
      "Ep 1 (Step 000160): Train loss 5.504, Val loss 5.584\n",
      "Ep 1 (Step 000170): Train loss 5.498, Val loss 5.546\n",
      "Ep 1 (Step 000180): Train loss 5.451, Val loss 5.512\n",
      "Ep 1 (Step 000190): Train loss 5.369, Val loss 5.517\n",
      "Ep 1 (Step 000200): Train loss 5.394, Val loss 5.468\n",
      "Ep 1 (Step 000210): Train loss 5.322, Val loss 5.464\n",
      "Ep 1 (Step 000220): Train loss 5.355, Val loss 5.438\n",
      "Ep 1 (Step 000230): Train loss 5.287, Val loss 5.424\n",
      "Ep 1 (Step 000240): Train loss 5.352, Val loss 5.402\n",
      "Ep 1 (Step 000250): Train loss 5.199, Val loss 5.396\n",
      "Ep 1 (Step 000260): Train loss 5.281, Val loss 5.387\n",
      "Ep 1 (Step 000270): Train loss 5.268, Val loss 5.383\n",
      "Ep 1 (Step 000280): Train loss 5.302, Val loss 5.353\n",
      "Ep 1 (Step 000290): Train loss 5.215, Val loss 5.345\n",
      "Ep 1 (Step 000300): Train loss 5.139, Val loss 5.342\n",
      "Ep 1 (Step 000310): Train loss 5.205, Val loss 5.334\n",
      "Ep 1 (Step 000320): Train loss 5.233, Val loss 5.330\n",
      "Ep 1 (Step 000330): Train loss 5.135, Val loss 5.295\n",
      "Ep 1 (Step 000340): Train loss 5.189, Val loss 5.291\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2915\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.534, Val loss 8.490\n",
      "Ep 1 (Step 000010): Train loss 6.843, Val loss 6.844\n",
      "Ep 1 (Step 000020): Train loss 6.539, Val loss 6.460\n",
      "Ep 1 (Step 000030): Train loss 6.461, Val loss 6.427\n",
      "Ep 1 (Step 000040): Train loss 6.244, Val loss 6.281\n",
      "Ep 1 (Step 000050): Train loss 6.201, Val loss 6.140\n",
      "Ep 1 (Step 000060): Train loss 6.023, Val loss 6.034\n",
      "Ep 1 (Step 000070): Train loss 6.005, Val loss 5.952\n",
      "Ep 1 (Step 000080): Train loss 5.832, Val loss 5.889\n",
      "Ep 1 (Step 000090): Train loss 5.767, Val loss 5.814\n",
      "Ep 1 (Step 000100): Train loss 5.620, Val loss 5.756\n",
      "Ep 1 (Step 000110): Train loss 5.676, Val loss 5.711\n",
      "Ep 1 (Step 000120): Train loss 5.623, Val loss 5.666\n",
      "Ep 1 (Step 000130): Train loss 5.585, Val loss 5.648\n",
      "Ep 1 (Step 000140): Train loss 5.563, Val loss 5.629\n",
      "Ep 1 (Step 000150): Train loss 5.534, Val loss 5.582\n",
      "Ep 1 (Step 000160): Train loss 5.386, Val loss 5.571\n",
      "Ep 1 (Step 000170): Train loss 5.349, Val loss 5.537\n",
      "Ep 1 (Step 000180): Train loss 5.368, Val loss 5.500\n",
      "Ep 1 (Step 000190): Train loss 5.382, Val loss 5.490\n",
      "Ep 1 (Step 000200): Train loss 5.408, Val loss 5.479\n",
      "Ep 1 (Step 000210): Train loss 5.334, Val loss 5.445\n",
      "Ep 1 (Step 000220): Train loss 5.421, Val loss 5.433\n",
      "Ep 1 (Step 000230): Train loss 5.427, Val loss 5.407\n",
      "Ep 1 (Step 000240): Train loss 5.346, Val loss 5.400\n",
      "Ep 1 (Step 000250): Train loss 5.281, Val loss 5.387\n",
      "Ep 1 (Step 000260): Train loss 5.241, Val loss 5.381\n",
      "Ep 1 (Step 000270): Train loss 5.301, Val loss 5.344\n",
      "Ep 1 (Step 000280): Train loss 5.248, Val loss 5.344\n",
      "Ep 1 (Step 000290): Train loss 5.212, Val loss 5.343\n",
      "Ep 1 (Step 000300): Train loss 5.281, Val loss 5.334\n",
      "Ep 1 (Step 000310): Train loss 5.144, Val loss 5.314\n",
      "Ep 1 (Step 000320): Train loss 5.187, Val loss 5.301\n",
      "Ep 1 (Step 000330): Train loss 5.138, Val loss 5.287\n",
      "Ep 1 (Step 000340): Train loss 5.163, Val loss 5.278\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2780\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.570, Val loss 8.542\n",
      "Ep 1 (Step 000010): Train loss 6.877, Val loss 6.822\n",
      "Ep 1 (Step 000020): Train loss 6.460, Val loss 6.454\n",
      "Ep 1 (Step 000030): Train loss 6.418, Val loss 6.370\n",
      "Ep 1 (Step 000040): Train loss 6.304, Val loss 6.248\n",
      "Ep 1 (Step 000050): Train loss 6.107, Val loss 6.108\n",
      "Ep 1 (Step 000060): Train loss 5.964, Val loss 6.012\n",
      "Ep 1 (Step 000070): Train loss 5.938, Val loss 5.907\n",
      "Ep 1 (Step 000080): Train loss 5.846, Val loss 5.834\n",
      "Ep 1 (Step 000090): Train loss 5.781, Val loss 5.765\n",
      "Ep 1 (Step 000100): Train loss 5.665, Val loss 5.742\n",
      "Ep 1 (Step 000110): Train loss 5.580, Val loss 5.680\n",
      "Ep 1 (Step 000120): Train loss 5.565, Val loss 5.643\n",
      "Ep 1 (Step 000130): Train loss 5.561, Val loss 5.603\n",
      "Ep 1 (Step 000140): Train loss 5.472, Val loss 5.576\n",
      "Ep 1 (Step 000150): Train loss 5.515, Val loss 5.554\n",
      "Ep 1 (Step 000160): Train loss 5.483, Val loss 5.533\n",
      "Ep 1 (Step 000170): Train loss 5.378, Val loss 5.497\n",
      "Ep 1 (Step 000180): Train loss 5.463, Val loss 5.477\n",
      "Ep 1 (Step 000190): Train loss 5.410, Val loss 5.438\n",
      "Ep 1 (Step 000200): Train loss 5.403, Val loss 5.449\n",
      "Ep 1 (Step 000210): Train loss 5.306, Val loss 5.433\n",
      "Ep 1 (Step 000220): Train loss 5.332, Val loss 5.420\n",
      "Ep 1 (Step 000230): Train loss 5.356, Val loss 5.410\n",
      "Ep 1 (Step 000240): Train loss 5.335, Val loss 5.379\n",
      "Ep 1 (Step 000250): Train loss 5.229, Val loss 5.387\n",
      "Ep 1 (Step 000260): Train loss 5.290, Val loss 5.370\n",
      "Ep 1 (Step 000270): Train loss 5.222, Val loss 5.353\n",
      "Ep 1 (Step 000280): Train loss 5.213, Val loss 5.356\n",
      "Ep 1 (Step 000290): Train loss 5.204, Val loss 5.322\n",
      "Ep 1 (Step 000300): Train loss 5.288, Val loss 5.321\n",
      "Ep 1 (Step 000310): Train loss 5.234, Val loss 5.329\n",
      "Ep 1 (Step 000320): Train loss 5.211, Val loss 5.312\n",
      "Ep 1 (Step 000330): Train loss 5.113, Val loss 5.296\n",
      "Ep 1 (Step 000340): Train loss 5.178, Val loss 5.304\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3039\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.539, Val loss 8.515\n",
      "Ep 1 (Step 000010): Train loss 6.867, Val loss 6.828\n",
      "Ep 1 (Step 000020): Train loss 6.493, Val loss 6.444\n",
      "Ep 1 (Step 000030): Train loss 6.413, Val loss 6.365\n",
      "Ep 1 (Step 000040): Train loss 6.136, Val loss 6.222\n",
      "Ep 1 (Step 000050): Train loss 6.100, Val loss 6.134\n",
      "Ep 1 (Step 000060): Train loss 5.940, Val loss 6.036\n",
      "Ep 1 (Step 000070): Train loss 6.000, Val loss 5.953\n",
      "Ep 1 (Step 000080): Train loss 5.774, Val loss 5.852\n",
      "Ep 1 (Step 000090): Train loss 5.767, Val loss 5.803\n",
      "Ep 1 (Step 000100): Train loss 5.742, Val loss 5.721\n",
      "Ep 1 (Step 000110): Train loss 5.600, Val loss 5.678\n",
      "Ep 1 (Step 000120): Train loss 5.577, Val loss 5.629\n",
      "Ep 1 (Step 000130): Train loss 5.594, Val loss 5.617\n",
      "Ep 1 (Step 000140): Train loss 5.524, Val loss 5.587\n",
      "Ep 1 (Step 000150): Train loss 5.498, Val loss 5.557\n",
      "Ep 1 (Step 000160): Train loss 5.451, Val loss 5.509\n",
      "Ep 1 (Step 000170): Train loss 5.312, Val loss 5.480\n",
      "Ep 1 (Step 000180): Train loss 5.367, Val loss 5.456\n",
      "Ep 1 (Step 000190): Train loss 5.331, Val loss 5.444\n",
      "Ep 1 (Step 000200): Train loss 5.320, Val loss 5.427\n",
      "Ep 1 (Step 000210): Train loss 5.343, Val loss 5.420\n",
      "Ep 1 (Step 000220): Train loss 5.218, Val loss 5.389\n",
      "Ep 1 (Step 000230): Train loss 5.313, Val loss 5.385\n",
      "Ep 1 (Step 000240): Train loss 5.250, Val loss 5.378\n",
      "Ep 1 (Step 000250): Train loss 5.210, Val loss 5.350\n",
      "Ep 1 (Step 000260): Train loss 5.213, Val loss 5.331\n",
      "Ep 1 (Step 000270): Train loss 5.224, Val loss 5.329\n",
      "Ep 1 (Step 000280): Train loss 5.271, Val loss 5.321\n",
      "Ep 1 (Step 000290): Train loss 5.213, Val loss 5.301\n",
      "Ep 1 (Step 000300): Train loss 5.106, Val loss 5.309\n",
      "Ep 1 (Step 000310): Train loss 5.187, Val loss 5.293\n",
      "Ep 1 (Step 000320): Train loss 5.124, Val loss 5.275\n",
      "Ep 1 (Step 000330): Train loss 5.159, Val loss 5.269\n",
      "Ep 1 (Step 000340): Train loss 5.077, Val loss 5.260\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2601\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.514, Val loss 8.499\n",
      "Ep 1 (Step 000010): Train loss 6.864, Val loss 6.828\n",
      "Ep 1 (Step 000020): Train loss 6.467, Val loss 6.453\n",
      "Ep 1 (Step 000030): Train loss 6.486, Val loss 6.437\n",
      "Ep 1 (Step 000040): Train loss 6.266, Val loss 6.296\n",
      "Ep 1 (Step 000050): Train loss 6.154, Val loss 6.211\n",
      "Ep 1 (Step 000060): Train loss 6.108, Val loss 6.099\n",
      "Ep 1 (Step 000070): Train loss 5.960, Val loss 5.998\n",
      "Ep 1 (Step 000080): Train loss 5.848, Val loss 5.928\n",
      "Ep 1 (Step 000090): Train loss 5.854, Val loss 5.843\n",
      "Ep 1 (Step 000100): Train loss 5.735, Val loss 5.787\n",
      "Ep 1 (Step 000110): Train loss 5.722, Val loss 5.728\n",
      "Ep 1 (Step 000120): Train loss 5.625, Val loss 5.703\n",
      "Ep 1 (Step 000130): Train loss 5.580, Val loss 5.657\n",
      "Ep 1 (Step 000140): Train loss 5.520, Val loss 5.604\n",
      "Ep 1 (Step 000150): Train loss 5.532, Val loss 5.576\n",
      "Ep 1 (Step 000160): Train loss 5.457, Val loss 5.557\n",
      "Ep 1 (Step 000170): Train loss 5.416, Val loss 5.529\n",
      "Ep 1 (Step 000180): Train loss 5.386, Val loss 5.516\n",
      "Ep 1 (Step 000190): Train loss 5.447, Val loss 5.495\n",
      "Ep 1 (Step 000200): Train loss 5.365, Val loss 5.473\n",
      "Ep 1 (Step 000210): Train loss 5.310, Val loss 5.457\n",
      "Ep 1 (Step 000220): Train loss 5.414, Val loss 5.447\n",
      "Ep 1 (Step 000230): Train loss 5.317, Val loss 5.420\n",
      "Ep 1 (Step 000240): Train loss 5.212, Val loss 5.399\n",
      "Ep 1 (Step 000250): Train loss 5.285, Val loss 5.381\n",
      "Ep 1 (Step 000260): Train loss 5.254, Val loss 5.357\n",
      "Ep 1 (Step 000270): Train loss 5.268, Val loss 5.358\n",
      "Ep 1 (Step 000280): Train loss 5.207, Val loss 5.359\n",
      "Ep 1 (Step 000290): Train loss 5.305, Val loss 5.334\n",
      "Ep 1 (Step 000300): Train loss 5.248, Val loss 5.345\n",
      "Ep 1 (Step 000310): Train loss 5.183, Val loss 5.317\n",
      "Ep 1 (Step 000320): Train loss 5.079, Val loss 5.295\n",
      "Ep 1 (Step 000330): Train loss 5.214, Val loss 5.308\n",
      "Ep 1 (Step 000340): Train loss 5.216, Val loss 5.302\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3019\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.480, Val loss 8.513\n",
      "Ep 1 (Step 000010): Train loss 6.821, Val loss 6.849\n",
      "Ep 1 (Step 000020): Train loss 6.447, Val loss 6.469\n",
      "Ep 1 (Step 000030): Train loss 6.394, Val loss 6.417\n",
      "Ep 1 (Step 000040): Train loss 6.205, Val loss 6.284\n",
      "Ep 1 (Step 000050): Train loss 6.198, Val loss 6.145\n",
      "Ep 1 (Step 000060): Train loss 5.975, Val loss 6.034\n",
      "Ep 1 (Step 000070): Train loss 5.897, Val loss 5.970\n",
      "Ep 1 (Step 000080): Train loss 5.858, Val loss 5.874\n",
      "Ep 1 (Step 000090): Train loss 5.769, Val loss 5.802\n",
      "Ep 1 (Step 000100): Train loss 5.699, Val loss 5.728\n",
      "Ep 1 (Step 000110): Train loss 5.622, Val loss 5.678\n",
      "Ep 1 (Step 000120): Train loss 5.641, Val loss 5.643\n",
      "Ep 1 (Step 000130): Train loss 5.542, Val loss 5.610\n",
      "Ep 1 (Step 000140): Train loss 5.483, Val loss 5.574\n",
      "Ep 1 (Step 000150): Train loss 5.537, Val loss 5.537\n",
      "Ep 1 (Step 000160): Train loss 5.500, Val loss 5.508\n",
      "Ep 1 (Step 000170): Train loss 5.457, Val loss 5.493\n",
      "Ep 1 (Step 000180): Train loss 5.477, Val loss 5.473\n",
      "Ep 1 (Step 000190): Train loss 5.378, Val loss 5.456\n",
      "Ep 1 (Step 000200): Train loss 5.414, Val loss 5.439\n",
      "Ep 1 (Step 000210): Train loss 5.322, Val loss 5.414\n",
      "Ep 1 (Step 000220): Train loss 5.328, Val loss 5.418\n",
      "Ep 1 (Step 000230): Train loss 5.324, Val loss 5.387\n",
      "Ep 1 (Step 000240): Train loss 5.250, Val loss 5.376\n",
      "Ep 1 (Step 000250): Train loss 5.361, Val loss 5.398\n",
      "Ep 1 (Step 000260): Train loss 5.247, Val loss 5.345\n",
      "Ep 1 (Step 000270): Train loss 5.221, Val loss 5.342\n",
      "Ep 1 (Step 000280): Train loss 5.166, Val loss 5.330\n",
      "Ep 1 (Step 000290): Train loss 5.205, Val loss 5.324\n",
      "Ep 1 (Step 000300): Train loss 5.110, Val loss 5.292\n",
      "Ep 1 (Step 000310): Train loss 5.177, Val loss 5.282\n",
      "Ep 1 (Step 000320): Train loss 5.194, Val loss 5.278\n",
      "Ep 1 (Step 000330): Train loss 5.182, Val loss 5.275\n",
      "Ep 1 (Step 000340): Train loss 5.086, Val loss 5.245\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2453\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.892, Val loss 8.859\n",
      "Ep 1 (Step 000010): Train loss 7.467, Val loss 7.452\n",
      "Ep 1 (Step 000020): Train loss 6.814, Val loss 6.791\n",
      "Ep 1 (Step 000030): Train loss 6.469, Val loss 6.453\n",
      "Ep 1 (Step 000040): Train loss 6.315, Val loss 6.350\n",
      "Ep 1 (Step 000050): Train loss 6.294, Val loss 6.263\n",
      "Ep 1 (Step 000060): Train loss 6.080, Val loss 6.129\n",
      "Ep 1 (Step 000070): Train loss 5.994, Val loss 6.047\n",
      "Ep 1 (Step 000080): Train loss 5.880, Val loss 5.961\n",
      "Ep 1 (Step 000090): Train loss 5.827, Val loss 5.887\n",
      "Ep 1 (Step 000100): Train loss 5.828, Val loss 5.828\n",
      "Ep 1 (Step 000110): Train loss 5.753, Val loss 5.767\n",
      "Ep 1 (Step 000120): Train loss 5.721, Val loss 5.754\n",
      "Ep 1 (Step 000130): Train loss 5.568, Val loss 5.697\n",
      "Ep 1 (Step 000140): Train loss 5.567, Val loss 5.643\n",
      "Ep 1 (Step 000150): Train loss 5.493, Val loss 5.609\n",
      "Ep 1 (Step 000160): Train loss 5.506, Val loss 5.575\n",
      "Ep 1 (Step 000170): Train loss 5.495, Val loss 5.568\n",
      "Ep 1 (Step 000180): Train loss 5.462, Val loss 5.540\n",
      "Ep 1 (Step 000190): Train loss 5.433, Val loss 5.505\n",
      "Ep 1 (Step 000200): Train loss 5.461, Val loss 5.474\n",
      "Ep 1 (Step 000210): Train loss 5.319, Val loss 5.467\n",
      "Ep 1 (Step 000220): Train loss 5.424, Val loss 5.449\n",
      "Ep 1 (Step 000230): Train loss 5.300, Val loss 5.434\n",
      "Ep 1 (Step 000240): Train loss 5.278, Val loss 5.420\n",
      "Ep 1 (Step 000250): Train loss 5.287, Val loss 5.408\n",
      "Ep 1 (Step 000260): Train loss 5.345, Val loss 5.394\n",
      "Ep 1 (Step 000270): Train loss 5.303, Val loss 5.369\n",
      "Ep 1 (Step 000280): Train loss 5.226, Val loss 5.360\n",
      "Ep 1 (Step 000290): Train loss 5.184, Val loss 5.360\n",
      "Ep 1 (Step 000300): Train loss 5.281, Val loss 5.346\n",
      "Ep 1 (Step 000310): Train loss 5.277, Val loss 5.335\n",
      "Ep 1 (Step 000320): Train loss 5.199, Val loss 5.326\n",
      "Ep 1 (Step 000330): Train loss 5.145, Val loss 5.308\n",
      "Ep 1 (Step 000340): Train loss 5.199, Val loss 5.295\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2954\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.741, Val loss 8.710\n",
      "Ep 1 (Step 000010): Train loss 7.401, Val loss 7.334\n",
      "Ep 1 (Step 000020): Train loss 6.768, Val loss 6.698\n",
      "Ep 1 (Step 000030): Train loss 6.462, Val loss 6.415\n",
      "Ep 1 (Step 000040): Train loss 6.367, Val loss 6.315\n",
      "Ep 1 (Step 000050): Train loss 6.174, Val loss 6.222\n",
      "Ep 1 (Step 000060): Train loss 6.006, Val loss 6.091\n",
      "Ep 1 (Step 000070): Train loss 5.914, Val loss 6.024\n",
      "Ep 1 (Step 000080): Train loss 5.983, Val loss 5.945\n",
      "Ep 1 (Step 000090): Train loss 5.873, Val loss 5.894\n",
      "Ep 1 (Step 000100): Train loss 5.834, Val loss 5.828\n",
      "Ep 1 (Step 000110): Train loss 5.705, Val loss 5.776\n",
      "Ep 1 (Step 000120): Train loss 5.602, Val loss 5.720\n",
      "Ep 1 (Step 000130): Train loss 5.688, Val loss 5.682\n",
      "Ep 1 (Step 000140): Train loss 5.589, Val loss 5.652\n",
      "Ep 1 (Step 000150): Train loss 5.543, Val loss 5.626\n",
      "Ep 1 (Step 000160): Train loss 5.499, Val loss 5.585\n",
      "Ep 1 (Step 000170): Train loss 5.483, Val loss 5.569\n",
      "Ep 1 (Step 000180): Train loss 5.444, Val loss 5.539\n",
      "Ep 1 (Step 000190): Train loss 5.471, Val loss 5.529\n",
      "Ep 1 (Step 000200): Train loss 5.440, Val loss 5.495\n",
      "Ep 1 (Step 000210): Train loss 5.418, Val loss 5.474\n",
      "Ep 1 (Step 000220): Train loss 5.453, Val loss 5.447\n",
      "Ep 1 (Step 000230): Train loss 5.396, Val loss 5.448\n",
      "Ep 1 (Step 000240): Train loss 5.276, Val loss 5.444\n",
      "Ep 1 (Step 000250): Train loss 5.272, Val loss 5.405\n",
      "Ep 1 (Step 000260): Train loss 5.312, Val loss 5.396\n",
      "Ep 1 (Step 000270): Train loss 5.230, Val loss 5.392\n",
      "Ep 1 (Step 000280): Train loss 5.215, Val loss 5.354\n",
      "Ep 1 (Step 000290): Train loss 5.306, Val loss 5.357\n",
      "Ep 1 (Step 000300): Train loss 5.226, Val loss 5.354\n",
      "Ep 1 (Step 000310): Train loss 5.279, Val loss 5.339\n",
      "Ep 1 (Step 000320): Train loss 5.216, Val loss 5.344\n",
      "Ep 1 (Step 000330): Train loss 5.243, Val loss 5.329\n",
      "Ep 1 (Step 000340): Train loss 5.192, Val loss 5.307\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3074\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.758, Val loss 8.760\n",
      "Ep 1 (Step 000010): Train loss 7.406, Val loss 7.406\n",
      "Ep 1 (Step 000020): Train loss 6.760, Val loss 6.734\n",
      "Ep 1 (Step 000030): Train loss 6.438, Val loss 6.414\n",
      "Ep 1 (Step 000040): Train loss 6.335, Val loss 6.309\n",
      "Ep 1 (Step 000050): Train loss 6.185, Val loss 6.216\n",
      "Ep 1 (Step 000060): Train loss 6.049, Val loss 6.095\n",
      "Ep 1 (Step 000070): Train loss 5.967, Val loss 5.998\n",
      "Ep 1 (Step 000080): Train loss 5.954, Val loss 5.935\n",
      "Ep 1 (Step 000090): Train loss 5.827, Val loss 5.868\n",
      "Ep 1 (Step 000100): Train loss 5.775, Val loss 5.835\n",
      "Ep 1 (Step 000110): Train loss 5.753, Val loss 5.775\n",
      "Ep 1 (Step 000120): Train loss 5.657, Val loss 5.724\n",
      "Ep 1 (Step 000130): Train loss 5.615, Val loss 5.687\n",
      "Ep 1 (Step 000140): Train loss 5.570, Val loss 5.637\n",
      "Ep 1 (Step 000150): Train loss 5.558, Val loss 5.603\n",
      "Ep 1 (Step 000160): Train loss 5.609, Val loss 5.567\n",
      "Ep 1 (Step 000170): Train loss 5.432, Val loss 5.533\n",
      "Ep 1 (Step 000180): Train loss 5.424, Val loss 5.515\n",
      "Ep 1 (Step 000190): Train loss 5.408, Val loss 5.494\n",
      "Ep 1 (Step 000200): Train loss 5.402, Val loss 5.484\n",
      "Ep 1 (Step 000210): Train loss 5.381, Val loss 5.480\n",
      "Ep 1 (Step 000220): Train loss 5.353, Val loss 5.454\n",
      "Ep 1 (Step 000230): Train loss 5.393, Val loss 5.444\n",
      "Ep 1 (Step 000240): Train loss 5.392, Val loss 5.423\n",
      "Ep 1 (Step 000250): Train loss 5.268, Val loss 5.400\n",
      "Ep 1 (Step 000260): Train loss 5.253, Val loss 5.386\n",
      "Ep 1 (Step 000270): Train loss 5.270, Val loss 5.377\n",
      "Ep 1 (Step 000280): Train loss 5.208, Val loss 5.366\n",
      "Ep 1 (Step 000290): Train loss 5.206, Val loss 5.346\n",
      "Ep 1 (Step 000300): Train loss 5.215, Val loss 5.345\n",
      "Ep 1 (Step 000310): Train loss 5.221, Val loss 5.328\n",
      "Ep 1 (Step 000320): Train loss 5.217, Val loss 5.312\n",
      "Ep 1 (Step 000330): Train loss 5.116, Val loss 5.306\n",
      "Ep 1 (Step 000340): Train loss 5.224, Val loss 5.284\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2840\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.842, Val loss 8.843\n",
      "Ep 1 (Step 000010): Train loss 7.404, Val loss 7.395\n",
      "Ep 1 (Step 000020): Train loss 6.710, Val loss 6.751\n",
      "Ep 1 (Step 000030): Train loss 6.463, Val loss 6.452\n",
      "Ep 1 (Step 000040): Train loss 6.359, Val loss 6.338\n",
      "Ep 1 (Step 000050): Train loss 6.282, Val loss 6.242\n",
      "Ep 1 (Step 000060): Train loss 6.082, Val loss 6.136\n",
      "Ep 1 (Step 000070): Train loss 6.010, Val loss 6.026\n",
      "Ep 1 (Step 000080): Train loss 5.865, Val loss 5.942\n",
      "Ep 1 (Step 000090): Train loss 5.832, Val loss 5.870\n",
      "Ep 1 (Step 000100): Train loss 5.797, Val loss 5.806\n",
      "Ep 1 (Step 000110): Train loss 5.686, Val loss 5.762\n",
      "Ep 1 (Step 000120): Train loss 5.628, Val loss 5.707\n",
      "Ep 1 (Step 000130): Train loss 5.550, Val loss 5.684\n",
      "Ep 1 (Step 000140): Train loss 5.564, Val loss 5.649\n",
      "Ep 1 (Step 000150): Train loss 5.479, Val loss 5.599\n",
      "Ep 1 (Step 000160): Train loss 5.429, Val loss 5.594\n",
      "Ep 1 (Step 000170): Train loss 5.492, Val loss 5.549\n",
      "Ep 1 (Step 000180): Train loss 5.447, Val loss 5.525\n",
      "Ep 1 (Step 000190): Train loss 5.438, Val loss 5.499\n",
      "Ep 1 (Step 000200): Train loss 5.444, Val loss 5.485\n",
      "Ep 1 (Step 000210): Train loss 5.399, Val loss 5.478\n",
      "Ep 1 (Step 000220): Train loss 5.438, Val loss 5.435\n",
      "Ep 1 (Step 000230): Train loss 5.325, Val loss 5.408\n",
      "Ep 1 (Step 000240): Train loss 5.255, Val loss 5.399\n",
      "Ep 1 (Step 000250): Train loss 5.304, Val loss 5.394\n",
      "Ep 1 (Step 000260): Train loss 5.298, Val loss 5.376\n",
      "Ep 1 (Step 000270): Train loss 5.234, Val loss 5.353\n",
      "Ep 1 (Step 000280): Train loss 5.217, Val loss 5.327\n",
      "Ep 1 (Step 000290): Train loss 5.221, Val loss 5.318\n",
      "Ep 1 (Step 000300): Train loss 5.256, Val loss 5.313\n",
      "Ep 1 (Step 000310): Train loss 5.277, Val loss 5.304\n",
      "Ep 1 (Step 000320): Train loss 5.176, Val loss 5.302\n",
      "Ep 1 (Step 000330): Train loss 5.248, Val loss 5.282\n",
      "Ep 1 (Step 000340): Train loss 5.149, Val loss 5.273\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2725\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.807, Val loss 8.796\n",
      "Ep 1 (Step 000010): Train loss 7.417, Val loss 7.390\n",
      "Ep 1 (Step 000020): Train loss 6.773, Val loss 6.737\n",
      "Ep 1 (Step 000030): Train loss 6.506, Val loss 6.441\n",
      "Ep 1 (Step 000040): Train loss 6.346, Val loss 6.351\n",
      "Ep 1 (Step 000050): Train loss 6.184, Val loss 6.237\n",
      "Ep 1 (Step 000060): Train loss 6.104, Val loss 6.138\n",
      "Ep 1 (Step 000070): Train loss 5.988, Val loss 6.018\n",
      "Ep 1 (Step 000080): Train loss 5.929, Val loss 5.983\n",
      "Ep 1 (Step 000090): Train loss 5.916, Val loss 5.888\n",
      "Ep 1 (Step 000100): Train loss 5.768, Val loss 5.818\n",
      "Ep 1 (Step 000110): Train loss 5.758, Val loss 5.768\n",
      "Ep 1 (Step 000120): Train loss 5.732, Val loss 5.724\n",
      "Ep 1 (Step 000130): Train loss 5.658, Val loss 5.694\n",
      "Ep 1 (Step 000140): Train loss 5.582, Val loss 5.651\n",
      "Ep 1 (Step 000150): Train loss 5.554, Val loss 5.597\n",
      "Ep 1 (Step 000160): Train loss 5.605, Val loss 5.575\n",
      "Ep 1 (Step 000170): Train loss 5.445, Val loss 5.557\n",
      "Ep 1 (Step 000180): Train loss 5.515, Val loss 5.520\n",
      "Ep 1 (Step 000190): Train loss 5.411, Val loss 5.494\n",
      "Ep 1 (Step 000200): Train loss 5.439, Val loss 5.483\n",
      "Ep 1 (Step 000210): Train loss 5.313, Val loss 5.460\n",
      "Ep 1 (Step 000220): Train loss 5.430, Val loss 5.452\n",
      "Ep 1 (Step 000230): Train loss 5.330, Val loss 5.420\n",
      "Ep 1 (Step 000240): Train loss 5.163, Val loss 5.407\n",
      "Ep 1 (Step 000250): Train loss 5.327, Val loss 5.398\n",
      "Ep 1 (Step 000260): Train loss 5.294, Val loss 5.393\n",
      "Ep 1 (Step 000270): Train loss 5.342, Val loss 5.359\n",
      "Ep 1 (Step 000280): Train loss 5.235, Val loss 5.354\n",
      "Ep 1 (Step 000290): Train loss 5.218, Val loss 5.328\n",
      "Ep 1 (Step 000300): Train loss 5.134, Val loss 5.334\n",
      "Ep 1 (Step 000310): Train loss 5.268, Val loss 5.305\n",
      "Ep 1 (Step 000320): Train loss 5.179, Val loss 5.293\n",
      "Ep 1 (Step 000330): Train loss 5.182, Val loss 5.282\n",
      "Ep 1 (Step 000340): Train loss 5.116, Val loss 5.276\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2758\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.730, Val loss 8.741\n",
      "Ep 1 (Step 000010): Train loss 7.398, Val loss 7.379\n",
      "Ep 1 (Step 000020): Train loss 6.766, Val loss 6.728\n",
      "Ep 1 (Step 000030): Train loss 6.418, Val loss 6.425\n",
      "Ep 1 (Step 000040): Train loss 6.326, Val loss 6.299\n",
      "Ep 1 (Step 000050): Train loss 6.181, Val loss 6.200\n",
      "Ep 1 (Step 000060): Train loss 6.129, Val loss 6.103\n",
      "Ep 1 (Step 000070): Train loss 5.993, Val loss 6.016\n",
      "Ep 1 (Step 000080): Train loss 5.950, Val loss 5.940\n",
      "Ep 1 (Step 000090): Train loss 5.784, Val loss 5.895\n",
      "Ep 1 (Step 000100): Train loss 5.686, Val loss 5.812\n",
      "Ep 1 (Step 000110): Train loss 5.733, Val loss 5.762\n",
      "Ep 1 (Step 000120): Train loss 5.641, Val loss 5.731\n",
      "Ep 1 (Step 000130): Train loss 5.617, Val loss 5.687\n",
      "Ep 1 (Step 000140): Train loss 5.579, Val loss 5.642\n",
      "Ep 1 (Step 000150): Train loss 5.575, Val loss 5.604\n",
      "Ep 1 (Step 000160): Train loss 5.506, Val loss 5.582\n",
      "Ep 1 (Step 000170): Train loss 5.434, Val loss 5.563\n",
      "Ep 1 (Step 000180): Train loss 5.402, Val loss 5.503\n",
      "Ep 1 (Step 000190): Train loss 5.433, Val loss 5.488\n",
      "Ep 1 (Step 000200): Train loss 5.396, Val loss 5.475\n",
      "Ep 1 (Step 000210): Train loss 5.293, Val loss 5.459\n",
      "Ep 1 (Step 000220): Train loss 5.254, Val loss 5.441\n",
      "Ep 1 (Step 000230): Train loss 5.277, Val loss 5.436\n",
      "Ep 1 (Step 000240): Train loss 5.231, Val loss 5.410\n",
      "Ep 1 (Step 000250): Train loss 5.268, Val loss 5.394\n",
      "Ep 1 (Step 000260): Train loss 5.231, Val loss 5.383\n",
      "Ep 1 (Step 000270): Train loss 5.263, Val loss 5.356\n",
      "Ep 1 (Step 000280): Train loss 5.221, Val loss 5.341\n",
      "Ep 1 (Step 000290): Train loss 5.196, Val loss 5.324\n",
      "Ep 1 (Step 000300): Train loss 5.195, Val loss 5.316\n",
      "Ep 1 (Step 000310): Train loss 5.182, Val loss 5.307\n",
      "Ep 1 (Step 000320): Train loss 5.200, Val loss 5.310\n",
      "Ep 1 (Step 000330): Train loss 5.136, Val loss 5.286\n",
      "Ep 1 (Step 000340): Train loss 5.057, Val loss 5.287\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2868\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.568, Val loss 8.523\n",
      "Ep 1 (Step 000010): Train loss 6.826, Val loss 6.835\n",
      "Ep 1 (Step 000020): Train loss 6.484, Val loss 6.439\n",
      "Ep 1 (Step 000030): Train loss 6.302, Val loss 6.340\n",
      "Ep 1 (Step 000040): Train loss 6.228, Val loss 6.230\n",
      "Ep 1 (Step 000050): Train loss 6.053, Val loss 6.084\n",
      "Ep 1 (Step 000060): Train loss 5.909, Val loss 5.971\n",
      "Ep 1 (Step 000070): Train loss 5.823, Val loss 5.872\n",
      "Ep 1 (Step 000080): Train loss 5.763, Val loss 5.812\n",
      "Ep 1 (Step 000090): Train loss 5.753, Val loss 5.747\n",
      "Ep 1 (Step 000100): Train loss 5.603, Val loss 5.677\n",
      "Ep 1 (Step 000110): Train loss 5.577, Val loss 5.638\n",
      "Ep 1 (Step 000120): Train loss 5.524, Val loss 5.621\n",
      "Ep 1 (Step 000130): Train loss 5.506, Val loss 5.588\n",
      "Ep 1 (Step 000140): Train loss 5.411, Val loss 5.543\n",
      "Ep 1 (Step 000150): Train loss 5.458, Val loss 5.520\n",
      "Ep 1 (Step 000160): Train loss 5.456, Val loss 5.493\n",
      "Ep 1 (Step 000170): Train loss 5.337, Val loss 5.479\n",
      "Ep 1 (Step 000180): Train loss 5.297, Val loss 5.456\n",
      "Ep 1 (Step 000190): Train loss 5.371, Val loss 5.442\n",
      "Ep 1 (Step 000200): Train loss 5.283, Val loss 5.424\n",
      "Ep 1 (Step 000210): Train loss 5.353, Val loss 5.416\n",
      "Ep 1 (Step 000220): Train loss 5.293, Val loss 5.408\n",
      "Ep 1 (Step 000230): Train loss 5.279, Val loss 5.387\n",
      "Ep 1 (Step 000240): Train loss 5.288, Val loss 5.377\n",
      "Ep 1 (Step 000250): Train loss 5.200, Val loss 5.360\n",
      "Ep 1 (Step 000260): Train loss 5.251, Val loss 5.330\n",
      "Ep 1 (Step 000270): Train loss 5.283, Val loss 5.323\n",
      "Ep 1 (Step 000280): Train loss 5.282, Val loss 5.314\n",
      "Ep 1 (Step 000290): Train loss 5.140, Val loss 5.314\n",
      "Ep 1 (Step 000300): Train loss 5.051, Val loss 5.290\n",
      "Ep 1 (Step 000310): Train loss 5.127, Val loss 5.284\n",
      "Ep 1 (Step 000320): Train loss 5.158, Val loss 5.275\n",
      "Ep 1 (Step 000330): Train loss 5.171, Val loss 5.271\n",
      "Ep 1 (Step 000340): Train loss 5.104, Val loss 5.240\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2399\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.566, Val loss 8.545\n",
      "Ep 1 (Step 000010): Train loss 6.868, Val loss 6.863\n",
      "Ep 1 (Step 000020): Train loss 6.417, Val loss 6.452\n",
      "Ep 1 (Step 000030): Train loss 6.358, Val loss 6.323\n",
      "Ep 1 (Step 000040): Train loss 6.208, Val loss 6.155\n",
      "Ep 1 (Step 000050): Train loss 6.006, Val loss 6.034\n",
      "Ep 1 (Step 000060): Train loss 5.900, Val loss 5.915\n",
      "Ep 1 (Step 000070): Train loss 5.815, Val loss 5.820\n",
      "Ep 1 (Step 000080): Train loss 5.740, Val loss 5.758\n",
      "Ep 1 (Step 000090): Train loss 5.686, Val loss 5.726\n",
      "Ep 1 (Step 000100): Train loss 5.581, Val loss 5.670\n",
      "Ep 1 (Step 000110): Train loss 5.511, Val loss 5.626\n",
      "Ep 1 (Step 000120): Train loss 5.494, Val loss 5.569\n",
      "Ep 1 (Step 000130): Train loss 5.482, Val loss 5.544\n",
      "Ep 1 (Step 000140): Train loss 5.440, Val loss 5.527\n",
      "Ep 1 (Step 000150): Train loss 5.501, Val loss 5.525\n",
      "Ep 1 (Step 000160): Train loss 5.487, Val loss 5.499\n",
      "Ep 1 (Step 000170): Train loss 5.402, Val loss 5.493\n",
      "Ep 1 (Step 000180): Train loss 5.419, Val loss 5.449\n",
      "Ep 1 (Step 000190): Train loss 5.324, Val loss 5.417\n",
      "Ep 1 (Step 000200): Train loss 5.290, Val loss 5.411\n",
      "Ep 1 (Step 000210): Train loss 5.218, Val loss 5.393\n",
      "Ep 1 (Step 000220): Train loss 5.255, Val loss 5.390\n",
      "Ep 1 (Step 000230): Train loss 5.161, Val loss 5.350\n",
      "Ep 1 (Step 000240): Train loss 5.301, Val loss 5.351\n",
      "Ep 1 (Step 000250): Train loss 5.190, Val loss 5.343\n",
      "Ep 1 (Step 000260): Train loss 5.158, Val loss 5.326\n",
      "Ep 1 (Step 000270): Train loss 5.227, Val loss 5.308\n",
      "Ep 1 (Step 000280): Train loss 5.248, Val loss 5.307\n",
      "Ep 1 (Step 000290): Train loss 5.136, Val loss 5.304\n",
      "Ep 1 (Step 000300): Train loss 5.136, Val loss 5.280\n",
      "Ep 1 (Step 000310): Train loss 5.076, Val loss 5.277\n",
      "Ep 1 (Step 000320): Train loss 5.107, Val loss 5.277\n",
      "Ep 1 (Step 000330): Train loss 5.197, Val loss 5.278\n",
      "Ep 1 (Step 000340): Train loss 5.117, Val loss 5.255\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2553\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.541, Val loss 8.572\n",
      "Ep 1 (Step 000010): Train loss 6.838, Val loss 6.827\n",
      "Ep 1 (Step 000020): Train loss 6.472, Val loss 6.454\n",
      "Ep 1 (Step 000030): Train loss 6.343, Val loss 6.344\n",
      "Ep 1 (Step 000040): Train loss 6.190, Val loss 6.176\n",
      "Ep 1 (Step 000050): Train loss 5.981, Val loss 6.039\n",
      "Ep 1 (Step 000060): Train loss 5.869, Val loss 5.927\n",
      "Ep 1 (Step 000070): Train loss 5.870, Val loss 5.856\n",
      "Ep 1 (Step 000080): Train loss 5.814, Val loss 5.803\n",
      "Ep 1 (Step 000090): Train loss 5.705, Val loss 5.726\n",
      "Ep 1 (Step 000100): Train loss 5.642, Val loss 5.681\n",
      "Ep 1 (Step 000110): Train loss 5.620, Val loss 5.646\n",
      "Ep 1 (Step 000120): Train loss 5.588, Val loss 5.619\n",
      "Ep 1 (Step 000130): Train loss 5.506, Val loss 5.578\n",
      "Ep 1 (Step 000140): Train loss 5.491, Val loss 5.535\n",
      "Ep 1 (Step 000150): Train loss 5.401, Val loss 5.521\n",
      "Ep 1 (Step 000160): Train loss 5.448, Val loss 5.488\n",
      "Ep 1 (Step 000170): Train loss 5.396, Val loss 5.458\n",
      "Ep 1 (Step 000180): Train loss 5.385, Val loss 5.447\n",
      "Ep 1 (Step 000190): Train loss 5.367, Val loss 5.426\n",
      "Ep 1 (Step 000200): Train loss 5.277, Val loss 5.406\n",
      "Ep 1 (Step 000210): Train loss 5.323, Val loss 5.397\n",
      "Ep 1 (Step 000220): Train loss 5.249, Val loss 5.368\n",
      "Ep 1 (Step 000230): Train loss 5.277, Val loss 5.372\n",
      "Ep 1 (Step 000240): Train loss 5.218, Val loss 5.353\n",
      "Ep 1 (Step 000250): Train loss 5.316, Val loss 5.332\n",
      "Ep 1 (Step 000260): Train loss 5.146, Val loss 5.323\n",
      "Ep 1 (Step 000270): Train loss 5.168, Val loss 5.305\n",
      "Ep 1 (Step 000280): Train loss 5.195, Val loss 5.298\n",
      "Ep 1 (Step 000290): Train loss 5.152, Val loss 5.281\n",
      "Ep 1 (Step 000300): Train loss 5.148, Val loss 5.272\n",
      "Ep 1 (Step 000310): Train loss 5.086, Val loss 5.270\n",
      "Ep 1 (Step 000320): Train loss 5.159, Val loss 5.305\n",
      "Ep 1 (Step 000330): Train loss 5.054, Val loss 5.250\n",
      "Ep 1 (Step 000340): Train loss 5.161, Val loss 5.276\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2757\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.568, Val loss 8.558\n",
      "Ep 1 (Step 000010): Train loss 6.858, Val loss 6.778\n",
      "Ep 1 (Step 000020): Train loss 6.476, Val loss 6.437\n",
      "Ep 1 (Step 000030): Train loss 6.396, Val loss 6.358\n",
      "Ep 1 (Step 000040): Train loss 6.181, Val loss 6.218\n",
      "Ep 1 (Step 000050): Train loss 6.086, Val loss 6.112\n",
      "Ep 1 (Step 000060): Train loss 5.941, Val loss 5.981\n",
      "Ep 1 (Step 000070): Train loss 5.895, Val loss 5.889\n",
      "Ep 1 (Step 000080): Train loss 5.779, Val loss 5.818\n",
      "Ep 1 (Step 000090): Train loss 5.643, Val loss 5.764\n",
      "Ep 1 (Step 000100): Train loss 5.680, Val loss 5.703\n",
      "Ep 1 (Step 000110): Train loss 5.555, Val loss 5.649\n",
      "Ep 1 (Step 000120): Train loss 5.474, Val loss 5.595\n",
      "Ep 1 (Step 000130): Train loss 5.526, Val loss 5.555\n",
      "Ep 1 (Step 000140): Train loss 5.457, Val loss 5.518\n",
      "Ep 1 (Step 000150): Train loss 5.425, Val loss 5.508\n",
      "Ep 1 (Step 000160): Train loss 5.357, Val loss 5.488\n",
      "Ep 1 (Step 000170): Train loss 5.359, Val loss 5.458\n",
      "Ep 1 (Step 000180): Train loss 5.378, Val loss 5.443\n",
      "Ep 1 (Step 000190): Train loss 5.252, Val loss 5.420\n",
      "Ep 1 (Step 000200): Train loss 5.250, Val loss 5.385\n",
      "Ep 1 (Step 000210): Train loss 5.340, Val loss 5.383\n",
      "Ep 1 (Step 000220): Train loss 5.228, Val loss 5.364\n",
      "Ep 1 (Step 000230): Train loss 5.259, Val loss 5.339\n",
      "Ep 1 (Step 000240): Train loss 5.280, Val loss 5.338\n",
      "Ep 1 (Step 000250): Train loss 5.171, Val loss 5.314\n",
      "Ep 1 (Step 000260): Train loss 5.168, Val loss 5.304\n",
      "Ep 1 (Step 000270): Train loss 5.201, Val loss 5.283\n",
      "Ep 1 (Step 000280): Train loss 5.118, Val loss 5.281\n",
      "Ep 1 (Step 000290): Train loss 5.167, Val loss 5.267\n",
      "Ep 1 (Step 000300): Train loss 5.154, Val loss 5.252\n",
      "Ep 1 (Step 000310): Train loss 5.121, Val loss 5.234\n",
      "Ep 1 (Step 000320): Train loss 5.112, Val loss 5.241\n",
      "Ep 1 (Step 000330): Train loss 5.087, Val loss 5.223\n",
      "Ep 1 (Step 000340): Train loss 4.978, Val loss 5.204\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2041\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.474, Val loss 8.478\n",
      "Ep 1 (Step 000010): Train loss 6.821, Val loss 6.811\n",
      "Ep 1 (Step 000020): Train loss 6.458, Val loss 6.439\n",
      "Ep 1 (Step 000030): Train loss 6.381, Val loss 6.347\n",
      "Ep 1 (Step 000040): Train loss 6.195, Val loss 6.207\n",
      "Ep 1 (Step 000050): Train loss 6.014, Val loss 6.072\n",
      "Ep 1 (Step 000060): Train loss 5.882, Val loss 5.958\n",
      "Ep 1 (Step 000070): Train loss 5.828, Val loss 5.871\n",
      "Ep 1 (Step 000080): Train loss 5.780, Val loss 5.809\n",
      "Ep 1 (Step 000090): Train loss 5.694, Val loss 5.759\n",
      "Ep 1 (Step 000100): Train loss 5.695, Val loss 5.698\n",
      "Ep 1 (Step 000110): Train loss 5.567, Val loss 5.644\n",
      "Ep 1 (Step 000120): Train loss 5.501, Val loss 5.599\n",
      "Ep 1 (Step 000130): Train loss 5.574, Val loss 5.586\n",
      "Ep 1 (Step 000140): Train loss 5.405, Val loss 5.555\n",
      "Ep 1 (Step 000150): Train loss 5.437, Val loss 5.531\n",
      "Ep 1 (Step 000160): Train loss 5.372, Val loss 5.488\n",
      "Ep 1 (Step 000170): Train loss 5.443, Val loss 5.456\n",
      "Ep 1 (Step 000180): Train loss 5.279, Val loss 5.444\n",
      "Ep 1 (Step 000190): Train loss 5.285, Val loss 5.414\n",
      "Ep 1 (Step 000200): Train loss 5.317, Val loss 5.406\n",
      "Ep 1 (Step 000210): Train loss 5.235, Val loss 5.376\n",
      "Ep 1 (Step 000220): Train loss 5.307, Val loss 5.359\n",
      "Ep 1 (Step 000230): Train loss 5.385, Val loss 5.349\n",
      "Ep 1 (Step 000240): Train loss 5.205, Val loss 5.324\n",
      "Ep 1 (Step 000250): Train loss 5.174, Val loss 5.321\n",
      "Ep 1 (Step 000260): Train loss 5.255, Val loss 5.309\n",
      "Ep 1 (Step 000270): Train loss 5.140, Val loss 5.287\n",
      "Ep 1 (Step 000280): Train loss 5.207, Val loss 5.279\n",
      "Ep 1 (Step 000290): Train loss 5.194, Val loss 5.272\n",
      "Ep 1 (Step 000300): Train loss 5.167, Val loss 5.254\n",
      "Ep 1 (Step 000310): Train loss 5.099, Val loss 5.252\n",
      "Ep 1 (Step 000320): Train loss 5.188, Val loss 5.221\n",
      "Ep 1 (Step 000330): Train loss 5.114, Val loss 5.218\n",
      "Ep 1 (Step 000340): Train loss 5.146, Val loss 5.219\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2190\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.552, Val loss 8.514\n",
      "Ep 1 (Step 000010): Train loss 6.822, Val loss 6.829\n",
      "Ep 1 (Step 000020): Train loss 6.489, Val loss 6.438\n",
      "Ep 1 (Step 000030): Train loss 6.369, Val loss 6.401\n",
      "Ep 1 (Step 000040): Train loss 6.264, Val loss 6.281\n",
      "Ep 1 (Step 000050): Train loss 6.118, Val loss 6.085\n",
      "Ep 1 (Step 000060): Train loss 5.999, Val loss 5.996\n",
      "Ep 1 (Step 000070): Train loss 5.962, Val loss 5.893\n",
      "Ep 1 (Step 000080): Train loss 5.689, Val loss 5.822\n",
      "Ep 1 (Step 000090): Train loss 5.738, Val loss 5.781\n",
      "Ep 1 (Step 000100): Train loss 5.613, Val loss 5.717\n",
      "Ep 1 (Step 000110): Train loss 5.568, Val loss 5.649\n",
      "Ep 1 (Step 000120): Train loss 5.493, Val loss 5.606\n",
      "Ep 1 (Step 000130): Train loss 5.593, Val loss 5.541\n",
      "Ep 1 (Step 000140): Train loss 5.452, Val loss 5.511\n",
      "Ep 1 (Step 000150): Train loss 5.433, Val loss 5.487\n",
      "Ep 1 (Step 000160): Train loss 5.420, Val loss 5.471\n",
      "Ep 1 (Step 000170): Train loss 5.369, Val loss 5.447\n",
      "Ep 1 (Step 000180): Train loss 5.396, Val loss 5.447\n",
      "Ep 1 (Step 000190): Train loss 5.357, Val loss 5.409\n",
      "Ep 1 (Step 000200): Train loss 5.338, Val loss 5.415\n",
      "Ep 1 (Step 000210): Train loss 5.297, Val loss 5.382\n",
      "Ep 1 (Step 000220): Train loss 5.229, Val loss 5.347\n",
      "Ep 1 (Step 000230): Train loss 5.282, Val loss 5.341\n",
      "Ep 1 (Step 000240): Train loss 5.213, Val loss 5.316\n",
      "Ep 1 (Step 000250): Train loss 5.128, Val loss 5.310\n",
      "Ep 1 (Step 000260): Train loss 5.197, Val loss 5.306\n",
      "Ep 1 (Step 000270): Train loss 5.150, Val loss 5.302\n",
      "Ep 1 (Step 000280): Train loss 5.070, Val loss 5.280\n",
      "Ep 1 (Step 000290): Train loss 5.131, Val loss 5.257\n",
      "Ep 1 (Step 000300): Train loss 5.087, Val loss 5.250\n",
      "Ep 1 (Step 000310): Train loss 5.193, Val loss 5.254\n",
      "Ep 1 (Step 000320): Train loss 5.129, Val loss 5.236\n",
      "Ep 1 (Step 000330): Train loss 5.102, Val loss 5.245\n",
      "Ep 1 (Step 000340): Train loss 5.108, Val loss 5.222\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2224\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.803, Val loss 8.786\n",
      "Ep 1 (Step 000010): Train loss 7.428, Val loss 7.425\n",
      "Ep 1 (Step 000020): Train loss 6.780, Val loss 6.761\n",
      "Ep 1 (Step 000030): Train loss 6.461, Val loss 6.455\n",
      "Ep 1 (Step 000040): Train loss 6.322, Val loss 6.307\n",
      "Ep 1 (Step 000050): Train loss 6.217, Val loss 6.223\n",
      "Ep 1 (Step 000060): Train loss 6.080, Val loss 6.083\n",
      "Ep 1 (Step 000070): Train loss 6.018, Val loss 6.002\n",
      "Ep 1 (Step 000080): Train loss 5.839, Val loss 5.933\n",
      "Ep 1 (Step 000090): Train loss 5.783, Val loss 5.857\n",
      "Ep 1 (Step 000100): Train loss 5.725, Val loss 5.814\n",
      "Ep 1 (Step 000110): Train loss 5.712, Val loss 5.771\n",
      "Ep 1 (Step 000120): Train loss 5.645, Val loss 5.723\n",
      "Ep 1 (Step 000130): Train loss 5.535, Val loss 5.675\n",
      "Ep 1 (Step 000140): Train loss 5.581, Val loss 5.635\n",
      "Ep 1 (Step 000150): Train loss 5.573, Val loss 5.607\n",
      "Ep 1 (Step 000160): Train loss 5.498, Val loss 5.581\n",
      "Ep 1 (Step 000170): Train loss 5.492, Val loss 5.552\n",
      "Ep 1 (Step 000180): Train loss 5.451, Val loss 5.521\n",
      "Ep 1 (Step 000190): Train loss 5.340, Val loss 5.501\n",
      "Ep 1 (Step 000200): Train loss 5.412, Val loss 5.494\n",
      "Ep 1 (Step 000210): Train loss 5.386, Val loss 5.461\n",
      "Ep 1 (Step 000220): Train loss 5.357, Val loss 5.441\n",
      "Ep 1 (Step 000230): Train loss 5.321, Val loss 5.428\n",
      "Ep 1 (Step 000240): Train loss 5.327, Val loss 5.407\n",
      "Ep 1 (Step 000250): Train loss 5.267, Val loss 5.399\n",
      "Ep 1 (Step 000260): Train loss 5.267, Val loss 5.389\n",
      "Ep 1 (Step 000270): Train loss 5.322, Val loss 5.387\n",
      "Ep 1 (Step 000280): Train loss 5.334, Val loss 5.368\n",
      "Ep 1 (Step 000290): Train loss 5.167, Val loss 5.345\n",
      "Ep 1 (Step 000300): Train loss 5.292, Val loss 5.350\n",
      "Ep 1 (Step 000310): Train loss 5.234, Val loss 5.327\n",
      "Ep 1 (Step 000320): Train loss 5.276, Val loss 5.311\n",
      "Ep 1 (Step 000330): Train loss 5.161, Val loss 5.313\n",
      "Ep 1 (Step 000340): Train loss 5.190, Val loss 5.316\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3157\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.777, Val loss 8.764\n",
      "Ep 1 (Step 000010): Train loss 7.398, Val loss 7.401\n",
      "Ep 1 (Step 000020): Train loss 6.775, Val loss 6.756\n",
      "Ep 1 (Step 000030): Train loss 6.487, Val loss 6.450\n",
      "Ep 1 (Step 000040): Train loss 6.407, Val loss 6.337\n",
      "Ep 1 (Step 000050): Train loss 6.127, Val loss 6.233\n",
      "Ep 1 (Step 000060): Train loss 6.130, Val loss 6.130\n",
      "Ep 1 (Step 000070): Train loss 6.023, Val loss 6.008\n",
      "Ep 1 (Step 000080): Train loss 5.962, Val loss 5.954\n",
      "Ep 1 (Step 000090): Train loss 5.888, Val loss 5.881\n",
      "Ep 1 (Step 000100): Train loss 5.792, Val loss 5.834\n",
      "Ep 1 (Step 000110): Train loss 5.705, Val loss 5.786\n",
      "Ep 1 (Step 000120): Train loss 5.791, Val loss 5.728\n",
      "Ep 1 (Step 000130): Train loss 5.614, Val loss 5.698\n",
      "Ep 1 (Step 000140): Train loss 5.525, Val loss 5.656\n",
      "Ep 1 (Step 000150): Train loss 5.544, Val loss 5.624\n",
      "Ep 1 (Step 000160): Train loss 5.525, Val loss 5.589\n",
      "Ep 1 (Step 000170): Train loss 5.447, Val loss 5.557\n",
      "Ep 1 (Step 000180): Train loss 5.491, Val loss 5.532\n",
      "Ep 1 (Step 000190): Train loss 5.454, Val loss 5.505\n",
      "Ep 1 (Step 000200): Train loss 5.397, Val loss 5.486\n",
      "Ep 1 (Step 000210): Train loss 5.392, Val loss 5.488\n",
      "Ep 1 (Step 000220): Train loss 5.364, Val loss 5.465\n",
      "Ep 1 (Step 000230): Train loss 5.297, Val loss 5.447\n",
      "Ep 1 (Step 000240): Train loss 5.323, Val loss 5.426\n",
      "Ep 1 (Step 000250): Train loss 5.250, Val loss 5.415\n",
      "Ep 1 (Step 000260): Train loss 5.359, Val loss 5.399\n",
      "Ep 1 (Step 000270): Train loss 5.304, Val loss 5.377\n",
      "Ep 1 (Step 000280): Train loss 5.255, Val loss 5.359\n",
      "Ep 1 (Step 000290): Train loss 5.140, Val loss 5.352\n",
      "Ep 1 (Step 000300): Train loss 5.291, Val loss 5.336\n",
      "Ep 1 (Step 000310): Train loss 5.231, Val loss 5.327\n",
      "Ep 1 (Step 000320): Train loss 5.220, Val loss 5.314\n",
      "Ep 1 (Step 000330): Train loss 5.217, Val loss 5.306\n",
      "Ep 1 (Step 000340): Train loss 5.193, Val loss 5.297\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2966\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.789, Val loss 8.764\n",
      "Ep 1 (Step 000010): Train loss 7.421, Val loss 7.388\n",
      "Ep 1 (Step 000020): Train loss 6.747, Val loss 6.724\n",
      "Ep 1 (Step 000030): Train loss 6.459, Val loss 6.445\n",
      "Ep 1 (Step 000040): Train loss 6.334, Val loss 6.346\n",
      "Ep 1 (Step 000050): Train loss 6.275, Val loss 6.253\n",
      "Ep 1 (Step 000060): Train loss 6.134, Val loss 6.126\n",
      "Ep 1 (Step 000070): Train loss 5.965, Val loss 6.036\n",
      "Ep 1 (Step 000080): Train loss 5.893, Val loss 5.964\n",
      "Ep 1 (Step 000090): Train loss 5.890, Val loss 5.881\n",
      "Ep 1 (Step 000100): Train loss 5.781, Val loss 5.834\n",
      "Ep 1 (Step 000110): Train loss 5.829, Val loss 5.799\n",
      "Ep 1 (Step 000120): Train loss 5.701, Val loss 5.726\n",
      "Ep 1 (Step 000130): Train loss 5.638, Val loss 5.687\n",
      "Ep 1 (Step 000140): Train loss 5.648, Val loss 5.651\n",
      "Ep 1 (Step 000150): Train loss 5.513, Val loss 5.623\n",
      "Ep 1 (Step 000160): Train loss 5.489, Val loss 5.583\n",
      "Ep 1 (Step 000170): Train loss 5.492, Val loss 5.573\n",
      "Ep 1 (Step 000180): Train loss 5.435, Val loss 5.532\n",
      "Ep 1 (Step 000190): Train loss 5.430, Val loss 5.498\n",
      "Ep 1 (Step 000200): Train loss 5.350, Val loss 5.477\n",
      "Ep 1 (Step 000210): Train loss 5.413, Val loss 5.471\n",
      "Ep 1 (Step 000220): Train loss 5.339, Val loss 5.455\n",
      "Ep 1 (Step 000230): Train loss 5.324, Val loss 5.450\n",
      "Ep 1 (Step 000240): Train loss 5.351, Val loss 5.421\n",
      "Ep 1 (Step 000250): Train loss 5.289, Val loss 5.421\n",
      "Ep 1 (Step 000260): Train loss 5.324, Val loss 5.397\n",
      "Ep 1 (Step 000270): Train loss 5.275, Val loss 5.388\n",
      "Ep 1 (Step 000280): Train loss 5.260, Val loss 5.380\n",
      "Ep 1 (Step 000290): Train loss 5.254, Val loss 5.366\n",
      "Ep 1 (Step 000300): Train loss 5.248, Val loss 5.350\n",
      "Ep 1 (Step 000310): Train loss 5.183, Val loss 5.339\n",
      "Ep 1 (Step 000320): Train loss 5.287, Val loss 5.326\n",
      "Ep 1 (Step 000330): Train loss 5.238, Val loss 5.313\n",
      "Ep 1 (Step 000340): Train loss 5.209, Val loss 5.308\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3083\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.805, Val loss 8.780\n",
      "Ep 1 (Step 000010): Train loss 7.405, Val loss 7.390\n",
      "Ep 1 (Step 000020): Train loss 6.794, Val loss 6.746\n",
      "Ep 1 (Step 000030): Train loss 6.431, Val loss 6.442\n",
      "Ep 1 (Step 000040): Train loss 6.381, Val loss 6.347\n",
      "Ep 1 (Step 000050): Train loss 6.249, Val loss 6.249\n",
      "Ep 1 (Step 000060): Train loss 6.098, Val loss 6.166\n",
      "Ep 1 (Step 000070): Train loss 5.947, Val loss 6.023\n",
      "Ep 1 (Step 000080): Train loss 5.879, Val loss 5.946\n",
      "Ep 1 (Step 000090): Train loss 5.864, Val loss 5.871\n",
      "Ep 1 (Step 000100): Train loss 5.823, Val loss 5.819\n",
      "Ep 1 (Step 000110): Train loss 5.708, Val loss 5.757\n",
      "Ep 1 (Step 000120): Train loss 5.676, Val loss 5.713\n",
      "Ep 1 (Step 000130): Train loss 5.579, Val loss 5.667\n",
      "Ep 1 (Step 000140): Train loss 5.623, Val loss 5.650\n",
      "Ep 1 (Step 000150): Train loss 5.556, Val loss 5.601\n",
      "Ep 1 (Step 000160): Train loss 5.513, Val loss 5.578\n",
      "Ep 1 (Step 000170): Train loss 5.455, Val loss 5.561\n",
      "Ep 1 (Step 000180): Train loss 5.434, Val loss 5.516\n",
      "Ep 1 (Step 000190): Train loss 5.444, Val loss 5.505\n",
      "Ep 1 (Step 000200): Train loss 5.330, Val loss 5.498\n",
      "Ep 1 (Step 000210): Train loss 5.413, Val loss 5.473\n",
      "Ep 1 (Step 000220): Train loss 5.318, Val loss 5.429\n",
      "Ep 1 (Step 000230): Train loss 5.333, Val loss 5.438\n",
      "Ep 1 (Step 000240): Train loss 5.275, Val loss 5.406\n",
      "Ep 1 (Step 000250): Train loss 5.297, Val loss 5.390\n",
      "Ep 1 (Step 000260): Train loss 5.288, Val loss 5.359\n",
      "Ep 1 (Step 000270): Train loss 5.286, Val loss 5.343\n",
      "Ep 1 (Step 000280): Train loss 5.212, Val loss 5.351\n",
      "Ep 1 (Step 000290): Train loss 5.276, Val loss 5.341\n",
      "Ep 1 (Step 000300): Train loss 5.184, Val loss 5.313\n",
      "Ep 1 (Step 000310): Train loss 5.133, Val loss 5.319\n",
      "Ep 1 (Step 000320): Train loss 5.245, Val loss 5.302\n",
      "Ep 1 (Step 000330): Train loss 5.223, Val loss 5.310\n",
      "Ep 1 (Step 000340): Train loss 5.154, Val loss 5.289\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2893\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.775, Val loss 8.765\n",
      "Ep 1 (Step 000010): Train loss 7.363, Val loss 7.358\n",
      "Ep 1 (Step 000020): Train loss 6.701, Val loss 6.725\n",
      "Ep 1 (Step 000030): Train loss 6.437, Val loss 6.432\n",
      "Ep 1 (Step 000040): Train loss 6.390, Val loss 6.342\n",
      "Ep 1 (Step 000050): Train loss 6.258, Val loss 6.266\n",
      "Ep 1 (Step 000060): Train loss 6.111, Val loss 6.142\n",
      "Ep 1 (Step 000070): Train loss 6.021, Val loss 6.030\n",
      "Ep 1 (Step 000080): Train loss 5.904, Val loss 5.953\n",
      "Ep 1 (Step 000090): Train loss 5.799, Val loss 5.884\n",
      "Ep 1 (Step 000100): Train loss 5.777, Val loss 5.819\n",
      "Ep 1 (Step 000110): Train loss 5.782, Val loss 5.776\n",
      "Ep 1 (Step 000120): Train loss 5.592, Val loss 5.725\n",
      "Ep 1 (Step 000130): Train loss 5.644, Val loss 5.670\n",
      "Ep 1 (Step 000140): Train loss 5.664, Val loss 5.626\n",
      "Ep 1 (Step 000150): Train loss 5.536, Val loss 5.601\n",
      "Ep 1 (Step 000160): Train loss 5.487, Val loss 5.588\n",
      "Ep 1 (Step 000170): Train loss 5.492, Val loss 5.543\n",
      "Ep 1 (Step 000180): Train loss 5.447, Val loss 5.517\n",
      "Ep 1 (Step 000190): Train loss 5.434, Val loss 5.499\n",
      "Ep 1 (Step 000200): Train loss 5.424, Val loss 5.487\n",
      "Ep 1 (Step 000210): Train loss 5.413, Val loss 5.459\n",
      "Ep 1 (Step 000220): Train loss 5.384, Val loss 5.440\n",
      "Ep 1 (Step 000230): Train loss 5.321, Val loss 5.417\n",
      "Ep 1 (Step 000240): Train loss 5.342, Val loss 5.415\n",
      "Ep 1 (Step 000250): Train loss 5.361, Val loss 5.387\n",
      "Ep 1 (Step 000260): Train loss 5.261, Val loss 5.375\n",
      "Ep 1 (Step 000270): Train loss 5.327, Val loss 5.363\n",
      "Ep 1 (Step 000280): Train loss 5.267, Val loss 5.360\n",
      "Ep 1 (Step 000290): Train loss 5.141, Val loss 5.332\n",
      "Ep 1 (Step 000300): Train loss 5.236, Val loss 5.321\n",
      "Ep 1 (Step 000310): Train loss 5.270, Val loss 5.324\n",
      "Ep 1 (Step 000320): Train loss 5.128, Val loss 5.302\n",
      "Ep 1 (Step 000330): Train loss 5.168, Val loss 5.297\n",
      "Ep 1 (Step 000340): Train loss 5.217, Val loss 5.285\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2851\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.735, Val loss 8.706\n",
      "Ep 1 (Step 000010): Train loss 7.371, Val loss 7.342\n",
      "Ep 1 (Step 000020): Train loss 6.791, Val loss 6.717\n",
      "Ep 1 (Step 000030): Train loss 6.457, Val loss 6.427\n",
      "Ep 1 (Step 000040): Train loss 6.398, Val loss 6.330\n",
      "Ep 1 (Step 000050): Train loss 6.282, Val loss 6.233\n",
      "Ep 1 (Step 000060): Train loss 6.090, Val loss 6.128\n",
      "Ep 1 (Step 000070): Train loss 6.002, Val loss 6.054\n",
      "Ep 1 (Step 000080): Train loss 6.025, Val loss 5.943\n",
      "Ep 1 (Step 000090): Train loss 5.841, Val loss 5.851\n",
      "Ep 1 (Step 000100): Train loss 5.757, Val loss 5.807\n",
      "Ep 1 (Step 000110): Train loss 5.736, Val loss 5.764\n",
      "Ep 1 (Step 000120): Train loss 5.578, Val loss 5.704\n",
      "Ep 1 (Step 000130): Train loss 5.648, Val loss 5.662\n",
      "Ep 1 (Step 000140): Train loss 5.625, Val loss 5.626\n",
      "Ep 1 (Step 000150): Train loss 5.513, Val loss 5.589\n",
      "Ep 1 (Step 000160): Train loss 5.581, Val loss 5.560\n",
      "Ep 1 (Step 000170): Train loss 5.436, Val loss 5.516\n",
      "Ep 1 (Step 000180): Train loss 5.500, Val loss 5.504\n",
      "Ep 1 (Step 000190): Train loss 5.415, Val loss 5.498\n",
      "Ep 1 (Step 000200): Train loss 5.470, Val loss 5.476\n",
      "Ep 1 (Step 000210): Train loss 5.453, Val loss 5.462\n",
      "Ep 1 (Step 000220): Train loss 5.384, Val loss 5.447\n",
      "Ep 1 (Step 000230): Train loss 5.359, Val loss 5.427\n",
      "Ep 1 (Step 000240): Train loss 5.350, Val loss 5.404\n",
      "Ep 1 (Step 000250): Train loss 5.249, Val loss 5.385\n",
      "Ep 1 (Step 000260): Train loss 5.310, Val loss 5.369\n",
      "Ep 1 (Step 000270): Train loss 5.274, Val loss 5.352\n",
      "Ep 1 (Step 000280): Train loss 5.242, Val loss 5.354\n",
      "Ep 1 (Step 000290): Train loss 5.194, Val loss 5.340\n",
      "Ep 1 (Step 000300): Train loss 5.272, Val loss 5.334\n",
      "Ep 1 (Step 000310): Train loss 5.138, Val loss 5.311\n",
      "Ep 1 (Step 000320): Train loss 5.177, Val loss 5.302\n",
      "Ep 1 (Step 000330): Train loss 5.108, Val loss 5.287\n",
      "Ep 1 (Step 000340): Train loss 5.181, Val loss 5.288\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2879\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.476, Val loss 8.488\n",
      "Ep 1 (Step 000010): Train loss 6.862, Val loss 6.792\n",
      "Ep 1 (Step 000020): Train loss 6.462, Val loss 6.438\n",
      "Ep 1 (Step 000030): Train loss 6.435, Val loss 6.428\n",
      "Ep 1 (Step 000040): Train loss 6.298, Val loss 6.258\n",
      "Ep 1 (Step 000050): Train loss 6.157, Val loss 6.144\n",
      "Ep 1 (Step 000060): Train loss 6.034, Val loss 6.011\n",
      "Ep 1 (Step 000070): Train loss 5.910, Val loss 5.899\n",
      "Ep 1 (Step 000080): Train loss 5.754, Val loss 5.822\n",
      "Ep 1 (Step 000090): Train loss 5.696, Val loss 5.779\n",
      "Ep 1 (Step 000100): Train loss 5.685, Val loss 5.725\n",
      "Ep 1 (Step 000110): Train loss 5.633, Val loss 5.687\n",
      "Ep 1 (Step 000120): Train loss 5.526, Val loss 5.630\n",
      "Ep 1 (Step 000130): Train loss 5.547, Val loss 5.592\n",
      "Ep 1 (Step 000140): Train loss 5.551, Val loss 5.561\n",
      "Ep 1 (Step 000150): Train loss 5.485, Val loss 5.534\n",
      "Ep 1 (Step 000160): Train loss 5.470, Val loss 5.509\n",
      "Ep 1 (Step 000170): Train loss 5.394, Val loss 5.486\n",
      "Ep 1 (Step 000180): Train loss 5.432, Val loss 5.462\n",
      "Ep 1 (Step 000190): Train loss 5.397, Val loss 5.428\n",
      "Ep 1 (Step 000200): Train loss 5.327, Val loss 5.437\n",
      "Ep 1 (Step 000210): Train loss 5.350, Val loss 5.408\n",
      "Ep 1 (Step 000220): Train loss 5.266, Val loss 5.398\n",
      "Ep 1 (Step 000230): Train loss 5.302, Val loss 5.367\n",
      "Ep 1 (Step 000240): Train loss 5.165, Val loss 5.351\n",
      "Ep 1 (Step 000250): Train loss 5.276, Val loss 5.342\n",
      "Ep 1 (Step 000260): Train loss 5.225, Val loss 5.344\n",
      "Ep 1 (Step 000270): Train loss 5.176, Val loss 5.328\n",
      "Ep 1 (Step 000280): Train loss 5.232, Val loss 5.322\n",
      "Ep 1 (Step 000290): Train loss 5.200, Val loss 5.299\n",
      "Ep 1 (Step 000300): Train loss 5.139, Val loss 5.274\n",
      "Ep 1 (Step 000310): Train loss 5.198, Val loss 5.273\n",
      "Ep 1 (Step 000320): Train loss 5.163, Val loss 5.276\n",
      "Ep 1 (Step 000330): Train loss 5.245, Val loss 5.281\n",
      "Ep 1 (Step 000340): Train loss 5.142, Val loss 5.257\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2566\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.641, Val loss 8.598\n",
      "Ep 1 (Step 000010): Train loss 6.927, Val loss 6.882\n",
      "Ep 1 (Step 000020): Train loss 6.455, Val loss 6.429\n",
      "Ep 1 (Step 000030): Train loss 6.404, Val loss 6.320\n",
      "Ep 1 (Step 000040): Train loss 6.200, Val loss 6.215\n",
      "Ep 1 (Step 000050): Train loss 6.060, Val loss 6.068\n",
      "Ep 1 (Step 000060): Train loss 5.909, Val loss 5.980\n",
      "Ep 1 (Step 000070): Train loss 5.864, Val loss 5.879\n",
      "Ep 1 (Step 000080): Train loss 5.812, Val loss 5.814\n",
      "Ep 1 (Step 000090): Train loss 5.697, Val loss 5.739\n",
      "Ep 1 (Step 000100): Train loss 5.622, Val loss 5.682\n",
      "Ep 1 (Step 000110): Train loss 5.600, Val loss 5.635\n",
      "Ep 1 (Step 000120): Train loss 5.571, Val loss 5.616\n",
      "Ep 1 (Step 000130): Train loss 5.507, Val loss 5.588\n",
      "Ep 1 (Step 000140): Train loss 5.449, Val loss 5.538\n",
      "Ep 1 (Step 000150): Train loss 5.502, Val loss 5.541\n",
      "Ep 1 (Step 000160): Train loss 5.435, Val loss 5.517\n",
      "Ep 1 (Step 000170): Train loss 5.344, Val loss 5.490\n",
      "Ep 1 (Step 000180): Train loss 5.380, Val loss 5.460\n",
      "Ep 1 (Step 000190): Train loss 5.330, Val loss 5.452\n",
      "Ep 1 (Step 000200): Train loss 5.243, Val loss 5.426\n",
      "Ep 1 (Step 000210): Train loss 5.303, Val loss 5.413\n",
      "Ep 1 (Step 000220): Train loss 5.230, Val loss 5.421\n",
      "Ep 1 (Step 000230): Train loss 5.321, Val loss 5.397\n",
      "Ep 1 (Step 000240): Train loss 5.292, Val loss 5.403\n",
      "Ep 1 (Step 000250): Train loss 5.290, Val loss 5.366\n",
      "Ep 1 (Step 000260): Train loss 5.255, Val loss 5.343\n",
      "Ep 1 (Step 000270): Train loss 5.189, Val loss 5.341\n",
      "Ep 1 (Step 000280): Train loss 5.281, Val loss 5.331\n",
      "Ep 1 (Step 000290): Train loss 5.230, Val loss 5.315\n",
      "Ep 1 (Step 000300): Train loss 5.153, Val loss 5.295\n",
      "Ep 1 (Step 000310): Train loss 5.198, Val loss 5.294\n",
      "Ep 1 (Step 000320): Train loss 5.078, Val loss 5.289\n",
      "Ep 1 (Step 000330): Train loss 5.232, Val loss 5.265\n",
      "Ep 1 (Step 000340): Train loss 5.074, Val loss 5.267\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2668\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.502, Val loss 8.485\n",
      "Ep 1 (Step 000010): Train loss 6.863, Val loss 6.847\n",
      "Ep 1 (Step 000020): Train loss 6.456, Val loss 6.448\n",
      "Ep 1 (Step 000030): Train loss 6.323, Val loss 6.341\n",
      "Ep 1 (Step 000040): Train loss 6.198, Val loss 6.208\n",
      "Ep 1 (Step 000050): Train loss 6.011, Val loss 6.054\n",
      "Ep 1 (Step 000060): Train loss 5.898, Val loss 5.950\n",
      "Ep 1 (Step 000070): Train loss 5.835, Val loss 5.872\n",
      "Ep 1 (Step 000080): Train loss 5.647, Val loss 5.795\n",
      "Ep 1 (Step 000090): Train loss 5.715, Val loss 5.741\n",
      "Ep 1 (Step 000100): Train loss 5.671, Val loss 5.703\n",
      "Ep 1 (Step 000110): Train loss 5.611, Val loss 5.642\n",
      "Ep 1 (Step 000120): Train loss 5.622, Val loss 5.589\n",
      "Ep 1 (Step 000130): Train loss 5.558, Val loss 5.563\n",
      "Ep 1 (Step 000140): Train loss 5.527, Val loss 5.548\n",
      "Ep 1 (Step 000150): Train loss 5.508, Val loss 5.510\n",
      "Ep 1 (Step 000160): Train loss 5.402, Val loss 5.482\n",
      "Ep 1 (Step 000170): Train loss 5.446, Val loss 5.467\n",
      "Ep 1 (Step 000180): Train loss 5.379, Val loss 5.438\n",
      "Ep 1 (Step 000190): Train loss 5.330, Val loss 5.408\n",
      "Ep 1 (Step 000200): Train loss 5.360, Val loss 5.393\n",
      "Ep 1 (Step 000210): Train loss 5.288, Val loss 5.381\n",
      "Ep 1 (Step 000220): Train loss 5.286, Val loss 5.376\n",
      "Ep 1 (Step 000230): Train loss 5.239, Val loss 5.380\n",
      "Ep 1 (Step 000240): Train loss 5.197, Val loss 5.368\n",
      "Ep 1 (Step 000250): Train loss 5.217, Val loss 5.367\n",
      "Ep 1 (Step 000260): Train loss 5.230, Val loss 5.338\n",
      "Ep 1 (Step 000270): Train loss 5.225, Val loss 5.330\n",
      "Ep 1 (Step 000280): Train loss 5.179, Val loss 5.325\n",
      "Ep 1 (Step 000290): Train loss 5.225, Val loss 5.314\n",
      "Ep 1 (Step 000300): Train loss 5.220, Val loss 5.315\n",
      "Ep 1 (Step 000310): Train loss 5.153, Val loss 5.283\n",
      "Ep 1 (Step 000320): Train loss 5.140, Val loss 5.285\n",
      "Ep 1 (Step 000330): Train loss 5.161, Val loss 5.263\n",
      "Ep 1 (Step 000340): Train loss 5.142, Val loss 5.262\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2618\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.554, Val loss 8.545\n",
      "Ep 1 (Step 000010): Train loss 6.899, Val loss 6.879\n",
      "Ep 1 (Step 000020): Train loss 6.428, Val loss 6.471\n",
      "Ep 1 (Step 000030): Train loss 6.344, Val loss 6.397\n",
      "Ep 1 (Step 000040): Train loss 6.333, Val loss 6.266\n",
      "Ep 1 (Step 000050): Train loss 6.077, Val loss 6.092\n",
      "Ep 1 (Step 000060): Train loss 5.920, Val loss 6.006\n",
      "Ep 1 (Step 000070): Train loss 5.884, Val loss 5.934\n",
      "Ep 1 (Step 000080): Train loss 5.753, Val loss 5.832\n",
      "Ep 1 (Step 000090): Train loss 5.712, Val loss 5.773\n",
      "Ep 1 (Step 000100): Train loss 5.681, Val loss 5.706\n",
      "Ep 1 (Step 000110): Train loss 5.534, Val loss 5.660\n",
      "Ep 1 (Step 000120): Train loss 5.522, Val loss 5.609\n",
      "Ep 1 (Step 000130): Train loss 5.530, Val loss 5.604\n",
      "Ep 1 (Step 000140): Train loss 5.449, Val loss 5.574\n",
      "Ep 1 (Step 000150): Train loss 5.395, Val loss 5.526\n",
      "Ep 1 (Step 000160): Train loss 5.476, Val loss 5.489\n",
      "Ep 1 (Step 000170): Train loss 5.388, Val loss 5.478\n",
      "Ep 1 (Step 000180): Train loss 5.330, Val loss 5.459\n",
      "Ep 1 (Step 000190): Train loss 5.362, Val loss 5.430\n",
      "Ep 1 (Step 000200): Train loss 5.334, Val loss 5.426\n",
      "Ep 1 (Step 000210): Train loss 5.283, Val loss 5.422\n",
      "Ep 1 (Step 000220): Train loss 5.255, Val loss 5.382\n",
      "Ep 1 (Step 000230): Train loss 5.232, Val loss 5.352\n",
      "Ep 1 (Step 000240): Train loss 5.235, Val loss 5.319\n",
      "Ep 1 (Step 000250): Train loss 5.213, Val loss 5.324\n",
      "Ep 1 (Step 000260): Train loss 5.259, Val loss 5.306\n",
      "Ep 1 (Step 000270): Train loss 5.242, Val loss 5.281\n",
      "Ep 1 (Step 000280): Train loss 5.193, Val loss 5.278\n",
      "Ep 1 (Step 000290): Train loss 5.164, Val loss 5.296\n",
      "Ep 1 (Step 000300): Train loss 5.133, Val loss 5.270\n",
      "Ep 1 (Step 000310): Train loss 5.147, Val loss 5.277\n",
      "Ep 1 (Step 000320): Train loss 5.016, Val loss 5.253\n",
      "Ep 1 (Step 000330): Train loss 5.084, Val loss 5.243\n",
      "Ep 1 (Step 000340): Train loss 5.082, Val loss 5.227\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2271\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.546, Val loss 8.536\n",
      "Ep 1 (Step 000010): Train loss 6.830, Val loss 6.804\n",
      "Ep 1 (Step 000020): Train loss 6.544, Val loss 6.487\n",
      "Ep 1 (Step 000030): Train loss 6.407, Val loss 6.420\n",
      "Ep 1 (Step 000040): Train loss 6.298, Val loss 6.266\n",
      "Ep 1 (Step 000050): Train loss 6.087, Val loss 6.094\n",
      "Ep 1 (Step 000060): Train loss 6.017, Val loss 6.017\n",
      "Ep 1 (Step 000070): Train loss 5.892, Val loss 5.895\n",
      "Ep 1 (Step 000080): Train loss 5.778, Val loss 5.807\n",
      "Ep 1 (Step 000090): Train loss 5.732, Val loss 5.751\n",
      "Ep 1 (Step 000100): Train loss 5.626, Val loss 5.696\n",
      "Ep 1 (Step 000110): Train loss 5.584, Val loss 5.653\n",
      "Ep 1 (Step 000120): Train loss 5.521, Val loss 5.590\n",
      "Ep 1 (Step 000130): Train loss 5.462, Val loss 5.558\n",
      "Ep 1 (Step 000140): Train loss 5.522, Val loss 5.534\n",
      "Ep 1 (Step 000150): Train loss 5.458, Val loss 5.503\n",
      "Ep 1 (Step 000160): Train loss 5.447, Val loss 5.492\n",
      "Ep 1 (Step 000170): Train loss 5.387, Val loss 5.467\n",
      "Ep 1 (Step 000180): Train loss 5.264, Val loss 5.456\n",
      "Ep 1 (Step 000190): Train loss 5.328, Val loss 5.441\n",
      "Ep 1 (Step 000200): Train loss 5.341, Val loss 5.418\n",
      "Ep 1 (Step 000210): Train loss 5.217, Val loss 5.407\n",
      "Ep 1 (Step 000220): Train loss 5.259, Val loss 5.386\n",
      "Ep 1 (Step 000230): Train loss 5.323, Val loss 5.369\n",
      "Ep 1 (Step 000240): Train loss 5.288, Val loss 5.336\n",
      "Ep 1 (Step 000250): Train loss 5.194, Val loss 5.344\n",
      "Ep 1 (Step 000260): Train loss 5.233, Val loss 5.312\n",
      "Ep 1 (Step 000270): Train loss 5.213, Val loss 5.316\n",
      "Ep 1 (Step 000280): Train loss 5.146, Val loss 5.298\n",
      "Ep 1 (Step 000290): Train loss 5.182, Val loss 5.277\n",
      "Ep 1 (Step 000300): Train loss 5.221, Val loss 5.262\n",
      "Ep 1 (Step 000310): Train loss 5.154, Val loss 5.257\n",
      "Ep 1 (Step 000320): Train loss 5.081, Val loss 5.254\n",
      "Ep 1 (Step 000330): Train loss 5.121, Val loss 5.235\n",
      "Ep 1 (Step 000340): Train loss 5.101, Val loss 5.222\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2218\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.485, Val loss 8.485\n",
      "Ep 1 (Step 000010): Train loss 6.836, Val loss 6.812\n",
      "Ep 1 (Step 000020): Train loss 6.476, Val loss 6.440\n",
      "Ep 1 (Step 000030): Train loss 6.392, Val loss 6.362\n",
      "Ep 1 (Step 000040): Train loss 6.197, Val loss 6.223\n",
      "Ep 1 (Step 000050): Train loss 6.188, Val loss 6.107\n",
      "Ep 1 (Step 000060): Train loss 5.911, Val loss 5.985\n",
      "Ep 1 (Step 000070): Train loss 5.911, Val loss 5.881\n",
      "Ep 1 (Step 000080): Train loss 5.820, Val loss 5.833\n",
      "Ep 1 (Step 000090): Train loss 5.708, Val loss 5.777\n",
      "Ep 1 (Step 000100): Train loss 5.603, Val loss 5.723\n",
      "Ep 1 (Step 000110): Train loss 5.699, Val loss 5.663\n",
      "Ep 1 (Step 000120): Train loss 5.525, Val loss 5.623\n",
      "Ep 1 (Step 000130): Train loss 5.595, Val loss 5.581\n",
      "Ep 1 (Step 000140): Train loss 5.526, Val loss 5.549\n",
      "Ep 1 (Step 000150): Train loss 5.399, Val loss 5.515\n",
      "Ep 1 (Step 000160): Train loss 5.427, Val loss 5.495\n",
      "Ep 1 (Step 000170): Train loss 5.421, Val loss 5.470\n",
      "Ep 1 (Step 000180): Train loss 5.386, Val loss 5.487\n",
      "Ep 1 (Step 000190): Train loss 5.408, Val loss 5.449\n",
      "Ep 1 (Step 000200): Train loss 5.421, Val loss 5.419\n",
      "Ep 1 (Step 000210): Train loss 5.272, Val loss 5.390\n",
      "Ep 1 (Step 000220): Train loss 5.275, Val loss 5.368\n",
      "Ep 1 (Step 000230): Train loss 5.267, Val loss 5.348\n",
      "Ep 1 (Step 000240): Train loss 5.200, Val loss 5.325\n",
      "Ep 1 (Step 000250): Train loss 5.164, Val loss 5.326\n",
      "Ep 1 (Step 000260): Train loss 5.208, Val loss 5.317\n",
      "Ep 1 (Step 000270): Train loss 5.207, Val loss 5.309\n",
      "Ep 1 (Step 000280): Train loss 5.273, Val loss 5.302\n",
      "Ep 1 (Step 000290): Train loss 5.194, Val loss 5.288\n",
      "Ep 1 (Step 000300): Train loss 5.152, Val loss 5.270\n",
      "Ep 1 (Step 000310): Train loss 5.224, Val loss 5.287\n",
      "Ep 1 (Step 000320): Train loss 5.158, Val loss 5.278\n",
      "Ep 1 (Step 000330): Train loss 5.046, Val loss 5.240\n",
      "Ep 1 (Step 000340): Train loss 5.058, Val loss 5.225\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2245\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.157, Val loss 9.134\n",
      "Ep 1 (Step 000010): Train loss 7.521, Val loss 7.531\n",
      "Ep 1 (Step 000020): Train loss 6.875, Val loss 6.840\n",
      "Ep 1 (Step 000030): Train loss 6.514, Val loss 6.501\n",
      "Ep 1 (Step 000040): Train loss 6.441, Val loss 6.399\n",
      "Ep 1 (Step 000050): Train loss 6.431, Val loss 6.376\n",
      "Ep 1 (Step 000060): Train loss 6.377, Val loss 6.340\n",
      "Ep 1 (Step 000070): Train loss 6.246, Val loss 6.232\n",
      "Ep 1 (Step 000080): Train loss 6.052, Val loss 6.141\n",
      "Ep 1 (Step 000090): Train loss 6.040, Val loss 6.062\n",
      "Ep 1 (Step 000100): Train loss 6.014, Val loss 5.994\n",
      "Ep 1 (Step 000110): Train loss 5.856, Val loss 5.946\n",
      "Ep 1 (Step 000120): Train loss 5.942, Val loss 5.883\n",
      "Ep 1 (Step 000130): Train loss 5.850, Val loss 5.835\n",
      "Ep 1 (Step 000140): Train loss 5.787, Val loss 5.808\n",
      "Ep 1 (Step 000150): Train loss 5.718, Val loss 5.755\n",
      "Ep 1 (Step 000160): Train loss 5.704, Val loss 5.737\n",
      "Ep 1 (Step 000170): Train loss 5.682, Val loss 5.702\n",
      "Ep 1 (Step 000180): Train loss 5.718, Val loss 5.679\n",
      "Ep 1 (Step 000190): Train loss 5.675, Val loss 5.647\n",
      "Ep 1 (Step 000200): Train loss 5.503, Val loss 5.630\n",
      "Ep 1 (Step 000210): Train loss 5.546, Val loss 5.595\n",
      "Ep 1 (Step 000220): Train loss 5.487, Val loss 5.580\n",
      "Ep 1 (Step 000230): Train loss 5.410, Val loss 5.553\n",
      "Ep 1 (Step 000240): Train loss 5.446, Val loss 5.558\n",
      "Ep 1 (Step 000250): Train loss 5.489, Val loss 5.531\n",
      "Ep 1 (Step 000260): Train loss 5.311, Val loss 5.510\n",
      "Ep 1 (Step 000270): Train loss 5.381, Val loss 5.494\n",
      "Ep 1 (Step 000280): Train loss 5.391, Val loss 5.486\n",
      "Ep 1 (Step 000290): Train loss 5.377, Val loss 5.468\n",
      "Ep 1 (Step 000300): Train loss 5.370, Val loss 5.447\n",
      "Ep 1 (Step 000310): Train loss 5.343, Val loss 5.449\n",
      "Ep 1 (Step 000320): Train loss 5.415, Val loss 5.435\n",
      "Ep 1 (Step 000330): Train loss 5.306, Val loss 5.429\n",
      "Ep 1 (Step 000340): Train loss 5.367, Val loss 5.408\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.4084\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.164, Val loss 9.151\n",
      "Ep 1 (Step 000010): Train loss 7.506, Val loss 7.522\n",
      "Ep 1 (Step 000020): Train loss 6.881, Val loss 6.864\n",
      "Ep 1 (Step 000030): Train loss 6.510, Val loss 6.536\n",
      "Ep 1 (Step 000040): Train loss 6.398, Val loss 6.414\n",
      "Ep 1 (Step 000050): Train loss 6.368, Val loss 6.405\n",
      "Ep 1 (Step 000060): Train loss 6.381, Val loss 6.368\n",
      "Ep 1 (Step 000070): Train loss 6.282, Val loss 6.286\n",
      "Ep 1 (Step 000080): Train loss 6.052, Val loss 6.179\n",
      "Ep 1 (Step 000090): Train loss 6.088, Val loss 6.112\n",
      "Ep 1 (Step 000100): Train loss 5.932, Val loss 6.029\n",
      "Ep 1 (Step 000110): Train loss 5.985, Val loss 5.977\n",
      "Ep 1 (Step 000120): Train loss 5.862, Val loss 5.921\n",
      "Ep 1 (Step 000130): Train loss 5.788, Val loss 5.864\n",
      "Ep 1 (Step 000140): Train loss 5.886, Val loss 5.834\n",
      "Ep 1 (Step 000150): Train loss 5.760, Val loss 5.794\n",
      "Ep 1 (Step 000160): Train loss 5.676, Val loss 5.749\n",
      "Ep 1 (Step 000170): Train loss 5.659, Val loss 5.717\n",
      "Ep 1 (Step 000180): Train loss 5.590, Val loss 5.690\n",
      "Ep 1 (Step 000190): Train loss 5.521, Val loss 5.667\n",
      "Ep 1 (Step 000200): Train loss 5.599, Val loss 5.637\n",
      "Ep 1 (Step 000210): Train loss 5.574, Val loss 5.618\n",
      "Ep 1 (Step 000220): Train loss 5.518, Val loss 5.601\n",
      "Ep 1 (Step 000230): Train loss 5.591, Val loss 5.577\n",
      "Ep 1 (Step 000240): Train loss 5.449, Val loss 5.548\n",
      "Ep 1 (Step 000250): Train loss 5.444, Val loss 5.546\n",
      "Ep 1 (Step 000260): Train loss 5.394, Val loss 5.513\n",
      "Ep 1 (Step 000270): Train loss 5.416, Val loss 5.492\n",
      "Ep 1 (Step 000280): Train loss 5.343, Val loss 5.482\n",
      "Ep 1 (Step 000290): Train loss 5.416, Val loss 5.480\n",
      "Ep 1 (Step 000300): Train loss 5.340, Val loss 5.448\n",
      "Ep 1 (Step 000310): Train loss 5.315, Val loss 5.457\n",
      "Ep 1 (Step 000320): Train loss 5.360, Val loss 5.437\n",
      "Ep 1 (Step 000330): Train loss 5.344, Val loss 5.426\n",
      "Ep 1 (Step 000340): Train loss 5.359, Val loss 5.400\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.4003\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.159, Val loss 9.127\n",
      "Ep 1 (Step 000010): Train loss 7.533, Val loss 7.499\n",
      "Ep 1 (Step 000020): Train loss 6.857, Val loss 6.812\n",
      "Ep 1 (Step 000030): Train loss 6.501, Val loss 6.489\n",
      "Ep 1 (Step 000040): Train loss 6.418, Val loss 6.405\n",
      "Ep 1 (Step 000050): Train loss 6.424, Val loss 6.381\n",
      "Ep 1 (Step 000060): Train loss 6.369, Val loss 6.345\n",
      "Ep 1 (Step 000070): Train loss 6.251, Val loss 6.271\n",
      "Ep 1 (Step 000080): Train loss 6.146, Val loss 6.167\n",
      "Ep 1 (Step 000090): Train loss 6.102, Val loss 6.079\n",
      "Ep 1 (Step 000100): Train loss 5.957, Val loss 6.013\n",
      "Ep 1 (Step 000110): Train loss 5.983, Val loss 5.988\n",
      "Ep 1 (Step 000120): Train loss 5.875, Val loss 5.906\n",
      "Ep 1 (Step 000130): Train loss 5.855, Val loss 5.855\n",
      "Ep 1 (Step 000140): Train loss 5.801, Val loss 5.814\n",
      "Ep 1 (Step 000150): Train loss 5.779, Val loss 5.782\n",
      "Ep 1 (Step 000160): Train loss 5.740, Val loss 5.741\n",
      "Ep 1 (Step 000170): Train loss 5.618, Val loss 5.719\n",
      "Ep 1 (Step 000180): Train loss 5.661, Val loss 5.690\n",
      "Ep 1 (Step 000190): Train loss 5.609, Val loss 5.667\n",
      "Ep 1 (Step 000200): Train loss 5.616, Val loss 5.629\n",
      "Ep 1 (Step 000210): Train loss 5.487, Val loss 5.613\n",
      "Ep 1 (Step 000220): Train loss 5.492, Val loss 5.593\n",
      "Ep 1 (Step 000230): Train loss 5.535, Val loss 5.576\n",
      "Ep 1 (Step 000240): Train loss 5.493, Val loss 5.559\n",
      "Ep 1 (Step 000250): Train loss 5.394, Val loss 5.531\n",
      "Ep 1 (Step 000260): Train loss 5.353, Val loss 5.525\n",
      "Ep 1 (Step 000270): Train loss 5.380, Val loss 5.502\n",
      "Ep 1 (Step 000280): Train loss 5.380, Val loss 5.480\n",
      "Ep 1 (Step 000290): Train loss 5.355, Val loss 5.475\n",
      "Ep 1 (Step 000300): Train loss 5.351, Val loss 5.472\n",
      "Ep 1 (Step 000310): Train loss 5.419, Val loss 5.452\n",
      "Ep 1 (Step 000320): Train loss 5.296, Val loss 5.454\n",
      "Ep 1 (Step 000330): Train loss 5.263, Val loss 5.427\n",
      "Ep 1 (Step 000340): Train loss 5.316, Val loss 5.428\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.4281\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.178, Val loss 9.180\n",
      "Ep 1 (Step 000010): Train loss 7.583, Val loss 7.568\n",
      "Ep 1 (Step 000020): Train loss 6.877, Val loss 6.871\n",
      "Ep 1 (Step 000030): Train loss 6.524, Val loss 6.502\n",
      "Ep 1 (Step 000040): Train loss 6.429, Val loss 6.396\n",
      "Ep 1 (Step 000050): Train loss 6.390, Val loss 6.379\n",
      "Ep 1 (Step 000060): Train loss 6.355, Val loss 6.339\n",
      "Ep 1 (Step 000070): Train loss 6.283, Val loss 6.275\n",
      "Ep 1 (Step 000080): Train loss 6.214, Val loss 6.177\n",
      "Ep 1 (Step 000090): Train loss 5.990, Val loss 6.097\n",
      "Ep 1 (Step 000100): Train loss 5.986, Val loss 6.012\n",
      "Ep 1 (Step 000110): Train loss 5.916, Val loss 5.956\n",
      "Ep 1 (Step 000120): Train loss 5.883, Val loss 5.920\n",
      "Ep 1 (Step 000130): Train loss 5.808, Val loss 5.853\n",
      "Ep 1 (Step 000140): Train loss 5.729, Val loss 5.816\n",
      "Ep 1 (Step 000150): Train loss 5.760, Val loss 5.764\n",
      "Ep 1 (Step 000160): Train loss 5.707, Val loss 5.734\n",
      "Ep 1 (Step 000170): Train loss 5.705, Val loss 5.704\n",
      "Ep 1 (Step 000180): Train loss 5.605, Val loss 5.681\n",
      "Ep 1 (Step 000190): Train loss 5.571, Val loss 5.649\n",
      "Ep 1 (Step 000200): Train loss 5.528, Val loss 5.629\n",
      "Ep 1 (Step 000210): Train loss 5.536, Val loss 5.596\n",
      "Ep 1 (Step 000220): Train loss 5.535, Val loss 5.569\n",
      "Ep 1 (Step 000230): Train loss 5.541, Val loss 5.559\n",
      "Ep 1 (Step 000240): Train loss 5.388, Val loss 5.537\n",
      "Ep 1 (Step 000250): Train loss 5.482, Val loss 5.521\n",
      "Ep 1 (Step 000260): Train loss 5.367, Val loss 5.510\n",
      "Ep 1 (Step 000270): Train loss 5.415, Val loss 5.478\n",
      "Ep 1 (Step 000280): Train loss 5.454, Val loss 5.465\n",
      "Ep 1 (Step 000290): Train loss 5.388, Val loss 5.452\n",
      "Ep 1 (Step 000300): Train loss 5.309, Val loss 5.435\n",
      "Ep 1 (Step 000310): Train loss 5.355, Val loss 5.435\n",
      "Ep 1 (Step 000320): Train loss 5.242, Val loss 5.413\n",
      "Ep 1 (Step 000330): Train loss 5.329, Val loss 5.403\n",
      "Ep 1 (Step 000340): Train loss 5.325, Val loss 5.391\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3910\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.134, Val loss 9.133\n",
      "Ep 1 (Step 000010): Train loss 7.554, Val loss 7.493\n",
      "Ep 1 (Step 000020): Train loss 6.865, Val loss 6.832\n",
      "Ep 1 (Step 000030): Train loss 6.521, Val loss 6.507\n",
      "Ep 1 (Step 000040): Train loss 6.421, Val loss 6.412\n",
      "Ep 1 (Step 000050): Train loss 6.421, Val loss 6.372\n",
      "Ep 1 (Step 000060): Train loss 6.361, Val loss 6.361\n",
      "Ep 1 (Step 000070): Train loss 6.280, Val loss 6.291\n",
      "Ep 1 (Step 000080): Train loss 6.180, Val loss 6.185\n",
      "Ep 1 (Step 000090): Train loss 6.132, Val loss 6.112\n",
      "Ep 1 (Step 000100): Train loss 5.963, Val loss 6.033\n",
      "Ep 1 (Step 000110): Train loss 5.976, Val loss 5.977\n",
      "Ep 1 (Step 000120): Train loss 5.945, Val loss 5.929\n",
      "Ep 1 (Step 000130): Train loss 5.898, Val loss 5.891\n",
      "Ep 1 (Step 000140): Train loss 5.814, Val loss 5.833\n",
      "Ep 1 (Step 000150): Train loss 5.781, Val loss 5.804\n",
      "Ep 1 (Step 000160): Train loss 5.681, Val loss 5.757\n",
      "Ep 1 (Step 000170): Train loss 5.653, Val loss 5.721\n",
      "Ep 1 (Step 000180): Train loss 5.625, Val loss 5.692\n",
      "Ep 1 (Step 000190): Train loss 5.593, Val loss 5.666\n",
      "Ep 1 (Step 000200): Train loss 5.533, Val loss 5.634\n",
      "Ep 1 (Step 000210): Train loss 5.585, Val loss 5.632\n",
      "Ep 1 (Step 000220): Train loss 5.582, Val loss 5.593\n",
      "Ep 1 (Step 000230): Train loss 5.566, Val loss 5.568\n",
      "Ep 1 (Step 000240): Train loss 5.444, Val loss 5.547\n",
      "Ep 1 (Step 000250): Train loss 5.473, Val loss 5.530\n",
      "Ep 1 (Step 000260): Train loss 5.400, Val loss 5.512\n",
      "Ep 1 (Step 000270): Train loss 5.393, Val loss 5.489\n",
      "Ep 1 (Step 000280): Train loss 5.397, Val loss 5.466\n",
      "Ep 1 (Step 000290): Train loss 5.438, Val loss 5.454\n",
      "Ep 1 (Step 000300): Train loss 5.433, Val loss 5.453\n",
      "Ep 1 (Step 000310): Train loss 5.356, Val loss 5.430\n",
      "Ep 1 (Step 000320): Train loss 5.301, Val loss 5.414\n",
      "Ep 1 (Step 000330): Train loss 5.298, Val loss 5.392\n",
      "Ep 1 (Step 000340): Train loss 5.280, Val loss 5.375\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3752\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.123, Val loss 9.122\n",
      "Ep 1 (Step 000010): Train loss 7.502, Val loss 7.475\n",
      "Ep 1 (Step 000020): Train loss 6.869, Val loss 6.817\n",
      "Ep 1 (Step 000030): Train loss 6.502, Val loss 6.503\n",
      "Ep 1 (Step 000040): Train loss 6.413, Val loss 6.409\n",
      "Ep 1 (Step 000050): Train loss 6.435, Val loss 6.381\n",
      "Ep 1 (Step 000060): Train loss 6.251, Val loss 6.335\n",
      "Ep 1 (Step 000070): Train loss 6.292, Val loss 6.282\n",
      "Ep 1 (Step 000080): Train loss 6.205, Val loss 6.176\n",
      "Ep 1 (Step 000090): Train loss 6.103, Val loss 6.086\n",
      "Ep 1 (Step 000100): Train loss 6.081, Val loss 6.050\n",
      "Ep 1 (Step 000110): Train loss 6.004, Val loss 5.976\n",
      "Ep 1 (Step 000120): Train loss 5.868, Val loss 5.926\n",
      "Ep 1 (Step 000130): Train loss 5.783, Val loss 5.879\n",
      "Ep 1 (Step 000140): Train loss 5.780, Val loss 5.832\n",
      "Ep 1 (Step 000150): Train loss 5.735, Val loss 5.791\n",
      "Ep 1 (Step 000160): Train loss 5.678, Val loss 5.753\n",
      "Ep 1 (Step 000170): Train loss 5.755, Val loss 5.707\n",
      "Ep 1 (Step 000180): Train loss 5.650, Val loss 5.682\n",
      "Ep 1 (Step 000190): Train loss 5.556, Val loss 5.639\n",
      "Ep 1 (Step 000200): Train loss 5.598, Val loss 5.613\n",
      "Ep 1 (Step 000210): Train loss 5.501, Val loss 5.603\n",
      "Ep 1 (Step 000220): Train loss 5.518, Val loss 5.565\n",
      "Ep 1 (Step 000230): Train loss 5.453, Val loss 5.546\n",
      "Ep 1 (Step 000240): Train loss 5.464, Val loss 5.531\n",
      "Ep 1 (Step 000250): Train loss 5.348, Val loss 5.522\n",
      "Ep 1 (Step 000260): Train loss 5.383, Val loss 5.492\n",
      "Ep 1 (Step 000270): Train loss 5.381, Val loss 5.470\n",
      "Ep 1 (Step 000280): Train loss 5.421, Val loss 5.454\n",
      "Ep 1 (Step 000290): Train loss 5.359, Val loss 5.451\n",
      "Ep 1 (Step 000300): Train loss 5.302, Val loss 5.437\n",
      "Ep 1 (Step 000310): Train loss 5.288, Val loss 5.425\n",
      "Ep 1 (Step 000320): Train loss 5.372, Val loss 5.424\n",
      "Ep 1 (Step 000330): Train loss 5.274, Val loss 5.407\n",
      "Ep 1 (Step 000340): Train loss 5.311, Val loss 5.399\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3990\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.928, Val loss 8.918\n",
      "Ep 1 (Step 000010): Train loss 6.898, Val loss 6.881\n",
      "Ep 1 (Step 000020): Train loss 6.509, Val loss 6.484\n",
      "Ep 1 (Step 000030): Train loss 6.502, Val loss 6.491\n",
      "Ep 1 (Step 000040): Train loss 6.403, Val loss 6.398\n",
      "Ep 1 (Step 000050): Train loss 6.260, Val loss 6.251\n",
      "Ep 1 (Step 000060): Train loss 6.137, Val loss 6.125\n",
      "Ep 1 (Step 000070): Train loss 5.979, Val loss 6.017\n",
      "Ep 1 (Step 000080): Train loss 5.896, Val loss 5.957\n",
      "Ep 1 (Step 000090): Train loss 5.782, Val loss 5.870\n",
      "Ep 1 (Step 000100): Train loss 5.726, Val loss 5.804\n",
      "Ep 1 (Step 000110): Train loss 5.685, Val loss 5.742\n",
      "Ep 1 (Step 000120): Train loss 5.586, Val loss 5.710\n",
      "Ep 1 (Step 000130): Train loss 5.616, Val loss 5.670\n",
      "Ep 1 (Step 000140): Train loss 5.525, Val loss 5.621\n",
      "Ep 1 (Step 000150): Train loss 5.507, Val loss 5.591\n",
      "Ep 1 (Step 000160): Train loss 5.371, Val loss 5.544\n",
      "Ep 1 (Step 000170): Train loss 5.360, Val loss 5.529\n",
      "Ep 1 (Step 000180): Train loss 5.325, Val loss 5.509\n",
      "Ep 1 (Step 000190): Train loss 5.342, Val loss 5.473\n",
      "Ep 1 (Step 000200): Train loss 5.349, Val loss 5.454\n",
      "Ep 1 (Step 000210): Train loss 5.291, Val loss 5.445\n",
      "Ep 1 (Step 000220): Train loss 5.274, Val loss 5.417\n",
      "Ep 1 (Step 000230): Train loss 5.277, Val loss 5.407\n",
      "Ep 1 (Step 000240): Train loss 5.238, Val loss 5.379\n",
      "Ep 1 (Step 000250): Train loss 5.285, Val loss 5.387\n",
      "Ep 1 (Step 000260): Train loss 5.293, Val loss 5.382\n",
      "Ep 1 (Step 000270): Train loss 5.257, Val loss 5.369\n",
      "Ep 1 (Step 000280): Train loss 5.250, Val loss 5.347\n",
      "Ep 1 (Step 000290): Train loss 5.213, Val loss 5.332\n",
      "Ep 1 (Step 000300): Train loss 5.221, Val loss 5.306\n",
      "Ep 1 (Step 000310): Train loss 5.182, Val loss 5.308\n",
      "Ep 1 (Step 000320): Train loss 5.127, Val loss 5.308\n",
      "Ep 1 (Step 000330): Train loss 5.192, Val loss 5.279\n",
      "Ep 1 (Step 000340): Train loss 5.153, Val loss 5.290\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2896\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.959, Val loss 8.943\n",
      "Ep 1 (Step 000010): Train loss 6.855, Val loss 6.854\n",
      "Ep 1 (Step 000020): Train loss 6.487, Val loss 6.484\n",
      "Ep 1 (Step 000030): Train loss 6.442, Val loss 6.485\n",
      "Ep 1 (Step 000040): Train loss 6.394, Val loss 6.418\n",
      "Ep 1 (Step 000050): Train loss 6.283, Val loss 6.284\n",
      "Ep 1 (Step 000060): Train loss 6.151, Val loss 6.139\n",
      "Ep 1 (Step 000070): Train loss 6.062, Val loss 6.044\n",
      "Ep 1 (Step 000080): Train loss 5.858, Val loss 5.941\n",
      "Ep 1 (Step 000090): Train loss 5.805, Val loss 5.871\n",
      "Ep 1 (Step 000100): Train loss 5.800, Val loss 5.814\n",
      "Ep 1 (Step 000110): Train loss 5.740, Val loss 5.754\n",
      "Ep 1 (Step 000120): Train loss 5.624, Val loss 5.689\n",
      "Ep 1 (Step 000130): Train loss 5.596, Val loss 5.648\n",
      "Ep 1 (Step 000140): Train loss 5.509, Val loss 5.602\n",
      "Ep 1 (Step 000150): Train loss 5.457, Val loss 5.595\n",
      "Ep 1 (Step 000160): Train loss 5.411, Val loss 5.536\n",
      "Ep 1 (Step 000170): Train loss 5.412, Val loss 5.543\n",
      "Ep 1 (Step 000180): Train loss 5.391, Val loss 5.506\n",
      "Ep 1 (Step 000190): Train loss 5.387, Val loss 5.474\n",
      "Ep 1 (Step 000200): Train loss 5.330, Val loss 5.463\n",
      "Ep 1 (Step 000210): Train loss 5.396, Val loss 5.450\n",
      "Ep 1 (Step 000220): Train loss 5.247, Val loss 5.433\n",
      "Ep 1 (Step 000230): Train loss 5.304, Val loss 5.411\n",
      "Ep 1 (Step 000240): Train loss 5.361, Val loss 5.388\n",
      "Ep 1 (Step 000250): Train loss 5.270, Val loss 5.380\n",
      "Ep 1 (Step 000260): Train loss 5.261, Val loss 5.367\n",
      "Ep 1 (Step 000270): Train loss 5.231, Val loss 5.341\n",
      "Ep 1 (Step 000280): Train loss 5.211, Val loss 5.333\n",
      "Ep 1 (Step 000290): Train loss 5.132, Val loss 5.346\n",
      "Ep 1 (Step 000300): Train loss 5.134, Val loss 5.312\n",
      "Ep 1 (Step 000310): Train loss 5.166, Val loss 5.296\n",
      "Ep 1 (Step 000320): Train loss 5.183, Val loss 5.286\n",
      "Ep 1 (Step 000330): Train loss 5.088, Val loss 5.278\n",
      "Ep 1 (Step 000340): Train loss 5.073, Val loss 5.286\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2862\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.964, Val loss 8.940\n",
      "Ep 1 (Step 000010): Train loss 6.869, Val loss 6.833\n",
      "Ep 1 (Step 000020): Train loss 6.522, Val loss 6.453\n",
      "Ep 1 (Step 000030): Train loss 6.493, Val loss 6.487\n",
      "Ep 1 (Step 000040): Train loss 6.414, Val loss 6.396\n",
      "Ep 1 (Step 000050): Train loss 6.320, Val loss 6.278\n",
      "Ep 1 (Step 000060): Train loss 6.165, Val loss 6.157\n",
      "Ep 1 (Step 000070): Train loss 6.063, Val loss 6.044\n",
      "Ep 1 (Step 000080): Train loss 5.905, Val loss 5.964\n",
      "Ep 1 (Step 000090): Train loss 5.829, Val loss 5.876\n",
      "Ep 1 (Step 000100): Train loss 5.786, Val loss 5.803\n",
      "Ep 1 (Step 000110): Train loss 5.715, Val loss 5.741\n",
      "Ep 1 (Step 000120): Train loss 5.698, Val loss 5.688\n",
      "Ep 1 (Step 000130): Train loss 5.642, Val loss 5.641\n",
      "Ep 1 (Step 000140): Train loss 5.686, Val loss 5.596\n",
      "Ep 1 (Step 000150): Train loss 5.496, Val loss 5.585\n",
      "Ep 1 (Step 000160): Train loss 5.419, Val loss 5.526\n",
      "Ep 1 (Step 000170): Train loss 5.434, Val loss 5.512\n",
      "Ep 1 (Step 000180): Train loss 5.397, Val loss 5.494\n",
      "Ep 1 (Step 000190): Train loss 5.364, Val loss 5.479\n",
      "Ep 1 (Step 000200): Train loss 5.414, Val loss 5.447\n",
      "Ep 1 (Step 000210): Train loss 5.356, Val loss 5.426\n",
      "Ep 1 (Step 000220): Train loss 5.305, Val loss 5.409\n",
      "Ep 1 (Step 000230): Train loss 5.343, Val loss 5.394\n",
      "Ep 1 (Step 000240): Train loss 5.222, Val loss 5.391\n",
      "Ep 1 (Step 000250): Train loss 5.225, Val loss 5.375\n",
      "Ep 1 (Step 000260): Train loss 5.153, Val loss 5.364\n",
      "Ep 1 (Step 000270): Train loss 5.185, Val loss 5.365\n",
      "Ep 1 (Step 000280): Train loss 5.207, Val loss 5.354\n",
      "Ep 1 (Step 000290): Train loss 5.168, Val loss 5.330\n",
      "Ep 1 (Step 000300): Train loss 5.127, Val loss 5.327\n",
      "Ep 1 (Step 000310): Train loss 5.295, Val loss 5.333\n",
      "Ep 1 (Step 000320): Train loss 5.050, Val loss 5.299\n",
      "Ep 1 (Step 000330): Train loss 5.156, Val loss 5.305\n",
      "Ep 1 (Step 000340): Train loss 5.134, Val loss 5.305\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3050\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.953, Val loss 8.925\n",
      "Ep 1 (Step 000010): Train loss 6.852, Val loss 6.897\n",
      "Ep 1 (Step 000020): Train loss 6.534, Val loss 6.507\n",
      "Ep 1 (Step 000030): Train loss 6.457, Val loss 6.482\n",
      "Ep 1 (Step 000040): Train loss 6.430, Val loss 6.396\n",
      "Ep 1 (Step 000050): Train loss 6.319, Val loss 6.301\n",
      "Ep 1 (Step 000060): Train loss 6.153, Val loss 6.202\n",
      "Ep 1 (Step 000070): Train loss 6.022, Val loss 6.075\n",
      "Ep 1 (Step 000080): Train loss 5.936, Val loss 5.959\n",
      "Ep 1 (Step 000090): Train loss 5.836, Val loss 5.872\n",
      "Ep 1 (Step 000100): Train loss 5.718, Val loss 5.798\n",
      "Ep 1 (Step 000110): Train loss 5.710, Val loss 5.748\n",
      "Ep 1 (Step 000120): Train loss 5.609, Val loss 5.706\n",
      "Ep 1 (Step 000130): Train loss 5.580, Val loss 5.655\n",
      "Ep 1 (Step 000140): Train loss 5.555, Val loss 5.615\n",
      "Ep 1 (Step 000150): Train loss 5.439, Val loss 5.567\n",
      "Ep 1 (Step 000160): Train loss 5.385, Val loss 5.539\n",
      "Ep 1 (Step 000170): Train loss 5.434, Val loss 5.510\n",
      "Ep 1 (Step 000180): Train loss 5.393, Val loss 5.498\n",
      "Ep 1 (Step 000190): Train loss 5.447, Val loss 5.459\n",
      "Ep 1 (Step 000200): Train loss 5.349, Val loss 5.460\n",
      "Ep 1 (Step 000210): Train loss 5.326, Val loss 5.416\n",
      "Ep 1 (Step 000220): Train loss 5.266, Val loss 5.401\n",
      "Ep 1 (Step 000230): Train loss 5.338, Val loss 5.391\n",
      "Ep 1 (Step 000240): Train loss 5.303, Val loss 5.375\n",
      "Ep 1 (Step 000250): Train loss 5.268, Val loss 5.367\n",
      "Ep 1 (Step 000260): Train loss 5.344, Val loss 5.345\n",
      "Ep 1 (Step 000270): Train loss 5.179, Val loss 5.339\n",
      "Ep 1 (Step 000280): Train loss 5.199, Val loss 5.342\n",
      "Ep 1 (Step 000290): Train loss 5.099, Val loss 5.309\n",
      "Ep 1 (Step 000300): Train loss 5.141, Val loss 5.303\n",
      "Ep 1 (Step 000310): Train loss 5.160, Val loss 5.314\n",
      "Ep 1 (Step 000320): Train loss 5.215, Val loss 5.279\n",
      "Ep 1 (Step 000330): Train loss 5.182, Val loss 5.273\n",
      "Ep 1 (Step 000340): Train loss 5.087, Val loss 5.269\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2690\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.930, Val loss 8.917\n",
      "Ep 1 (Step 000010): Train loss 6.881, Val loss 6.856\n",
      "Ep 1 (Step 000020): Train loss 6.459, Val loss 6.466\n",
      "Ep 1 (Step 000030): Train loss 6.390, Val loss 6.463\n",
      "Ep 1 (Step 000040): Train loss 6.366, Val loss 6.400\n",
      "Ep 1 (Step 000050): Train loss 6.332, Val loss 6.270\n",
      "Ep 1 (Step 000060): Train loss 6.138, Val loss 6.166\n",
      "Ep 1 (Step 000070): Train loss 5.974, Val loss 6.033\n",
      "Ep 1 (Step 000080): Train loss 5.915, Val loss 5.964\n",
      "Ep 1 (Step 000090): Train loss 5.801, Val loss 5.877\n",
      "Ep 1 (Step 000100): Train loss 5.715, Val loss 5.820\n",
      "Ep 1 (Step 000110): Train loss 5.713, Val loss 5.747\n",
      "Ep 1 (Step 000120): Train loss 5.607, Val loss 5.702\n",
      "Ep 1 (Step 000130): Train loss 5.530, Val loss 5.639\n",
      "Ep 1 (Step 000140): Train loss 5.549, Val loss 5.602\n",
      "Ep 1 (Step 000150): Train loss 5.471, Val loss 5.544\n",
      "Ep 1 (Step 000160): Train loss 5.470, Val loss 5.520\n",
      "Ep 1 (Step 000170): Train loss 5.438, Val loss 5.512\n",
      "Ep 1 (Step 000180): Train loss 5.411, Val loss 5.487\n",
      "Ep 1 (Step 000190): Train loss 5.278, Val loss 5.446\n",
      "Ep 1 (Step 000200): Train loss 5.328, Val loss 5.439\n",
      "Ep 1 (Step 000210): Train loss 5.439, Val loss 5.450\n",
      "Ep 1 (Step 000220): Train loss 5.165, Val loss 5.405\n",
      "Ep 1 (Step 000230): Train loss 5.346, Val loss 5.372\n",
      "Ep 1 (Step 000240): Train loss 5.268, Val loss 5.369\n",
      "Ep 1 (Step 000250): Train loss 5.223, Val loss 5.377\n",
      "Ep 1 (Step 000260): Train loss 5.271, Val loss 5.346\n",
      "Ep 1 (Step 000270): Train loss 5.177, Val loss 5.342\n",
      "Ep 1 (Step 000280): Train loss 5.159, Val loss 5.324\n",
      "Ep 1 (Step 000290): Train loss 5.225, Val loss 5.310\n",
      "Ep 1 (Step 000300): Train loss 5.128, Val loss 5.287\n",
      "Ep 1 (Step 000310): Train loss 5.173, Val loss 5.268\n",
      "Ep 1 (Step 000320): Train loss 5.081, Val loss 5.262\n",
      "Ep 1 (Step 000330): Train loss 5.008, Val loss 5.253\n",
      "Ep 1 (Step 000340): Train loss 5.028, Val loss 5.249\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2495\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.932, Val loss 8.930\n",
      "Ep 1 (Step 000010): Train loss 6.854, Val loss 6.876\n",
      "Ep 1 (Step 000020): Train loss 6.453, Val loss 6.477\n",
      "Ep 1 (Step 000030): Train loss 6.471, Val loss 6.443\n",
      "Ep 1 (Step 000040): Train loss 6.389, Val loss 6.364\n",
      "Ep 1 (Step 000050): Train loss 6.253, Val loss 6.278\n",
      "Ep 1 (Step 000060): Train loss 6.073, Val loss 6.137\n",
      "Ep 1 (Step 000070): Train loss 6.002, Val loss 6.017\n",
      "Ep 1 (Step 000080): Train loss 5.908, Val loss 5.926\n",
      "Ep 1 (Step 000090): Train loss 5.809, Val loss 5.849\n",
      "Ep 1 (Step 000100): Train loss 5.658, Val loss 5.789\n",
      "Ep 1 (Step 000110): Train loss 5.628, Val loss 5.733\n",
      "Ep 1 (Step 000120): Train loss 5.610, Val loss 5.674\n",
      "Ep 1 (Step 000130): Train loss 5.565, Val loss 5.642\n",
      "Ep 1 (Step 000140): Train loss 5.471, Val loss 5.598\n",
      "Ep 1 (Step 000150): Train loss 5.492, Val loss 5.572\n",
      "Ep 1 (Step 000160): Train loss 5.472, Val loss 5.541\n",
      "Ep 1 (Step 000170): Train loss 5.465, Val loss 5.495\n",
      "Ep 1 (Step 000180): Train loss 5.355, Val loss 5.454\n",
      "Ep 1 (Step 000190): Train loss 5.356, Val loss 5.465\n",
      "Ep 1 (Step 000200): Train loss 5.324, Val loss 5.438\n",
      "Ep 1 (Step 000210): Train loss 5.319, Val loss 5.423\n",
      "Ep 1 (Step 000220): Train loss 5.280, Val loss 5.402\n",
      "Ep 1 (Step 000230): Train loss 5.302, Val loss 5.380\n",
      "Ep 1 (Step 000240): Train loss 5.272, Val loss 5.389\n",
      "Ep 1 (Step 000250): Train loss 5.275, Val loss 5.352\n",
      "Ep 1 (Step 000260): Train loss 5.173, Val loss 5.350\n",
      "Ep 1 (Step 000270): Train loss 5.232, Val loss 5.345\n",
      "Ep 1 (Step 000280): Train loss 5.206, Val loss 5.335\n",
      "Ep 1 (Step 000290): Train loss 5.172, Val loss 5.308\n",
      "Ep 1 (Step 000300): Train loss 5.223, Val loss 5.311\n",
      "Ep 1 (Step 000310): Train loss 5.163, Val loss 5.271\n",
      "Ep 1 (Step 000320): Train loss 5.119, Val loss 5.262\n",
      "Ep 1 (Step 000330): Train loss 5.098, Val loss 5.259\n",
      "Ep 1 (Step 000340): Train loss 5.113, Val loss 5.254\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2537\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.155, Val loss 9.157\n",
      "Ep 1 (Step 000010): Train loss 7.482, Val loss 7.518\n",
      "Ep 1 (Step 000020): Train loss 6.858, Val loss 6.851\n",
      "Ep 1 (Step 000030): Train loss 6.539, Val loss 6.517\n",
      "Ep 1 (Step 000040): Train loss 6.391, Val loss 6.413\n",
      "Ep 1 (Step 000050): Train loss 6.395, Val loss 6.393\n",
      "Ep 1 (Step 000060): Train loss 6.351, Val loss 6.355\n",
      "Ep 1 (Step 000070): Train loss 6.298, Val loss 6.276\n",
      "Ep 1 (Step 000080): Train loss 6.223, Val loss 6.175\n",
      "Ep 1 (Step 000090): Train loss 6.076, Val loss 6.079\n",
      "Ep 1 (Step 000100): Train loss 5.998, Val loss 6.027\n",
      "Ep 1 (Step 000110): Train loss 5.955, Val loss 5.974\n",
      "Ep 1 (Step 000120): Train loss 5.883, Val loss 5.911\n",
      "Ep 1 (Step 000130): Train loss 5.863, Val loss 5.852\n",
      "Ep 1 (Step 000140): Train loss 5.769, Val loss 5.833\n",
      "Ep 1 (Step 000150): Train loss 5.809, Val loss 5.785\n",
      "Ep 1 (Step 000160): Train loss 5.713, Val loss 5.755\n",
      "Ep 1 (Step 000170): Train loss 5.646, Val loss 5.740\n",
      "Ep 1 (Step 000180): Train loss 5.648, Val loss 5.699\n",
      "Ep 1 (Step 000190): Train loss 5.592, Val loss 5.663\n",
      "Ep 1 (Step 000200): Train loss 5.548, Val loss 5.634\n",
      "Ep 1 (Step 000210): Train loss 5.568, Val loss 5.620\n",
      "Ep 1 (Step 000220): Train loss 5.482, Val loss 5.578\n",
      "Ep 1 (Step 000230): Train loss 5.515, Val loss 5.570\n",
      "Ep 1 (Step 000240): Train loss 5.468, Val loss 5.541\n",
      "Ep 1 (Step 000250): Train loss 5.353, Val loss 5.537\n",
      "Ep 1 (Step 000260): Train loss 5.443, Val loss 5.529\n",
      "Ep 1 (Step 000270): Train loss 5.362, Val loss 5.513\n",
      "Ep 1 (Step 000280): Train loss 5.413, Val loss 5.499\n",
      "Ep 1 (Step 000290): Train loss 5.393, Val loss 5.493\n",
      "Ep 1 (Step 000300): Train loss 5.442, Val loss 5.475\n",
      "Ep 1 (Step 000310): Train loss 5.328, Val loss 5.450\n",
      "Ep 1 (Step 000320): Train loss 5.313, Val loss 5.448\n",
      "Ep 1 (Step 000330): Train loss 5.387, Val loss 5.409\n",
      "Ep 1 (Step 000340): Train loss 5.324, Val loss 5.409\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.4094\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.114, Val loss 9.119\n",
      "Ep 1 (Step 000010): Train loss 7.518, Val loss 7.457\n",
      "Ep 1 (Step 000020): Train loss 6.828, Val loss 6.807\n",
      "Ep 1 (Step 000030): Train loss 6.520, Val loss 6.485\n",
      "Ep 1 (Step 000040): Train loss 6.399, Val loss 6.404\n",
      "Ep 1 (Step 000050): Train loss 6.359, Val loss 6.371\n",
      "Ep 1 (Step 000060): Train loss 6.358, Val loss 6.326\n",
      "Ep 1 (Step 000070): Train loss 6.222, Val loss 6.258\n",
      "Ep 1 (Step 000080): Train loss 6.180, Val loss 6.173\n",
      "Ep 1 (Step 000090): Train loss 6.085, Val loss 6.083\n",
      "Ep 1 (Step 000100): Train loss 6.073, Val loss 6.017\n",
      "Ep 1 (Step 000110): Train loss 5.991, Val loss 5.962\n",
      "Ep 1 (Step 000120): Train loss 5.914, Val loss 5.904\n",
      "Ep 1 (Step 000130): Train loss 5.806, Val loss 5.860\n",
      "Ep 1 (Step 000140): Train loss 5.777, Val loss 5.827\n",
      "Ep 1 (Step 000150): Train loss 5.759, Val loss 5.776\n",
      "Ep 1 (Step 000160): Train loss 5.698, Val loss 5.759\n",
      "Ep 1 (Step 000170): Train loss 5.693, Val loss 5.733\n",
      "Ep 1 (Step 000180): Train loss 5.684, Val loss 5.685\n",
      "Ep 1 (Step 000190): Train loss 5.576, Val loss 5.674\n",
      "Ep 1 (Step 000200): Train loss 5.557, Val loss 5.649\n",
      "Ep 1 (Step 000210): Train loss 5.539, Val loss 5.612\n",
      "Ep 1 (Step 000220): Train loss 5.528, Val loss 5.597\n",
      "Ep 1 (Step 000230): Train loss 5.550, Val loss 5.577\n",
      "Ep 1 (Step 000240): Train loss 5.531, Val loss 5.551\n",
      "Ep 1 (Step 000250): Train loss 5.483, Val loss 5.541\n",
      "Ep 1 (Step 000260): Train loss 5.411, Val loss 5.513\n",
      "Ep 1 (Step 000270): Train loss 5.427, Val loss 5.507\n",
      "Ep 1 (Step 000280): Train loss 5.400, Val loss 5.484\n",
      "Ep 1 (Step 000290): Train loss 5.386, Val loss 5.476\n",
      "Ep 1 (Step 000300): Train loss 5.310, Val loss 5.469\n",
      "Ep 1 (Step 000310): Train loss 5.352, Val loss 5.447\n",
      "Ep 1 (Step 000320): Train loss 5.287, Val loss 5.436\n",
      "Ep 1 (Step 000330): Train loss 5.211, Val loss 5.424\n",
      "Ep 1 (Step 000340): Train loss 5.359, Val loss 5.419\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.4189\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.146, Val loss 9.133\n",
      "Ep 1 (Step 000010): Train loss 7.514, Val loss 7.458\n",
      "Ep 1 (Step 000020): Train loss 6.883, Val loss 6.789\n",
      "Ep 1 (Step 000030): Train loss 6.514, Val loss 6.479\n",
      "Ep 1 (Step 000040): Train loss 6.445, Val loss 6.400\n",
      "Ep 1 (Step 000050): Train loss 6.376, Val loss 6.383\n",
      "Ep 1 (Step 000060): Train loss 6.341, Val loss 6.354\n",
      "Ep 1 (Step 000070): Train loss 6.286, Val loss 6.278\n",
      "Ep 1 (Step 000080): Train loss 6.144, Val loss 6.161\n",
      "Ep 1 (Step 000090): Train loss 6.012, Val loss 6.071\n",
      "Ep 1 (Step 000100): Train loss 5.955, Val loss 6.006\n",
      "Ep 1 (Step 000110): Train loss 5.957, Val loss 5.973\n",
      "Ep 1 (Step 000120): Train loss 5.870, Val loss 5.928\n",
      "Ep 1 (Step 000130): Train loss 5.822, Val loss 5.875\n",
      "Ep 1 (Step 000140): Train loss 5.837, Val loss 5.844\n",
      "Ep 1 (Step 000150): Train loss 5.757, Val loss 5.800\n",
      "Ep 1 (Step 000160): Train loss 5.651, Val loss 5.761\n",
      "Ep 1 (Step 000170): Train loss 5.679, Val loss 5.734\n",
      "Ep 1 (Step 000180): Train loss 5.579, Val loss 5.702\n",
      "Ep 1 (Step 000190): Train loss 5.550, Val loss 5.673\n",
      "Ep 1 (Step 000200): Train loss 5.549, Val loss 5.653\n",
      "Ep 1 (Step 000210): Train loss 5.554, Val loss 5.610\n",
      "Ep 1 (Step 000220): Train loss 5.537, Val loss 5.597\n",
      "Ep 1 (Step 000230): Train loss 5.624, Val loss 5.573\n",
      "Ep 1 (Step 000240): Train loss 5.530, Val loss 5.544\n",
      "Ep 1 (Step 000250): Train loss 5.445, Val loss 5.533\n",
      "Ep 1 (Step 000260): Train loss 5.397, Val loss 5.520\n",
      "Ep 1 (Step 000270): Train loss 5.399, Val loss 5.514\n",
      "Ep 1 (Step 000280): Train loss 5.367, Val loss 5.494\n",
      "Ep 1 (Step 000290): Train loss 5.331, Val loss 5.486\n",
      "Ep 1 (Step 000300): Train loss 5.329, Val loss 5.467\n",
      "Ep 1 (Step 000310): Train loss 5.321, Val loss 5.457\n",
      "Ep 1 (Step 000320): Train loss 5.248, Val loss 5.434\n",
      "Ep 1 (Step 000330): Train loss 5.283, Val loss 5.428\n",
      "Ep 1 (Step 000340): Train loss 5.353, Val loss 5.407\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.4066\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.158, Val loss 9.156\n",
      "Ep 1 (Step 000010): Train loss 7.608, Val loss 7.555\n",
      "Ep 1 (Step 000020): Train loss 6.900, Val loss 6.854\n",
      "Ep 1 (Step 000030): Train loss 6.519, Val loss 6.499\n",
      "Ep 1 (Step 000040): Train loss 6.462, Val loss 6.403\n",
      "Ep 1 (Step 000050): Train loss 6.371, Val loss 6.373\n",
      "Ep 1 (Step 000060): Train loss 6.349, Val loss 6.357\n",
      "Ep 1 (Step 000070): Train loss 6.269, Val loss 6.300\n",
      "Ep 1 (Step 000080): Train loss 6.180, Val loss 6.196\n",
      "Ep 1 (Step 000090): Train loss 6.066, Val loss 6.109\n",
      "Ep 1 (Step 000100): Train loss 6.028, Val loss 6.036\n",
      "Ep 1 (Step 000110): Train loss 5.938, Val loss 5.972\n",
      "Ep 1 (Step 000120): Train loss 5.908, Val loss 5.928\n",
      "Ep 1 (Step 000130): Train loss 5.810, Val loss 5.878\n",
      "Ep 1 (Step 000140): Train loss 5.771, Val loss 5.844\n",
      "Ep 1 (Step 000150): Train loss 5.664, Val loss 5.784\n",
      "Ep 1 (Step 000160): Train loss 5.650, Val loss 5.748\n",
      "Ep 1 (Step 000170): Train loss 5.737, Val loss 5.724\n",
      "Ep 1 (Step 000180): Train loss 5.607, Val loss 5.694\n",
      "Ep 1 (Step 000190): Train loss 5.614, Val loss 5.649\n",
      "Ep 1 (Step 000200): Train loss 5.595, Val loss 5.632\n",
      "Ep 1 (Step 000210): Train loss 5.524, Val loss 5.604\n",
      "Ep 1 (Step 000220): Train loss 5.516, Val loss 5.573\n",
      "Ep 1 (Step 000230): Train loss 5.516, Val loss 5.543\n",
      "Ep 1 (Step 000240): Train loss 5.409, Val loss 5.532\n",
      "Ep 1 (Step 000250): Train loss 5.393, Val loss 5.518\n",
      "Ep 1 (Step 000260): Train loss 5.399, Val loss 5.498\n",
      "Ep 1 (Step 000270): Train loss 5.401, Val loss 5.476\n",
      "Ep 1 (Step 000280): Train loss 5.335, Val loss 5.460\n",
      "Ep 1 (Step 000290): Train loss 5.415, Val loss 5.445\n",
      "Ep 1 (Step 000300): Train loss 5.265, Val loss 5.439\n",
      "Ep 1 (Step 000310): Train loss 5.320, Val loss 5.430\n",
      "Ep 1 (Step 000320): Train loss 5.307, Val loss 5.414\n",
      "Ep 1 (Step 000330): Train loss 5.341, Val loss 5.399\n",
      "Ep 1 (Step 000340): Train loss 5.293, Val loss 5.399\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3994\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.166, Val loss 9.153\n",
      "Ep 1 (Step 000010): Train loss 7.506, Val loss 7.497\n",
      "Ep 1 (Step 000020): Train loss 6.823, Val loss 6.820\n",
      "Ep 1 (Step 000030): Train loss 6.525, Val loss 6.485\n",
      "Ep 1 (Step 000040): Train loss 6.399, Val loss 6.394\n",
      "Ep 1 (Step 000050): Train loss 6.378, Val loss 6.378\n",
      "Ep 1 (Step 000060): Train loss 6.386, Val loss 6.356\n",
      "Ep 1 (Step 000070): Train loss 6.300, Val loss 6.283\n",
      "Ep 1 (Step 000080): Train loss 6.263, Val loss 6.191\n",
      "Ep 1 (Step 000090): Train loss 6.124, Val loss 6.089\n",
      "Ep 1 (Step 000100): Train loss 5.979, Val loss 6.025\n",
      "Ep 1 (Step 000110): Train loss 5.930, Val loss 5.973\n",
      "Ep 1 (Step 000120): Train loss 5.809, Val loss 5.917\n",
      "Ep 1 (Step 000130): Train loss 5.799, Val loss 5.886\n",
      "Ep 1 (Step 000140): Train loss 5.686, Val loss 5.827\n",
      "Ep 1 (Step 000150): Train loss 5.640, Val loss 5.791\n",
      "Ep 1 (Step 000160): Train loss 5.690, Val loss 5.748\n",
      "Ep 1 (Step 000170): Train loss 5.703, Val loss 5.711\n",
      "Ep 1 (Step 000180): Train loss 5.604, Val loss 5.673\n",
      "Ep 1 (Step 000190): Train loss 5.568, Val loss 5.653\n",
      "Ep 1 (Step 000200): Train loss 5.631, Val loss 5.634\n",
      "Ep 1 (Step 000210): Train loss 5.529, Val loss 5.592\n",
      "Ep 1 (Step 000220): Train loss 5.472, Val loss 5.565\n",
      "Ep 1 (Step 000230): Train loss 5.480, Val loss 5.555\n",
      "Ep 1 (Step 000240): Train loss 5.448, Val loss 5.525\n",
      "Ep 1 (Step 000250): Train loss 5.480, Val loss 5.511\n",
      "Ep 1 (Step 000260): Train loss 5.404, Val loss 5.500\n",
      "Ep 1 (Step 000270): Train loss 5.402, Val loss 5.482\n",
      "Ep 1 (Step 000280): Train loss 5.378, Val loss 5.466\n",
      "Ep 1 (Step 000290): Train loss 5.304, Val loss 5.446\n",
      "Ep 1 (Step 000300): Train loss 5.263, Val loss 5.432\n",
      "Ep 1 (Step 000310): Train loss 5.369, Val loss 5.419\n",
      "Ep 1 (Step 000320): Train loss 5.327, Val loss 5.418\n",
      "Ep 1 (Step 000330): Train loss 5.258, Val loss 5.407\n",
      "Ep 1 (Step 000340): Train loss 5.240, Val loss 5.399\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3990\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.150, Val loss 9.131\n",
      "Ep 1 (Step 000010): Train loss 7.543, Val loss 7.472\n",
      "Ep 1 (Step 000020): Train loss 6.823, Val loss 6.799\n",
      "Ep 1 (Step 000030): Train loss 6.494, Val loss 6.470\n",
      "Ep 1 (Step 000040): Train loss 6.392, Val loss 6.406\n",
      "Ep 1 (Step 000050): Train loss 6.375, Val loss 6.381\n",
      "Ep 1 (Step 000060): Train loss 6.385, Val loss 6.351\n",
      "Ep 1 (Step 000070): Train loss 6.329, Val loss 6.288\n",
      "Ep 1 (Step 000080): Train loss 6.239, Val loss 6.173\n",
      "Ep 1 (Step 000090): Train loss 6.089, Val loss 6.083\n",
      "Ep 1 (Step 000100): Train loss 6.064, Val loss 6.009\n",
      "Ep 1 (Step 000110): Train loss 5.853, Val loss 5.966\n",
      "Ep 1 (Step 000120): Train loss 5.876, Val loss 5.903\n",
      "Ep 1 (Step 000130): Train loss 5.777, Val loss 5.857\n",
      "Ep 1 (Step 000140): Train loss 5.733, Val loss 5.820\n",
      "Ep 1 (Step 000150): Train loss 5.650, Val loss 5.771\n",
      "Ep 1 (Step 000160): Train loss 5.661, Val loss 5.742\n",
      "Ep 1 (Step 000170): Train loss 5.663, Val loss 5.705\n",
      "Ep 1 (Step 000180): Train loss 5.557, Val loss 5.686\n",
      "Ep 1 (Step 000190): Train loss 5.567, Val loss 5.645\n",
      "Ep 1 (Step 000200): Train loss 5.550, Val loss 5.617\n",
      "Ep 1 (Step 000210): Train loss 5.504, Val loss 5.577\n",
      "Ep 1 (Step 000220): Train loss 5.406, Val loss 5.567\n",
      "Ep 1 (Step 000230): Train loss 5.412, Val loss 5.545\n",
      "Ep 1 (Step 000240): Train loss 5.458, Val loss 5.525\n",
      "Ep 1 (Step 000250): Train loss 5.450, Val loss 5.510\n",
      "Ep 1 (Step 000260): Train loss 5.375, Val loss 5.493\n",
      "Ep 1 (Step 000270): Train loss 5.438, Val loss 5.492\n",
      "Ep 1 (Step 000280): Train loss 5.450, Val loss 5.451\n",
      "Ep 1 (Step 000290): Train loss 5.396, Val loss 5.441\n",
      "Ep 1 (Step 000300): Train loss 5.383, Val loss 5.435\n",
      "Ep 1 (Step 000310): Train loss 5.349, Val loss 5.432\n",
      "Ep 1 (Step 000320): Train loss 5.294, Val loss 5.415\n",
      "Ep 1 (Step 000330): Train loss 5.296, Val loss 5.395\n",
      "Ep 1 (Step 000340): Train loss 5.315, Val loss 5.390\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3902\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.893, Val loss 8.889\n",
      "Ep 1 (Step 000010): Train loss 6.847, Val loss 6.886\n",
      "Ep 1 (Step 000020): Train loss 6.509, Val loss 6.499\n",
      "Ep 1 (Step 000030): Train loss 6.526, Val loss 6.471\n",
      "Ep 1 (Step 000040): Train loss 6.402, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.270, Val loss 6.279\n",
      "Ep 1 (Step 000060): Train loss 6.102, Val loss 6.128\n",
      "Ep 1 (Step 000070): Train loss 5.914, Val loss 6.000\n",
      "Ep 1 (Step 000080): Train loss 5.855, Val loss 5.896\n",
      "Ep 1 (Step 000090): Train loss 5.753, Val loss 5.824\n",
      "Ep 1 (Step 000100): Train loss 5.783, Val loss 5.799\n",
      "Ep 1 (Step 000110): Train loss 5.631, Val loss 5.723\n",
      "Ep 1 (Step 000120): Train loss 5.567, Val loss 5.692\n",
      "Ep 1 (Step 000130): Train loss 5.601, Val loss 5.640\n",
      "Ep 1 (Step 000140): Train loss 5.523, Val loss 5.605\n",
      "Ep 1 (Step 000150): Train loss 5.502, Val loss 5.589\n",
      "Ep 1 (Step 000160): Train loss 5.443, Val loss 5.546\n",
      "Ep 1 (Step 000170): Train loss 5.512, Val loss 5.517\n",
      "Ep 1 (Step 000180): Train loss 5.345, Val loss 5.512\n",
      "Ep 1 (Step 000190): Train loss 5.481, Val loss 5.480\n",
      "Ep 1 (Step 000200): Train loss 5.343, Val loss 5.465\n",
      "Ep 1 (Step 000210): Train loss 5.318, Val loss 5.474\n",
      "Ep 1 (Step 000220): Train loss 5.299, Val loss 5.442\n",
      "Ep 1 (Step 000230): Train loss 5.268, Val loss 5.418\n",
      "Ep 1 (Step 000240): Train loss 5.203, Val loss 5.396\n",
      "Ep 1 (Step 000250): Train loss 5.240, Val loss 5.371\n",
      "Ep 1 (Step 000260): Train loss 5.255, Val loss 5.371\n",
      "Ep 1 (Step 000270): Train loss 5.177, Val loss 5.358\n",
      "Ep 1 (Step 000280): Train loss 5.290, Val loss 5.335\n",
      "Ep 1 (Step 000290): Train loss 5.257, Val loss 5.323\n",
      "Ep 1 (Step 000300): Train loss 5.154, Val loss 5.313\n",
      "Ep 1 (Step 000310): Train loss 5.228, Val loss 5.319\n",
      "Ep 1 (Step 000320): Train loss 5.236, Val loss 5.300\n",
      "Ep 1 (Step 000330): Train loss 5.143, Val loss 5.305\n",
      "Ep 1 (Step 000340): Train loss 5.154, Val loss 5.275\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2749\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.971, Val loss 8.955\n",
      "Ep 1 (Step 000010): Train loss 6.895, Val loss 6.900\n",
      "Ep 1 (Step 000020): Train loss 6.463, Val loss 6.480\n",
      "Ep 1 (Step 000030): Train loss 6.496, Val loss 6.476\n",
      "Ep 1 (Step 000040): Train loss 6.382, Val loss 6.399\n",
      "Ep 1 (Step 000050): Train loss 6.291, Val loss 6.260\n",
      "Ep 1 (Step 000060): Train loss 6.184, Val loss 6.165\n",
      "Ep 1 (Step 000070): Train loss 5.998, Val loss 6.033\n",
      "Ep 1 (Step 000080): Train loss 5.884, Val loss 5.928\n",
      "Ep 1 (Step 000090): Train loss 5.758, Val loss 5.838\n",
      "Ep 1 (Step 000100): Train loss 5.748, Val loss 5.776\n",
      "Ep 1 (Step 000110): Train loss 5.684, Val loss 5.708\n",
      "Ep 1 (Step 000120): Train loss 5.596, Val loss 5.659\n",
      "Ep 1 (Step 000130): Train loss 5.550, Val loss 5.621\n",
      "Ep 1 (Step 000140): Train loss 5.538, Val loss 5.616\n",
      "Ep 1 (Step 000150): Train loss 5.506, Val loss 5.583\n",
      "Ep 1 (Step 000160): Train loss 5.400, Val loss 5.550\n",
      "Ep 1 (Step 000170): Train loss 5.359, Val loss 5.529\n",
      "Ep 1 (Step 000180): Train loss 5.403, Val loss 5.506\n",
      "Ep 1 (Step 000190): Train loss 5.334, Val loss 5.501\n",
      "Ep 1 (Step 000200): Train loss 5.333, Val loss 5.471\n",
      "Ep 1 (Step 000210): Train loss 5.432, Val loss 5.450\n",
      "Ep 1 (Step 000220): Train loss 5.341, Val loss 5.418\n",
      "Ep 1 (Step 000230): Train loss 5.318, Val loss 5.423\n",
      "Ep 1 (Step 000240): Train loss 5.336, Val loss 5.413\n",
      "Ep 1 (Step 000250): Train loss 5.296, Val loss 5.390\n",
      "Ep 1 (Step 000260): Train loss 5.303, Val loss 5.381\n",
      "Ep 1 (Step 000270): Train loss 5.203, Val loss 5.362\n",
      "Ep 1 (Step 000280): Train loss 5.274, Val loss 5.354\n",
      "Ep 1 (Step 000290): Train loss 5.239, Val loss 5.350\n",
      "Ep 1 (Step 000300): Train loss 5.223, Val loss 5.325\n",
      "Ep 1 (Step 000310): Train loss 5.120, Val loss 5.301\n",
      "Ep 1 (Step 000320): Train loss 5.080, Val loss 5.311\n",
      "Ep 1 (Step 000330): Train loss 5.153, Val loss 5.299\n",
      "Ep 1 (Step 000340): Train loss 5.084, Val loss 5.292\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2924\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.901, Val loss 8.884\n",
      "Ep 1 (Step 000010): Train loss 6.867, Val loss 6.835\n",
      "Ep 1 (Step 000020): Train loss 6.494, Val loss 6.459\n",
      "Ep 1 (Step 000030): Train loss 6.537, Val loss 6.472\n",
      "Ep 1 (Step 000040): Train loss 6.401, Val loss 6.416\n",
      "Ep 1 (Step 000050): Train loss 6.342, Val loss 6.309\n",
      "Ep 1 (Step 000060): Train loss 6.205, Val loss 6.169\n",
      "Ep 1 (Step 000070): Train loss 6.009, Val loss 6.043\n",
      "Ep 1 (Step 000080): Train loss 5.950, Val loss 5.963\n",
      "Ep 1 (Step 000090): Train loss 5.791, Val loss 5.890\n",
      "Ep 1 (Step 000100): Train loss 5.762, Val loss 5.821\n",
      "Ep 1 (Step 000110): Train loss 5.687, Val loss 5.792\n",
      "Ep 1 (Step 000120): Train loss 5.673, Val loss 5.738\n",
      "Ep 1 (Step 000130): Train loss 5.593, Val loss 5.674\n",
      "Ep 1 (Step 000140): Train loss 5.552, Val loss 5.648\n",
      "Ep 1 (Step 000150): Train loss 5.518, Val loss 5.606\n",
      "Ep 1 (Step 000160): Train loss 5.520, Val loss 5.572\n",
      "Ep 1 (Step 000170): Train loss 5.489, Val loss 5.559\n",
      "Ep 1 (Step 000180): Train loss 5.411, Val loss 5.514\n",
      "Ep 1 (Step 000190): Train loss 5.411, Val loss 5.508\n",
      "Ep 1 (Step 000200): Train loss 5.377, Val loss 5.472\n",
      "Ep 1 (Step 000210): Train loss 5.439, Val loss 5.462\n",
      "Ep 1 (Step 000220): Train loss 5.432, Val loss 5.442\n",
      "Ep 1 (Step 000230): Train loss 5.315, Val loss 5.421\n",
      "Ep 1 (Step 000240): Train loss 5.183, Val loss 5.416\n",
      "Ep 1 (Step 000250): Train loss 5.264, Val loss 5.389\n",
      "Ep 1 (Step 000260): Train loss 5.184, Val loss 5.377\n",
      "Ep 1 (Step 000270): Train loss 5.242, Val loss 5.370\n",
      "Ep 1 (Step 000280): Train loss 5.231, Val loss 5.354\n",
      "Ep 1 (Step 000290): Train loss 5.213, Val loss 5.330\n",
      "Ep 1 (Step 000300): Train loss 5.202, Val loss 5.333\n",
      "Ep 1 (Step 000310): Train loss 5.106, Val loss 5.329\n",
      "Ep 1 (Step 000320): Train loss 5.174, Val loss 5.316\n",
      "Ep 1 (Step 000330): Train loss 5.139, Val loss 5.311\n",
      "Ep 1 (Step 000340): Train loss 5.090, Val loss 5.287\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2874\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.975, Val loss 8.969\n",
      "Ep 1 (Step 000010): Train loss 6.918, Val loss 6.902\n",
      "Ep 1 (Step 000020): Train loss 6.485, Val loss 6.485\n",
      "Ep 1 (Step 000030): Train loss 6.512, Val loss 6.467\n",
      "Ep 1 (Step 000040): Train loss 6.392, Val loss 6.399\n",
      "Ep 1 (Step 000050): Train loss 6.204, Val loss 6.314\n",
      "Ep 1 (Step 000060): Train loss 6.156, Val loss 6.189\n",
      "Ep 1 (Step 000070): Train loss 6.045, Val loss 6.041\n",
      "Ep 1 (Step 000080): Train loss 5.890, Val loss 5.943\n",
      "Ep 1 (Step 000090): Train loss 5.915, Val loss 5.885\n",
      "Ep 1 (Step 000100): Train loss 5.763, Val loss 5.819\n",
      "Ep 1 (Step 000110): Train loss 5.679, Val loss 5.734\n",
      "Ep 1 (Step 000120): Train loss 5.578, Val loss 5.686\n",
      "Ep 1 (Step 000130): Train loss 5.569, Val loss 5.636\n",
      "Ep 1 (Step 000140): Train loss 5.568, Val loss 5.595\n",
      "Ep 1 (Step 000150): Train loss 5.549, Val loss 5.577\n",
      "Ep 1 (Step 000160): Train loss 5.481, Val loss 5.538\n",
      "Ep 1 (Step 000170): Train loss 5.500, Val loss 5.497\n",
      "Ep 1 (Step 000180): Train loss 5.450, Val loss 5.480\n",
      "Ep 1 (Step 000190): Train loss 5.349, Val loss 5.456\n",
      "Ep 1 (Step 000200): Train loss 5.330, Val loss 5.432\n",
      "Ep 1 (Step 000210): Train loss 5.354, Val loss 5.407\n",
      "Ep 1 (Step 000220): Train loss 5.349, Val loss 5.403\n",
      "Ep 1 (Step 000230): Train loss 5.196, Val loss 5.384\n",
      "Ep 1 (Step 000240): Train loss 5.245, Val loss 5.370\n",
      "Ep 1 (Step 000250): Train loss 5.248, Val loss 5.360\n",
      "Ep 1 (Step 000260): Train loss 5.214, Val loss 5.338\n",
      "Ep 1 (Step 000270): Train loss 5.220, Val loss 5.322\n",
      "Ep 1 (Step 000280): Train loss 5.174, Val loss 5.307\n",
      "Ep 1 (Step 000290): Train loss 5.158, Val loss 5.287\n",
      "Ep 1 (Step 000300): Train loss 5.155, Val loss 5.276\n",
      "Ep 1 (Step 000310): Train loss 5.197, Val loss 5.271\n",
      "Ep 1 (Step 000320): Train loss 5.150, Val loss 5.257\n",
      "Ep 1 (Step 000330): Train loss 5.077, Val loss 5.263\n",
      "Ep 1 (Step 000340): Train loss 5.130, Val loss 5.247\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2469\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.957, Val loss 8.937\n",
      "Ep 1 (Step 000010): Train loss 6.923, Val loss 6.876\n",
      "Ep 1 (Step 000020): Train loss 6.445, Val loss 6.448\n",
      "Ep 1 (Step 000030): Train loss 6.468, Val loss 6.452\n",
      "Ep 1 (Step 000040): Train loss 6.379, Val loss 6.385\n",
      "Ep 1 (Step 000050): Train loss 6.278, Val loss 6.282\n",
      "Ep 1 (Step 000060): Train loss 6.142, Val loss 6.159\n",
      "Ep 1 (Step 000070): Train loss 6.025, Val loss 6.049\n",
      "Ep 1 (Step 000080): Train loss 5.862, Val loss 5.950\n",
      "Ep 1 (Step 000090): Train loss 5.861, Val loss 5.890\n",
      "Ep 1 (Step 000100): Train loss 5.693, Val loss 5.815\n",
      "Ep 1 (Step 000110): Train loss 5.664, Val loss 5.730\n",
      "Ep 1 (Step 000120): Train loss 5.598, Val loss 5.676\n",
      "Ep 1 (Step 000130): Train loss 5.575, Val loss 5.623\n",
      "Ep 1 (Step 000140): Train loss 5.588, Val loss 5.600\n",
      "Ep 1 (Step 000150): Train loss 5.541, Val loss 5.569\n",
      "Ep 1 (Step 000160): Train loss 5.459, Val loss 5.545\n",
      "Ep 1 (Step 000170): Train loss 5.435, Val loss 5.508\n",
      "Ep 1 (Step 000180): Train loss 5.344, Val loss 5.489\n",
      "Ep 1 (Step 000190): Train loss 5.283, Val loss 5.461\n",
      "Ep 1 (Step 000200): Train loss 5.324, Val loss 5.450\n",
      "Ep 1 (Step 000210): Train loss 5.296, Val loss 5.427\n",
      "Ep 1 (Step 000220): Train loss 5.346, Val loss 5.421\n",
      "Ep 1 (Step 000230): Train loss 5.276, Val loss 5.390\n",
      "Ep 1 (Step 000240): Train loss 5.338, Val loss 5.371\n",
      "Ep 1 (Step 000250): Train loss 5.336, Val loss 5.370\n",
      "Ep 1 (Step 000260): Train loss 5.255, Val loss 5.344\n",
      "Ep 1 (Step 000270): Train loss 5.156, Val loss 5.340\n",
      "Ep 1 (Step 000280): Train loss 5.188, Val loss 5.325\n",
      "Ep 1 (Step 000290): Train loss 5.102, Val loss 5.293\n",
      "Ep 1 (Step 000300): Train loss 5.219, Val loss 5.297\n",
      "Ep 1 (Step 000310): Train loss 5.177, Val loss 5.277\n",
      "Ep 1 (Step 000320): Train loss 5.117, Val loss 5.275\n",
      "Ep 1 (Step 000330): Train loss 5.253, Val loss 5.260\n",
      "Ep 1 (Step 000340): Train loss 5.168, Val loss 5.254\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2537\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.990, Val loss 8.983\n",
      "Ep 1 (Step 000010): Train loss 6.908, Val loss 6.890\n",
      "Ep 1 (Step 000020): Train loss 6.492, Val loss 6.467\n",
      "Ep 1 (Step 000030): Train loss 6.442, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.390, Val loss 6.410\n",
      "Ep 1 (Step 000050): Train loss 6.272, Val loss 6.236\n",
      "Ep 1 (Step 000060): Train loss 6.080, Val loss 6.116\n",
      "Ep 1 (Step 000070): Train loss 6.030, Val loss 5.993\n",
      "Ep 1 (Step 000080): Train loss 5.890, Val loss 5.890\n",
      "Ep 1 (Step 000090): Train loss 5.739, Val loss 5.818\n",
      "Ep 1 (Step 000100): Train loss 5.650, Val loss 5.756\n",
      "Ep 1 (Step 000110): Train loss 5.719, Val loss 5.725\n",
      "Ep 1 (Step 000120): Train loss 5.589, Val loss 5.667\n",
      "Ep 1 (Step 000130): Train loss 5.527, Val loss 5.619\n",
      "Ep 1 (Step 000140): Train loss 5.476, Val loss 5.584\n",
      "Ep 1 (Step 000150): Train loss 5.439, Val loss 5.547\n",
      "Ep 1 (Step 000160): Train loss 5.464, Val loss 5.529\n",
      "Ep 1 (Step 000170): Train loss 5.470, Val loss 5.514\n",
      "Ep 1 (Step 000180): Train loss 5.446, Val loss 5.488\n",
      "Ep 1 (Step 000190): Train loss 5.295, Val loss 5.454\n",
      "Ep 1 (Step 000200): Train loss 5.405, Val loss 5.412\n",
      "Ep 1 (Step 000210): Train loss 5.252, Val loss 5.404\n",
      "Ep 1 (Step 000220): Train loss 5.335, Val loss 5.394\n",
      "Ep 1 (Step 000230): Train loss 5.254, Val loss 5.371\n",
      "Ep 1 (Step 000240): Train loss 5.279, Val loss 5.357\n",
      "Ep 1 (Step 000250): Train loss 5.282, Val loss 5.352\n",
      "Ep 1 (Step 000260): Train loss 5.262, Val loss 5.329\n",
      "Ep 1 (Step 000270): Train loss 5.201, Val loss 5.304\n",
      "Ep 1 (Step 000280): Train loss 5.200, Val loss 5.313\n",
      "Ep 1 (Step 000290): Train loss 5.154, Val loss 5.334\n",
      "Ep 1 (Step 000300): Train loss 5.133, Val loss 5.292\n",
      "Ep 1 (Step 000310): Train loss 5.127, Val loss 5.275\n",
      "Ep 1 (Step 000320): Train loss 5.091, Val loss 5.278\n",
      "Ep 1 (Step 000330): Train loss 5.033, Val loss 5.275\n",
      "Ep 1 (Step 000340): Train loss 5.069, Val loss 5.257\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2575\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.103, Val loss 9.088\n",
      "Ep 1 (Step 000010): Train loss 7.448, Val loss 7.426\n",
      "Ep 1 (Step 000020): Train loss 6.820, Val loss 6.777\n",
      "Ep 1 (Step 000030): Train loss 6.496, Val loss 6.470\n",
      "Ep 1 (Step 000040): Train loss 6.411, Val loss 6.393\n",
      "Ep 1 (Step 000050): Train loss 6.399, Val loss 6.359\n",
      "Ep 1 (Step 000060): Train loss 6.292, Val loss 6.293\n",
      "Ep 1 (Step 000070): Train loss 6.159, Val loss 6.157\n",
      "Ep 1 (Step 000080): Train loss 6.101, Val loss 6.058\n",
      "Ep 1 (Step 000090): Train loss 5.946, Val loss 5.982\n",
      "Ep 1 (Step 000100): Train loss 5.852, Val loss 5.939\n",
      "Ep 1 (Step 000110): Train loss 5.831, Val loss 5.867\n",
      "Ep 1 (Step 000120): Train loss 5.740, Val loss 5.811\n",
      "Ep 1 (Step 000130): Train loss 5.774, Val loss 5.781\n",
      "Ep 1 (Step 000140): Train loss 5.709, Val loss 5.740\n",
      "Ep 1 (Step 000150): Train loss 5.649, Val loss 5.712\n",
      "Ep 1 (Step 000160): Train loss 5.662, Val loss 5.675\n",
      "Ep 1 (Step 000170): Train loss 5.577, Val loss 5.648\n",
      "Ep 1 (Step 000180): Train loss 5.495, Val loss 5.621\n",
      "Ep 1 (Step 000190): Train loss 5.521, Val loss 5.600\n",
      "Ep 1 (Step 000200): Train loss 5.475, Val loss 5.576\n",
      "Ep 1 (Step 000210): Train loss 5.413, Val loss 5.567\n",
      "Ep 1 (Step 000220): Train loss 5.369, Val loss 5.540\n",
      "Ep 1 (Step 000230): Train loss 5.385, Val loss 5.525\n",
      "Ep 1 (Step 000240): Train loss 5.398, Val loss 5.501\n",
      "Ep 1 (Step 000250): Train loss 5.404, Val loss 5.477\n",
      "Ep 1 (Step 000260): Train loss 5.344, Val loss 5.464\n",
      "Ep 1 (Step 000270): Train loss 5.299, Val loss 5.446\n",
      "Ep 1 (Step 000280): Train loss 5.285, Val loss 5.440\n",
      "Ep 1 (Step 000290): Train loss 5.309, Val loss 5.417\n",
      "Ep 1 (Step 000300): Train loss 5.250, Val loss 5.402\n",
      "Ep 1 (Step 000310): Train loss 5.278, Val loss 5.397\n",
      "Ep 1 (Step 000320): Train loss 5.263, Val loss 5.387\n",
      "Ep 1 (Step 000330): Train loss 5.277, Val loss 5.365\n",
      "Ep 1 (Step 000340): Train loss 5.208, Val loss 5.354\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3536\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.148, Val loss 9.144\n",
      "Ep 1 (Step 000010): Train loss 7.600, Val loss 7.525\n",
      "Ep 1 (Step 000020): Train loss 6.887, Val loss 6.851\n",
      "Ep 1 (Step 000030): Train loss 6.496, Val loss 6.487\n",
      "Ep 1 (Step 000040): Train loss 6.421, Val loss 6.373\n",
      "Ep 1 (Step 000050): Train loss 6.374, Val loss 6.346\n",
      "Ep 1 (Step 000060): Train loss 6.272, Val loss 6.270\n",
      "Ep 1 (Step 000070): Train loss 6.138, Val loss 6.145\n",
      "Ep 1 (Step 000080): Train loss 6.140, Val loss 6.083\n",
      "Ep 1 (Step 000090): Train loss 6.015, Val loss 6.015\n",
      "Ep 1 (Step 000100): Train loss 5.918, Val loss 5.955\n",
      "Ep 1 (Step 000110): Train loss 5.825, Val loss 5.896\n",
      "Ep 1 (Step 000120): Train loss 5.794, Val loss 5.841\n",
      "Ep 1 (Step 000130): Train loss 5.782, Val loss 5.789\n",
      "Ep 1 (Step 000140): Train loss 5.681, Val loss 5.756\n",
      "Ep 1 (Step 000150): Train loss 5.691, Val loss 5.722\n",
      "Ep 1 (Step 000160): Train loss 5.607, Val loss 5.703\n",
      "Ep 1 (Step 000170): Train loss 5.578, Val loss 5.664\n",
      "Ep 1 (Step 000180): Train loss 5.573, Val loss 5.609\n",
      "Ep 1 (Step 000190): Train loss 5.526, Val loss 5.588\n",
      "Ep 1 (Step 000200): Train loss 5.473, Val loss 5.583\n",
      "Ep 1 (Step 000210): Train loss 5.446, Val loss 5.549\n",
      "Ep 1 (Step 000220): Train loss 5.431, Val loss 5.539\n",
      "Ep 1 (Step 000230): Train loss 5.384, Val loss 5.494\n",
      "Ep 1 (Step 000240): Train loss 5.436, Val loss 5.482\n",
      "Ep 1 (Step 000250): Train loss 5.389, Val loss 5.473\n",
      "Ep 1 (Step 000260): Train loss 5.332, Val loss 5.452\n",
      "Ep 1 (Step 000270): Train loss 5.373, Val loss 5.446\n",
      "Ep 1 (Step 000280): Train loss 5.308, Val loss 5.429\n",
      "Ep 1 (Step 000290): Train loss 5.272, Val loss 5.419\n",
      "Ep 1 (Step 000300): Train loss 5.306, Val loss 5.395\n",
      "Ep 1 (Step 000310): Train loss 5.316, Val loss 5.388\n",
      "Ep 1 (Step 000320): Train loss 5.234, Val loss 5.392\n",
      "Ep 1 (Step 000330): Train loss 5.244, Val loss 5.371\n",
      "Ep 1 (Step 000340): Train loss 5.138, Val loss 5.363\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3632\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.116, Val loss 9.117\n",
      "Ep 1 (Step 000010): Train loss 7.497, Val loss 7.501\n",
      "Ep 1 (Step 000020): Train loss 6.818, Val loss 6.824\n",
      "Ep 1 (Step 000030): Train loss 6.469, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.490, Val loss 6.392\n",
      "Ep 1 (Step 000050): Train loss 6.376, Val loss 6.377\n",
      "Ep 1 (Step 000060): Train loss 6.276, Val loss 6.300\n",
      "Ep 1 (Step 000070): Train loss 6.147, Val loss 6.176\n",
      "Ep 1 (Step 000080): Train loss 6.036, Val loss 6.078\n",
      "Ep 1 (Step 000090): Train loss 5.965, Val loss 6.007\n",
      "Ep 1 (Step 000100): Train loss 5.922, Val loss 5.940\n",
      "Ep 1 (Step 000110): Train loss 5.898, Val loss 5.899\n",
      "Ep 1 (Step 000120): Train loss 5.756, Val loss 5.847\n",
      "Ep 1 (Step 000130): Train loss 5.799, Val loss 5.803\n",
      "Ep 1 (Step 000140): Train loss 5.704, Val loss 5.752\n",
      "Ep 1 (Step 000150): Train loss 5.606, Val loss 5.716\n",
      "Ep 1 (Step 000160): Train loss 5.708, Val loss 5.694\n",
      "Ep 1 (Step 000170): Train loss 5.638, Val loss 5.645\n",
      "Ep 1 (Step 000180): Train loss 5.520, Val loss 5.619\n",
      "Ep 1 (Step 000190): Train loss 5.485, Val loss 5.593\n",
      "Ep 1 (Step 000200): Train loss 5.476, Val loss 5.592\n",
      "Ep 1 (Step 000210): Train loss 5.475, Val loss 5.576\n",
      "Ep 1 (Step 000220): Train loss 5.466, Val loss 5.542\n",
      "Ep 1 (Step 000230): Train loss 5.415, Val loss 5.525\n",
      "Ep 1 (Step 000240): Train loss 5.440, Val loss 5.508\n",
      "Ep 1 (Step 000250): Train loss 5.370, Val loss 5.479\n",
      "Ep 1 (Step 000260): Train loss 5.431, Val loss 5.461\n",
      "Ep 1 (Step 000270): Train loss 5.370, Val loss 5.433\n",
      "Ep 1 (Step 000280): Train loss 5.311, Val loss 5.429\n",
      "Ep 1 (Step 000290): Train loss 5.242, Val loss 5.406\n",
      "Ep 1 (Step 000300): Train loss 5.252, Val loss 5.392\n",
      "Ep 1 (Step 000310): Train loss 5.277, Val loss 5.376\n",
      "Ep 1 (Step 000320): Train loss 5.255, Val loss 5.371\n",
      "Ep 1 (Step 000330): Train loss 5.226, Val loss 5.374\n",
      "Ep 1 (Step 000340): Train loss 5.128, Val loss 5.356\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3558\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.141, Val loss 9.136\n",
      "Ep 1 (Step 000010): Train loss 7.482, Val loss 7.466\n",
      "Ep 1 (Step 000020): Train loss 6.809, Val loss 6.812\n",
      "Ep 1 (Step 000030): Train loss 6.509, Val loss 6.487\n",
      "Ep 1 (Step 000040): Train loss 6.398, Val loss 6.398\n",
      "Ep 1 (Step 000050): Train loss 6.371, Val loss 6.359\n",
      "Ep 1 (Step 000060): Train loss 6.317, Val loss 6.318\n",
      "Ep 1 (Step 000070): Train loss 6.250, Val loss 6.213\n",
      "Ep 1 (Step 000080): Train loss 6.106, Val loss 6.093\n",
      "Ep 1 (Step 000090): Train loss 6.066, Val loss 6.025\n",
      "Ep 1 (Step 000100): Train loss 5.919, Val loss 5.961\n",
      "Ep 1 (Step 000110): Train loss 5.863, Val loss 5.895\n",
      "Ep 1 (Step 000120): Train loss 5.810, Val loss 5.839\n",
      "Ep 1 (Step 000130): Train loss 5.713, Val loss 5.791\n",
      "Ep 1 (Step 000140): Train loss 5.712, Val loss 5.742\n",
      "Ep 1 (Step 000150): Train loss 5.663, Val loss 5.717\n",
      "Ep 1 (Step 000160): Train loss 5.643, Val loss 5.674\n",
      "Ep 1 (Step 000170): Train loss 5.656, Val loss 5.640\n",
      "Ep 1 (Step 000180): Train loss 5.491, Val loss 5.621\n",
      "Ep 1 (Step 000190): Train loss 5.519, Val loss 5.583\n",
      "Ep 1 (Step 000200): Train loss 5.506, Val loss 5.548\n",
      "Ep 1 (Step 000210): Train loss 5.533, Val loss 5.529\n",
      "Ep 1 (Step 000220): Train loss 5.444, Val loss 5.499\n",
      "Ep 1 (Step 000230): Train loss 5.443, Val loss 5.493\n",
      "Ep 1 (Step 000240): Train loss 5.355, Val loss 5.465\n",
      "Ep 1 (Step 000250): Train loss 5.377, Val loss 5.450\n",
      "Ep 1 (Step 000260): Train loss 5.335, Val loss 5.439\n",
      "Ep 1 (Step 000270): Train loss 5.321, Val loss 5.409\n",
      "Ep 1 (Step 000280): Train loss 5.219, Val loss 5.391\n",
      "Ep 1 (Step 000290): Train loss 5.316, Val loss 5.381\n",
      "Ep 1 (Step 000300): Train loss 5.300, Val loss 5.377\n",
      "Ep 1 (Step 000310): Train loss 5.201, Val loss 5.361\n",
      "Ep 1 (Step 000320): Train loss 5.209, Val loss 5.360\n",
      "Ep 1 (Step 000330): Train loss 5.240, Val loss 5.350\n",
      "Ep 1 (Step 000340): Train loss 5.170, Val loss 5.332\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3316\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.143, Val loss 9.127\n",
      "Ep 1 (Step 000010): Train loss 7.483, Val loss 7.471\n",
      "Ep 1 (Step 000020): Train loss 6.829, Val loss 6.805\n",
      "Ep 1 (Step 000030): Train loss 6.538, Val loss 6.472\n",
      "Ep 1 (Step 000040): Train loss 6.376, Val loss 6.361\n",
      "Ep 1 (Step 000050): Train loss 6.264, Val loss 6.322\n",
      "Ep 1 (Step 000060): Train loss 6.229, Val loss 6.251\n",
      "Ep 1 (Step 000070): Train loss 6.193, Val loss 6.144\n",
      "Ep 1 (Step 000080): Train loss 6.023, Val loss 6.043\n",
      "Ep 1 (Step 000090): Train loss 5.995, Val loss 6.002\n",
      "Ep 1 (Step 000100): Train loss 5.945, Val loss 5.944\n",
      "Ep 1 (Step 000110): Train loss 5.859, Val loss 5.867\n",
      "Ep 1 (Step 000120): Train loss 5.777, Val loss 5.819\n",
      "Ep 1 (Step 000130): Train loss 5.771, Val loss 5.777\n",
      "Ep 1 (Step 000140): Train loss 5.696, Val loss 5.725\n",
      "Ep 1 (Step 000150): Train loss 5.622, Val loss 5.704\n",
      "Ep 1 (Step 000160): Train loss 5.708, Val loss 5.673\n",
      "Ep 1 (Step 000170): Train loss 5.687, Val loss 5.629\n",
      "Ep 1 (Step 000180): Train loss 5.505, Val loss 5.597\n",
      "Ep 1 (Step 000190): Train loss 5.598, Val loss 5.575\n",
      "Ep 1 (Step 000200): Train loss 5.569, Val loss 5.551\n",
      "Ep 1 (Step 000210): Train loss 5.433, Val loss 5.534\n",
      "Ep 1 (Step 000220): Train loss 5.494, Val loss 5.507\n",
      "Ep 1 (Step 000230): Train loss 5.410, Val loss 5.494\n",
      "Ep 1 (Step 000240): Train loss 5.401, Val loss 5.480\n",
      "Ep 1 (Step 000250): Train loss 5.437, Val loss 5.459\n",
      "Ep 1 (Step 000260): Train loss 5.310, Val loss 5.438\n",
      "Ep 1 (Step 000270): Train loss 5.264, Val loss 5.417\n",
      "Ep 1 (Step 000280): Train loss 5.290, Val loss 5.392\n",
      "Ep 1 (Step 000290): Train loss 5.283, Val loss 5.383\n",
      "Ep 1 (Step 000300): Train loss 5.266, Val loss 5.363\n",
      "Ep 1 (Step 000310): Train loss 5.300, Val loss 5.350\n",
      "Ep 1 (Step 000320): Train loss 5.238, Val loss 5.337\n",
      "Ep 1 (Step 000330): Train loss 5.260, Val loss 5.325\n",
      "Ep 1 (Step 000340): Train loss 5.219, Val loss 5.318\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3181\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.124, Val loss 9.123\n",
      "Ep 1 (Step 000010): Train loss 7.478, Val loss 7.476\n",
      "Ep 1 (Step 000020): Train loss 6.815, Val loss 6.792\n",
      "Ep 1 (Step 000030): Train loss 6.481, Val loss 6.479\n",
      "Ep 1 (Step 000040): Train loss 6.387, Val loss 6.388\n",
      "Ep 1 (Step 000050): Train loss 6.382, Val loss 6.360\n",
      "Ep 1 (Step 000060): Train loss 6.302, Val loss 6.319\n",
      "Ep 1 (Step 000070): Train loss 6.196, Val loss 6.212\n",
      "Ep 1 (Step 000080): Train loss 6.075, Val loss 6.100\n",
      "Ep 1 (Step 000090): Train loss 6.018, Val loss 6.022\n",
      "Ep 1 (Step 000100): Train loss 5.952, Val loss 5.951\n",
      "Ep 1 (Step 000110): Train loss 5.874, Val loss 5.906\n",
      "Ep 1 (Step 000120): Train loss 5.808, Val loss 5.855\n",
      "Ep 1 (Step 000130): Train loss 5.777, Val loss 5.803\n",
      "Ep 1 (Step 000140): Train loss 5.628, Val loss 5.753\n",
      "Ep 1 (Step 000150): Train loss 5.647, Val loss 5.705\n",
      "Ep 1 (Step 000160): Train loss 5.564, Val loss 5.679\n",
      "Ep 1 (Step 000170): Train loss 5.537, Val loss 5.647\n",
      "Ep 1 (Step 000180): Train loss 5.545, Val loss 5.640\n",
      "Ep 1 (Step 000190): Train loss 5.494, Val loss 5.596\n",
      "Ep 1 (Step 000200): Train loss 5.462, Val loss 5.570\n",
      "Ep 1 (Step 000210): Train loss 5.428, Val loss 5.556\n",
      "Ep 1 (Step 000220): Train loss 5.390, Val loss 5.514\n",
      "Ep 1 (Step 000230): Train loss 5.372, Val loss 5.501\n",
      "Ep 1 (Step 000240): Train loss 5.396, Val loss 5.485\n",
      "Ep 1 (Step 000250): Train loss 5.348, Val loss 5.460\n",
      "Ep 1 (Step 000260): Train loss 5.280, Val loss 5.441\n",
      "Ep 1 (Step 000270): Train loss 5.329, Val loss 5.429\n",
      "Ep 1 (Step 000280): Train loss 5.365, Val loss 5.412\n",
      "Ep 1 (Step 000290): Train loss 5.296, Val loss 5.406\n",
      "Ep 1 (Step 000300): Train loss 5.226, Val loss 5.395\n",
      "Ep 1 (Step 000310): Train loss 5.281, Val loss 5.379\n",
      "Ep 1 (Step 000320): Train loss 5.227, Val loss 5.356\n",
      "Ep 1 (Step 000330): Train loss 5.234, Val loss 5.348\n",
      "Ep 1 (Step 000340): Train loss 5.220, Val loss 5.336\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3355\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.908, Val loss 8.909\n",
      "Ep 1 (Step 000010): Train loss 6.856, Val loss 6.838\n",
      "Ep 1 (Step 000020): Train loss 6.464, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.407, Val loss 6.436\n",
      "Ep 1 (Step 000040): Train loss 6.309, Val loss 6.287\n",
      "Ep 1 (Step 000050): Train loss 6.098, Val loss 6.164\n",
      "Ep 1 (Step 000060): Train loss 6.006, Val loss 5.998\n",
      "Ep 1 (Step 000070): Train loss 5.905, Val loss 5.918\n",
      "Ep 1 (Step 000080): Train loss 5.873, Val loss 5.864\n",
      "Ep 1 (Step 000090): Train loss 5.714, Val loss 5.789\n",
      "Ep 1 (Step 000100): Train loss 5.662, Val loss 5.714\n",
      "Ep 1 (Step 000110): Train loss 5.577, Val loss 5.688\n",
      "Ep 1 (Step 000120): Train loss 5.574, Val loss 5.645\n",
      "Ep 1 (Step 000130): Train loss 5.458, Val loss 5.598\n",
      "Ep 1 (Step 000140): Train loss 5.456, Val loss 5.578\n",
      "Ep 1 (Step 000150): Train loss 5.434, Val loss 5.540\n",
      "Ep 1 (Step 000160): Train loss 5.431, Val loss 5.502\n",
      "Ep 1 (Step 000170): Train loss 5.367, Val loss 5.482\n",
      "Ep 1 (Step 000180): Train loss 5.398, Val loss 5.474\n",
      "Ep 1 (Step 000190): Train loss 5.262, Val loss 5.439\n",
      "Ep 1 (Step 000200): Train loss 5.270, Val loss 5.422\n",
      "Ep 1 (Step 000210): Train loss 5.283, Val loss 5.383\n",
      "Ep 1 (Step 000220): Train loss 5.305, Val loss 5.379\n",
      "Ep 1 (Step 000230): Train loss 5.192, Val loss 5.357\n",
      "Ep 1 (Step 000240): Train loss 5.237, Val loss 5.351\n",
      "Ep 1 (Step 000250): Train loss 5.196, Val loss 5.325\n",
      "Ep 1 (Step 000260): Train loss 5.175, Val loss 5.313\n",
      "Ep 1 (Step 000270): Train loss 5.175, Val loss 5.299\n",
      "Ep 1 (Step 000280): Train loss 5.128, Val loss 5.304\n",
      "Ep 1 (Step 000290): Train loss 5.099, Val loss 5.289\n",
      "Ep 1 (Step 000300): Train loss 5.172, Val loss 5.276\n",
      "Ep 1 (Step 000310): Train loss 5.024, Val loss 5.261\n",
      "Ep 1 (Step 000320): Train loss 5.102, Val loss 5.258\n",
      "Ep 1 (Step 000330): Train loss 5.144, Val loss 5.240\n",
      "Ep 1 (Step 000340): Train loss 5.099, Val loss 5.244\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2444\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.959, Val loss 8.938\n",
      "Ep 1 (Step 000010): Train loss 6.855, Val loss 6.858\n",
      "Ep 1 (Step 000020): Train loss 6.500, Val loss 6.471\n",
      "Ep 1 (Step 000030): Train loss 6.457, Val loss 6.467\n",
      "Ep 1 (Step 000040): Train loss 6.377, Val loss 6.354\n",
      "Ep 1 (Step 000050): Train loss 6.183, Val loss 6.208\n",
      "Ep 1 (Step 000060): Train loss 6.021, Val loss 6.059\n",
      "Ep 1 (Step 000070): Train loss 5.906, Val loss 5.973\n",
      "Ep 1 (Step 000080): Train loss 5.727, Val loss 5.863\n",
      "Ep 1 (Step 000090): Train loss 5.761, Val loss 5.788\n",
      "Ep 1 (Step 000100): Train loss 5.666, Val loss 5.741\n",
      "Ep 1 (Step 000110): Train loss 5.582, Val loss 5.684\n",
      "Ep 1 (Step 000120): Train loss 5.588, Val loss 5.641\n",
      "Ep 1 (Step 000130): Train loss 5.510, Val loss 5.599\n",
      "Ep 1 (Step 000140): Train loss 5.405, Val loss 5.552\n",
      "Ep 1 (Step 000150): Train loss 5.417, Val loss 5.526\n",
      "Ep 1 (Step 000160): Train loss 5.477, Val loss 5.494\n",
      "Ep 1 (Step 000170): Train loss 5.446, Val loss 5.493\n",
      "Ep 1 (Step 000180): Train loss 5.358, Val loss 5.458\n",
      "Ep 1 (Step 000190): Train loss 5.375, Val loss 5.442\n",
      "Ep 1 (Step 000200): Train loss 5.344, Val loss 5.422\n",
      "Ep 1 (Step 000210): Train loss 5.297, Val loss 5.405\n",
      "Ep 1 (Step 000220): Train loss 5.189, Val loss 5.378\n",
      "Ep 1 (Step 000230): Train loss 5.189, Val loss 5.359\n",
      "Ep 1 (Step 000240): Train loss 5.135, Val loss 5.360\n",
      "Ep 1 (Step 000250): Train loss 5.211, Val loss 5.332\n",
      "Ep 1 (Step 000260): Train loss 5.069, Val loss 5.330\n",
      "Ep 1 (Step 000270): Train loss 5.153, Val loss 5.312\n",
      "Ep 1 (Step 000280): Train loss 5.216, Val loss 5.296\n",
      "Ep 1 (Step 000290): Train loss 5.222, Val loss 5.289\n",
      "Ep 1 (Step 000300): Train loss 5.210, Val loss 5.292\n",
      "Ep 1 (Step 000310): Train loss 5.168, Val loss 5.263\n",
      "Ep 1 (Step 000320): Train loss 5.050, Val loss 5.238\n",
      "Ep 1 (Step 000330): Train loss 5.099, Val loss 5.238\n",
      "Ep 1 (Step 000340): Train loss 5.040, Val loss 5.222\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2218\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.892, Val loss 8.887\n",
      "Ep 1 (Step 000010): Train loss 6.940, Val loss 6.895\n",
      "Ep 1 (Step 000020): Train loss 6.489, Val loss 6.496\n",
      "Ep 1 (Step 000030): Train loss 6.521, Val loss 6.467\n",
      "Ep 1 (Step 000040): Train loss 6.312, Val loss 6.352\n",
      "Ep 1 (Step 000050): Train loss 6.272, Val loss 6.177\n",
      "Ep 1 (Step 000060): Train loss 6.019, Val loss 6.040\n",
      "Ep 1 (Step 000070): Train loss 5.906, Val loss 5.929\n",
      "Ep 1 (Step 000080): Train loss 5.749, Val loss 5.847\n",
      "Ep 1 (Step 000090): Train loss 5.748, Val loss 5.779\n",
      "Ep 1 (Step 000100): Train loss 5.602, Val loss 5.730\n",
      "Ep 1 (Step 000110): Train loss 5.625, Val loss 5.683\n",
      "Ep 1 (Step 000120): Train loss 5.623, Val loss 5.632\n",
      "Ep 1 (Step 000130): Train loss 5.446, Val loss 5.600\n",
      "Ep 1 (Step 000140): Train loss 5.542, Val loss 5.567\n",
      "Ep 1 (Step 000150): Train loss 5.362, Val loss 5.533\n",
      "Ep 1 (Step 000160): Train loss 5.372, Val loss 5.517\n",
      "Ep 1 (Step 000170): Train loss 5.334, Val loss 5.480\n",
      "Ep 1 (Step 000180): Train loss 5.339, Val loss 5.451\n",
      "Ep 1 (Step 000190): Train loss 5.364, Val loss 5.415\n",
      "Ep 1 (Step 000200): Train loss 5.267, Val loss 5.396\n",
      "Ep 1 (Step 000210): Train loss 5.253, Val loss 5.387\n",
      "Ep 1 (Step 000220): Train loss 5.174, Val loss 5.378\n",
      "Ep 1 (Step 000230): Train loss 5.135, Val loss 5.355\n",
      "Ep 1 (Step 000240): Train loss 5.205, Val loss 5.339\n",
      "Ep 1 (Step 000250): Train loss 5.181, Val loss 5.330\n",
      "Ep 1 (Step 000260): Train loss 5.148, Val loss 5.321\n",
      "Ep 1 (Step 000270): Train loss 5.210, Val loss 5.311\n",
      "Ep 1 (Step 000280): Train loss 5.228, Val loss 5.306\n",
      "Ep 1 (Step 000290): Train loss 5.142, Val loss 5.283\n",
      "Ep 1 (Step 000300): Train loss 5.103, Val loss 5.279\n",
      "Ep 1 (Step 000310): Train loss 5.056, Val loss 5.252\n",
      "Ep 1 (Step 000320): Train loss 5.167, Val loss 5.248\n",
      "Ep 1 (Step 000330): Train loss 5.136, Val loss 5.231\n",
      "Ep 1 (Step 000340): Train loss 5.010, Val loss 5.224\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2240\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.943, Val loss 8.952\n",
      "Ep 1 (Step 000010): Train loss 6.921, Val loss 6.854\n",
      "Ep 1 (Step 000020): Train loss 6.474, Val loss 6.478\n",
      "Ep 1 (Step 000030): Train loss 6.459, Val loss 6.445\n",
      "Ep 1 (Step 000040): Train loss 6.407, Val loss 6.351\n",
      "Ep 1 (Step 000050): Train loss 6.215, Val loss 6.202\n",
      "Ep 1 (Step 000060): Train loss 6.112, Val loss 6.071\n",
      "Ep 1 (Step 000070): Train loss 5.994, Val loss 5.942\n",
      "Ep 1 (Step 000080): Train loss 5.768, Val loss 5.824\n",
      "Ep 1 (Step 000090): Train loss 5.719, Val loss 5.768\n",
      "Ep 1 (Step 000100): Train loss 5.712, Val loss 5.711\n",
      "Ep 1 (Step 000110): Train loss 5.534, Val loss 5.642\n",
      "Ep 1 (Step 000120): Train loss 5.464, Val loss 5.601\n",
      "Ep 1 (Step 000130): Train loss 5.462, Val loss 5.562\n",
      "Ep 1 (Step 000140): Train loss 5.380, Val loss 5.514\n",
      "Ep 1 (Step 000150): Train loss 5.374, Val loss 5.519\n",
      "Ep 1 (Step 000160): Train loss 5.321, Val loss 5.468\n",
      "Ep 1 (Step 000170): Train loss 5.436, Val loss 5.440\n",
      "Ep 1 (Step 000180): Train loss 5.401, Val loss 5.423\n",
      "Ep 1 (Step 000190): Train loss 5.202, Val loss 5.405\n",
      "Ep 1 (Step 000200): Train loss 5.299, Val loss 5.400\n",
      "Ep 1 (Step 000210): Train loss 5.295, Val loss 5.380\n",
      "Ep 1 (Step 000220): Train loss 5.184, Val loss 5.363\n",
      "Ep 1 (Step 000230): Train loss 5.201, Val loss 5.333\n",
      "Ep 1 (Step 000240): Train loss 5.145, Val loss 5.307\n",
      "Ep 1 (Step 000250): Train loss 5.185, Val loss 5.301\n",
      "Ep 1 (Step 000260): Train loss 5.090, Val loss 5.291\n",
      "Ep 1 (Step 000270): Train loss 5.165, Val loss 5.286\n",
      "Ep 1 (Step 000280): Train loss 5.227, Val loss 5.268\n",
      "Ep 1 (Step 000290): Train loss 5.026, Val loss 5.264\n",
      "Ep 1 (Step 000300): Train loss 5.102, Val loss 5.237\n",
      "Ep 1 (Step 000310): Train loss 5.122, Val loss 5.226\n",
      "Ep 1 (Step 000320): Train loss 5.074, Val loss 5.209\n",
      "Ep 1 (Step 000330): Train loss 5.072, Val loss 5.204\n",
      "Ep 1 (Step 000340): Train loss 5.092, Val loss 5.187\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1865\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.932, Val loss 8.937\n",
      "Ep 1 (Step 000010): Train loss 6.872, Val loss 6.856\n",
      "Ep 1 (Step 000020): Train loss 6.439, Val loss 6.467\n",
      "Ep 1 (Step 000030): Train loss 6.414, Val loss 6.454\n",
      "Ep 1 (Step 000040): Train loss 6.357, Val loss 6.353\n",
      "Ep 1 (Step 000050): Train loss 6.159, Val loss 6.212\n",
      "Ep 1 (Step 000060): Train loss 6.017, Val loss 6.074\n",
      "Ep 1 (Step 000070): Train loss 5.917, Val loss 5.969\n",
      "Ep 1 (Step 000080): Train loss 5.801, Val loss 5.879\n",
      "Ep 1 (Step 000090): Train loss 5.821, Val loss 5.785\n",
      "Ep 1 (Step 000100): Train loss 5.657, Val loss 5.705\n",
      "Ep 1 (Step 000110): Train loss 5.612, Val loss 5.677\n",
      "Ep 1 (Step 000120): Train loss 5.469, Val loss 5.614\n",
      "Ep 1 (Step 000130): Train loss 5.484, Val loss 5.584\n",
      "Ep 1 (Step 000140): Train loss 5.475, Val loss 5.545\n",
      "Ep 1 (Step 000150): Train loss 5.449, Val loss 5.512\n",
      "Ep 1 (Step 000160): Train loss 5.354, Val loss 5.500\n",
      "Ep 1 (Step 000170): Train loss 5.364, Val loss 5.447\n",
      "Ep 1 (Step 000180): Train loss 5.347, Val loss 5.422\n",
      "Ep 1 (Step 000190): Train loss 5.346, Val loss 5.396\n",
      "Ep 1 (Step 000200): Train loss 5.226, Val loss 5.392\n",
      "Ep 1 (Step 000210): Train loss 5.276, Val loss 5.368\n",
      "Ep 1 (Step 000220): Train loss 5.251, Val loss 5.345\n",
      "Ep 1 (Step 000230): Train loss 5.213, Val loss 5.335\n",
      "Ep 1 (Step 000240): Train loss 5.162, Val loss 5.327\n",
      "Ep 1 (Step 000250): Train loss 5.166, Val loss 5.313\n",
      "Ep 1 (Step 000260): Train loss 5.173, Val loss 5.292\n",
      "Ep 1 (Step 000270): Train loss 5.078, Val loss 5.273\n",
      "Ep 1 (Step 000280): Train loss 5.115, Val loss 5.282\n",
      "Ep 1 (Step 000290): Train loss 5.139, Val loss 5.275\n",
      "Ep 1 (Step 000300): Train loss 5.073, Val loss 5.248\n",
      "Ep 1 (Step 000310): Train loss 5.150, Val loss 5.233\n",
      "Ep 1 (Step 000320): Train loss 5.003, Val loss 5.235\n",
      "Ep 1 (Step 000330): Train loss 5.019, Val loss 5.216\n",
      "Ep 1 (Step 000340): Train loss 5.025, Val loss 5.220\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2203\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.879, Val loss 8.867\n",
      "Ep 1 (Step 000010): Train loss 6.855, Val loss 6.838\n",
      "Ep 1 (Step 000020): Train loss 6.497, Val loss 6.475\n",
      "Ep 1 (Step 000030): Train loss 6.540, Val loss 6.473\n",
      "Ep 1 (Step 000040): Train loss 6.377, Val loss 6.356\n",
      "Ep 1 (Step 000050): Train loss 6.252, Val loss 6.194\n",
      "Ep 1 (Step 000060): Train loss 6.036, Val loss 6.026\n",
      "Ep 1 (Step 000070): Train loss 5.894, Val loss 5.940\n",
      "Ep 1 (Step 000080): Train loss 5.827, Val loss 5.850\n",
      "Ep 1 (Step 000090): Train loss 5.737, Val loss 5.767\n",
      "Ep 1 (Step 000100): Train loss 5.745, Val loss 5.723\n",
      "Ep 1 (Step 000110): Train loss 5.594, Val loss 5.646\n",
      "Ep 1 (Step 000120): Train loss 5.527, Val loss 5.606\n",
      "Ep 1 (Step 000130): Train loss 5.452, Val loss 5.557\n",
      "Ep 1 (Step 000140): Train loss 5.503, Val loss 5.514\n",
      "Ep 1 (Step 000150): Train loss 5.509, Val loss 5.501\n",
      "Ep 1 (Step 000160): Train loss 5.448, Val loss 5.461\n",
      "Ep 1 (Step 000170): Train loss 5.394, Val loss 5.424\n",
      "Ep 1 (Step 000180): Train loss 5.385, Val loss 5.402\n",
      "Ep 1 (Step 000190): Train loss 5.317, Val loss 5.399\n",
      "Ep 1 (Step 000200): Train loss 5.308, Val loss 5.377\n",
      "Ep 1 (Step 000210): Train loss 5.214, Val loss 5.352\n",
      "Ep 1 (Step 000220): Train loss 5.293, Val loss 5.348\n",
      "Ep 1 (Step 000230): Train loss 5.247, Val loss 5.330\n",
      "Ep 1 (Step 000240): Train loss 5.202, Val loss 5.313\n",
      "Ep 1 (Step 000250): Train loss 5.187, Val loss 5.308\n",
      "Ep 1 (Step 000260): Train loss 5.146, Val loss 5.277\n",
      "Ep 1 (Step 000270): Train loss 5.167, Val loss 5.258\n",
      "Ep 1 (Step 000280): Train loss 5.162, Val loss 5.263\n",
      "Ep 1 (Step 000290): Train loss 5.161, Val loss 5.242\n",
      "Ep 1 (Step 000300): Train loss 5.053, Val loss 5.248\n",
      "Ep 1 (Step 000310): Train loss 5.089, Val loss 5.223\n",
      "Ep 1 (Step 000320): Train loss 5.135, Val loss 5.219\n",
      "Ep 1 (Step 000330): Train loss 5.049, Val loss 5.201\n",
      "Ep 1 (Step 000340): Train loss 5.047, Val loss 5.192\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1916\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.139, Val loss 9.131\n",
      "Ep 1 (Step 000010): Train loss 7.512, Val loss 7.501\n",
      "Ep 1 (Step 000020): Train loss 6.892, Val loss 6.821\n",
      "Ep 1 (Step 000030): Train loss 6.483, Val loss 6.483\n",
      "Ep 1 (Step 000040): Train loss 6.423, Val loss 6.390\n",
      "Ep 1 (Step 000050): Train loss 6.311, Val loss 6.344\n",
      "Ep 1 (Step 000060): Train loss 6.235, Val loss 6.270\n",
      "Ep 1 (Step 000070): Train loss 6.161, Val loss 6.157\n",
      "Ep 1 (Step 000080): Train loss 6.059, Val loss 6.069\n",
      "Ep 1 (Step 000090): Train loss 6.003, Val loss 6.001\n",
      "Ep 1 (Step 000100): Train loss 5.974, Val loss 5.941\n",
      "Ep 1 (Step 000110): Train loss 5.815, Val loss 5.897\n",
      "Ep 1 (Step 000120): Train loss 5.774, Val loss 5.842\n",
      "Ep 1 (Step 000130): Train loss 5.778, Val loss 5.806\n",
      "Ep 1 (Step 000140): Train loss 5.791, Val loss 5.753\n",
      "Ep 1 (Step 000150): Train loss 5.656, Val loss 5.725\n",
      "Ep 1 (Step 000160): Train loss 5.662, Val loss 5.694\n",
      "Ep 1 (Step 000170): Train loss 5.577, Val loss 5.662\n",
      "Ep 1 (Step 000180): Train loss 5.561, Val loss 5.624\n",
      "Ep 1 (Step 000190): Train loss 5.455, Val loss 5.608\n",
      "Ep 1 (Step 000200): Train loss 5.435, Val loss 5.568\n",
      "Ep 1 (Step 000210): Train loss 5.427, Val loss 5.552\n",
      "Ep 1 (Step 000220): Train loss 5.500, Val loss 5.533\n",
      "Ep 1 (Step 000230): Train loss 5.434, Val loss 5.515\n",
      "Ep 1 (Step 000240): Train loss 5.373, Val loss 5.488\n",
      "Ep 1 (Step 000250): Train loss 5.374, Val loss 5.464\n",
      "Ep 1 (Step 000260): Train loss 5.378, Val loss 5.455\n",
      "Ep 1 (Step 000270): Train loss 5.352, Val loss 5.426\n",
      "Ep 1 (Step 000280): Train loss 5.367, Val loss 5.423\n",
      "Ep 1 (Step 000290): Train loss 5.322, Val loss 5.404\n",
      "Ep 1 (Step 000300): Train loss 5.325, Val loss 5.385\n",
      "Ep 1 (Step 000310): Train loss 5.281, Val loss 5.370\n",
      "Ep 1 (Step 000320): Train loss 5.310, Val loss 5.365\n",
      "Ep 1 (Step 000330): Train loss 5.280, Val loss 5.354\n",
      "Ep 1 (Step 000340): Train loss 5.197, Val loss 5.348\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3485\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.135, Val loss 9.142\n",
      "Ep 1 (Step 000010): Train loss 7.535, Val loss 7.536\n",
      "Ep 1 (Step 000020): Train loss 6.857, Val loss 6.861\n",
      "Ep 1 (Step 000030): Train loss 6.540, Val loss 6.500\n",
      "Ep 1 (Step 000040): Train loss 6.324, Val loss 6.386\n",
      "Ep 1 (Step 000050): Train loss 6.348, Val loss 6.338\n",
      "Ep 1 (Step 000060): Train loss 6.235, Val loss 6.249\n",
      "Ep 1 (Step 000070): Train loss 6.048, Val loss 6.143\n",
      "Ep 1 (Step 000080): Train loss 6.001, Val loss 6.057\n",
      "Ep 1 (Step 000090): Train loss 6.031, Val loss 6.009\n",
      "Ep 1 (Step 000100): Train loss 5.819, Val loss 5.946\n",
      "Ep 1 (Step 000110): Train loss 5.902, Val loss 5.882\n",
      "Ep 1 (Step 000120): Train loss 5.852, Val loss 5.824\n",
      "Ep 1 (Step 000130): Train loss 5.715, Val loss 5.797\n",
      "Ep 1 (Step 000140): Train loss 5.603, Val loss 5.748\n",
      "Ep 1 (Step 000150): Train loss 5.600, Val loss 5.720\n",
      "Ep 1 (Step 000160): Train loss 5.610, Val loss 5.680\n",
      "Ep 1 (Step 000170): Train loss 5.620, Val loss 5.646\n",
      "Ep 1 (Step 000180): Train loss 5.547, Val loss 5.614\n",
      "Ep 1 (Step 000190): Train loss 5.419, Val loss 5.584\n",
      "Ep 1 (Step 000200): Train loss 5.455, Val loss 5.555\n",
      "Ep 1 (Step 000210): Train loss 5.482, Val loss 5.546\n",
      "Ep 1 (Step 000220): Train loss 5.419, Val loss 5.516\n",
      "Ep 1 (Step 000230): Train loss 5.349, Val loss 5.500\n",
      "Ep 1 (Step 000240): Train loss 5.442, Val loss 5.480\n",
      "Ep 1 (Step 000250): Train loss 5.433, Val loss 5.468\n",
      "Ep 1 (Step 000260): Train loss 5.415, Val loss 5.444\n",
      "Ep 1 (Step 000270): Train loss 5.339, Val loss 5.426\n",
      "Ep 1 (Step 000280): Train loss 5.285, Val loss 5.421\n",
      "Ep 1 (Step 000290): Train loss 5.245, Val loss 5.407\n",
      "Ep 1 (Step 000300): Train loss 5.375, Val loss 5.379\n",
      "Ep 1 (Step 000310): Train loss 5.253, Val loss 5.384\n",
      "Ep 1 (Step 000320): Train loss 5.244, Val loss 5.370\n",
      "Ep 1 (Step 000330): Train loss 5.224, Val loss 5.349\n",
      "Ep 1 (Step 000340): Train loss 5.205, Val loss 5.350\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3502\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.133, Val loss 9.117\n",
      "Ep 1 (Step 000010): Train loss 7.478, Val loss 7.464\n",
      "Ep 1 (Step 000020): Train loss 6.786, Val loss 6.804\n",
      "Ep 1 (Step 000030): Train loss 6.552, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.406, Val loss 6.398\n",
      "Ep 1 (Step 000050): Train loss 6.276, Val loss 6.341\n",
      "Ep 1 (Step 000060): Train loss 6.313, Val loss 6.268\n",
      "Ep 1 (Step 000070): Train loss 6.163, Val loss 6.161\n",
      "Ep 1 (Step 000080): Train loss 6.111, Val loss 6.074\n",
      "Ep 1 (Step 000090): Train loss 6.028, Val loss 6.016\n",
      "Ep 1 (Step 000100): Train loss 5.923, Val loss 5.942\n",
      "Ep 1 (Step 000110): Train loss 5.919, Val loss 5.886\n",
      "Ep 1 (Step 000120): Train loss 5.788, Val loss 5.832\n",
      "Ep 1 (Step 000130): Train loss 5.751, Val loss 5.798\n",
      "Ep 1 (Step 000140): Train loss 5.665, Val loss 5.773\n",
      "Ep 1 (Step 000150): Train loss 5.713, Val loss 5.722\n",
      "Ep 1 (Step 000160): Train loss 5.650, Val loss 5.681\n",
      "Ep 1 (Step 000170): Train loss 5.615, Val loss 5.654\n",
      "Ep 1 (Step 000180): Train loss 5.550, Val loss 5.629\n",
      "Ep 1 (Step 000190): Train loss 5.581, Val loss 5.614\n",
      "Ep 1 (Step 000200): Train loss 5.402, Val loss 5.577\n",
      "Ep 1 (Step 000210): Train loss 5.436, Val loss 5.561\n",
      "Ep 1 (Step 000220): Train loss 5.379, Val loss 5.534\n",
      "Ep 1 (Step 000230): Train loss 5.431, Val loss 5.505\n",
      "Ep 1 (Step 000240): Train loss 5.409, Val loss 5.491\n",
      "Ep 1 (Step 000250): Train loss 5.386, Val loss 5.473\n",
      "Ep 1 (Step 000260): Train loss 5.377, Val loss 5.468\n",
      "Ep 1 (Step 000270): Train loss 5.308, Val loss 5.449\n",
      "Ep 1 (Step 000280): Train loss 5.272, Val loss 5.438\n",
      "Ep 1 (Step 000290): Train loss 5.307, Val loss 5.430\n",
      "Ep 1 (Step 000300): Train loss 5.270, Val loss 5.407\n",
      "Ep 1 (Step 000310): Train loss 5.320, Val loss 5.402\n",
      "Ep 1 (Step 000320): Train loss 5.281, Val loss 5.380\n",
      "Ep 1 (Step 000330): Train loss 5.275, Val loss 5.370\n",
      "Ep 1 (Step 000340): Train loss 5.247, Val loss 5.359\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3586\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.157, Val loss 9.136\n",
      "Ep 1 (Step 000010): Train loss 7.506, Val loss 7.463\n",
      "Ep 1 (Step 000020): Train loss 6.813, Val loss 6.786\n",
      "Ep 1 (Step 000030): Train loss 6.435, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.401, Val loss 6.403\n",
      "Ep 1 (Step 000050): Train loss 6.344, Val loss 6.368\n",
      "Ep 1 (Step 000060): Train loss 6.318, Val loss 6.315\n",
      "Ep 1 (Step 000070): Train loss 6.236, Val loss 6.222\n",
      "Ep 1 (Step 000080): Train loss 6.123, Val loss 6.136\n",
      "Ep 1 (Step 000090): Train loss 5.992, Val loss 6.034\n",
      "Ep 1 (Step 000100): Train loss 5.945, Val loss 5.969\n",
      "Ep 1 (Step 000110): Train loss 5.901, Val loss 5.923\n",
      "Ep 1 (Step 000120): Train loss 5.858, Val loss 5.853\n",
      "Ep 1 (Step 000130): Train loss 5.708, Val loss 5.805\n",
      "Ep 1 (Step 000140): Train loss 5.743, Val loss 5.756\n",
      "Ep 1 (Step 000150): Train loss 5.594, Val loss 5.718\n",
      "Ep 1 (Step 000160): Train loss 5.601, Val loss 5.688\n",
      "Ep 1 (Step 000170): Train loss 5.590, Val loss 5.646\n",
      "Ep 1 (Step 000180): Train loss 5.604, Val loss 5.621\n",
      "Ep 1 (Step 000190): Train loss 5.529, Val loss 5.611\n",
      "Ep 1 (Step 000200): Train loss 5.482, Val loss 5.581\n",
      "Ep 1 (Step 000210): Train loss 5.482, Val loss 5.556\n",
      "Ep 1 (Step 000220): Train loss 5.437, Val loss 5.526\n",
      "Ep 1 (Step 000230): Train loss 5.384, Val loss 5.504\n",
      "Ep 1 (Step 000240): Train loss 5.308, Val loss 5.495\n",
      "Ep 1 (Step 000250): Train loss 5.346, Val loss 5.470\n",
      "Ep 1 (Step 000260): Train loss 5.298, Val loss 5.454\n",
      "Ep 1 (Step 000270): Train loss 5.263, Val loss 5.427\n",
      "Ep 1 (Step 000280): Train loss 5.246, Val loss 5.420\n",
      "Ep 1 (Step 000290): Train loss 5.289, Val loss 5.400\n",
      "Ep 1 (Step 000300): Train loss 5.292, Val loss 5.390\n",
      "Ep 1 (Step 000310): Train loss 5.310, Val loss 5.373\n",
      "Ep 1 (Step 000320): Train loss 5.263, Val loss 5.368\n",
      "Ep 1 (Step 000330): Train loss 5.243, Val loss 5.358\n",
      "Ep 1 (Step 000340): Train loss 5.178, Val loss 5.363\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3628\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.155, Val loss 9.149\n",
      "Ep 1 (Step 000010): Train loss 7.489, Val loss 7.449\n",
      "Ep 1 (Step 000020): Train loss 6.845, Val loss 6.784\n",
      "Ep 1 (Step 000030): Train loss 6.529, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.405, Val loss 6.402\n",
      "Ep 1 (Step 000050): Train loss 6.348, Val loss 6.375\n",
      "Ep 1 (Step 000060): Train loss 6.349, Val loss 6.312\n",
      "Ep 1 (Step 000070): Train loss 6.178, Val loss 6.208\n",
      "Ep 1 (Step 000080): Train loss 6.055, Val loss 6.126\n",
      "Ep 1 (Step 000090): Train loss 6.000, Val loss 6.028\n",
      "Ep 1 (Step 000100): Train loss 5.962, Val loss 5.956\n",
      "Ep 1 (Step 000110): Train loss 5.892, Val loss 5.907\n",
      "Ep 1 (Step 000120): Train loss 5.775, Val loss 5.866\n",
      "Ep 1 (Step 000130): Train loss 5.749, Val loss 5.808\n",
      "Ep 1 (Step 000140): Train loss 5.715, Val loss 5.774\n",
      "Ep 1 (Step 000150): Train loss 5.658, Val loss 5.719\n",
      "Ep 1 (Step 000160): Train loss 5.686, Val loss 5.685\n",
      "Ep 1 (Step 000170): Train loss 5.552, Val loss 5.650\n",
      "Ep 1 (Step 000180): Train loss 5.590, Val loss 5.622\n",
      "Ep 1 (Step 000190): Train loss 5.453, Val loss 5.585\n",
      "Ep 1 (Step 000200): Train loss 5.477, Val loss 5.579\n",
      "Ep 1 (Step 000210): Train loss 5.473, Val loss 5.542\n",
      "Ep 1 (Step 000220): Train loss 5.406, Val loss 5.522\n",
      "Ep 1 (Step 000230): Train loss 5.304, Val loss 5.507\n",
      "Ep 1 (Step 000240): Train loss 5.371, Val loss 5.488\n",
      "Ep 1 (Step 000250): Train loss 5.309, Val loss 5.476\n",
      "Ep 1 (Step 000260): Train loss 5.323, Val loss 5.457\n",
      "Ep 1 (Step 000270): Train loss 5.326, Val loss 5.442\n",
      "Ep 1 (Step 000280): Train loss 5.318, Val loss 5.424\n",
      "Ep 1 (Step 000290): Train loss 5.295, Val loss 5.407\n",
      "Ep 1 (Step 000300): Train loss 5.221, Val loss 5.389\n",
      "Ep 1 (Step 000310): Train loss 5.278, Val loss 5.379\n",
      "Ep 1 (Step 000320): Train loss 5.256, Val loss 5.372\n",
      "Ep 1 (Step 000330): Train loss 5.217, Val loss 5.361\n",
      "Ep 1 (Step 000340): Train loss 5.215, Val loss 5.342\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3415\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.130, Val loss 9.134\n",
      "Ep 1 (Step 000010): Train loss 7.518, Val loss 7.490\n",
      "Ep 1 (Step 000020): Train loss 6.878, Val loss 6.824\n",
      "Ep 1 (Step 000030): Train loss 6.557, Val loss 6.503\n",
      "Ep 1 (Step 000040): Train loss 6.354, Val loss 6.412\n",
      "Ep 1 (Step 000050): Train loss 6.399, Val loss 6.372\n",
      "Ep 1 (Step 000060): Train loss 6.322, Val loss 6.316\n",
      "Ep 1 (Step 000070): Train loss 6.127, Val loss 6.221\n",
      "Ep 1 (Step 000080): Train loss 6.154, Val loss 6.107\n",
      "Ep 1 (Step 000090): Train loss 5.916, Val loss 6.010\n",
      "Ep 1 (Step 000100): Train loss 5.915, Val loss 5.941\n",
      "Ep 1 (Step 000110): Train loss 5.833, Val loss 5.893\n",
      "Ep 1 (Step 000120): Train loss 5.819, Val loss 5.825\n",
      "Ep 1 (Step 000130): Train loss 5.700, Val loss 5.787\n",
      "Ep 1 (Step 000140): Train loss 5.730, Val loss 5.758\n",
      "Ep 1 (Step 000150): Train loss 5.617, Val loss 5.702\n",
      "Ep 1 (Step 000160): Train loss 5.663, Val loss 5.659\n",
      "Ep 1 (Step 000170): Train loss 5.536, Val loss 5.645\n",
      "Ep 1 (Step 000180): Train loss 5.515, Val loss 5.619\n",
      "Ep 1 (Step 000190): Train loss 5.461, Val loss 5.575\n",
      "Ep 1 (Step 000200): Train loss 5.478, Val loss 5.571\n",
      "Ep 1 (Step 000210): Train loss 5.487, Val loss 5.545\n",
      "Ep 1 (Step 000220): Train loss 5.484, Val loss 5.512\n",
      "Ep 1 (Step 000230): Train loss 5.395, Val loss 5.492\n",
      "Ep 1 (Step 000240): Train loss 5.327, Val loss 5.473\n",
      "Ep 1 (Step 000250): Train loss 5.415, Val loss 5.457\n",
      "Ep 1 (Step 000260): Train loss 5.348, Val loss 5.435\n",
      "Ep 1 (Step 000270): Train loss 5.435, Val loss 5.425\n",
      "Ep 1 (Step 000280): Train loss 5.293, Val loss 5.412\n",
      "Ep 1 (Step 000290): Train loss 5.210, Val loss 5.394\n",
      "Ep 1 (Step 000300): Train loss 5.324, Val loss 5.393\n",
      "Ep 1 (Step 000310): Train loss 5.213, Val loss 5.367\n",
      "Ep 1 (Step 000320): Train loss 5.186, Val loss 5.365\n",
      "Ep 1 (Step 000330): Train loss 5.206, Val loss 5.347\n",
      "Ep 1 (Step 000340): Train loss 5.232, Val loss 5.347\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3469\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.955, Val loss 8.941\n",
      "Ep 1 (Step 000010): Train loss 6.901, Val loss 6.898\n",
      "Ep 1 (Step 000020): Train loss 6.508, Val loss 6.493\n",
      "Ep 1 (Step 000030): Train loss 6.468, Val loss 6.471\n",
      "Ep 1 (Step 000040): Train loss 6.343, Val loss 6.328\n",
      "Ep 1 (Step 000050): Train loss 6.134, Val loss 6.151\n",
      "Ep 1 (Step 000060): Train loss 6.033, Val loss 6.021\n",
      "Ep 1 (Step 000070): Train loss 5.829, Val loss 5.933\n",
      "Ep 1 (Step 000080): Train loss 5.785, Val loss 5.859\n",
      "Ep 1 (Step 000090): Train loss 5.667, Val loss 5.781\n",
      "Ep 1 (Step 000100): Train loss 5.669, Val loss 5.706\n",
      "Ep 1 (Step 000110): Train loss 5.561, Val loss 5.654\n",
      "Ep 1 (Step 000120): Train loss 5.591, Val loss 5.605\n",
      "Ep 1 (Step 000130): Train loss 5.470, Val loss 5.582\n",
      "Ep 1 (Step 000140): Train loss 5.512, Val loss 5.559\n",
      "Ep 1 (Step 000150): Train loss 5.361, Val loss 5.519\n",
      "Ep 1 (Step 000160): Train loss 5.343, Val loss 5.487\n",
      "Ep 1 (Step 000170): Train loss 5.295, Val loss 5.486\n",
      "Ep 1 (Step 000180): Train loss 5.295, Val loss 5.453\n",
      "Ep 1 (Step 000190): Train loss 5.334, Val loss 5.405\n",
      "Ep 1 (Step 000200): Train loss 5.361, Val loss 5.401\n",
      "Ep 1 (Step 000210): Train loss 5.236, Val loss 5.398\n",
      "Ep 1 (Step 000220): Train loss 5.250, Val loss 5.375\n",
      "Ep 1 (Step 000230): Train loss 5.261, Val loss 5.366\n",
      "Ep 1 (Step 000240): Train loss 5.171, Val loss 5.326\n",
      "Ep 1 (Step 000250): Train loss 5.205, Val loss 5.350\n",
      "Ep 1 (Step 000260): Train loss 5.144, Val loss 5.318\n",
      "Ep 1 (Step 000270): Train loss 5.200, Val loss 5.320\n",
      "Ep 1 (Step 000280): Train loss 5.192, Val loss 5.293\n",
      "Ep 1 (Step 000290): Train loss 5.085, Val loss 5.293\n",
      "Ep 1 (Step 000300): Train loss 5.158, Val loss 5.263\n",
      "Ep 1 (Step 000310): Train loss 5.070, Val loss 5.269\n",
      "Ep 1 (Step 000320): Train loss 5.103, Val loss 5.239\n",
      "Ep 1 (Step 000330): Train loss 5.077, Val loss 5.229\n",
      "Ep 1 (Step 000340): Train loss 5.052, Val loss 5.227\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2273\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.963, Val loss 8.969\n",
      "Ep 1 (Step 000010): Train loss 6.964, Val loss 6.912\n",
      "Ep 1 (Step 000020): Train loss 6.464, Val loss 6.464\n",
      "Ep 1 (Step 000030): Train loss 6.430, Val loss 6.426\n",
      "Ep 1 (Step 000040): Train loss 6.344, Val loss 6.297\n",
      "Ep 1 (Step 000050): Train loss 6.082, Val loss 6.145\n",
      "Ep 1 (Step 000060): Train loss 6.020, Val loss 6.020\n",
      "Ep 1 (Step 000070): Train loss 5.954, Val loss 5.899\n",
      "Ep 1 (Step 000080): Train loss 5.802, Val loss 5.822\n",
      "Ep 1 (Step 000090): Train loss 5.625, Val loss 5.764\n",
      "Ep 1 (Step 000100): Train loss 5.647, Val loss 5.710\n",
      "Ep 1 (Step 000110): Train loss 5.631, Val loss 5.647\n",
      "Ep 1 (Step 000120): Train loss 5.536, Val loss 5.609\n",
      "Ep 1 (Step 000130): Train loss 5.459, Val loss 5.576\n",
      "Ep 1 (Step 000140): Train loss 5.373, Val loss 5.554\n",
      "Ep 1 (Step 000150): Train loss 5.392, Val loss 5.524\n",
      "Ep 1 (Step 000160): Train loss 5.405, Val loss 5.500\n",
      "Ep 1 (Step 000170): Train loss 5.305, Val loss 5.479\n",
      "Ep 1 (Step 000180): Train loss 5.344, Val loss 5.452\n",
      "Ep 1 (Step 000190): Train loss 5.250, Val loss 5.426\n",
      "Ep 1 (Step 000200): Train loss 5.280, Val loss 5.397\n",
      "Ep 1 (Step 000210): Train loss 5.309, Val loss 5.379\n",
      "Ep 1 (Step 000220): Train loss 5.344, Val loss 5.371\n",
      "Ep 1 (Step 000230): Train loss 5.297, Val loss 5.351\n",
      "Ep 1 (Step 000240): Train loss 5.230, Val loss 5.352\n",
      "Ep 1 (Step 000250): Train loss 5.092, Val loss 5.325\n",
      "Ep 1 (Step 000260): Train loss 5.116, Val loss 5.315\n",
      "Ep 1 (Step 000270): Train loss 5.148, Val loss 5.298\n",
      "Ep 1 (Step 000280): Train loss 5.166, Val loss 5.287\n",
      "Ep 1 (Step 000290): Train loss 5.126, Val loss 5.273\n",
      "Ep 1 (Step 000300): Train loss 5.148, Val loss 5.285\n",
      "Ep 1 (Step 000310): Train loss 5.093, Val loss 5.277\n",
      "Ep 1 (Step 000320): Train loss 5.106, Val loss 5.256\n",
      "Ep 1 (Step 000330): Train loss 5.073, Val loss 5.248\n",
      "Ep 1 (Step 000340): Train loss 5.092, Val loss 5.231\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2306\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.907, Val loss 8.887\n",
      "Ep 1 (Step 000010): Train loss 6.865, Val loss 6.837\n",
      "Ep 1 (Step 000020): Train loss 6.486, Val loss 6.444\n",
      "Ep 1 (Step 000030): Train loss 6.482, Val loss 6.440\n",
      "Ep 1 (Step 000040): Train loss 6.427, Val loss 6.328\n",
      "Ep 1 (Step 000050): Train loss 6.155, Val loss 6.158\n",
      "Ep 1 (Step 000060): Train loss 6.054, Val loss 6.043\n",
      "Ep 1 (Step 000070): Train loss 5.912, Val loss 5.952\n",
      "Ep 1 (Step 000080): Train loss 5.795, Val loss 5.849\n",
      "Ep 1 (Step 000090): Train loss 5.671, Val loss 5.775\n",
      "Ep 1 (Step 000100): Train loss 5.611, Val loss 5.709\n",
      "Ep 1 (Step 000110): Train loss 5.699, Val loss 5.668\n",
      "Ep 1 (Step 000120): Train loss 5.562, Val loss 5.615\n",
      "Ep 1 (Step 000130): Train loss 5.413, Val loss 5.578\n",
      "Ep 1 (Step 000140): Train loss 5.555, Val loss 5.546\n",
      "Ep 1 (Step 000150): Train loss 5.420, Val loss 5.516\n",
      "Ep 1 (Step 000160): Train loss 5.379, Val loss 5.507\n",
      "Ep 1 (Step 000170): Train loss 5.455, Val loss 5.455\n",
      "Ep 1 (Step 000180): Train loss 5.300, Val loss 5.429\n",
      "Ep 1 (Step 000190): Train loss 5.319, Val loss 5.422\n",
      "Ep 1 (Step 000200): Train loss 5.304, Val loss 5.405\n",
      "Ep 1 (Step 000210): Train loss 5.270, Val loss 5.382\n",
      "Ep 1 (Step 000220): Train loss 5.248, Val loss 5.355\n",
      "Ep 1 (Step 000230): Train loss 5.261, Val loss 5.354\n",
      "Ep 1 (Step 000240): Train loss 5.182, Val loss 5.351\n",
      "Ep 1 (Step 000250): Train loss 5.157, Val loss 5.324\n",
      "Ep 1 (Step 000260): Train loss 5.232, Val loss 5.314\n",
      "Ep 1 (Step 000270): Train loss 5.163, Val loss 5.292\n",
      "Ep 1 (Step 000280): Train loss 5.239, Val loss 5.269\n",
      "Ep 1 (Step 000290): Train loss 5.104, Val loss 5.270\n",
      "Ep 1 (Step 000300): Train loss 5.044, Val loss 5.241\n",
      "Ep 1 (Step 000310): Train loss 5.063, Val loss 5.258\n",
      "Ep 1 (Step 000320): Train loss 5.035, Val loss 5.264\n",
      "Ep 1 (Step 000330): Train loss 5.033, Val loss 5.251\n",
      "Ep 1 (Step 000340): Train loss 5.052, Val loss 5.238\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2383\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.896, Val loss 8.884\n",
      "Ep 1 (Step 000010): Train loss 6.923, Val loss 6.895\n",
      "Ep 1 (Step 000020): Train loss 6.479, Val loss 6.467\n",
      "Ep 1 (Step 000030): Train loss 6.429, Val loss 6.444\n",
      "Ep 1 (Step 000040): Train loss 6.331, Val loss 6.299\n",
      "Ep 1 (Step 000050): Train loss 6.132, Val loss 6.164\n",
      "Ep 1 (Step 000060): Train loss 5.960, Val loss 6.012\n",
      "Ep 1 (Step 000070): Train loss 5.870, Val loss 5.926\n",
      "Ep 1 (Step 000080): Train loss 5.805, Val loss 5.833\n",
      "Ep 1 (Step 000090): Train loss 5.709, Val loss 5.759\n",
      "Ep 1 (Step 000100): Train loss 5.629, Val loss 5.695\n",
      "Ep 1 (Step 000110): Train loss 5.579, Val loss 5.633\n",
      "Ep 1 (Step 000120): Train loss 5.547, Val loss 5.593\n",
      "Ep 1 (Step 000130): Train loss 5.486, Val loss 5.563\n",
      "Ep 1 (Step 000140): Train loss 5.435, Val loss 5.537\n",
      "Ep 1 (Step 000150): Train loss 5.319, Val loss 5.501\n",
      "Ep 1 (Step 000160): Train loss 5.410, Val loss 5.489\n",
      "Ep 1 (Step 000170): Train loss 5.422, Val loss 5.464\n",
      "Ep 1 (Step 000180): Train loss 5.390, Val loss 5.444\n",
      "Ep 1 (Step 000190): Train loss 5.363, Val loss 5.415\n",
      "Ep 1 (Step 000200): Train loss 5.243, Val loss 5.377\n",
      "Ep 1 (Step 000210): Train loss 5.240, Val loss 5.368\n",
      "Ep 1 (Step 000220): Train loss 5.244, Val loss 5.350\n",
      "Ep 1 (Step 000230): Train loss 5.225, Val loss 5.334\n",
      "Ep 1 (Step 000240): Train loss 5.221, Val loss 5.318\n",
      "Ep 1 (Step 000250): Train loss 5.076, Val loss 5.309\n",
      "Ep 1 (Step 000260): Train loss 5.122, Val loss 5.281\n",
      "Ep 1 (Step 000270): Train loss 5.070, Val loss 5.273\n",
      "Ep 1 (Step 000280): Train loss 5.097, Val loss 5.248\n",
      "Ep 1 (Step 000290): Train loss 5.170, Val loss 5.249\n",
      "Ep 1 (Step 000300): Train loss 5.125, Val loss 5.241\n",
      "Ep 1 (Step 000310): Train loss 5.029, Val loss 5.238\n",
      "Ep 1 (Step 000320): Train loss 5.107, Val loss 5.216\n",
      "Ep 1 (Step 000330): Train loss 5.079, Val loss 5.224\n",
      "Ep 1 (Step 000340): Train loss 5.067, Val loss 5.200\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2002\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.875, Val loss 8.864\n",
      "Ep 1 (Step 000010): Train loss 6.878, Val loss 6.819\n",
      "Ep 1 (Step 000020): Train loss 6.442, Val loss 6.452\n",
      "Ep 1 (Step 000030): Train loss 6.450, Val loss 6.429\n",
      "Ep 1 (Step 000040): Train loss 6.330, Val loss 6.330\n",
      "Ep 1 (Step 000050): Train loss 6.241, Val loss 6.198\n",
      "Ep 1 (Step 000060): Train loss 5.958, Val loss 6.007\n",
      "Ep 1 (Step 000070): Train loss 5.898, Val loss 5.922\n",
      "Ep 1 (Step 000080): Train loss 5.813, Val loss 5.838\n",
      "Ep 1 (Step 000090): Train loss 5.703, Val loss 5.763\n",
      "Ep 1 (Step 000100): Train loss 5.699, Val loss 5.691\n",
      "Ep 1 (Step 000110): Train loss 5.588, Val loss 5.636\n",
      "Ep 1 (Step 000120): Train loss 5.560, Val loss 5.591\n",
      "Ep 1 (Step 000130): Train loss 5.564, Val loss 5.571\n",
      "Ep 1 (Step 000140): Train loss 5.360, Val loss 5.538\n",
      "Ep 1 (Step 000150): Train loss 5.372, Val loss 5.512\n",
      "Ep 1 (Step 000160): Train loss 5.349, Val loss 5.482\n",
      "Ep 1 (Step 000170): Train loss 5.295, Val loss 5.442\n",
      "Ep 1 (Step 000180): Train loss 5.271, Val loss 5.417\n",
      "Ep 1 (Step 000190): Train loss 5.304, Val loss 5.400\n",
      "Ep 1 (Step 000200): Train loss 5.210, Val loss 5.380\n",
      "Ep 1 (Step 000210): Train loss 5.284, Val loss 5.378\n",
      "Ep 1 (Step 000220): Train loss 5.244, Val loss 5.347\n",
      "Ep 1 (Step 000230): Train loss 5.210, Val loss 5.329\n",
      "Ep 1 (Step 000240): Train loss 5.189, Val loss 5.314\n",
      "Ep 1 (Step 000250): Train loss 5.154, Val loss 5.309\n",
      "Ep 1 (Step 000260): Train loss 5.155, Val loss 5.295\n",
      "Ep 1 (Step 000270): Train loss 5.244, Val loss 5.266\n",
      "Ep 1 (Step 000280): Train loss 5.225, Val loss 5.268\n",
      "Ep 1 (Step 000290): Train loss 5.159, Val loss 5.251\n",
      "Ep 1 (Step 000300): Train loss 5.080, Val loss 5.257\n",
      "Ep 1 (Step 000310): Train loss 5.128, Val loss 5.252\n",
      "Ep 1 (Step 000320): Train loss 5.082, Val loss 5.231\n",
      "Ep 1 (Step 000330): Train loss 5.064, Val loss 5.209\n",
      "Ep 1 (Step 000340): Train loss 5.033, Val loss 5.224\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2244\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.899, Val loss 8.890\n",
      "Ep 1 (Step 000010): Train loss 6.850, Val loss 6.848\n",
      "Ep 1 (Step 000020): Train loss 6.540, Val loss 6.487\n",
      "Ep 1 (Step 000030): Train loss 6.497, Val loss 6.420\n",
      "Ep 1 (Step 000040): Train loss 6.383, Val loss 6.320\n",
      "Ep 1 (Step 000050): Train loss 6.173, Val loss 6.181\n",
      "Ep 1 (Step 000060): Train loss 5.972, Val loss 6.025\n",
      "Ep 1 (Step 000070): Train loss 5.911, Val loss 5.923\n",
      "Ep 1 (Step 000080): Train loss 5.792, Val loss 5.839\n",
      "Ep 1 (Step 000090): Train loss 5.706, Val loss 5.751\n",
      "Ep 1 (Step 000100): Train loss 5.674, Val loss 5.690\n",
      "Ep 1 (Step 000110): Train loss 5.601, Val loss 5.646\n",
      "Ep 1 (Step 000120): Train loss 5.461, Val loss 5.583\n",
      "Ep 1 (Step 000130): Train loss 5.516, Val loss 5.560\n",
      "Ep 1 (Step 000140): Train loss 5.452, Val loss 5.539\n",
      "Ep 1 (Step 000150): Train loss 5.368, Val loss 5.495\n",
      "Ep 1 (Step 000160): Train loss 5.322, Val loss 5.471\n",
      "Ep 1 (Step 000170): Train loss 5.335, Val loss 5.431\n",
      "Ep 1 (Step 000180): Train loss 5.283, Val loss 5.401\n",
      "Ep 1 (Step 000190): Train loss 5.302, Val loss 5.392\n",
      "Ep 1 (Step 000200): Train loss 5.194, Val loss 5.362\n",
      "Ep 1 (Step 000210): Train loss 5.240, Val loss 5.342\n",
      "Ep 1 (Step 000220): Train loss 5.257, Val loss 5.327\n",
      "Ep 1 (Step 000230): Train loss 5.193, Val loss 5.326\n",
      "Ep 1 (Step 000240): Train loss 5.302, Val loss 5.308\n",
      "Ep 1 (Step 000250): Train loss 5.213, Val loss 5.300\n",
      "Ep 1 (Step 000260): Train loss 5.107, Val loss 5.287\n",
      "Ep 1 (Step 000270): Train loss 5.111, Val loss 5.254\n",
      "Ep 1 (Step 000280): Train loss 5.086, Val loss 5.259\n",
      "Ep 1 (Step 000290): Train loss 5.177, Val loss 5.263\n",
      "Ep 1 (Step 000300): Train loss 5.126, Val loss 5.253\n",
      "Ep 1 (Step 000310): Train loss 5.083, Val loss 5.224\n",
      "Ep 1 (Step 000320): Train loss 5.049, Val loss 5.210\n",
      "Ep 1 (Step 000330): Train loss 5.094, Val loss 5.200\n",
      "Ep 1 (Step 000340): Train loss 5.059, Val loss 5.208\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2079\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.069, Val loss 9.055\n",
      "Ep 1 (Step 000010): Train loss 7.468, Val loss 7.426\n",
      "Ep 1 (Step 000020): Train loss 6.828, Val loss 6.784\n",
      "Ep 1 (Step 000030): Train loss 6.516, Val loss 6.484\n",
      "Ep 1 (Step 000040): Train loss 6.432, Val loss 6.408\n",
      "Ep 1 (Step 000050): Train loss 6.399, Val loss 6.374\n",
      "Ep 1 (Step 000060): Train loss 6.323, Val loss 6.327\n",
      "Ep 1 (Step 000070): Train loss 6.174, Val loss 6.222\n",
      "Ep 1 (Step 000080): Train loss 6.113, Val loss 6.113\n",
      "Ep 1 (Step 000090): Train loss 6.163, Val loss 6.035\n",
      "Ep 1 (Step 000100): Train loss 5.925, Val loss 5.966\n",
      "Ep 1 (Step 000110): Train loss 5.901, Val loss 5.916\n",
      "Ep 1 (Step 000120): Train loss 5.840, Val loss 5.861\n",
      "Ep 1 (Step 000130): Train loss 5.791, Val loss 5.813\n",
      "Ep 1 (Step 000140): Train loss 5.685, Val loss 5.777\n",
      "Ep 1 (Step 000150): Train loss 5.789, Val loss 5.752\n",
      "Ep 1 (Step 000160): Train loss 5.654, Val loss 5.706\n",
      "Ep 1 (Step 000170): Train loss 5.634, Val loss 5.682\n",
      "Ep 1 (Step 000180): Train loss 5.616, Val loss 5.658\n",
      "Ep 1 (Step 000190): Train loss 5.654, Val loss 5.631\n",
      "Ep 1 (Step 000200): Train loss 5.517, Val loss 5.599\n",
      "Ep 1 (Step 000210): Train loss 5.439, Val loss 5.571\n",
      "Ep 1 (Step 000220): Train loss 5.453, Val loss 5.548\n",
      "Ep 1 (Step 000230): Train loss 5.460, Val loss 5.536\n",
      "Ep 1 (Step 000240): Train loss 5.372, Val loss 5.524\n",
      "Ep 1 (Step 000250): Train loss 5.450, Val loss 5.509\n",
      "Ep 1 (Step 000260): Train loss 5.479, Val loss 5.491\n",
      "Ep 1 (Step 000270): Train loss 5.375, Val loss 5.475\n",
      "Ep 1 (Step 000280): Train loss 5.350, Val loss 5.469\n",
      "Ep 1 (Step 000290): Train loss 5.314, Val loss 5.431\n",
      "Ep 1 (Step 000300): Train loss 5.338, Val loss 5.415\n",
      "Ep 1 (Step 000310): Train loss 5.307, Val loss 5.407\n",
      "Ep 1 (Step 000320): Train loss 5.301, Val loss 5.418\n",
      "Ep 1 (Step 000330): Train loss 5.265, Val loss 5.393\n",
      "Ep 1 (Step 000340): Train loss 5.207, Val loss 5.380\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3796\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.064, Val loss 9.034\n",
      "Ep 1 (Step 000010): Train loss 7.459, Val loss 7.442\n",
      "Ep 1 (Step 000020): Train loss 6.847, Val loss 6.780\n",
      "Ep 1 (Step 000030): Train loss 6.464, Val loss 6.468\n",
      "Ep 1 (Step 000040): Train loss 6.429, Val loss 6.391\n",
      "Ep 1 (Step 000050): Train loss 6.380, Val loss 6.361\n",
      "Ep 1 (Step 000060): Train loss 6.292, Val loss 6.312\n",
      "Ep 1 (Step 000070): Train loss 6.219, Val loss 6.207\n",
      "Ep 1 (Step 000080): Train loss 6.134, Val loss 6.098\n",
      "Ep 1 (Step 000090): Train loss 5.980, Val loss 6.035\n",
      "Ep 1 (Step 000100): Train loss 5.940, Val loss 5.961\n",
      "Ep 1 (Step 000110): Train loss 5.902, Val loss 5.924\n",
      "Ep 1 (Step 000120): Train loss 5.800, Val loss 5.873\n",
      "Ep 1 (Step 000130): Train loss 5.796, Val loss 5.815\n",
      "Ep 1 (Step 000140): Train loss 5.711, Val loss 5.781\n",
      "Ep 1 (Step 000150): Train loss 5.721, Val loss 5.758\n",
      "Ep 1 (Step 000160): Train loss 5.661, Val loss 5.706\n",
      "Ep 1 (Step 000170): Train loss 5.599, Val loss 5.673\n",
      "Ep 1 (Step 000180): Train loss 5.535, Val loss 5.637\n",
      "Ep 1 (Step 000190): Train loss 5.526, Val loss 5.619\n",
      "Ep 1 (Step 000200): Train loss 5.500, Val loss 5.609\n",
      "Ep 1 (Step 000210): Train loss 5.523, Val loss 5.584\n",
      "Ep 1 (Step 000220): Train loss 5.435, Val loss 5.556\n",
      "Ep 1 (Step 000230): Train loss 5.466, Val loss 5.537\n",
      "Ep 1 (Step 000240): Train loss 5.452, Val loss 5.529\n",
      "Ep 1 (Step 000250): Train loss 5.478, Val loss 5.522\n",
      "Ep 1 (Step 000260): Train loss 5.365, Val loss 5.493\n",
      "Ep 1 (Step 000270): Train loss 5.339, Val loss 5.487\n",
      "Ep 1 (Step 000280): Train loss 5.313, Val loss 5.467\n",
      "Ep 1 (Step 000290): Train loss 5.386, Val loss 5.462\n",
      "Ep 1 (Step 000300): Train loss 5.401, Val loss 5.462\n",
      "Ep 1 (Step 000310): Train loss 5.278, Val loss 5.432\n",
      "Ep 1 (Step 000320): Train loss 5.314, Val loss 5.441\n",
      "Ep 1 (Step 000330): Train loss 5.289, Val loss 5.427\n",
      "Ep 1 (Step 000340): Train loss 5.284, Val loss 5.404\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.4040\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.070, Val loss 9.057\n",
      "Ep 1 (Step 000010): Train loss 7.472, Val loss 7.474\n",
      "Ep 1 (Step 000020): Train loss 6.831, Val loss 6.805\n",
      "Ep 1 (Step 000030): Train loss 6.464, Val loss 6.475\n",
      "Ep 1 (Step 000040): Train loss 6.390, Val loss 6.388\n",
      "Ep 1 (Step 000050): Train loss 6.410, Val loss 6.353\n",
      "Ep 1 (Step 000060): Train loss 6.324, Val loss 6.284\n",
      "Ep 1 (Step 000070): Train loss 6.128, Val loss 6.170\n",
      "Ep 1 (Step 000080): Train loss 6.200, Val loss 6.089\n",
      "Ep 1 (Step 000090): Train loss 6.007, Val loss 6.025\n",
      "Ep 1 (Step 000100): Train loss 5.955, Val loss 5.961\n",
      "Ep 1 (Step 000110): Train loss 5.873, Val loss 5.908\n",
      "Ep 1 (Step 000120): Train loss 5.786, Val loss 5.862\n",
      "Ep 1 (Step 000130): Train loss 5.740, Val loss 5.820\n",
      "Ep 1 (Step 000140): Train loss 5.774, Val loss 5.784\n",
      "Ep 1 (Step 000150): Train loss 5.775, Val loss 5.743\n",
      "Ep 1 (Step 000160): Train loss 5.661, Val loss 5.714\n",
      "Ep 1 (Step 000170): Train loss 5.636, Val loss 5.668\n",
      "Ep 1 (Step 000180): Train loss 5.538, Val loss 5.657\n",
      "Ep 1 (Step 000190): Train loss 5.535, Val loss 5.613\n",
      "Ep 1 (Step 000200): Train loss 5.533, Val loss 5.589\n",
      "Ep 1 (Step 000210): Train loss 5.469, Val loss 5.559\n",
      "Ep 1 (Step 000220): Train loss 5.476, Val loss 5.551\n",
      "Ep 1 (Step 000230): Train loss 5.399, Val loss 5.531\n",
      "Ep 1 (Step 000240): Train loss 5.397, Val loss 5.520\n",
      "Ep 1 (Step 000250): Train loss 5.380, Val loss 5.502\n",
      "Ep 1 (Step 000260): Train loss 5.378, Val loss 5.495\n",
      "Ep 1 (Step 000270): Train loss 5.360, Val loss 5.476\n",
      "Ep 1 (Step 000280): Train loss 5.340, Val loss 5.457\n",
      "Ep 1 (Step 000290): Train loss 5.359, Val loss 5.452\n",
      "Ep 1 (Step 000300): Train loss 5.331, Val loss 5.427\n",
      "Ep 1 (Step 000310): Train loss 5.203, Val loss 5.423\n",
      "Ep 1 (Step 000320): Train loss 5.302, Val loss 5.403\n",
      "Ep 1 (Step 000330): Train loss 5.257, Val loss 5.411\n",
      "Ep 1 (Step 000340): Train loss 5.258, Val loss 5.372\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3719\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.053, Val loss 9.046\n",
      "Ep 1 (Step 000010): Train loss 7.456, Val loss 7.410\n",
      "Ep 1 (Step 000020): Train loss 6.782, Val loss 6.785\n",
      "Ep 1 (Step 000030): Train loss 6.475, Val loss 6.489\n",
      "Ep 1 (Step 000040): Train loss 6.467, Val loss 6.419\n",
      "Ep 1 (Step 000050): Train loss 6.364, Val loss 6.391\n",
      "Ep 1 (Step 000060): Train loss 6.363, Val loss 6.344\n",
      "Ep 1 (Step 000070): Train loss 6.188, Val loss 6.256\n",
      "Ep 1 (Step 000080): Train loss 6.212, Val loss 6.147\n",
      "Ep 1 (Step 000090): Train loss 6.040, Val loss 6.061\n",
      "Ep 1 (Step 000100): Train loss 5.981, Val loss 5.990\n",
      "Ep 1 (Step 000110): Train loss 5.892, Val loss 5.935\n",
      "Ep 1 (Step 000120): Train loss 5.832, Val loss 5.879\n",
      "Ep 1 (Step 000130): Train loss 5.776, Val loss 5.829\n",
      "Ep 1 (Step 000140): Train loss 5.740, Val loss 5.810\n",
      "Ep 1 (Step 000150): Train loss 5.769, Val loss 5.762\n",
      "Ep 1 (Step 000160): Train loss 5.739, Val loss 5.721\n",
      "Ep 1 (Step 000170): Train loss 5.658, Val loss 5.693\n",
      "Ep 1 (Step 000180): Train loss 5.615, Val loss 5.654\n",
      "Ep 1 (Step 000190): Train loss 5.575, Val loss 5.640\n",
      "Ep 1 (Step 000200): Train loss 5.586, Val loss 5.618\n",
      "Ep 1 (Step 000210): Train loss 5.478, Val loss 5.586\n",
      "Ep 1 (Step 000220): Train loss 5.411, Val loss 5.561\n",
      "Ep 1 (Step 000230): Train loss 5.486, Val loss 5.544\n",
      "Ep 1 (Step 000240): Train loss 5.398, Val loss 5.534\n",
      "Ep 1 (Step 000250): Train loss 5.457, Val loss 5.499\n",
      "Ep 1 (Step 000260): Train loss 5.359, Val loss 5.472\n",
      "Ep 1 (Step 000270): Train loss 5.321, Val loss 5.467\n",
      "Ep 1 (Step 000280): Train loss 5.304, Val loss 5.455\n",
      "Ep 1 (Step 000290): Train loss 5.296, Val loss 5.435\n",
      "Ep 1 (Step 000300): Train loss 5.324, Val loss 5.416\n",
      "Ep 1 (Step 000310): Train loss 5.398, Val loss 5.418\n",
      "Ep 1 (Step 000320): Train loss 5.231, Val loss 5.388\n",
      "Ep 1 (Step 000330): Train loss 5.282, Val loss 5.390\n",
      "Ep 1 (Step 000340): Train loss 5.283, Val loss 5.367\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3674\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.096, Val loss 9.079\n",
      "Ep 1 (Step 000010): Train loss 7.457, Val loss 7.478\n",
      "Ep 1 (Step 000020): Train loss 6.862, Val loss 6.814\n",
      "Ep 1 (Step 000030): Train loss 6.532, Val loss 6.471\n",
      "Ep 1 (Step 000040): Train loss 6.435, Val loss 6.396\n",
      "Ep 1 (Step 000050): Train loss 6.433, Val loss 6.376\n",
      "Ep 1 (Step 000060): Train loss 6.283, Val loss 6.325\n",
      "Ep 1 (Step 000070): Train loss 6.239, Val loss 6.238\n",
      "Ep 1 (Step 000080): Train loss 6.156, Val loss 6.149\n",
      "Ep 1 (Step 000090): Train loss 6.015, Val loss 6.052\n",
      "Ep 1 (Step 000100): Train loss 5.972, Val loss 5.992\n",
      "Ep 1 (Step 000110): Train loss 5.876, Val loss 5.939\n",
      "Ep 1 (Step 000120): Train loss 5.825, Val loss 5.868\n",
      "Ep 1 (Step 000130): Train loss 5.792, Val loss 5.834\n",
      "Ep 1 (Step 000140): Train loss 5.717, Val loss 5.783\n",
      "Ep 1 (Step 000150): Train loss 5.702, Val loss 5.754\n",
      "Ep 1 (Step 000160): Train loss 5.646, Val loss 5.719\n",
      "Ep 1 (Step 000170): Train loss 5.591, Val loss 5.688\n",
      "Ep 1 (Step 000180): Train loss 5.539, Val loss 5.648\n",
      "Ep 1 (Step 000190): Train loss 5.574, Val loss 5.621\n",
      "Ep 1 (Step 000200): Train loss 5.472, Val loss 5.584\n",
      "Ep 1 (Step 000210): Train loss 5.420, Val loss 5.556\n",
      "Ep 1 (Step 000220): Train loss 5.366, Val loss 5.536\n",
      "Ep 1 (Step 000230): Train loss 5.407, Val loss 5.535\n",
      "Ep 1 (Step 000240): Train loss 5.349, Val loss 5.505\n",
      "Ep 1 (Step 000250): Train loss 5.395, Val loss 5.492\n",
      "Ep 1 (Step 000260): Train loss 5.384, Val loss 5.486\n",
      "Ep 1 (Step 000270): Train loss 5.386, Val loss 5.465\n",
      "Ep 1 (Step 000280): Train loss 5.424, Val loss 5.451\n",
      "Ep 1 (Step 000290): Train loss 5.252, Val loss 5.445\n",
      "Ep 1 (Step 000300): Train loss 5.368, Val loss 5.423\n",
      "Ep 1 (Step 000310): Train loss 5.300, Val loss 5.414\n",
      "Ep 1 (Step 000320): Train loss 5.301, Val loss 5.399\n",
      "Ep 1 (Step 000330): Train loss 5.256, Val loss 5.379\n",
      "Ep 1 (Step 000340): Train loss 5.224, Val loss 5.370\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3700\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.046, Val loss 9.035\n",
      "Ep 1 (Step 000010): Train loss 7.454, Val loss 7.412\n",
      "Ep 1 (Step 000020): Train loss 6.781, Val loss 6.769\n",
      "Ep 1 (Step 000030): Train loss 6.455, Val loss 6.469\n",
      "Ep 1 (Step 000040): Train loss 6.434, Val loss 6.402\n",
      "Ep 1 (Step 000050): Train loss 6.412, Val loss 6.378\n",
      "Ep 1 (Step 000060): Train loss 6.352, Val loss 6.337\n",
      "Ep 1 (Step 000070): Train loss 6.183, Val loss 6.231\n",
      "Ep 1 (Step 000080): Train loss 6.130, Val loss 6.121\n",
      "Ep 1 (Step 000090): Train loss 6.013, Val loss 6.038\n",
      "Ep 1 (Step 000100): Train loss 5.926, Val loss 5.979\n",
      "Ep 1 (Step 000110): Train loss 5.881, Val loss 5.911\n",
      "Ep 1 (Step 000120): Train loss 5.836, Val loss 5.873\n",
      "Ep 1 (Step 000130): Train loss 5.830, Val loss 5.854\n",
      "Ep 1 (Step 000140): Train loss 5.740, Val loss 5.795\n",
      "Ep 1 (Step 000150): Train loss 5.675, Val loss 5.751\n",
      "Ep 1 (Step 000160): Train loss 5.562, Val loss 5.700\n",
      "Ep 1 (Step 000170): Train loss 5.562, Val loss 5.668\n",
      "Ep 1 (Step 000180): Train loss 5.641, Val loss 5.648\n",
      "Ep 1 (Step 000190): Train loss 5.561, Val loss 5.629\n",
      "Ep 1 (Step 000200): Train loss 5.541, Val loss 5.620\n",
      "Ep 1 (Step 000210): Train loss 5.517, Val loss 5.577\n",
      "Ep 1 (Step 000220): Train loss 5.474, Val loss 5.554\n",
      "Ep 1 (Step 000230): Train loss 5.433, Val loss 5.527\n",
      "Ep 1 (Step 000240): Train loss 5.422, Val loss 5.514\n",
      "Ep 1 (Step 000250): Train loss 5.524, Val loss 5.504\n",
      "Ep 1 (Step 000260): Train loss 5.325, Val loss 5.476\n",
      "Ep 1 (Step 000270): Train loss 5.357, Val loss 5.459\n",
      "Ep 1 (Step 000280): Train loss 5.373, Val loss 5.446\n",
      "Ep 1 (Step 000290): Train loss 5.309, Val loss 5.436\n",
      "Ep 1 (Step 000300): Train loss 5.339, Val loss 5.427\n",
      "Ep 1 (Step 000310): Train loss 5.359, Val loss 5.407\n",
      "Ep 1 (Step 000320): Train loss 5.273, Val loss 5.384\n",
      "Ep 1 (Step 000330): Train loss 5.376, Val loss 5.362\n",
      "Ep 1 (Step 000340): Train loss 5.238, Val loss 5.355\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3553\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.879, Val loss 8.833\n",
      "Ep 1 (Step 000010): Train loss 6.883, Val loss 6.834\n",
      "Ep 1 (Step 000020): Train loss 6.450, Val loss 6.473\n",
      "Ep 1 (Step 000030): Train loss 6.483, Val loss 6.479\n",
      "Ep 1 (Step 000040): Train loss 6.438, Val loss 6.395\n",
      "Ep 1 (Step 000050): Train loss 6.346, Val loss 6.272\n",
      "Ep 1 (Step 000060): Train loss 6.096, Val loss 6.064\n",
      "Ep 1 (Step 000070): Train loss 5.972, Val loss 5.992\n",
      "Ep 1 (Step 000080): Train loss 5.929, Val loss 5.915\n",
      "Ep 1 (Step 000090): Train loss 5.824, Val loss 5.819\n",
      "Ep 1 (Step 000100): Train loss 5.668, Val loss 5.763\n",
      "Ep 1 (Step 000110): Train loss 5.592, Val loss 5.716\n",
      "Ep 1 (Step 000120): Train loss 5.592, Val loss 5.653\n",
      "Ep 1 (Step 000130): Train loss 5.639, Val loss 5.637\n",
      "Ep 1 (Step 000140): Train loss 5.543, Val loss 5.591\n",
      "Ep 1 (Step 000150): Train loss 5.506, Val loss 5.558\n",
      "Ep 1 (Step 000160): Train loss 5.469, Val loss 5.545\n",
      "Ep 1 (Step 000170): Train loss 5.399, Val loss 5.528\n",
      "Ep 1 (Step 000180): Train loss 5.370, Val loss 5.492\n",
      "Ep 1 (Step 000190): Train loss 5.336, Val loss 5.477\n",
      "Ep 1 (Step 000200): Train loss 5.402, Val loss 5.466\n",
      "Ep 1 (Step 000210): Train loss 5.345, Val loss 5.435\n",
      "Ep 1 (Step 000220): Train loss 5.173, Val loss 5.410\n",
      "Ep 1 (Step 000230): Train loss 5.332, Val loss 5.418\n",
      "Ep 1 (Step 000240): Train loss 5.243, Val loss 5.405\n",
      "Ep 1 (Step 000250): Train loss 5.166, Val loss 5.371\n",
      "Ep 1 (Step 000260): Train loss 5.252, Val loss 5.370\n",
      "Ep 1 (Step 000270): Train loss 5.296, Val loss 5.376\n",
      "Ep 1 (Step 000280): Train loss 5.175, Val loss 5.360\n",
      "Ep 1 (Step 000290): Train loss 5.161, Val loss 5.319\n",
      "Ep 1 (Step 000300): Train loss 5.198, Val loss 5.307\n",
      "Ep 1 (Step 000310): Train loss 5.117, Val loss 5.303\n",
      "Ep 1 (Step 000320): Train loss 5.226, Val loss 5.316\n",
      "Ep 1 (Step 000330): Train loss 5.174, Val loss 5.304\n",
      "Ep 1 (Step 000340): Train loss 5.104, Val loss 5.290\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2903\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.836, Val loss 8.803\n",
      "Ep 1 (Step 000010): Train loss 6.906, Val loss 6.834\n",
      "Ep 1 (Step 000020): Train loss 6.541, Val loss 6.473\n",
      "Ep 1 (Step 000030): Train loss 6.484, Val loss 6.444\n",
      "Ep 1 (Step 000040): Train loss 6.388, Val loss 6.382\n",
      "Ep 1 (Step 000050): Train loss 6.170, Val loss 6.235\n",
      "Ep 1 (Step 000060): Train loss 6.052, Val loss 6.089\n",
      "Ep 1 (Step 000070): Train loss 5.940, Val loss 5.987\n",
      "Ep 1 (Step 000080): Train loss 5.808, Val loss 5.901\n",
      "Ep 1 (Step 000090): Train loss 5.803, Val loss 5.832\n",
      "Ep 1 (Step 000100): Train loss 5.747, Val loss 5.763\n",
      "Ep 1 (Step 000110): Train loss 5.678, Val loss 5.702\n",
      "Ep 1 (Step 000120): Train loss 5.571, Val loss 5.667\n",
      "Ep 1 (Step 000130): Train loss 5.477, Val loss 5.622\n",
      "Ep 1 (Step 000140): Train loss 5.575, Val loss 5.586\n",
      "Ep 1 (Step 000150): Train loss 5.500, Val loss 5.546\n",
      "Ep 1 (Step 000160): Train loss 5.447, Val loss 5.547\n",
      "Ep 1 (Step 000170): Train loss 5.402, Val loss 5.541\n",
      "Ep 1 (Step 000180): Train loss 5.375, Val loss 5.479\n",
      "Ep 1 (Step 000190): Train loss 5.340, Val loss 5.455\n",
      "Ep 1 (Step 000200): Train loss 5.365, Val loss 5.458\n",
      "Ep 1 (Step 000210): Train loss 5.257, Val loss 5.431\n",
      "Ep 1 (Step 000220): Train loss 5.308, Val loss 5.401\n",
      "Ep 1 (Step 000230): Train loss 5.365, Val loss 5.394\n",
      "Ep 1 (Step 000240): Train loss 5.253, Val loss 5.382\n",
      "Ep 1 (Step 000250): Train loss 5.252, Val loss 5.374\n",
      "Ep 1 (Step 000260): Train loss 5.228, Val loss 5.378\n",
      "Ep 1 (Step 000270): Train loss 5.350, Val loss 5.357\n",
      "Ep 1 (Step 000280): Train loss 5.276, Val loss 5.331\n",
      "Ep 1 (Step 000290): Train loss 5.056, Val loss 5.313\n",
      "Ep 1 (Step 000300): Train loss 5.226, Val loss 5.309\n",
      "Ep 1 (Step 000310): Train loss 5.155, Val loss 5.296\n",
      "Ep 1 (Step 000320): Train loss 5.221, Val loss 5.302\n",
      "Ep 1 (Step 000330): Train loss 5.144, Val loss 5.290\n",
      "Ep 1 (Step 000340): Train loss 5.152, Val loss 5.287\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2870\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.848, Val loss 8.808\n",
      "Ep 1 (Step 000010): Train loss 6.887, Val loss 6.842\n",
      "Ep 1 (Step 000020): Train loss 6.572, Val loss 6.470\n",
      "Ep 1 (Step 000030): Train loss 6.503, Val loss 6.495\n",
      "Ep 1 (Step 000040): Train loss 6.354, Val loss 6.395\n",
      "Ep 1 (Step 000050): Train loss 6.310, Val loss 6.280\n",
      "Ep 1 (Step 000060): Train loss 6.085, Val loss 6.102\n",
      "Ep 1 (Step 000070): Train loss 5.957, Val loss 6.006\n",
      "Ep 1 (Step 000080): Train loss 5.843, Val loss 5.908\n",
      "Ep 1 (Step 000090): Train loss 5.790, Val loss 5.849\n",
      "Ep 1 (Step 000100): Train loss 5.730, Val loss 5.774\n",
      "Ep 1 (Step 000110): Train loss 5.731, Val loss 5.710\n",
      "Ep 1 (Step 000120): Train loss 5.627, Val loss 5.684\n",
      "Ep 1 (Step 000130): Train loss 5.556, Val loss 5.664\n",
      "Ep 1 (Step 000140): Train loss 5.539, Val loss 5.616\n",
      "Ep 1 (Step 000150): Train loss 5.554, Val loss 5.593\n",
      "Ep 1 (Step 000160): Train loss 5.463, Val loss 5.540\n",
      "Ep 1 (Step 000170): Train loss 5.511, Val loss 5.532\n",
      "Ep 1 (Step 000180): Train loss 5.466, Val loss 5.517\n",
      "Ep 1 (Step 000190): Train loss 5.448, Val loss 5.484\n",
      "Ep 1 (Step 000200): Train loss 5.433, Val loss 5.464\n",
      "Ep 1 (Step 000210): Train loss 5.323, Val loss 5.456\n",
      "Ep 1 (Step 000220): Train loss 5.324, Val loss 5.436\n",
      "Ep 1 (Step 000230): Train loss 5.284, Val loss 5.421\n",
      "Ep 1 (Step 000240): Train loss 5.337, Val loss 5.416\n",
      "Ep 1 (Step 000250): Train loss 5.356, Val loss 5.391\n",
      "Ep 1 (Step 000260): Train loss 5.290, Val loss 5.368\n",
      "Ep 1 (Step 000270): Train loss 5.200, Val loss 5.369\n",
      "Ep 1 (Step 000280): Train loss 5.183, Val loss 5.370\n",
      "Ep 1 (Step 000290): Train loss 5.301, Val loss 5.335\n",
      "Ep 1 (Step 000300): Train loss 5.185, Val loss 5.354\n",
      "Ep 1 (Step 000310): Train loss 5.172, Val loss 5.328\n",
      "Ep 1 (Step 000320): Train loss 5.158, Val loss 5.302\n",
      "Ep 1 (Step 000330): Train loss 5.141, Val loss 5.306\n",
      "Ep 1 (Step 000340): Train loss 5.177, Val loss 5.274\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2745\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.826, Val loss 8.797\n",
      "Ep 1 (Step 000010): Train loss 6.886, Val loss 6.862\n",
      "Ep 1 (Step 000020): Train loss 6.506, Val loss 6.470\n",
      "Ep 1 (Step 000030): Train loss 6.497, Val loss 6.468\n",
      "Ep 1 (Step 000040): Train loss 6.435, Val loss 6.385\n",
      "Ep 1 (Step 000050): Train loss 6.225, Val loss 6.252\n",
      "Ep 1 (Step 000060): Train loss 6.137, Val loss 6.097\n",
      "Ep 1 (Step 000070): Train loss 5.940, Val loss 5.997\n",
      "Ep 1 (Step 000080): Train loss 5.944, Val loss 5.906\n",
      "Ep 1 (Step 000090): Train loss 5.722, Val loss 5.809\n",
      "Ep 1 (Step 000100): Train loss 5.705, Val loss 5.746\n",
      "Ep 1 (Step 000110): Train loss 5.630, Val loss 5.695\n",
      "Ep 1 (Step 000120): Train loss 5.589, Val loss 5.648\n",
      "Ep 1 (Step 000130): Train loss 5.558, Val loss 5.608\n",
      "Ep 1 (Step 000140): Train loss 5.509, Val loss 5.578\n",
      "Ep 1 (Step 000150): Train loss 5.511, Val loss 5.553\n",
      "Ep 1 (Step 000160): Train loss 5.412, Val loss 5.512\n",
      "Ep 1 (Step 000170): Train loss 5.380, Val loss 5.484\n",
      "Ep 1 (Step 000180): Train loss 5.334, Val loss 5.446\n",
      "Ep 1 (Step 000190): Train loss 5.377, Val loss 5.419\n",
      "Ep 1 (Step 000200): Train loss 5.363, Val loss 5.429\n",
      "Ep 1 (Step 000210): Train loss 5.280, Val loss 5.393\n",
      "Ep 1 (Step 000220): Train loss 5.235, Val loss 5.395\n",
      "Ep 1 (Step 000230): Train loss 5.266, Val loss 5.376\n",
      "Ep 1 (Step 000240): Train loss 5.304, Val loss 5.350\n",
      "Ep 1 (Step 000250): Train loss 5.303, Val loss 5.357\n",
      "Ep 1 (Step 000260): Train loss 5.168, Val loss 5.340\n",
      "Ep 1 (Step 000270): Train loss 5.191, Val loss 5.324\n",
      "Ep 1 (Step 000280): Train loss 5.172, Val loss 5.319\n",
      "Ep 1 (Step 000290): Train loss 5.106, Val loss 5.310\n",
      "Ep 1 (Step 000300): Train loss 5.139, Val loss 5.285\n",
      "Ep 1 (Step 000310): Train loss 5.068, Val loss 5.269\n",
      "Ep 1 (Step 000320): Train loss 5.222, Val loss 5.255\n",
      "Ep 1 (Step 000330): Train loss 5.123, Val loss 5.257\n",
      "Ep 1 (Step 000340): Train loss 5.027, Val loss 5.243\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2427\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.771, Val loss 8.743\n",
      "Ep 1 (Step 000010): Train loss 6.846, Val loss 6.838\n",
      "Ep 1 (Step 000020): Train loss 6.456, Val loss 6.430\n",
      "Ep 1 (Step 000030): Train loss 6.450, Val loss 6.394\n",
      "Ep 1 (Step 000040): Train loss 6.274, Val loss 6.314\n",
      "Ep 1 (Step 000050): Train loss 6.163, Val loss 6.161\n",
      "Ep 1 (Step 000060): Train loss 6.041, Val loss 6.032\n",
      "Ep 1 (Step 000070): Train loss 5.906, Val loss 5.964\n",
      "Ep 1 (Step 000080): Train loss 5.839, Val loss 5.883\n",
      "Ep 1 (Step 000090): Train loss 5.727, Val loss 5.798\n",
      "Ep 1 (Step 000100): Train loss 5.684, Val loss 5.741\n",
      "Ep 1 (Step 000110): Train loss 5.693, Val loss 5.689\n",
      "Ep 1 (Step 000120): Train loss 5.597, Val loss 5.636\n",
      "Ep 1 (Step 000130): Train loss 5.558, Val loss 5.604\n",
      "Ep 1 (Step 000140): Train loss 5.566, Val loss 5.570\n",
      "Ep 1 (Step 000150): Train loss 5.451, Val loss 5.528\n",
      "Ep 1 (Step 000160): Train loss 5.451, Val loss 5.487\n",
      "Ep 1 (Step 000170): Train loss 5.378, Val loss 5.466\n",
      "Ep 1 (Step 000180): Train loss 5.494, Val loss 5.461\n",
      "Ep 1 (Step 000190): Train loss 5.314, Val loss 5.426\n",
      "Ep 1 (Step 000200): Train loss 5.244, Val loss 5.398\n",
      "Ep 1 (Step 000210): Train loss 5.250, Val loss 5.389\n",
      "Ep 1 (Step 000220): Train loss 5.310, Val loss 5.383\n",
      "Ep 1 (Step 000230): Train loss 5.304, Val loss 5.374\n",
      "Ep 1 (Step 000240): Train loss 5.358, Val loss 5.360\n",
      "Ep 1 (Step 000250): Train loss 5.197, Val loss 5.341\n",
      "Ep 1 (Step 000260): Train loss 5.204, Val loss 5.315\n",
      "Ep 1 (Step 000270): Train loss 5.148, Val loss 5.292\n",
      "Ep 1 (Step 000280): Train loss 5.130, Val loss 5.295\n",
      "Ep 1 (Step 000290): Train loss 5.154, Val loss 5.260\n",
      "Ep 1 (Step 000300): Train loss 5.113, Val loss 5.280\n",
      "Ep 1 (Step 000310): Train loss 5.111, Val loss 5.255\n",
      "Ep 1 (Step 000320): Train loss 5.060, Val loss 5.264\n",
      "Ep 1 (Step 000330): Train loss 5.082, Val loss 5.242\n",
      "Ep 1 (Step 000340): Train loss 5.084, Val loss 5.241\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2415\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.850, Val loss 8.826\n",
      "Ep 1 (Step 000010): Train loss 6.881, Val loss 6.839\n",
      "Ep 1 (Step 000020): Train loss 6.492, Val loss 6.469\n",
      "Ep 1 (Step 000030): Train loss 6.506, Val loss 6.470\n",
      "Ep 1 (Step 000040): Train loss 6.368, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.187, Val loss 6.273\n",
      "Ep 1 (Step 000060): Train loss 6.186, Val loss 6.109\n",
      "Ep 1 (Step 000070): Train loss 5.934, Val loss 5.988\n",
      "Ep 1 (Step 000080): Train loss 5.902, Val loss 5.921\n",
      "Ep 1 (Step 000090): Train loss 5.880, Val loss 5.809\n",
      "Ep 1 (Step 000100): Train loss 5.811, Val loss 5.730\n",
      "Ep 1 (Step 000110): Train loss 5.700, Val loss 5.703\n",
      "Ep 1 (Step 000120): Train loss 5.624, Val loss 5.644\n",
      "Ep 1 (Step 000130): Train loss 5.502, Val loss 5.615\n",
      "Ep 1 (Step 000140): Train loss 5.444, Val loss 5.567\n",
      "Ep 1 (Step 000150): Train loss 5.506, Val loss 5.544\n",
      "Ep 1 (Step 000160): Train loss 5.418, Val loss 5.505\n",
      "Ep 1 (Step 000170): Train loss 5.349, Val loss 5.479\n",
      "Ep 1 (Step 000180): Train loss 5.307, Val loss 5.452\n",
      "Ep 1 (Step 000190): Train loss 5.377, Val loss 5.425\n",
      "Ep 1 (Step 000200): Train loss 5.232, Val loss 5.413\n",
      "Ep 1 (Step 000210): Train loss 5.365, Val loss 5.411\n",
      "Ep 1 (Step 000220): Train loss 5.454, Val loss 5.394\n",
      "Ep 1 (Step 000230): Train loss 5.306, Val loss 5.386\n",
      "Ep 1 (Step 000240): Train loss 5.193, Val loss 5.364\n",
      "Ep 1 (Step 000250): Train loss 5.314, Val loss 5.360\n",
      "Ep 1 (Step 000260): Train loss 5.177, Val loss 5.364\n",
      "Ep 1 (Step 000270): Train loss 5.221, Val loss 5.336\n",
      "Ep 1 (Step 000280): Train loss 5.146, Val loss 5.322\n",
      "Ep 1 (Step 000290): Train loss 5.159, Val loss 5.307\n",
      "Ep 1 (Step 000300): Train loss 5.130, Val loss 5.308\n",
      "Ep 1 (Step 000310): Train loss 5.175, Val loss 5.300\n",
      "Ep 1 (Step 000320): Train loss 5.206, Val loss 5.278\n",
      "Ep 1 (Step 000330): Train loss 5.110, Val loss 5.258\n",
      "Ep 1 (Step 000340): Train loss 5.050, Val loss 5.279\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2795\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.082, Val loss 9.056\n",
      "Ep 1 (Step 000010): Train loss 7.507, Val loss 7.459\n",
      "Ep 1 (Step 000020): Train loss 6.860, Val loss 6.797\n",
      "Ep 1 (Step 000030): Train loss 6.454, Val loss 6.471\n",
      "Ep 1 (Step 000040): Train loss 6.437, Val loss 6.382\n",
      "Ep 1 (Step 000050): Train loss 6.450, Val loss 6.365\n",
      "Ep 1 (Step 000060): Train loss 6.281, Val loss 6.284\n",
      "Ep 1 (Step 000070): Train loss 6.179, Val loss 6.173\n",
      "Ep 1 (Step 000080): Train loss 6.104, Val loss 6.100\n",
      "Ep 1 (Step 000090): Train loss 5.976, Val loss 6.011\n",
      "Ep 1 (Step 000100): Train loss 6.009, Val loss 5.971\n",
      "Ep 1 (Step 000110): Train loss 5.966, Val loss 5.915\n",
      "Ep 1 (Step 000120): Train loss 5.758, Val loss 5.855\n",
      "Ep 1 (Step 000130): Train loss 5.663, Val loss 5.822\n",
      "Ep 1 (Step 000140): Train loss 5.653, Val loss 5.765\n",
      "Ep 1 (Step 000150): Train loss 5.651, Val loss 5.729\n",
      "Ep 1 (Step 000160): Train loss 5.611, Val loss 5.707\n",
      "Ep 1 (Step 000170): Train loss 5.626, Val loss 5.684\n",
      "Ep 1 (Step 000180): Train loss 5.568, Val loss 5.648\n",
      "Ep 1 (Step 000190): Train loss 5.523, Val loss 5.625\n",
      "Ep 1 (Step 000200): Train loss 5.442, Val loss 5.613\n",
      "Ep 1 (Step 000210): Train loss 5.584, Val loss 5.580\n",
      "Ep 1 (Step 000220): Train loss 5.409, Val loss 5.546\n",
      "Ep 1 (Step 000230): Train loss 5.340, Val loss 5.538\n",
      "Ep 1 (Step 000240): Train loss 5.474, Val loss 5.526\n",
      "Ep 1 (Step 000250): Train loss 5.436, Val loss 5.493\n",
      "Ep 1 (Step 000260): Train loss 5.428, Val loss 5.488\n",
      "Ep 1 (Step 000270): Train loss 5.446, Val loss 5.468\n",
      "Ep 1 (Step 000280): Train loss 5.318, Val loss 5.453\n",
      "Ep 1 (Step 000290): Train loss 5.281, Val loss 5.436\n",
      "Ep 1 (Step 000300): Train loss 5.328, Val loss 5.428\n",
      "Ep 1 (Step 000310): Train loss 5.212, Val loss 5.403\n",
      "Ep 1 (Step 000320): Train loss 5.289, Val loss 5.409\n",
      "Ep 1 (Step 000330): Train loss 5.241, Val loss 5.389\n",
      "Ep 1 (Step 000340): Train loss 5.207, Val loss 5.391\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3912\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.064, Val loss 9.067\n",
      "Ep 1 (Step 000010): Train loss 7.484, Val loss 7.478\n",
      "Ep 1 (Step 000020): Train loss 6.878, Val loss 6.817\n",
      "Ep 1 (Step 000030): Train loss 6.431, Val loss 6.491\n",
      "Ep 1 (Step 000040): Train loss 6.395, Val loss 6.392\n",
      "Ep 1 (Step 000050): Train loss 6.319, Val loss 6.345\n",
      "Ep 1 (Step 000060): Train loss 6.301, Val loss 6.290\n",
      "Ep 1 (Step 000070): Train loss 6.172, Val loss 6.167\n",
      "Ep 1 (Step 000080): Train loss 6.064, Val loss 6.070\n",
      "Ep 1 (Step 000090): Train loss 5.976, Val loss 5.996\n",
      "Ep 1 (Step 000100): Train loss 5.931, Val loss 5.932\n",
      "Ep 1 (Step 000110): Train loss 5.893, Val loss 5.881\n",
      "Ep 1 (Step 000120): Train loss 5.771, Val loss 5.841\n",
      "Ep 1 (Step 000130): Train loss 5.736, Val loss 5.823\n",
      "Ep 1 (Step 000140): Train loss 5.673, Val loss 5.772\n",
      "Ep 1 (Step 000150): Train loss 5.654, Val loss 5.736\n",
      "Ep 1 (Step 000160): Train loss 5.678, Val loss 5.698\n",
      "Ep 1 (Step 000170): Train loss 5.597, Val loss 5.659\n",
      "Ep 1 (Step 000180): Train loss 5.558, Val loss 5.637\n",
      "Ep 1 (Step 000190): Train loss 5.500, Val loss 5.601\n",
      "Ep 1 (Step 000200): Train loss 5.537, Val loss 5.587\n",
      "Ep 1 (Step 000210): Train loss 5.412, Val loss 5.554\n",
      "Ep 1 (Step 000220): Train loss 5.471, Val loss 5.539\n",
      "Ep 1 (Step 000230): Train loss 5.344, Val loss 5.529\n",
      "Ep 1 (Step 000240): Train loss 5.461, Val loss 5.511\n",
      "Ep 1 (Step 000250): Train loss 5.370, Val loss 5.504\n",
      "Ep 1 (Step 000260): Train loss 5.440, Val loss 5.468\n",
      "Ep 1 (Step 000270): Train loss 5.404, Val loss 5.471\n",
      "Ep 1 (Step 000280): Train loss 5.365, Val loss 5.455\n",
      "Ep 1 (Step 000290): Train loss 5.348, Val loss 5.435\n",
      "Ep 1 (Step 000300): Train loss 5.323, Val loss 5.421\n",
      "Ep 1 (Step 000310): Train loss 5.444, Val loss 5.419\n",
      "Ep 1 (Step 000320): Train loss 5.337, Val loss 5.403\n",
      "Ep 1 (Step 000330): Train loss 5.331, Val loss 5.399\n",
      "Ep 1 (Step 000340): Train loss 5.209, Val loss 5.390\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3896\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.071, Val loss 9.058\n",
      "Ep 1 (Step 000010): Train loss 7.453, Val loss 7.430\n",
      "Ep 1 (Step 000020): Train loss 6.810, Val loss 6.787\n",
      "Ep 1 (Step 000030): Train loss 6.536, Val loss 6.468\n",
      "Ep 1 (Step 000040): Train loss 6.430, Val loss 6.381\n",
      "Ep 1 (Step 000050): Train loss 6.389, Val loss 6.371\n",
      "Ep 1 (Step 000060): Train loss 6.340, Val loss 6.303\n",
      "Ep 1 (Step 000070): Train loss 6.188, Val loss 6.181\n",
      "Ep 1 (Step 000080): Train loss 6.119, Val loss 6.090\n",
      "Ep 1 (Step 000090): Train loss 5.968, Val loss 6.018\n",
      "Ep 1 (Step 000100): Train loss 5.942, Val loss 5.947\n",
      "Ep 1 (Step 000110): Train loss 5.860, Val loss 5.900\n",
      "Ep 1 (Step 000120): Train loss 5.774, Val loss 5.847\n",
      "Ep 1 (Step 000130): Train loss 5.777, Val loss 5.803\n",
      "Ep 1 (Step 000140): Train loss 5.640, Val loss 5.758\n",
      "Ep 1 (Step 000150): Train loss 5.665, Val loss 5.708\n",
      "Ep 1 (Step 000160): Train loss 5.653, Val loss 5.694\n",
      "Ep 1 (Step 000170): Train loss 5.680, Val loss 5.655\n",
      "Ep 1 (Step 000180): Train loss 5.611, Val loss 5.624\n",
      "Ep 1 (Step 000190): Train loss 5.506, Val loss 5.608\n",
      "Ep 1 (Step 000200): Train loss 5.569, Val loss 5.583\n",
      "Ep 1 (Step 000210): Train loss 5.451, Val loss 5.559\n",
      "Ep 1 (Step 000220): Train loss 5.458, Val loss 5.548\n",
      "Ep 1 (Step 000230): Train loss 5.457, Val loss 5.536\n",
      "Ep 1 (Step 000240): Train loss 5.378, Val loss 5.522\n",
      "Ep 1 (Step 000250): Train loss 5.492, Val loss 5.506\n",
      "Ep 1 (Step 000260): Train loss 5.441, Val loss 5.500\n",
      "Ep 1 (Step 000270): Train loss 5.474, Val loss 5.477\n",
      "Ep 1 (Step 000280): Train loss 5.348, Val loss 5.461\n",
      "Ep 1 (Step 000290): Train loss 5.339, Val loss 5.437\n",
      "Ep 1 (Step 000300): Train loss 5.346, Val loss 5.419\n",
      "Ep 1 (Step 000310): Train loss 5.299, Val loss 5.413\n",
      "Ep 1 (Step 000320): Train loss 5.264, Val loss 5.403\n",
      "Ep 1 (Step 000330): Train loss 5.257, Val loss 5.395\n",
      "Ep 1 (Step 000340): Train loss 5.207, Val loss 5.401\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.4012\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.058, Val loss 9.038\n",
      "Ep 1 (Step 000010): Train loss 7.514, Val loss 7.469\n",
      "Ep 1 (Step 000020): Train loss 6.821, Val loss 6.831\n",
      "Ep 1 (Step 000030): Train loss 6.480, Val loss 6.500\n",
      "Ep 1 (Step 000040): Train loss 6.355, Val loss 6.402\n",
      "Ep 1 (Step 000050): Train loss 6.371, Val loss 6.386\n",
      "Ep 1 (Step 000060): Train loss 6.339, Val loss 6.340\n",
      "Ep 1 (Step 000070): Train loss 6.280, Val loss 6.242\n",
      "Ep 1 (Step 000080): Train loss 6.147, Val loss 6.134\n",
      "Ep 1 (Step 000090): Train loss 6.052, Val loss 6.061\n",
      "Ep 1 (Step 000100): Train loss 5.945, Val loss 5.987\n",
      "Ep 1 (Step 000110): Train loss 5.859, Val loss 5.930\n",
      "Ep 1 (Step 000120): Train loss 5.802, Val loss 5.877\n",
      "Ep 1 (Step 000130): Train loss 5.824, Val loss 5.854\n",
      "Ep 1 (Step 000140): Train loss 5.737, Val loss 5.773\n",
      "Ep 1 (Step 000150): Train loss 5.712, Val loss 5.767\n",
      "Ep 1 (Step 000160): Train loss 5.676, Val loss 5.717\n",
      "Ep 1 (Step 000170): Train loss 5.607, Val loss 5.700\n",
      "Ep 1 (Step 000180): Train loss 5.476, Val loss 5.660\n",
      "Ep 1 (Step 000190): Train loss 5.478, Val loss 5.641\n",
      "Ep 1 (Step 000200): Train loss 5.534, Val loss 5.616\n",
      "Ep 1 (Step 000210): Train loss 5.537, Val loss 5.587\n",
      "Ep 1 (Step 000220): Train loss 5.450, Val loss 5.571\n",
      "Ep 1 (Step 000230): Train loss 5.487, Val loss 5.546\n",
      "Ep 1 (Step 000240): Train loss 5.397, Val loss 5.520\n",
      "Ep 1 (Step 000250): Train loss 5.437, Val loss 5.512\n",
      "Ep 1 (Step 000260): Train loss 5.381, Val loss 5.483\n",
      "Ep 1 (Step 000270): Train loss 5.262, Val loss 5.462\n",
      "Ep 1 (Step 000280): Train loss 5.356, Val loss 5.445\n",
      "Ep 1 (Step 000290): Train loss 5.359, Val loss 5.432\n",
      "Ep 1 (Step 000300): Train loss 5.311, Val loss 5.427\n",
      "Ep 1 (Step 000310): Train loss 5.377, Val loss 5.415\n",
      "Ep 1 (Step 000320): Train loss 5.274, Val loss 5.405\n",
      "Ep 1 (Step 000330): Train loss 5.330, Val loss 5.388\n",
      "Ep 1 (Step 000340): Train loss 5.247, Val loss 5.373\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3726\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.023, Val loss 9.004\n",
      "Ep 1 (Step 000010): Train loss 7.385, Val loss 7.363\n",
      "Ep 1 (Step 000020): Train loss 6.745, Val loss 6.743\n",
      "Ep 1 (Step 000030): Train loss 6.481, Val loss 6.463\n",
      "Ep 1 (Step 000040): Train loss 6.382, Val loss 6.400\n",
      "Ep 1 (Step 000050): Train loss 6.413, Val loss 6.367\n",
      "Ep 1 (Step 000060): Train loss 6.342, Val loss 6.332\n",
      "Ep 1 (Step 000070): Train loss 6.203, Val loss 6.235\n",
      "Ep 1 (Step 000080): Train loss 6.084, Val loss 6.140\n",
      "Ep 1 (Step 000090): Train loss 6.081, Val loss 6.037\n",
      "Ep 1 (Step 000100): Train loss 5.948, Val loss 5.973\n",
      "Ep 1 (Step 000110): Train loss 5.843, Val loss 5.922\n",
      "Ep 1 (Step 000120): Train loss 5.865, Val loss 5.875\n",
      "Ep 1 (Step 000130): Train loss 5.719, Val loss 5.808\n",
      "Ep 1 (Step 000140): Train loss 5.719, Val loss 5.757\n",
      "Ep 1 (Step 000150): Train loss 5.701, Val loss 5.724\n",
      "Ep 1 (Step 000160): Train loss 5.597, Val loss 5.697\n",
      "Ep 1 (Step 000170): Train loss 5.650, Val loss 5.650\n",
      "Ep 1 (Step 000180): Train loss 5.581, Val loss 5.635\n",
      "Ep 1 (Step 000190): Train loss 5.538, Val loss 5.607\n",
      "Ep 1 (Step 000200): Train loss 5.426, Val loss 5.582\n",
      "Ep 1 (Step 000210): Train loss 5.553, Val loss 5.558\n",
      "Ep 1 (Step 000220): Train loss 5.535, Val loss 5.538\n",
      "Ep 1 (Step 000230): Train loss 5.456, Val loss 5.534\n",
      "Ep 1 (Step 000240): Train loss 5.440, Val loss 5.504\n",
      "Ep 1 (Step 000250): Train loss 5.475, Val loss 5.487\n",
      "Ep 1 (Step 000260): Train loss 5.292, Val loss 5.483\n",
      "Ep 1 (Step 000270): Train loss 5.304, Val loss 5.457\n",
      "Ep 1 (Step 000280): Train loss 5.427, Val loss 5.444\n",
      "Ep 1 (Step 000290): Train loss 5.293, Val loss 5.417\n",
      "Ep 1 (Step 000300): Train loss 5.356, Val loss 5.406\n",
      "Ep 1 (Step 000310): Train loss 5.273, Val loss 5.409\n",
      "Ep 1 (Step 000320): Train loss 5.287, Val loss 5.399\n",
      "Ep 1 (Step 000330): Train loss 5.383, Val loss 5.388\n",
      "Ep 1 (Step 000340): Train loss 5.183, Val loss 5.365\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3650\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.077, Val loss 9.063\n",
      "Ep 1 (Step 000010): Train loss 7.558, Val loss 7.504\n",
      "Ep 1 (Step 000020): Train loss 6.847, Val loss 6.821\n",
      "Ep 1 (Step 000030): Train loss 6.531, Val loss 6.485\n",
      "Ep 1 (Step 000040): Train loss 6.347, Val loss 6.400\n",
      "Ep 1 (Step 000050): Train loss 6.377, Val loss 6.385\n",
      "Ep 1 (Step 000060): Train loss 6.271, Val loss 6.331\n",
      "Ep 1 (Step 000070): Train loss 6.296, Val loss 6.254\n",
      "Ep 1 (Step 000080): Train loss 6.123, Val loss 6.137\n",
      "Ep 1 (Step 000090): Train loss 6.106, Val loss 6.048\n",
      "Ep 1 (Step 000100): Train loss 5.982, Val loss 5.995\n",
      "Ep 1 (Step 000110): Train loss 5.892, Val loss 5.909\n",
      "Ep 1 (Step 000120): Train loss 5.875, Val loss 5.847\n",
      "Ep 1 (Step 000130): Train loss 5.854, Val loss 5.795\n",
      "Ep 1 (Step 000140): Train loss 5.759, Val loss 5.766\n",
      "Ep 1 (Step 000150): Train loss 5.696, Val loss 5.739\n",
      "Ep 1 (Step 000160): Train loss 5.640, Val loss 5.709\n",
      "Ep 1 (Step 000170): Train loss 5.560, Val loss 5.694\n",
      "Ep 1 (Step 000180): Train loss 5.616, Val loss 5.657\n",
      "Ep 1 (Step 000190): Train loss 5.596, Val loss 5.610\n",
      "Ep 1 (Step 000200): Train loss 5.545, Val loss 5.588\n",
      "Ep 1 (Step 000210): Train loss 5.520, Val loss 5.574\n",
      "Ep 1 (Step 000220): Train loss 5.470, Val loss 5.553\n",
      "Ep 1 (Step 000230): Train loss 5.509, Val loss 5.529\n",
      "Ep 1 (Step 000240): Train loss 5.421, Val loss 5.507\n",
      "Ep 1 (Step 000250): Train loss 5.475, Val loss 5.487\n",
      "Ep 1 (Step 000260): Train loss 5.336, Val loss 5.475\n",
      "Ep 1 (Step 000270): Train loss 5.436, Val loss 5.473\n",
      "Ep 1 (Step 000280): Train loss 5.345, Val loss 5.459\n",
      "Ep 1 (Step 000290): Train loss 5.335, Val loss 5.431\n",
      "Ep 1 (Step 000300): Train loss 5.349, Val loss 5.416\n",
      "Ep 1 (Step 000310): Train loss 5.325, Val loss 5.410\n",
      "Ep 1 (Step 000320): Train loss 5.252, Val loss 5.391\n",
      "Ep 1 (Step 000330): Train loss 5.239, Val loss 5.384\n",
      "Ep 1 (Step 000340): Train loss 5.284, Val loss 5.368\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3678\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.810, Val loss 8.773\n",
      "Ep 1 (Step 000010): Train loss 6.937, Val loss 6.852\n",
      "Ep 1 (Step 000020): Train loss 6.467, Val loss 6.460\n",
      "Ep 1 (Step 000030): Train loss 6.528, Val loss 6.469\n",
      "Ep 1 (Step 000040): Train loss 6.336, Val loss 6.359\n",
      "Ep 1 (Step 000050): Train loss 6.283, Val loss 6.199\n",
      "Ep 1 (Step 000060): Train loss 6.118, Val loss 6.070\n",
      "Ep 1 (Step 000070): Train loss 5.955, Val loss 5.954\n",
      "Ep 1 (Step 000080): Train loss 5.809, Val loss 5.886\n",
      "Ep 1 (Step 000090): Train loss 5.729, Val loss 5.815\n",
      "Ep 1 (Step 000100): Train loss 5.636, Val loss 5.765\n",
      "Ep 1 (Step 000110): Train loss 5.607, Val loss 5.710\n",
      "Ep 1 (Step 000120): Train loss 5.606, Val loss 5.662\n",
      "Ep 1 (Step 000130): Train loss 5.638, Val loss 5.625\n",
      "Ep 1 (Step 000140): Train loss 5.524, Val loss 5.580\n",
      "Ep 1 (Step 000150): Train loss 5.530, Val loss 5.569\n",
      "Ep 1 (Step 000160): Train loss 5.422, Val loss 5.544\n",
      "Ep 1 (Step 000170): Train loss 5.297, Val loss 5.514\n",
      "Ep 1 (Step 000180): Train loss 5.423, Val loss 5.480\n",
      "Ep 1 (Step 000190): Train loss 5.370, Val loss 5.463\n",
      "Ep 1 (Step 000200): Train loss 5.330, Val loss 5.445\n",
      "Ep 1 (Step 000210): Train loss 5.366, Val loss 5.421\n",
      "Ep 1 (Step 000220): Train loss 5.296, Val loss 5.408\n",
      "Ep 1 (Step 000230): Train loss 5.289, Val loss 5.404\n",
      "Ep 1 (Step 000240): Train loss 5.292, Val loss 5.381\n",
      "Ep 1 (Step 000250): Train loss 5.284, Val loss 5.366\n",
      "Ep 1 (Step 000260): Train loss 5.236, Val loss 5.339\n",
      "Ep 1 (Step 000270): Train loss 5.163, Val loss 5.337\n",
      "Ep 1 (Step 000280): Train loss 5.231, Val loss 5.323\n",
      "Ep 1 (Step 000290): Train loss 5.161, Val loss 5.314\n",
      "Ep 1 (Step 000300): Train loss 5.186, Val loss 5.302\n",
      "Ep 1 (Step 000310): Train loss 5.151, Val loss 5.302\n",
      "Ep 1 (Step 000320): Train loss 5.096, Val loss 5.291\n",
      "Ep 1 (Step 000330): Train loss 5.197, Val loss 5.293\n",
      "Ep 1 (Step 000340): Train loss 5.079, Val loss 5.271\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2709\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.842, Val loss 8.860\n",
      "Ep 1 (Step 000010): Train loss 6.897, Val loss 6.872\n",
      "Ep 1 (Step 000020): Train loss 6.488, Val loss 6.505\n",
      "Ep 1 (Step 000030): Train loss 6.532, Val loss 6.489\n",
      "Ep 1 (Step 000040): Train loss 6.449, Val loss 6.385\n",
      "Ep 1 (Step 000050): Train loss 6.211, Val loss 6.256\n",
      "Ep 1 (Step 000060): Train loss 6.099, Val loss 6.120\n",
      "Ep 1 (Step 000070): Train loss 6.042, Val loss 6.016\n",
      "Ep 1 (Step 000080): Train loss 5.857, Val loss 5.895\n",
      "Ep 1 (Step 000090): Train loss 5.757, Val loss 5.841\n",
      "Ep 1 (Step 000100): Train loss 5.658, Val loss 5.774\n",
      "Ep 1 (Step 000110): Train loss 5.655, Val loss 5.733\n",
      "Ep 1 (Step 000120): Train loss 5.626, Val loss 5.679\n",
      "Ep 1 (Step 000130): Train loss 5.587, Val loss 5.641\n",
      "Ep 1 (Step 000140): Train loss 5.556, Val loss 5.614\n",
      "Ep 1 (Step 000150): Train loss 5.467, Val loss 5.553\n",
      "Ep 1 (Step 000160): Train loss 5.420, Val loss 5.528\n",
      "Ep 1 (Step 000170): Train loss 5.418, Val loss 5.525\n",
      "Ep 1 (Step 000180): Train loss 5.425, Val loss 5.483\n",
      "Ep 1 (Step 000190): Train loss 5.413, Val loss 5.474\n",
      "Ep 1 (Step 000200): Train loss 5.179, Val loss 5.453\n",
      "Ep 1 (Step 000210): Train loss 5.377, Val loss 5.429\n",
      "Ep 1 (Step 000220): Train loss 5.306, Val loss 5.423\n",
      "Ep 1 (Step 000230): Train loss 5.318, Val loss 5.395\n",
      "Ep 1 (Step 000240): Train loss 5.271, Val loss 5.397\n",
      "Ep 1 (Step 000250): Train loss 5.314, Val loss 5.398\n",
      "Ep 1 (Step 000260): Train loss 5.248, Val loss 5.366\n",
      "Ep 1 (Step 000270): Train loss 5.119, Val loss 5.335\n",
      "Ep 1 (Step 000280): Train loss 5.129, Val loss 5.326\n",
      "Ep 1 (Step 000290): Train loss 5.213, Val loss 5.317\n",
      "Ep 1 (Step 000300): Train loss 5.173, Val loss 5.319\n",
      "Ep 1 (Step 000310): Train loss 5.210, Val loss 5.305\n",
      "Ep 1 (Step 000320): Train loss 5.203, Val loss 5.292\n",
      "Ep 1 (Step 000330): Train loss 5.243, Val loss 5.296\n",
      "Ep 1 (Step 000340): Train loss 5.103, Val loss 5.284\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2836\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.871, Val loss 8.860\n",
      "Ep 1 (Step 000010): Train loss 6.823, Val loss 6.830\n",
      "Ep 1 (Step 000020): Train loss 6.495, Val loss 6.490\n",
      "Ep 1 (Step 000030): Train loss 6.562, Val loss 6.459\n",
      "Ep 1 (Step 000040): Train loss 6.441, Val loss 6.368\n",
      "Ep 1 (Step 000050): Train loss 6.266, Val loss 6.241\n",
      "Ep 1 (Step 000060): Train loss 6.153, Val loss 6.120\n",
      "Ep 1 (Step 000070): Train loss 5.904, Val loss 6.004\n",
      "Ep 1 (Step 000080): Train loss 5.897, Val loss 5.903\n",
      "Ep 1 (Step 000090): Train loss 5.842, Val loss 5.849\n",
      "Ep 1 (Step 000100): Train loss 5.730, Val loss 5.788\n",
      "Ep 1 (Step 000110): Train loss 5.650, Val loss 5.752\n",
      "Ep 1 (Step 000120): Train loss 5.601, Val loss 5.705\n",
      "Ep 1 (Step 000130): Train loss 5.538, Val loss 5.669\n",
      "Ep 1 (Step 000140): Train loss 5.498, Val loss 5.603\n",
      "Ep 1 (Step 000150): Train loss 5.426, Val loss 5.575\n",
      "Ep 1 (Step 000160): Train loss 5.344, Val loss 5.559\n",
      "Ep 1 (Step 000170): Train loss 5.400, Val loss 5.515\n",
      "Ep 1 (Step 000180): Train loss 5.360, Val loss 5.489\n",
      "Ep 1 (Step 000190): Train loss 5.389, Val loss 5.467\n",
      "Ep 1 (Step 000200): Train loss 5.361, Val loss 5.455\n",
      "Ep 1 (Step 000210): Train loss 5.294, Val loss 5.449\n",
      "Ep 1 (Step 000220): Train loss 5.379, Val loss 5.426\n",
      "Ep 1 (Step 000230): Train loss 5.325, Val loss 5.386\n",
      "Ep 1 (Step 000240): Train loss 5.267, Val loss 5.388\n",
      "Ep 1 (Step 000250): Train loss 5.329, Val loss 5.340\n",
      "Ep 1 (Step 000260): Train loss 5.278, Val loss 5.340\n",
      "Ep 1 (Step 000270): Train loss 5.242, Val loss 5.336\n",
      "Ep 1 (Step 000280): Train loss 5.209, Val loss 5.332\n",
      "Ep 1 (Step 000290): Train loss 5.287, Val loss 5.328\n",
      "Ep 1 (Step 000300): Train loss 5.152, Val loss 5.317\n",
      "Ep 1 (Step 000310): Train loss 5.211, Val loss 5.315\n",
      "Ep 1 (Step 000320): Train loss 5.108, Val loss 5.312\n",
      "Ep 1 (Step 000330): Train loss 5.101, Val loss 5.298\n",
      "Ep 1 (Step 000340): Train loss 5.084, Val loss 5.293\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2928\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.829, Val loss 8.809\n",
      "Ep 1 (Step 000010): Train loss 6.892, Val loss 6.872\n",
      "Ep 1 (Step 000020): Train loss 6.496, Val loss 6.485\n",
      "Ep 1 (Step 000030): Train loss 6.497, Val loss 6.448\n",
      "Ep 1 (Step 000040): Train loss 6.350, Val loss 6.395\n",
      "Ep 1 (Step 000050): Train loss 6.291, Val loss 6.279\n",
      "Ep 1 (Step 000060): Train loss 6.152, Val loss 6.105\n",
      "Ep 1 (Step 000070): Train loss 5.997, Val loss 5.987\n",
      "Ep 1 (Step 000080): Train loss 5.905, Val loss 5.902\n",
      "Ep 1 (Step 000090): Train loss 5.772, Val loss 5.822\n",
      "Ep 1 (Step 000100): Train loss 5.697, Val loss 5.758\n",
      "Ep 1 (Step 000110): Train loss 5.613, Val loss 5.710\n",
      "Ep 1 (Step 000120): Train loss 5.554, Val loss 5.652\n",
      "Ep 1 (Step 000130): Train loss 5.579, Val loss 5.610\n",
      "Ep 1 (Step 000140): Train loss 5.519, Val loss 5.584\n",
      "Ep 1 (Step 000150): Train loss 5.459, Val loss 5.552\n",
      "Ep 1 (Step 000160): Train loss 5.450, Val loss 5.527\n",
      "Ep 1 (Step 000170): Train loss 5.372, Val loss 5.477\n",
      "Ep 1 (Step 000180): Train loss 5.334, Val loss 5.466\n",
      "Ep 1 (Step 000190): Train loss 5.334, Val loss 5.451\n",
      "Ep 1 (Step 000200): Train loss 5.284, Val loss 5.422\n",
      "Ep 1 (Step 000210): Train loss 5.315, Val loss 5.407\n",
      "Ep 1 (Step 000220): Train loss 5.332, Val loss 5.388\n",
      "Ep 1 (Step 000230): Train loss 5.275, Val loss 5.358\n",
      "Ep 1 (Step 000240): Train loss 5.155, Val loss 5.359\n",
      "Ep 1 (Step 000250): Train loss 5.187, Val loss 5.331\n",
      "Ep 1 (Step 000260): Train loss 5.328, Val loss 5.338\n",
      "Ep 1 (Step 000270): Train loss 5.132, Val loss 5.302\n",
      "Ep 1 (Step 000280): Train loss 5.194, Val loss 5.314\n",
      "Ep 1 (Step 000290): Train loss 5.263, Val loss 5.311\n",
      "Ep 1 (Step 000300): Train loss 5.143, Val loss 5.306\n",
      "Ep 1 (Step 000310): Train loss 5.169, Val loss 5.268\n",
      "Ep 1 (Step 000320): Train loss 5.128, Val loss 5.262\n",
      "Ep 1 (Step 000330): Train loss 5.127, Val loss 5.258\n",
      "Ep 1 (Step 000340): Train loss 5.137, Val loss 5.234\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2338\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.836, Val loss 8.801\n",
      "Ep 1 (Step 000010): Train loss 6.965, Val loss 6.902\n",
      "Ep 1 (Step 000020): Train loss 6.452, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.522, Val loss 6.467\n",
      "Ep 1 (Step 000040): Train loss 6.401, Val loss 6.365\n",
      "Ep 1 (Step 000050): Train loss 6.257, Val loss 6.254\n",
      "Ep 1 (Step 000060): Train loss 6.089, Val loss 6.104\n",
      "Ep 1 (Step 000070): Train loss 5.986, Val loss 6.009\n",
      "Ep 1 (Step 000080): Train loss 5.938, Val loss 5.917\n",
      "Ep 1 (Step 000090): Train loss 5.720, Val loss 5.826\n",
      "Ep 1 (Step 000100): Train loss 5.767, Val loss 5.786\n",
      "Ep 1 (Step 000110): Train loss 5.690, Val loss 5.712\n",
      "Ep 1 (Step 000120): Train loss 5.629, Val loss 5.686\n",
      "Ep 1 (Step 000130): Train loss 5.514, Val loss 5.627\n",
      "Ep 1 (Step 000140): Train loss 5.495, Val loss 5.593\n",
      "Ep 1 (Step 000150): Train loss 5.514, Val loss 5.544\n",
      "Ep 1 (Step 000160): Train loss 5.459, Val loss 5.551\n",
      "Ep 1 (Step 000170): Train loss 5.458, Val loss 5.515\n",
      "Ep 1 (Step 000180): Train loss 5.400, Val loss 5.466\n",
      "Ep 1 (Step 000190): Train loss 5.310, Val loss 5.446\n",
      "Ep 1 (Step 000200): Train loss 5.382, Val loss 5.423\n",
      "Ep 1 (Step 000210): Train loss 5.249, Val loss 5.407\n",
      "Ep 1 (Step 000220): Train loss 5.250, Val loss 5.395\n",
      "Ep 1 (Step 000230): Train loss 5.252, Val loss 5.403\n",
      "Ep 1 (Step 000240): Train loss 5.220, Val loss 5.365\n",
      "Ep 1 (Step 000250): Train loss 5.262, Val loss 5.353\n",
      "Ep 1 (Step 000260): Train loss 5.125, Val loss 5.332\n",
      "Ep 1 (Step 000270): Train loss 5.203, Val loss 5.317\n",
      "Ep 1 (Step 000280): Train loss 5.185, Val loss 5.314\n",
      "Ep 1 (Step 000290): Train loss 5.198, Val loss 5.300\n",
      "Ep 1 (Step 000300): Train loss 5.145, Val loss 5.294\n",
      "Ep 1 (Step 000310): Train loss 5.169, Val loss 5.285\n",
      "Ep 1 (Step 000320): Train loss 5.086, Val loss 5.279\n",
      "Ep 1 (Step 000330): Train loss 5.082, Val loss 5.253\n",
      "Ep 1 (Step 000340): Train loss 5.129, Val loss 5.233\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2330\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.759, Val loss 8.743\n",
      "Ep 1 (Step 000010): Train loss 6.833, Val loss 6.799\n",
      "Ep 1 (Step 000020): Train loss 6.490, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.424, Val loss 6.460\n",
      "Ep 1 (Step 000040): Train loss 6.453, Val loss 6.385\n",
      "Ep 1 (Step 000050): Train loss 6.282, Val loss 6.260\n",
      "Ep 1 (Step 000060): Train loss 6.036, Val loss 6.106\n",
      "Ep 1 (Step 000070): Train loss 5.984, Val loss 5.988\n",
      "Ep 1 (Step 000080): Train loss 5.835, Val loss 5.912\n",
      "Ep 1 (Step 000090): Train loss 5.846, Val loss 5.864\n",
      "Ep 1 (Step 000100): Train loss 5.746, Val loss 5.776\n",
      "Ep 1 (Step 000110): Train loss 5.629, Val loss 5.716\n",
      "Ep 1 (Step 000120): Train loss 5.583, Val loss 5.667\n",
      "Ep 1 (Step 000130): Train loss 5.471, Val loss 5.621\n",
      "Ep 1 (Step 000140): Train loss 5.552, Val loss 5.573\n",
      "Ep 1 (Step 000150): Train loss 5.523, Val loss 5.534\n",
      "Ep 1 (Step 000160): Train loss 5.437, Val loss 5.530\n",
      "Ep 1 (Step 000170): Train loss 5.394, Val loss 5.492\n",
      "Ep 1 (Step 000180): Train loss 5.393, Val loss 5.483\n",
      "Ep 1 (Step 000190): Train loss 5.380, Val loss 5.463\n",
      "Ep 1 (Step 000200): Train loss 5.406, Val loss 5.451\n",
      "Ep 1 (Step 000210): Train loss 5.281, Val loss 5.418\n",
      "Ep 1 (Step 000220): Train loss 5.311, Val loss 5.395\n",
      "Ep 1 (Step 000230): Train loss 5.174, Val loss 5.363\n",
      "Ep 1 (Step 000240): Train loss 5.245, Val loss 5.364\n",
      "Ep 1 (Step 000250): Train loss 5.261, Val loss 5.331\n",
      "Ep 1 (Step 000260): Train loss 5.201, Val loss 5.320\n",
      "Ep 1 (Step 000270): Train loss 5.265, Val loss 5.325\n",
      "Ep 1 (Step 000280): Train loss 5.219, Val loss 5.308\n",
      "Ep 1 (Step 000290): Train loss 5.163, Val loss 5.301\n",
      "Ep 1 (Step 000300): Train loss 5.028, Val loss 5.270\n",
      "Ep 1 (Step 000310): Train loss 5.106, Val loss 5.263\n",
      "Ep 1 (Step 000320): Train loss 5.053, Val loss 5.250\n",
      "Ep 1 (Step 000330): Train loss 5.053, Val loss 5.245\n",
      "Ep 1 (Step 000340): Train loss 5.066, Val loss 5.238\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2382\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.017, Val loss 9.010\n",
      "Ep 1 (Step 000010): Train loss 7.415, Val loss 7.392\n",
      "Ep 1 (Step 000020): Train loss 6.742, Val loss 6.756\n",
      "Ep 1 (Step 000030): Train loss 6.507, Val loss 6.464\n",
      "Ep 1 (Step 000040): Train loss 6.355, Val loss 6.373\n",
      "Ep 1 (Step 000050): Train loss 6.248, Val loss 6.318\n",
      "Ep 1 (Step 000060): Train loss 6.210, Val loss 6.217\n",
      "Ep 1 (Step 000070): Train loss 6.149, Val loss 6.126\n",
      "Ep 1 (Step 000080): Train loss 6.032, Val loss 6.044\n",
      "Ep 1 (Step 000090): Train loss 5.941, Val loss 5.959\n",
      "Ep 1 (Step 000100): Train loss 5.905, Val loss 5.900\n",
      "Ep 1 (Step 000110): Train loss 5.817, Val loss 5.845\n",
      "Ep 1 (Step 000120): Train loss 5.717, Val loss 5.791\n",
      "Ep 1 (Step 000130): Train loss 5.662, Val loss 5.766\n",
      "Ep 1 (Step 000140): Train loss 5.657, Val loss 5.730\n",
      "Ep 1 (Step 000150): Train loss 5.575, Val loss 5.695\n",
      "Ep 1 (Step 000160): Train loss 5.584, Val loss 5.644\n",
      "Ep 1 (Step 000170): Train loss 5.482, Val loss 5.628\n",
      "Ep 1 (Step 000180): Train loss 5.523, Val loss 5.596\n",
      "Ep 1 (Step 000190): Train loss 5.475, Val loss 5.564\n",
      "Ep 1 (Step 000200): Train loss 5.451, Val loss 5.536\n",
      "Ep 1 (Step 000210): Train loss 5.453, Val loss 5.510\n",
      "Ep 1 (Step 000220): Train loss 5.400, Val loss 5.480\n",
      "Ep 1 (Step 000230): Train loss 5.412, Val loss 5.472\n",
      "Ep 1 (Step 000240): Train loss 5.361, Val loss 5.458\n",
      "Ep 1 (Step 000250): Train loss 5.304, Val loss 5.438\n",
      "Ep 1 (Step 000260): Train loss 5.331, Val loss 5.438\n",
      "Ep 1 (Step 000270): Train loss 5.340, Val loss 5.427\n",
      "Ep 1 (Step 000280): Train loss 5.265, Val loss 5.408\n",
      "Ep 1 (Step 000290): Train loss 5.254, Val loss 5.400\n",
      "Ep 1 (Step 000300): Train loss 5.290, Val loss 5.366\n",
      "Ep 1 (Step 000310): Train loss 5.241, Val loss 5.359\n",
      "Ep 1 (Step 000320): Train loss 5.227, Val loss 5.353\n",
      "Ep 1 (Step 000330): Train loss 5.383, Val loss 5.337\n",
      "Ep 1 (Step 000340): Train loss 5.254, Val loss 5.321\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3213\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.044, Val loss 9.049\n",
      "Ep 1 (Step 000010): Train loss 7.412, Val loss 7.443\n",
      "Ep 1 (Step 000020): Train loss 6.771, Val loss 6.802\n",
      "Ep 1 (Step 000030): Train loss 6.492, Val loss 6.482\n",
      "Ep 1 (Step 000040): Train loss 6.342, Val loss 6.380\n",
      "Ep 1 (Step 000050): Train loss 6.362, Val loss 6.320\n",
      "Ep 1 (Step 000060): Train loss 6.173, Val loss 6.225\n",
      "Ep 1 (Step 000070): Train loss 6.092, Val loss 6.087\n",
      "Ep 1 (Step 000080): Train loss 5.968, Val loss 6.010\n",
      "Ep 1 (Step 000090): Train loss 5.869, Val loss 5.929\n",
      "Ep 1 (Step 000100): Train loss 5.848, Val loss 5.862\n",
      "Ep 1 (Step 000110): Train loss 5.814, Val loss 5.841\n",
      "Ep 1 (Step 000120): Train loss 5.669, Val loss 5.790\n",
      "Ep 1 (Step 000130): Train loss 5.718, Val loss 5.754\n",
      "Ep 1 (Step 000140): Train loss 5.612, Val loss 5.703\n",
      "Ep 1 (Step 000150): Train loss 5.625, Val loss 5.658\n",
      "Ep 1 (Step 000160): Train loss 5.533, Val loss 5.626\n",
      "Ep 1 (Step 000170): Train loss 5.509, Val loss 5.602\n",
      "Ep 1 (Step 000180): Train loss 5.518, Val loss 5.575\n",
      "Ep 1 (Step 000190): Train loss 5.518, Val loss 5.552\n",
      "Ep 1 (Step 000200): Train loss 5.458, Val loss 5.520\n",
      "Ep 1 (Step 000210): Train loss 5.381, Val loss 5.496\n",
      "Ep 1 (Step 000220): Train loss 5.368, Val loss 5.464\n",
      "Ep 1 (Step 000230): Train loss 5.386, Val loss 5.454\n",
      "Ep 1 (Step 000240): Train loss 5.346, Val loss 5.432\n",
      "Ep 1 (Step 000250): Train loss 5.370, Val loss 5.423\n",
      "Ep 1 (Step 000260): Train loss 5.328, Val loss 5.406\n",
      "Ep 1 (Step 000270): Train loss 5.286, Val loss 5.392\n",
      "Ep 1 (Step 000280): Train loss 5.304, Val loss 5.387\n",
      "Ep 1 (Step 000290): Train loss 5.236, Val loss 5.375\n",
      "Ep 1 (Step 000300): Train loss 5.200, Val loss 5.373\n",
      "Ep 1 (Step 000310): Train loss 5.258, Val loss 5.353\n",
      "Ep 1 (Step 000320): Train loss 5.226, Val loss 5.347\n",
      "Ep 1 (Step 000330): Train loss 5.198, Val loss 5.339\n",
      "Ep 1 (Step 000340): Train loss 5.139, Val loss 5.320\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3196\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.072, Val loss 9.044\n",
      "Ep 1 (Step 000010): Train loss 7.490, Val loss 7.455\n",
      "Ep 1 (Step 000020): Train loss 6.808, Val loss 6.788\n",
      "Ep 1 (Step 000030): Train loss 6.471, Val loss 6.454\n",
      "Ep 1 (Step 000040): Train loss 6.378, Val loss 6.385\n",
      "Ep 1 (Step 000050): Train loss 6.344, Val loss 6.316\n",
      "Ep 1 (Step 000060): Train loss 6.150, Val loss 6.195\n",
      "Ep 1 (Step 000070): Train loss 6.165, Val loss 6.082\n",
      "Ep 1 (Step 000080): Train loss 5.974, Val loss 6.009\n",
      "Ep 1 (Step 000090): Train loss 5.832, Val loss 5.943\n",
      "Ep 1 (Step 000100): Train loss 5.844, Val loss 5.875\n",
      "Ep 1 (Step 000110): Train loss 5.841, Val loss 5.839\n",
      "Ep 1 (Step 000120): Train loss 5.716, Val loss 5.781\n",
      "Ep 1 (Step 000130): Train loss 5.746, Val loss 5.733\n",
      "Ep 1 (Step 000140): Train loss 5.633, Val loss 5.696\n",
      "Ep 1 (Step 000150): Train loss 5.595, Val loss 5.661\n",
      "Ep 1 (Step 000160): Train loss 5.538, Val loss 5.621\n",
      "Ep 1 (Step 000170): Train loss 5.517, Val loss 5.595\n",
      "Ep 1 (Step 000180): Train loss 5.484, Val loss 5.567\n",
      "Ep 1 (Step 000190): Train loss 5.486, Val loss 5.548\n",
      "Ep 1 (Step 000200): Train loss 5.450, Val loss 5.528\n",
      "Ep 1 (Step 000210): Train loss 5.409, Val loss 5.506\n",
      "Ep 1 (Step 000220): Train loss 5.366, Val loss 5.483\n",
      "Ep 1 (Step 000230): Train loss 5.415, Val loss 5.466\n",
      "Ep 1 (Step 000240): Train loss 5.394, Val loss 5.446\n",
      "Ep 1 (Step 000250): Train loss 5.307, Val loss 5.430\n",
      "Ep 1 (Step 000260): Train loss 5.356, Val loss 5.424\n",
      "Ep 1 (Step 000270): Train loss 5.344, Val loss 5.405\n",
      "Ep 1 (Step 000280): Train loss 5.318, Val loss 5.394\n",
      "Ep 1 (Step 000290): Train loss 5.203, Val loss 5.379\n",
      "Ep 1 (Step 000300): Train loss 5.281, Val loss 5.369\n",
      "Ep 1 (Step 000310): Train loss 5.193, Val loss 5.358\n",
      "Ep 1 (Step 000320): Train loss 5.170, Val loss 5.344\n",
      "Ep 1 (Step 000330): Train loss 5.229, Val loss 5.336\n",
      "Ep 1 (Step 000340): Train loss 5.185, Val loss 5.322\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3218\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.095, Val loss 9.095\n",
      "Ep 1 (Step 000010): Train loss 7.526, Val loss 7.511\n",
      "Ep 1 (Step 000020): Train loss 6.882, Val loss 6.841\n",
      "Ep 1 (Step 000030): Train loss 6.509, Val loss 6.485\n",
      "Ep 1 (Step 000040): Train loss 6.374, Val loss 6.374\n",
      "Ep 1 (Step 000050): Train loss 6.313, Val loss 6.334\n",
      "Ep 1 (Step 000060): Train loss 6.239, Val loss 6.248\n",
      "Ep 1 (Step 000070): Train loss 6.143, Val loss 6.115\n",
      "Ep 1 (Step 000080): Train loss 6.056, Val loss 6.030\n",
      "Ep 1 (Step 000090): Train loss 5.909, Val loss 5.961\n",
      "Ep 1 (Step 000100): Train loss 5.830, Val loss 5.885\n",
      "Ep 1 (Step 000110): Train loss 5.822, Val loss 5.835\n",
      "Ep 1 (Step 000120): Train loss 5.734, Val loss 5.794\n",
      "Ep 1 (Step 000130): Train loss 5.725, Val loss 5.756\n",
      "Ep 1 (Step 000140): Train loss 5.699, Val loss 5.707\n",
      "Ep 1 (Step 000150): Train loss 5.656, Val loss 5.662\n",
      "Ep 1 (Step 000160): Train loss 5.608, Val loss 5.635\n",
      "Ep 1 (Step 000170): Train loss 5.622, Val loss 5.599\n",
      "Ep 1 (Step 000180): Train loss 5.568, Val loss 5.571\n",
      "Ep 1 (Step 000190): Train loss 5.539, Val loss 5.537\n",
      "Ep 1 (Step 000200): Train loss 5.419, Val loss 5.521\n",
      "Ep 1 (Step 000210): Train loss 5.424, Val loss 5.506\n",
      "Ep 1 (Step 000220): Train loss 5.374, Val loss 5.476\n",
      "Ep 1 (Step 000230): Train loss 5.292, Val loss 5.460\n",
      "Ep 1 (Step 000240): Train loss 5.359, Val loss 5.442\n",
      "Ep 1 (Step 000250): Train loss 5.336, Val loss 5.437\n",
      "Ep 1 (Step 000260): Train loss 5.349, Val loss 5.423\n",
      "Ep 1 (Step 000270): Train loss 5.265, Val loss 5.406\n",
      "Ep 1 (Step 000280): Train loss 5.271, Val loss 5.394\n",
      "Ep 1 (Step 000290): Train loss 5.284, Val loss 5.372\n",
      "Ep 1 (Step 000300): Train loss 5.173, Val loss 5.357\n",
      "Ep 1 (Step 000310): Train loss 5.214, Val loss 5.358\n",
      "Ep 1 (Step 000320): Train loss 5.108, Val loss 5.339\n",
      "Ep 1 (Step 000330): Train loss 5.205, Val loss 5.331\n",
      "Ep 1 (Step 000340): Train loss 5.234, Val loss 5.324\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3244\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.054, Val loss 9.046\n",
      "Ep 1 (Step 000010): Train loss 7.475, Val loss 7.469\n",
      "Ep 1 (Step 000020): Train loss 6.836, Val loss 6.788\n",
      "Ep 1 (Step 000030): Train loss 6.483, Val loss 6.473\n",
      "Ep 1 (Step 000040): Train loss 6.368, Val loss 6.380\n",
      "Ep 1 (Step 000050): Train loss 6.329, Val loss 6.332\n",
      "Ep 1 (Step 000060): Train loss 6.254, Val loss 6.246\n",
      "Ep 1 (Step 000070): Train loss 6.126, Val loss 6.132\n",
      "Ep 1 (Step 000080): Train loss 6.051, Val loss 6.041\n",
      "Ep 1 (Step 000090): Train loss 5.982, Val loss 5.956\n",
      "Ep 1 (Step 000100): Train loss 5.886, Val loss 5.909\n",
      "Ep 1 (Step 000110): Train loss 5.883, Val loss 5.846\n",
      "Ep 1 (Step 000120): Train loss 5.805, Val loss 5.808\n",
      "Ep 1 (Step 000130): Train loss 5.630, Val loss 5.758\n",
      "Ep 1 (Step 000140): Train loss 5.628, Val loss 5.721\n",
      "Ep 1 (Step 000150): Train loss 5.657, Val loss 5.688\n",
      "Ep 1 (Step 000160): Train loss 5.624, Val loss 5.657\n",
      "Ep 1 (Step 000170): Train loss 5.574, Val loss 5.613\n",
      "Ep 1 (Step 000180): Train loss 5.570, Val loss 5.585\n",
      "Ep 1 (Step 000190): Train loss 5.480, Val loss 5.578\n",
      "Ep 1 (Step 000200): Train loss 5.425, Val loss 5.540\n",
      "Ep 1 (Step 000210): Train loss 5.393, Val loss 5.506\n",
      "Ep 1 (Step 000220): Train loss 5.435, Val loss 5.500\n",
      "Ep 1 (Step 000230): Train loss 5.425, Val loss 5.479\n",
      "Ep 1 (Step 000240): Train loss 5.364, Val loss 5.458\n",
      "Ep 1 (Step 000250): Train loss 5.377, Val loss 5.436\n",
      "Ep 1 (Step 000260): Train loss 5.293, Val loss 5.417\n",
      "Ep 1 (Step 000270): Train loss 5.417, Val loss 5.406\n",
      "Ep 1 (Step 000280): Train loss 5.259, Val loss 5.384\n",
      "Ep 1 (Step 000290): Train loss 5.353, Val loss 5.376\n",
      "Ep 1 (Step 000300): Train loss 5.211, Val loss 5.355\n",
      "Ep 1 (Step 000310): Train loss 5.148, Val loss 5.347\n",
      "Ep 1 (Step 000320): Train loss 5.255, Val loss 5.338\n",
      "Ep 1 (Step 000330): Train loss 5.149, Val loss 5.322\n",
      "Ep 1 (Step 000340): Train loss 5.221, Val loss 5.316\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3159\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.064, Val loss 9.043\n",
      "Ep 1 (Step 000010): Train loss 7.459, Val loss 7.398\n",
      "Ep 1 (Step 000020): Train loss 6.753, Val loss 6.751\n",
      "Ep 1 (Step 000030): Train loss 6.436, Val loss 6.449\n",
      "Ep 1 (Step 000040): Train loss 6.382, Val loss 6.385\n",
      "Ep 1 (Step 000050): Train loss 6.370, Val loss 6.341\n",
      "Ep 1 (Step 000060): Train loss 6.215, Val loss 6.244\n",
      "Ep 1 (Step 000070): Train loss 6.123, Val loss 6.125\n",
      "Ep 1 (Step 000080): Train loss 6.028, Val loss 6.051\n",
      "Ep 1 (Step 000090): Train loss 5.920, Val loss 5.974\n",
      "Ep 1 (Step 000100): Train loss 5.846, Val loss 5.899\n",
      "Ep 1 (Step 000110): Train loss 5.760, Val loss 5.849\n",
      "Ep 1 (Step 000120): Train loss 5.820, Val loss 5.795\n",
      "Ep 1 (Step 000130): Train loss 5.683, Val loss 5.767\n",
      "Ep 1 (Step 000140): Train loss 5.579, Val loss 5.716\n",
      "Ep 1 (Step 000150): Train loss 5.640, Val loss 5.695\n",
      "Ep 1 (Step 000160): Train loss 5.569, Val loss 5.658\n",
      "Ep 1 (Step 000170): Train loss 5.561, Val loss 5.622\n",
      "Ep 1 (Step 000180): Train loss 5.537, Val loss 5.592\n",
      "Ep 1 (Step 000190): Train loss 5.500, Val loss 5.561\n",
      "Ep 1 (Step 000200): Train loss 5.404, Val loss 5.540\n",
      "Ep 1 (Step 000210): Train loss 5.385, Val loss 5.510\n",
      "Ep 1 (Step 000220): Train loss 5.417, Val loss 5.490\n",
      "Ep 1 (Step 000230): Train loss 5.409, Val loss 5.465\n",
      "Ep 1 (Step 000240): Train loss 5.400, Val loss 5.461\n",
      "Ep 1 (Step 000250): Train loss 5.319, Val loss 5.435\n",
      "Ep 1 (Step 000260): Train loss 5.297, Val loss 5.420\n",
      "Ep 1 (Step 000270): Train loss 5.250, Val loss 5.417\n",
      "Ep 1 (Step 000280): Train loss 5.262, Val loss 5.392\n",
      "Ep 1 (Step 000290): Train loss 5.229, Val loss 5.376\n",
      "Ep 1 (Step 000300): Train loss 5.239, Val loss 5.370\n",
      "Ep 1 (Step 000310): Train loss 5.215, Val loss 5.348\n",
      "Ep 1 (Step 000320): Train loss 5.207, Val loss 5.341\n",
      "Ep 1 (Step 000330): Train loss 5.238, Val loss 5.324\n",
      "Ep 1 (Step 000340): Train loss 5.140, Val loss 5.316\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3163\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.786, Val loss 8.771\n",
      "Ep 1 (Step 000010): Train loss 6.910, Val loss 6.888\n",
      "Ep 1 (Step 000020): Train loss 6.449, Val loss 6.466\n",
      "Ep 1 (Step 000030): Train loss 6.415, Val loss 6.413\n",
      "Ep 1 (Step 000040): Train loss 6.297, Val loss 6.263\n",
      "Ep 1 (Step 000050): Train loss 6.016, Val loss 6.106\n",
      "Ep 1 (Step 000060): Train loss 5.958, Val loss 5.995\n",
      "Ep 1 (Step 000070): Train loss 5.908, Val loss 5.916\n",
      "Ep 1 (Step 000080): Train loss 5.752, Val loss 5.807\n",
      "Ep 1 (Step 000090): Train loss 5.748, Val loss 5.760\n",
      "Ep 1 (Step 000100): Train loss 5.619, Val loss 5.691\n",
      "Ep 1 (Step 000110): Train loss 5.499, Val loss 5.649\n",
      "Ep 1 (Step 000120): Train loss 5.534, Val loss 5.610\n",
      "Ep 1 (Step 000130): Train loss 5.427, Val loss 5.579\n",
      "Ep 1 (Step 000140): Train loss 5.541, Val loss 5.542\n",
      "Ep 1 (Step 000150): Train loss 5.364, Val loss 5.514\n",
      "Ep 1 (Step 000160): Train loss 5.286, Val loss 5.487\n",
      "Ep 1 (Step 000170): Train loss 5.353, Val loss 5.474\n",
      "Ep 1 (Step 000180): Train loss 5.397, Val loss 5.443\n",
      "Ep 1 (Step 000190): Train loss 5.267, Val loss 5.416\n",
      "Ep 1 (Step 000200): Train loss 5.249, Val loss 5.384\n",
      "Ep 1 (Step 000210): Train loss 5.236, Val loss 5.381\n",
      "Ep 1 (Step 000220): Train loss 5.270, Val loss 5.384\n",
      "Ep 1 (Step 000230): Train loss 5.218, Val loss 5.351\n",
      "Ep 1 (Step 000240): Train loss 5.273, Val loss 5.330\n",
      "Ep 1 (Step 000250): Train loss 5.187, Val loss 5.329\n",
      "Ep 1 (Step 000260): Train loss 5.151, Val loss 5.307\n",
      "Ep 1 (Step 000270): Train loss 5.168, Val loss 5.287\n",
      "Ep 1 (Step 000280): Train loss 5.158, Val loss 5.287\n",
      "Ep 1 (Step 000290): Train loss 5.199, Val loss 5.266\n",
      "Ep 1 (Step 000300): Train loss 5.097, Val loss 5.268\n",
      "Ep 1 (Step 000310): Train loss 5.055, Val loss 5.262\n",
      "Ep 1 (Step 000320): Train loss 5.128, Val loss 5.240\n",
      "Ep 1 (Step 000330): Train loss 5.069, Val loss 5.220\n",
      "Ep 1 (Step 000340): Train loss 5.004, Val loss 5.219\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2190\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.800, Val loss 8.794\n",
      "Ep 1 (Step 000010): Train loss 6.901, Val loss 6.880\n",
      "Ep 1 (Step 000020): Train loss 6.461, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.407, Val loss 6.403\n",
      "Ep 1 (Step 000040): Train loss 6.240, Val loss 6.246\n",
      "Ep 1 (Step 000050): Train loss 6.181, Val loss 6.099\n",
      "Ep 1 (Step 000060): Train loss 5.987, Val loss 5.974\n",
      "Ep 1 (Step 000070): Train loss 5.853, Val loss 5.919\n",
      "Ep 1 (Step 000080): Train loss 5.714, Val loss 5.804\n",
      "Ep 1 (Step 000090): Train loss 5.706, Val loss 5.710\n",
      "Ep 1 (Step 000100): Train loss 5.562, Val loss 5.663\n",
      "Ep 1 (Step 000110): Train loss 5.572, Val loss 5.639\n",
      "Ep 1 (Step 000120): Train loss 5.527, Val loss 5.588\n",
      "Ep 1 (Step 000130): Train loss 5.383, Val loss 5.537\n",
      "Ep 1 (Step 000140): Train loss 5.397, Val loss 5.524\n",
      "Ep 1 (Step 000150): Train loss 5.416, Val loss 5.503\n",
      "Ep 1 (Step 000160): Train loss 5.314, Val loss 5.456\n",
      "Ep 1 (Step 000170): Train loss 5.413, Val loss 5.436\n",
      "Ep 1 (Step 000180): Train loss 5.344, Val loss 5.430\n",
      "Ep 1 (Step 000190): Train loss 5.290, Val loss 5.415\n",
      "Ep 1 (Step 000200): Train loss 5.345, Val loss 5.388\n",
      "Ep 1 (Step 000210): Train loss 5.294, Val loss 5.384\n",
      "Ep 1 (Step 000220): Train loss 5.170, Val loss 5.351\n",
      "Ep 1 (Step 000230): Train loss 5.165, Val loss 5.337\n",
      "Ep 1 (Step 000240): Train loss 5.228, Val loss 5.332\n",
      "Ep 1 (Step 000250): Train loss 5.121, Val loss 5.324\n",
      "Ep 1 (Step 000260): Train loss 5.160, Val loss 5.294\n",
      "Ep 1 (Step 000270): Train loss 5.211, Val loss 5.305\n",
      "Ep 1 (Step 000280): Train loss 5.096, Val loss 5.282\n",
      "Ep 1 (Step 000290): Train loss 5.112, Val loss 5.286\n",
      "Ep 1 (Step 000300): Train loss 5.080, Val loss 5.274\n",
      "Ep 1 (Step 000310): Train loss 5.142, Val loss 5.261\n",
      "Ep 1 (Step 000320): Train loss 5.097, Val loss 5.241\n",
      "Ep 1 (Step 000330): Train loss 5.084, Val loss 5.225\n",
      "Ep 1 (Step 000340): Train loss 4.993, Val loss 5.218\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2177\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.755, Val loss 8.733\n",
      "Ep 1 (Step 000010): Train loss 6.878, Val loss 6.859\n",
      "Ep 1 (Step 000020): Train loss 6.430, Val loss 6.458\n",
      "Ep 1 (Step 000030): Train loss 6.440, Val loss 6.436\n",
      "Ep 1 (Step 000040): Train loss 6.355, Val loss 6.319\n",
      "Ep 1 (Step 000050): Train loss 6.212, Val loss 6.159\n",
      "Ep 1 (Step 000060): Train loss 5.951, Val loss 6.016\n",
      "Ep 1 (Step 000070): Train loss 5.888, Val loss 5.915\n",
      "Ep 1 (Step 000080): Train loss 5.804, Val loss 5.847\n",
      "Ep 1 (Step 000090): Train loss 5.719, Val loss 5.758\n",
      "Ep 1 (Step 000100): Train loss 5.721, Val loss 5.695\n",
      "Ep 1 (Step 000110): Train loss 5.615, Val loss 5.651\n",
      "Ep 1 (Step 000120): Train loss 5.545, Val loss 5.590\n",
      "Ep 1 (Step 000130): Train loss 5.501, Val loss 5.568\n",
      "Ep 1 (Step 000140): Train loss 5.411, Val loss 5.536\n",
      "Ep 1 (Step 000150): Train loss 5.496, Val loss 5.506\n",
      "Ep 1 (Step 000160): Train loss 5.426, Val loss 5.487\n",
      "Ep 1 (Step 000170): Train loss 5.329, Val loss 5.463\n",
      "Ep 1 (Step 000180): Train loss 5.268, Val loss 5.442\n",
      "Ep 1 (Step 000190): Train loss 5.344, Val loss 5.429\n",
      "Ep 1 (Step 000200): Train loss 5.317, Val loss 5.391\n",
      "Ep 1 (Step 000210): Train loss 5.186, Val loss 5.375\n",
      "Ep 1 (Step 000220): Train loss 5.223, Val loss 5.352\n",
      "Ep 1 (Step 000230): Train loss 5.218, Val loss 5.348\n",
      "Ep 1 (Step 000240): Train loss 5.185, Val loss 5.347\n",
      "Ep 1 (Step 000250): Train loss 5.150, Val loss 5.317\n",
      "Ep 1 (Step 000260): Train loss 5.176, Val loss 5.321\n",
      "Ep 1 (Step 000270): Train loss 5.142, Val loss 5.289\n",
      "Ep 1 (Step 000280): Train loss 5.075, Val loss 5.292\n",
      "Ep 1 (Step 000290): Train loss 5.070, Val loss 5.292\n",
      "Ep 1 (Step 000300): Train loss 5.054, Val loss 5.271\n",
      "Ep 1 (Step 000310): Train loss 5.067, Val loss 5.274\n",
      "Ep 1 (Step 000320): Train loss 5.083, Val loss 5.261\n",
      "Ep 1 (Step 000330): Train loss 5.060, Val loss 5.244\n",
      "Ep 1 (Step 000340): Train loss 5.019, Val loss 5.253\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2526\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.836, Val loss 8.839\n",
      "Ep 1 (Step 000010): Train loss 6.861, Val loss 6.836\n",
      "Ep 1 (Step 000020): Train loss 6.538, Val loss 6.459\n",
      "Ep 1 (Step 000030): Train loss 6.472, Val loss 6.412\n",
      "Ep 1 (Step 000040): Train loss 6.316, Val loss 6.276\n",
      "Ep 1 (Step 000050): Train loss 6.102, Val loss 6.132\n",
      "Ep 1 (Step 000060): Train loss 5.993, Val loss 6.042\n",
      "Ep 1 (Step 000070): Train loss 5.918, Val loss 5.944\n",
      "Ep 1 (Step 000080): Train loss 5.728, Val loss 5.860\n",
      "Ep 1 (Step 000090): Train loss 5.719, Val loss 5.768\n",
      "Ep 1 (Step 000100): Train loss 5.617, Val loss 5.715\n",
      "Ep 1 (Step 000110): Train loss 5.546, Val loss 5.655\n",
      "Ep 1 (Step 000120): Train loss 5.552, Val loss 5.607\n",
      "Ep 1 (Step 000130): Train loss 5.489, Val loss 5.580\n",
      "Ep 1 (Step 000140): Train loss 5.421, Val loss 5.524\n",
      "Ep 1 (Step 000150): Train loss 5.337, Val loss 5.496\n",
      "Ep 1 (Step 000160): Train loss 5.314, Val loss 5.475\n",
      "Ep 1 (Step 000170): Train loss 5.442, Val loss 5.443\n",
      "Ep 1 (Step 000180): Train loss 5.322, Val loss 5.435\n",
      "Ep 1 (Step 000190): Train loss 5.267, Val loss 5.409\n",
      "Ep 1 (Step 000200): Train loss 5.223, Val loss 5.380\n",
      "Ep 1 (Step 000210): Train loss 5.209, Val loss 5.365\n",
      "Ep 1 (Step 000220): Train loss 5.294, Val loss 5.350\n",
      "Ep 1 (Step 000230): Train loss 5.305, Val loss 5.326\n",
      "Ep 1 (Step 000240): Train loss 5.076, Val loss 5.300\n",
      "Ep 1 (Step 000250): Train loss 5.172, Val loss 5.303\n",
      "Ep 1 (Step 000260): Train loss 5.089, Val loss 5.298\n",
      "Ep 1 (Step 000270): Train loss 5.140, Val loss 5.284\n",
      "Ep 1 (Step 000280): Train loss 5.207, Val loss 5.272\n",
      "Ep 1 (Step 000290): Train loss 4.965, Val loss 5.263\n",
      "Ep 1 (Step 000300): Train loss 5.098, Val loss 5.266\n",
      "Ep 1 (Step 000310): Train loss 5.098, Val loss 5.239\n",
      "Ep 1 (Step 000320): Train loss 5.095, Val loss 5.237\n",
      "Ep 1 (Step 000330): Train loss 5.030, Val loss 5.197\n",
      "Ep 1 (Step 000340): Train loss 4.979, Val loss 5.199\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1985\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.754, Val loss 8.748\n",
      "Ep 1 (Step 000010): Train loss 6.895, Val loss 6.860\n",
      "Ep 1 (Step 000020): Train loss 6.471, Val loss 6.463\n",
      "Ep 1 (Step 000030): Train loss 6.412, Val loss 6.413\n",
      "Ep 1 (Step 000040): Train loss 6.276, Val loss 6.237\n",
      "Ep 1 (Step 000050): Train loss 6.052, Val loss 6.080\n",
      "Ep 1 (Step 000060): Train loss 5.891, Val loss 5.971\n",
      "Ep 1 (Step 000070): Train loss 5.876, Val loss 5.892\n",
      "Ep 1 (Step 000080): Train loss 5.709, Val loss 5.839\n",
      "Ep 1 (Step 000090): Train loss 5.738, Val loss 5.736\n",
      "Ep 1 (Step 000100): Train loss 5.628, Val loss 5.693\n",
      "Ep 1 (Step 000110): Train loss 5.480, Val loss 5.647\n",
      "Ep 1 (Step 000120): Train loss 5.481, Val loss 5.590\n",
      "Ep 1 (Step 000130): Train loss 5.539, Val loss 5.556\n",
      "Ep 1 (Step 000140): Train loss 5.527, Val loss 5.529\n",
      "Ep 1 (Step 000150): Train loss 5.355, Val loss 5.513\n",
      "Ep 1 (Step 000160): Train loss 5.405, Val loss 5.476\n",
      "Ep 1 (Step 000170): Train loss 5.435, Val loss 5.440\n",
      "Ep 1 (Step 000180): Train loss 5.322, Val loss 5.416\n",
      "Ep 1 (Step 000190): Train loss 5.190, Val loss 5.395\n",
      "Ep 1 (Step 000200): Train loss 5.333, Val loss 5.374\n",
      "Ep 1 (Step 000210): Train loss 5.253, Val loss 5.354\n",
      "Ep 1 (Step 000220): Train loss 5.245, Val loss 5.344\n",
      "Ep 1 (Step 000230): Train loss 5.237, Val loss 5.331\n",
      "Ep 1 (Step 000240): Train loss 5.257, Val loss 5.332\n",
      "Ep 1 (Step 000250): Train loss 5.216, Val loss 5.314\n",
      "Ep 1 (Step 000260): Train loss 5.186, Val loss 5.300\n",
      "Ep 1 (Step 000270): Train loss 5.145, Val loss 5.283\n",
      "Ep 1 (Step 000280): Train loss 5.145, Val loss 5.271\n",
      "Ep 1 (Step 000290): Train loss 5.178, Val loss 5.236\n",
      "Ep 1 (Step 000300): Train loss 5.063, Val loss 5.214\n",
      "Ep 1 (Step 000310): Train loss 5.155, Val loss 5.220\n",
      "Ep 1 (Step 000320): Train loss 5.058, Val loss 5.213\n",
      "Ep 1 (Step 000330): Train loss 5.061, Val loss 5.182\n",
      "Ep 1 (Step 000340): Train loss 5.061, Val loss 5.166\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1662\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.789, Val loss 8.772\n",
      "Ep 1 (Step 000010): Train loss 6.872, Val loss 6.830\n",
      "Ep 1 (Step 000020): Train loss 6.498, Val loss 6.463\n",
      "Ep 1 (Step 000030): Train loss 6.443, Val loss 6.429\n",
      "Ep 1 (Step 000040): Train loss 6.271, Val loss 6.288\n",
      "Ep 1 (Step 000050): Train loss 6.115, Val loss 6.135\n",
      "Ep 1 (Step 000060): Train loss 5.958, Val loss 6.019\n",
      "Ep 1 (Step 000070): Train loss 5.870, Val loss 5.921\n",
      "Ep 1 (Step 000080): Train loss 5.801, Val loss 5.849\n",
      "Ep 1 (Step 000090): Train loss 5.709, Val loss 5.761\n",
      "Ep 1 (Step 000100): Train loss 5.687, Val loss 5.707\n",
      "Ep 1 (Step 000110): Train loss 5.463, Val loss 5.652\n",
      "Ep 1 (Step 000120): Train loss 5.579, Val loss 5.597\n",
      "Ep 1 (Step 000130): Train loss 5.563, Val loss 5.551\n",
      "Ep 1 (Step 000140): Train loss 5.479, Val loss 5.534\n",
      "Ep 1 (Step 000150): Train loss 5.500, Val loss 5.483\n",
      "Ep 1 (Step 000160): Train loss 5.449, Val loss 5.464\n",
      "Ep 1 (Step 000170): Train loss 5.299, Val loss 5.447\n",
      "Ep 1 (Step 000180): Train loss 5.368, Val loss 5.403\n",
      "Ep 1 (Step 000190): Train loss 5.280, Val loss 5.397\n",
      "Ep 1 (Step 000200): Train loss 5.294, Val loss 5.377\n",
      "Ep 1 (Step 000210): Train loss 5.227, Val loss 5.358\n",
      "Ep 1 (Step 000220): Train loss 5.150, Val loss 5.362\n",
      "Ep 1 (Step 000230): Train loss 5.237, Val loss 5.313\n",
      "Ep 1 (Step 000240): Train loss 5.178, Val loss 5.298\n",
      "Ep 1 (Step 000250): Train loss 5.141, Val loss 5.295\n",
      "Ep 1 (Step 000260): Train loss 5.209, Val loss 5.273\n",
      "Ep 1 (Step 000270): Train loss 5.201, Val loss 5.254\n",
      "Ep 1 (Step 000280): Train loss 5.088, Val loss 5.266\n",
      "Ep 1 (Step 000290): Train loss 5.113, Val loss 5.259\n",
      "Ep 1 (Step 000300): Train loss 5.072, Val loss 5.251\n",
      "Ep 1 (Step 000310): Train loss 5.072, Val loss 5.234\n",
      "Ep 1 (Step 000320): Train loss 5.111, Val loss 5.228\n",
      "Ep 1 (Step 000330): Train loss 4.969, Val loss 5.212\n",
      "Ep 1 (Step 000340): Train loss 4.989, Val loss 5.182\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1824\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.066, Val loss 9.061\n",
      "Ep 1 (Step 000010): Train loss 7.452, Val loss 7.445\n",
      "Ep 1 (Step 000020): Train loss 6.821, Val loss 6.807\n",
      "Ep 1 (Step 000030): Train loss 6.450, Val loss 6.487\n",
      "Ep 1 (Step 000040): Train loss 6.439, Val loss 6.387\n",
      "Ep 1 (Step 000050): Train loss 6.276, Val loss 6.331\n",
      "Ep 1 (Step 000060): Train loss 6.217, Val loss 6.239\n",
      "Ep 1 (Step 000070): Train loss 6.069, Val loss 6.128\n",
      "Ep 1 (Step 000080): Train loss 5.990, Val loss 6.042\n",
      "Ep 1 (Step 000090): Train loss 5.936, Val loss 5.975\n",
      "Ep 1 (Step 000100): Train loss 5.867, Val loss 5.921\n",
      "Ep 1 (Step 000110): Train loss 5.804, Val loss 5.865\n",
      "Ep 1 (Step 000120): Train loss 5.734, Val loss 5.827\n",
      "Ep 1 (Step 000130): Train loss 5.773, Val loss 5.768\n",
      "Ep 1 (Step 000140): Train loss 5.652, Val loss 5.721\n",
      "Ep 1 (Step 000150): Train loss 5.563, Val loss 5.683\n",
      "Ep 1 (Step 000160): Train loss 5.518, Val loss 5.660\n",
      "Ep 1 (Step 000170): Train loss 5.495, Val loss 5.612\n",
      "Ep 1 (Step 000180): Train loss 5.510, Val loss 5.586\n",
      "Ep 1 (Step 000190): Train loss 5.566, Val loss 5.564\n",
      "Ep 1 (Step 000200): Train loss 5.449, Val loss 5.538\n",
      "Ep 1 (Step 000210): Train loss 5.442, Val loss 5.527\n",
      "Ep 1 (Step 000220): Train loss 5.400, Val loss 5.513\n",
      "Ep 1 (Step 000230): Train loss 5.294, Val loss 5.493\n",
      "Ep 1 (Step 000240): Train loss 5.357, Val loss 5.475\n",
      "Ep 1 (Step 000250): Train loss 5.326, Val loss 5.466\n",
      "Ep 1 (Step 000260): Train loss 5.391, Val loss 5.446\n",
      "Ep 1 (Step 000270): Train loss 5.266, Val loss 5.423\n",
      "Ep 1 (Step 000280): Train loss 5.313, Val loss 5.419\n",
      "Ep 1 (Step 000290): Train loss 5.281, Val loss 5.402\n",
      "Ep 1 (Step 000300): Train loss 5.235, Val loss 5.375\n",
      "Ep 1 (Step 000310): Train loss 5.358, Val loss 5.375\n",
      "Ep 1 (Step 000320): Train loss 5.201, Val loss 5.356\n",
      "Ep 1 (Step 000330): Train loss 5.293, Val loss 5.343\n",
      "Ep 1 (Step 000340): Train loss 5.231, Val loss 5.343\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3434\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.071, Val loss 9.052\n",
      "Ep 1 (Step 000010): Train loss 7.472, Val loss 7.482\n",
      "Ep 1 (Step 000020): Train loss 6.874, Val loss 6.797\n",
      "Ep 1 (Step 000030): Train loss 6.496, Val loss 6.473\n",
      "Ep 1 (Step 000040): Train loss 6.373, Val loss 6.378\n",
      "Ep 1 (Step 000050): Train loss 6.328, Val loss 6.333\n",
      "Ep 1 (Step 000060): Train loss 6.218, Val loss 6.241\n",
      "Ep 1 (Step 000070): Train loss 6.159, Val loss 6.114\n",
      "Ep 1 (Step 000080): Train loss 6.008, Val loss 6.058\n",
      "Ep 1 (Step 000090): Train loss 5.926, Val loss 5.970\n",
      "Ep 1 (Step 000100): Train loss 5.860, Val loss 5.920\n",
      "Ep 1 (Step 000110): Train loss 5.819, Val loss 5.865\n",
      "Ep 1 (Step 000120): Train loss 5.778, Val loss 5.810\n",
      "Ep 1 (Step 000130): Train loss 5.675, Val loss 5.771\n",
      "Ep 1 (Step 000140): Train loss 5.725, Val loss 5.728\n",
      "Ep 1 (Step 000150): Train loss 5.661, Val loss 5.685\n",
      "Ep 1 (Step 000160): Train loss 5.521, Val loss 5.670\n",
      "Ep 1 (Step 000170): Train loss 5.511, Val loss 5.619\n",
      "Ep 1 (Step 000180): Train loss 5.502, Val loss 5.590\n",
      "Ep 1 (Step 000190): Train loss 5.483, Val loss 5.576\n",
      "Ep 1 (Step 000200): Train loss 5.410, Val loss 5.548\n",
      "Ep 1 (Step 000210): Train loss 5.402, Val loss 5.517\n",
      "Ep 1 (Step 000220): Train loss 5.322, Val loss 5.506\n",
      "Ep 1 (Step 000230): Train loss 5.369, Val loss 5.499\n",
      "Ep 1 (Step 000240): Train loss 5.343, Val loss 5.481\n",
      "Ep 1 (Step 000250): Train loss 5.293, Val loss 5.451\n",
      "Ep 1 (Step 000260): Train loss 5.324, Val loss 5.441\n",
      "Ep 1 (Step 000270): Train loss 5.306, Val loss 5.435\n",
      "Ep 1 (Step 000280): Train loss 5.261, Val loss 5.420\n",
      "Ep 1 (Step 000290): Train loss 5.318, Val loss 5.390\n",
      "Ep 1 (Step 000300): Train loss 5.309, Val loss 5.380\n",
      "Ep 1 (Step 000310): Train loss 5.244, Val loss 5.375\n",
      "Ep 1 (Step 000320): Train loss 5.272, Val loss 5.364\n",
      "Ep 1 (Step 000330): Train loss 5.258, Val loss 5.361\n",
      "Ep 1 (Step 000340): Train loss 5.140, Val loss 5.341\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3409\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.053, Val loss 9.044\n",
      "Ep 1 (Step 000010): Train loss 7.502, Val loss 7.465\n",
      "Ep 1 (Step 000020): Train loss 6.815, Val loss 6.809\n",
      "Ep 1 (Step 000030): Train loss 6.447, Val loss 6.477\n",
      "Ep 1 (Step 000040): Train loss 6.453, Val loss 6.384\n",
      "Ep 1 (Step 000050): Train loss 6.270, Val loss 6.334\n",
      "Ep 1 (Step 000060): Train loss 6.204, Val loss 6.228\n",
      "Ep 1 (Step 000070): Train loss 6.094, Val loss 6.091\n",
      "Ep 1 (Step 000080): Train loss 6.000, Val loss 6.025\n",
      "Ep 1 (Step 000090): Train loss 5.892, Val loss 5.944\n",
      "Ep 1 (Step 000100): Train loss 5.879, Val loss 5.898\n",
      "Ep 1 (Step 000110): Train loss 5.774, Val loss 5.834\n",
      "Ep 1 (Step 000120): Train loss 5.796, Val loss 5.794\n",
      "Ep 1 (Step 000130): Train loss 5.693, Val loss 5.763\n",
      "Ep 1 (Step 000140): Train loss 5.583, Val loss 5.713\n",
      "Ep 1 (Step 000150): Train loss 5.638, Val loss 5.680\n",
      "Ep 1 (Step 000160): Train loss 5.544, Val loss 5.652\n",
      "Ep 1 (Step 000170): Train loss 5.536, Val loss 5.628\n",
      "Ep 1 (Step 000180): Train loss 5.581, Val loss 5.580\n",
      "Ep 1 (Step 000190): Train loss 5.409, Val loss 5.556\n",
      "Ep 1 (Step 000200): Train loss 5.466, Val loss 5.544\n",
      "Ep 1 (Step 000210): Train loss 5.423, Val loss 5.520\n",
      "Ep 1 (Step 000220): Train loss 5.414, Val loss 5.502\n",
      "Ep 1 (Step 000230): Train loss 5.331, Val loss 5.483\n",
      "Ep 1 (Step 000240): Train loss 5.291, Val loss 5.480\n",
      "Ep 1 (Step 000250): Train loss 5.318, Val loss 5.450\n",
      "Ep 1 (Step 000260): Train loss 5.371, Val loss 5.432\n",
      "Ep 1 (Step 000270): Train loss 5.314, Val loss 5.428\n",
      "Ep 1 (Step 000280): Train loss 5.337, Val loss 5.407\n",
      "Ep 1 (Step 000290): Train loss 5.236, Val loss 5.393\n",
      "Ep 1 (Step 000300): Train loss 5.216, Val loss 5.384\n",
      "Ep 1 (Step 000310): Train loss 5.239, Val loss 5.376\n",
      "Ep 1 (Step 000320): Train loss 5.207, Val loss 5.364\n",
      "Ep 1 (Step 000330): Train loss 5.271, Val loss 5.365\n",
      "Ep 1 (Step 000340): Train loss 5.267, Val loss 5.346\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3457\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.063, Val loss 9.046\n",
      "Ep 1 (Step 000010): Train loss 7.481, Val loss 7.432\n",
      "Ep 1 (Step 000020): Train loss 6.845, Val loss 6.773\n",
      "Ep 1 (Step 000030): Train loss 6.456, Val loss 6.459\n",
      "Ep 1 (Step 000040): Train loss 6.424, Val loss 6.380\n",
      "Ep 1 (Step 000050): Train loss 6.317, Val loss 6.327\n",
      "Ep 1 (Step 000060): Train loss 6.198, Val loss 6.250\n",
      "Ep 1 (Step 000070): Train loss 6.144, Val loss 6.141\n",
      "Ep 1 (Step 000080): Train loss 5.965, Val loss 6.042\n",
      "Ep 1 (Step 000090): Train loss 5.964, Val loss 5.959\n",
      "Ep 1 (Step 000100): Train loss 5.839, Val loss 5.908\n",
      "Ep 1 (Step 000110): Train loss 5.794, Val loss 5.855\n",
      "Ep 1 (Step 000120): Train loss 5.751, Val loss 5.785\n",
      "Ep 1 (Step 000130): Train loss 5.662, Val loss 5.742\n",
      "Ep 1 (Step 000140): Train loss 5.635, Val loss 5.709\n",
      "Ep 1 (Step 000150): Train loss 5.709, Val loss 5.677\n",
      "Ep 1 (Step 000160): Train loss 5.600, Val loss 5.630\n",
      "Ep 1 (Step 000170): Train loss 5.508, Val loss 5.596\n",
      "Ep 1 (Step 000180): Train loss 5.469, Val loss 5.571\n",
      "Ep 1 (Step 000190): Train loss 5.478, Val loss 5.545\n",
      "Ep 1 (Step 000200): Train loss 5.441, Val loss 5.514\n",
      "Ep 1 (Step 000210): Train loss 5.459, Val loss 5.491\n",
      "Ep 1 (Step 000220): Train loss 5.395, Val loss 5.471\n",
      "Ep 1 (Step 000230): Train loss 5.333, Val loss 5.465\n",
      "Ep 1 (Step 000240): Train loss 5.383, Val loss 5.451\n",
      "Ep 1 (Step 000250): Train loss 5.223, Val loss 5.425\n",
      "Ep 1 (Step 000260): Train loss 5.302, Val loss 5.395\n",
      "Ep 1 (Step 000270): Train loss 5.277, Val loss 5.382\n",
      "Ep 1 (Step 000280): Train loss 5.233, Val loss 5.365\n",
      "Ep 1 (Step 000290): Train loss 5.229, Val loss 5.353\n",
      "Ep 1 (Step 000300): Train loss 5.212, Val loss 5.342\n",
      "Ep 1 (Step 000310): Train loss 5.221, Val loss 5.338\n",
      "Ep 1 (Step 000320): Train loss 5.300, Val loss 5.330\n",
      "Ep 1 (Step 000330): Train loss 5.208, Val loss 5.329\n",
      "Ep 1 (Step 000340): Train loss 5.106, Val loss 5.303\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3031\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.031, Val loss 9.034\n",
      "Ep 1 (Step 000010): Train loss 7.473, Val loss 7.432\n",
      "Ep 1 (Step 000020): Train loss 6.830, Val loss 6.779\n",
      "Ep 1 (Step 000030): Train loss 6.481, Val loss 6.481\n",
      "Ep 1 (Step 000040): Train loss 6.373, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.263, Val loss 6.336\n",
      "Ep 1 (Step 000060): Train loss 6.248, Val loss 6.238\n",
      "Ep 1 (Step 000070): Train loss 6.148, Val loss 6.166\n",
      "Ep 1 (Step 000080): Train loss 5.999, Val loss 6.027\n",
      "Ep 1 (Step 000090): Train loss 5.889, Val loss 5.959\n",
      "Ep 1 (Step 000100): Train loss 5.839, Val loss 5.879\n",
      "Ep 1 (Step 000110): Train loss 5.767, Val loss 5.834\n",
      "Ep 1 (Step 000120): Train loss 5.780, Val loss 5.789\n",
      "Ep 1 (Step 000130): Train loss 5.721, Val loss 5.751\n",
      "Ep 1 (Step 000140): Train loss 5.677, Val loss 5.715\n",
      "Ep 1 (Step 000150): Train loss 5.617, Val loss 5.678\n",
      "Ep 1 (Step 000160): Train loss 5.516, Val loss 5.639\n",
      "Ep 1 (Step 000170): Train loss 5.525, Val loss 5.605\n",
      "Ep 1 (Step 000180): Train loss 5.407, Val loss 5.576\n",
      "Ep 1 (Step 000190): Train loss 5.453, Val loss 5.557\n",
      "Ep 1 (Step 000200): Train loss 5.525, Val loss 5.532\n",
      "Ep 1 (Step 000210): Train loss 5.392, Val loss 5.501\n",
      "Ep 1 (Step 000220): Train loss 5.282, Val loss 5.494\n",
      "Ep 1 (Step 000230): Train loss 5.360, Val loss 5.471\n",
      "Ep 1 (Step 000240): Train loss 5.401, Val loss 5.445\n",
      "Ep 1 (Step 000250): Train loss 5.382, Val loss 5.425\n",
      "Ep 1 (Step 000260): Train loss 5.368, Val loss 5.409\n",
      "Ep 1 (Step 000270): Train loss 5.401, Val loss 5.389\n",
      "Ep 1 (Step 000280): Train loss 5.249, Val loss 5.374\n",
      "Ep 1 (Step 000290): Train loss 5.303, Val loss 5.360\n",
      "Ep 1 (Step 000300): Train loss 5.273, Val loss 5.345\n",
      "Ep 1 (Step 000310): Train loss 5.253, Val loss 5.340\n",
      "Ep 1 (Step 000320): Train loss 5.307, Val loss 5.335\n",
      "Ep 1 (Step 000330): Train loss 5.203, Val loss 5.315\n",
      "Ep 1 (Step 000340): Train loss 5.212, Val loss 5.319\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3192\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.043, Val loss 9.035\n",
      "Ep 1 (Step 000010): Train loss 7.490, Val loss 7.447\n",
      "Ep 1 (Step 000020): Train loss 6.800, Val loss 6.798\n",
      "Ep 1 (Step 000030): Train loss 6.513, Val loss 6.483\n",
      "Ep 1 (Step 000040): Train loss 6.379, Val loss 6.388\n",
      "Ep 1 (Step 000050): Train loss 6.332, Val loss 6.321\n",
      "Ep 1 (Step 000060): Train loss 6.261, Val loss 6.233\n",
      "Ep 1 (Step 000070): Train loss 6.085, Val loss 6.117\n",
      "Ep 1 (Step 000080): Train loss 6.031, Val loss 6.039\n",
      "Ep 1 (Step 000090): Train loss 5.916, Val loss 5.961\n",
      "Ep 1 (Step 000100): Train loss 5.958, Val loss 5.903\n",
      "Ep 1 (Step 000110): Train loss 5.743, Val loss 5.835\n",
      "Ep 1 (Step 000120): Train loss 5.729, Val loss 5.793\n",
      "Ep 1 (Step 000130): Train loss 5.742, Val loss 5.761\n",
      "Ep 1 (Step 000140): Train loss 5.714, Val loss 5.716\n",
      "Ep 1 (Step 000150): Train loss 5.590, Val loss 5.685\n",
      "Ep 1 (Step 000160): Train loss 5.525, Val loss 5.645\n",
      "Ep 1 (Step 000170): Train loss 5.577, Val loss 5.621\n",
      "Ep 1 (Step 000180): Train loss 5.484, Val loss 5.584\n",
      "Ep 1 (Step 000190): Train loss 5.395, Val loss 5.559\n",
      "Ep 1 (Step 000200): Train loss 5.408, Val loss 5.525\n",
      "Ep 1 (Step 000210): Train loss 5.382, Val loss 5.497\n",
      "Ep 1 (Step 000220): Train loss 5.402, Val loss 5.481\n",
      "Ep 1 (Step 000230): Train loss 5.434, Val loss 5.468\n",
      "Ep 1 (Step 000240): Train loss 5.397, Val loss 5.444\n",
      "Ep 1 (Step 000250): Train loss 5.245, Val loss 5.429\n",
      "Ep 1 (Step 000260): Train loss 5.381, Val loss 5.406\n",
      "Ep 1 (Step 000270): Train loss 5.290, Val loss 5.383\n",
      "Ep 1 (Step 000280): Train loss 5.274, Val loss 5.375\n",
      "Ep 1 (Step 000290): Train loss 5.180, Val loss 5.373\n",
      "Ep 1 (Step 000300): Train loss 5.267, Val loss 5.353\n",
      "Ep 1 (Step 000310): Train loss 5.255, Val loss 5.335\n",
      "Ep 1 (Step 000320): Train loss 5.149, Val loss 5.342\n",
      "Ep 1 (Step 000330): Train loss 5.253, Val loss 5.320\n",
      "Ep 1 (Step 000340): Train loss 5.146, Val loss 5.300\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2999\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.824, Val loss 8.791\n",
      "Ep 1 (Step 000010): Train loss 6.869, Val loss 6.873\n",
      "Ep 1 (Step 000020): Train loss 6.460, Val loss 6.452\n",
      "Ep 1 (Step 000030): Train loss 6.406, Val loss 6.411\n",
      "Ep 1 (Step 000040): Train loss 6.193, Val loss 6.281\n",
      "Ep 1 (Step 000050): Train loss 6.139, Val loss 6.162\n",
      "Ep 1 (Step 000060): Train loss 5.980, Val loss 6.000\n",
      "Ep 1 (Step 000070): Train loss 5.887, Val loss 5.896\n",
      "Ep 1 (Step 000080): Train loss 5.715, Val loss 5.825\n",
      "Ep 1 (Step 000090): Train loss 5.736, Val loss 5.750\n",
      "Ep 1 (Step 000100): Train loss 5.618, Val loss 5.694\n",
      "Ep 1 (Step 000110): Train loss 5.527, Val loss 5.629\n",
      "Ep 1 (Step 000120): Train loss 5.608, Val loss 5.588\n",
      "Ep 1 (Step 000130): Train loss 5.520, Val loss 5.543\n",
      "Ep 1 (Step 000140): Train loss 5.433, Val loss 5.532\n",
      "Ep 1 (Step 000150): Train loss 5.448, Val loss 5.496\n",
      "Ep 1 (Step 000160): Train loss 5.430, Val loss 5.497\n",
      "Ep 1 (Step 000170): Train loss 5.289, Val loss 5.456\n",
      "Ep 1 (Step 000180): Train loss 5.291, Val loss 5.440\n",
      "Ep 1 (Step 000190): Train loss 5.325, Val loss 5.406\n",
      "Ep 1 (Step 000200): Train loss 5.265, Val loss 5.395\n",
      "Ep 1 (Step 000210): Train loss 5.193, Val loss 5.370\n",
      "Ep 1 (Step 000220): Train loss 5.232, Val loss 5.364\n",
      "Ep 1 (Step 000230): Train loss 5.187, Val loss 5.344\n",
      "Ep 1 (Step 000240): Train loss 5.149, Val loss 5.329\n",
      "Ep 1 (Step 000250): Train loss 5.211, Val loss 5.317\n",
      "Ep 1 (Step 000260): Train loss 5.172, Val loss 5.303\n",
      "Ep 1 (Step 000270): Train loss 5.181, Val loss 5.290\n",
      "Ep 1 (Step 000280): Train loss 5.082, Val loss 5.298\n",
      "Ep 1 (Step 000290): Train loss 5.109, Val loss 5.272\n",
      "Ep 1 (Step 000300): Train loss 5.215, Val loss 5.259\n",
      "Ep 1 (Step 000310): Train loss 5.136, Val loss 5.258\n",
      "Ep 1 (Step 000320): Train loss 5.107, Val loss 5.272\n",
      "Ep 1 (Step 000330): Train loss 5.097, Val loss 5.238\n",
      "Ep 1 (Step 000340): Train loss 5.148, Val loss 5.237\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2368\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.797, Val loss 8.786\n",
      "Ep 1 (Step 000010): Train loss 6.859, Val loss 6.864\n",
      "Ep 1 (Step 000020): Train loss 6.513, Val loss 6.476\n",
      "Ep 1 (Step 000030): Train loss 6.417, Val loss 6.443\n",
      "Ep 1 (Step 000040): Train loss 6.194, Val loss 6.285\n",
      "Ep 1 (Step 000050): Train loss 6.062, Val loss 6.141\n",
      "Ep 1 (Step 000060): Train loss 5.986, Val loss 6.020\n",
      "Ep 1 (Step 000070): Train loss 5.850, Val loss 5.917\n",
      "Ep 1 (Step 000080): Train loss 5.786, Val loss 5.840\n",
      "Ep 1 (Step 000090): Train loss 5.744, Val loss 5.775\n",
      "Ep 1 (Step 000100): Train loss 5.688, Val loss 5.705\n",
      "Ep 1 (Step 000110): Train loss 5.576, Val loss 5.629\n",
      "Ep 1 (Step 000120): Train loss 5.596, Val loss 5.610\n",
      "Ep 1 (Step 000130): Train loss 5.459, Val loss 5.550\n",
      "Ep 1 (Step 000140): Train loss 5.434, Val loss 5.516\n",
      "Ep 1 (Step 000150): Train loss 5.370, Val loss 5.500\n",
      "Ep 1 (Step 000160): Train loss 5.380, Val loss 5.486\n",
      "Ep 1 (Step 000170): Train loss 5.360, Val loss 5.456\n",
      "Ep 1 (Step 000180): Train loss 5.315, Val loss 5.444\n",
      "Ep 1 (Step 000190): Train loss 5.296, Val loss 5.423\n",
      "Ep 1 (Step 000200): Train loss 5.351, Val loss 5.392\n",
      "Ep 1 (Step 000210): Train loss 5.209, Val loss 5.374\n",
      "Ep 1 (Step 000220): Train loss 5.284, Val loss 5.377\n",
      "Ep 1 (Step 000230): Train loss 5.249, Val loss 5.362\n",
      "Ep 1 (Step 000240): Train loss 5.161, Val loss 5.361\n",
      "Ep 1 (Step 000250): Train loss 5.228, Val loss 5.339\n",
      "Ep 1 (Step 000260): Train loss 5.260, Val loss 5.330\n",
      "Ep 1 (Step 000270): Train loss 5.234, Val loss 5.308\n",
      "Ep 1 (Step 000280): Train loss 5.108, Val loss 5.283\n",
      "Ep 1 (Step 000290): Train loss 5.061, Val loss 5.272\n",
      "Ep 1 (Step 000300): Train loss 5.217, Val loss 5.268\n",
      "Ep 1 (Step 000310): Train loss 5.143, Val loss 5.260\n",
      "Ep 1 (Step 000320): Train loss 5.059, Val loss 5.250\n",
      "Ep 1 (Step 000330): Train loss 5.131, Val loss 5.224\n",
      "Ep 1 (Step 000340): Train loss 5.049, Val loss 5.230\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2299\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.802, Val loss 8.800\n",
      "Ep 1 (Step 000010): Train loss 6.876, Val loss 6.824\n",
      "Ep 1 (Step 000020): Train loss 6.443, Val loss 6.432\n",
      "Ep 1 (Step 000030): Train loss 6.357, Val loss 6.382\n",
      "Ep 1 (Step 000040): Train loss 6.260, Val loss 6.228\n",
      "Ep 1 (Step 000050): Train loss 6.102, Val loss 6.079\n",
      "Ep 1 (Step 000060): Train loss 5.950, Val loss 5.967\n",
      "Ep 1 (Step 000070): Train loss 5.768, Val loss 5.884\n",
      "Ep 1 (Step 000080): Train loss 5.776, Val loss 5.812\n",
      "Ep 1 (Step 000090): Train loss 5.693, Val loss 5.737\n",
      "Ep 1 (Step 000100): Train loss 5.502, Val loss 5.678\n",
      "Ep 1 (Step 000110): Train loss 5.532, Val loss 5.624\n",
      "Ep 1 (Step 000120): Train loss 5.593, Val loss 5.592\n",
      "Ep 1 (Step 000130): Train loss 5.415, Val loss 5.547\n",
      "Ep 1 (Step 000140): Train loss 5.417, Val loss 5.512\n",
      "Ep 1 (Step 000150): Train loss 5.405, Val loss 5.507\n",
      "Ep 1 (Step 000160): Train loss 5.365, Val loss 5.471\n",
      "Ep 1 (Step 000170): Train loss 5.353, Val loss 5.452\n",
      "Ep 1 (Step 000180): Train loss 5.342, Val loss 5.439\n",
      "Ep 1 (Step 000190): Train loss 5.329, Val loss 5.412\n",
      "Ep 1 (Step 000200): Train loss 5.290, Val loss 5.394\n",
      "Ep 1 (Step 000210): Train loss 5.310, Val loss 5.387\n",
      "Ep 1 (Step 000220): Train loss 5.251, Val loss 5.356\n",
      "Ep 1 (Step 000230): Train loss 5.215, Val loss 5.340\n",
      "Ep 1 (Step 000240): Train loss 5.240, Val loss 5.325\n",
      "Ep 1 (Step 000250): Train loss 5.191, Val loss 5.321\n",
      "Ep 1 (Step 000260): Train loss 5.105, Val loss 5.310\n",
      "Ep 1 (Step 000270): Train loss 5.189, Val loss 5.296\n",
      "Ep 1 (Step 000280): Train loss 5.185, Val loss 5.293\n",
      "Ep 1 (Step 000290): Train loss 5.132, Val loss 5.279\n",
      "Ep 1 (Step 000300): Train loss 5.103, Val loss 5.272\n",
      "Ep 1 (Step 000310): Train loss 5.193, Val loss 5.276\n",
      "Ep 1 (Step 000320): Train loss 5.053, Val loss 5.243\n",
      "Ep 1 (Step 000330): Train loss 5.161, Val loss 5.237\n",
      "Ep 1 (Step 000340): Train loss 5.110, Val loss 5.230\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2300\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.799, Val loss 8.799\n",
      "Ep 1 (Step 000010): Train loss 6.873, Val loss 6.852\n",
      "Ep 1 (Step 000020): Train loss 6.448, Val loss 6.464\n",
      "Ep 1 (Step 000030): Train loss 6.392, Val loss 6.427\n",
      "Ep 1 (Step 000040): Train loss 6.274, Val loss 6.275\n",
      "Ep 1 (Step 000050): Train loss 6.086, Val loss 6.116\n",
      "Ep 1 (Step 000060): Train loss 5.897, Val loss 5.969\n",
      "Ep 1 (Step 000070): Train loss 5.934, Val loss 5.868\n",
      "Ep 1 (Step 000080): Train loss 5.740, Val loss 5.804\n",
      "Ep 1 (Step 000090): Train loss 5.570, Val loss 5.722\n",
      "Ep 1 (Step 000100): Train loss 5.598, Val loss 5.690\n",
      "Ep 1 (Step 000110): Train loss 5.597, Val loss 5.631\n",
      "Ep 1 (Step 000120): Train loss 5.548, Val loss 5.585\n",
      "Ep 1 (Step 000130): Train loss 5.475, Val loss 5.563\n",
      "Ep 1 (Step 000140): Train loss 5.465, Val loss 5.520\n",
      "Ep 1 (Step 000150): Train loss 5.458, Val loss 5.512\n",
      "Ep 1 (Step 000160): Train loss 5.365, Val loss 5.461\n",
      "Ep 1 (Step 000170): Train loss 5.299, Val loss 5.435\n",
      "Ep 1 (Step 000180): Train loss 5.344, Val loss 5.431\n",
      "Ep 1 (Step 000190): Train loss 5.266, Val loss 5.396\n",
      "Ep 1 (Step 000200): Train loss 5.167, Val loss 5.362\n",
      "Ep 1 (Step 000210): Train loss 5.291, Val loss 5.346\n",
      "Ep 1 (Step 000220): Train loss 5.223, Val loss 5.333\n",
      "Ep 1 (Step 000230): Train loss 5.282, Val loss 5.322\n",
      "Ep 1 (Step 000240): Train loss 5.207, Val loss 5.305\n",
      "Ep 1 (Step 000250): Train loss 5.158, Val loss 5.302\n",
      "Ep 1 (Step 000260): Train loss 5.151, Val loss 5.287\n",
      "Ep 1 (Step 000270): Train loss 5.209, Val loss 5.263\n",
      "Ep 1 (Step 000280): Train loss 5.193, Val loss 5.240\n",
      "Ep 1 (Step 000290): Train loss 5.087, Val loss 5.258\n",
      "Ep 1 (Step 000300): Train loss 5.057, Val loss 5.246\n",
      "Ep 1 (Step 000310): Train loss 5.016, Val loss 5.235\n",
      "Ep 1 (Step 000320): Train loss 5.064, Val loss 5.192\n",
      "Ep 1 (Step 000330): Train loss 5.157, Val loss 5.173\n",
      "Ep 1 (Step 000340): Train loss 5.062, Val loss 5.175\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1751\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.780, Val loss 8.769\n",
      "Ep 1 (Step 000010): Train loss 6.880, Val loss 6.850\n",
      "Ep 1 (Step 000020): Train loss 6.439, Val loss 6.461\n",
      "Ep 1 (Step 000030): Train loss 6.390, Val loss 6.418\n",
      "Ep 1 (Step 000040): Train loss 6.264, Val loss 6.234\n",
      "Ep 1 (Step 000050): Train loss 6.075, Val loss 6.098\n",
      "Ep 1 (Step 000060): Train loss 5.938, Val loss 5.989\n",
      "Ep 1 (Step 000070): Train loss 5.886, Val loss 5.877\n",
      "Ep 1 (Step 000080): Train loss 5.834, Val loss 5.807\n",
      "Ep 1 (Step 000090): Train loss 5.663, Val loss 5.720\n",
      "Ep 1 (Step 000100): Train loss 5.604, Val loss 5.689\n",
      "Ep 1 (Step 000110): Train loss 5.523, Val loss 5.655\n",
      "Ep 1 (Step 000120): Train loss 5.470, Val loss 5.594\n",
      "Ep 1 (Step 000130): Train loss 5.475, Val loss 5.531\n",
      "Ep 1 (Step 000140): Train loss 5.438, Val loss 5.497\n",
      "Ep 1 (Step 000150): Train loss 5.388, Val loss 5.468\n",
      "Ep 1 (Step 000160): Train loss 5.415, Val loss 5.442\n",
      "Ep 1 (Step 000170): Train loss 5.420, Val loss 5.422\n",
      "Ep 1 (Step 000180): Train loss 5.358, Val loss 5.407\n",
      "Ep 1 (Step 000190): Train loss 5.264, Val loss 5.376\n",
      "Ep 1 (Step 000200): Train loss 5.314, Val loss 5.358\n",
      "Ep 1 (Step 000210): Train loss 5.192, Val loss 5.338\n",
      "Ep 1 (Step 000220): Train loss 5.242, Val loss 5.350\n",
      "Ep 1 (Step 000230): Train loss 5.131, Val loss 5.333\n",
      "Ep 1 (Step 000240): Train loss 5.163, Val loss 5.325\n",
      "Ep 1 (Step 000250): Train loss 5.176, Val loss 5.303\n",
      "Ep 1 (Step 000260): Train loss 5.141, Val loss 5.288\n",
      "Ep 1 (Step 000270): Train loss 5.199, Val loss 5.279\n",
      "Ep 1 (Step 000280): Train loss 5.094, Val loss 5.262\n",
      "Ep 1 (Step 000290): Train loss 5.111, Val loss 5.259\n",
      "Ep 1 (Step 000300): Train loss 5.070, Val loss 5.246\n",
      "Ep 1 (Step 000310): Train loss 5.085, Val loss 5.229\n",
      "Ep 1 (Step 000320): Train loss 4.984, Val loss 5.218\n",
      "Ep 1 (Step 000330): Train loss 5.158, Val loss 5.210\n",
      "Ep 1 (Step 000340): Train loss 5.002, Val loss 5.193\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1933\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.863, Val loss 8.856\n",
      "Ep 1 (Step 000010): Train loss 6.837, Val loss 6.864\n",
      "Ep 1 (Step 000020): Train loss 6.518, Val loss 6.451\n",
      "Ep 1 (Step 000030): Train loss 6.459, Val loss 6.422\n",
      "Ep 1 (Step 000040): Train loss 6.244, Val loss 6.246\n",
      "Ep 1 (Step 000050): Train loss 6.188, Val loss 6.146\n",
      "Ep 1 (Step 000060): Train loss 5.995, Val loss 6.027\n",
      "Ep 1 (Step 000070): Train loss 5.943, Val loss 5.902\n",
      "Ep 1 (Step 000080): Train loss 5.834, Val loss 5.822\n",
      "Ep 1 (Step 000090): Train loss 5.654, Val loss 5.755\n",
      "Ep 1 (Step 000100): Train loss 5.603, Val loss 5.692\n",
      "Ep 1 (Step 000110): Train loss 5.482, Val loss 5.635\n",
      "Ep 1 (Step 000120): Train loss 5.539, Val loss 5.574\n",
      "Ep 1 (Step 000130): Train loss 5.520, Val loss 5.548\n",
      "Ep 1 (Step 000140): Train loss 5.482, Val loss 5.509\n",
      "Ep 1 (Step 000150): Train loss 5.403, Val loss 5.484\n",
      "Ep 1 (Step 000160): Train loss 5.372, Val loss 5.454\n",
      "Ep 1 (Step 000170): Train loss 5.408, Val loss 5.441\n",
      "Ep 1 (Step 000180): Train loss 5.332, Val loss 5.420\n",
      "Ep 1 (Step 000190): Train loss 5.291, Val loss 5.408\n",
      "Ep 1 (Step 000200): Train loss 5.315, Val loss 5.378\n",
      "Ep 1 (Step 000210): Train loss 5.298, Val loss 5.361\n",
      "Ep 1 (Step 000220): Train loss 5.227, Val loss 5.351\n",
      "Ep 1 (Step 000230): Train loss 5.203, Val loss 5.341\n",
      "Ep 1 (Step 000240): Train loss 5.150, Val loss 5.314\n",
      "Ep 1 (Step 000250): Train loss 5.079, Val loss 5.305\n",
      "Ep 1 (Step 000260): Train loss 5.173, Val loss 5.275\n",
      "Ep 1 (Step 000270): Train loss 5.161, Val loss 5.260\n",
      "Ep 1 (Step 000280): Train loss 5.132, Val loss 5.266\n",
      "Ep 1 (Step 000290): Train loss 5.166, Val loss 5.244\n",
      "Ep 1 (Step 000300): Train loss 5.081, Val loss 5.227\n",
      "Ep 1 (Step 000310): Train loss 5.079, Val loss 5.219\n",
      "Ep 1 (Step 000320): Train loss 5.058, Val loss 5.218\n",
      "Ep 1 (Step 000330): Train loss 5.084, Val loss 5.217\n",
      "Ep 1 (Step 000340): Train loss 5.066, Val loss 5.193\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1932\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.898, Val loss 8.880\n",
      "Ep 1 (Step 000010): Train loss 7.376, Val loss 7.373\n",
      "Ep 1 (Step 000020): Train loss 6.783, Val loss 6.742\n",
      "Ep 1 (Step 000030): Train loss 6.481, Val loss 6.463\n",
      "Ep 1 (Step 000040): Train loss 6.365, Val loss 6.373\n",
      "Ep 1 (Step 000050): Train loss 6.358, Val loss 6.334\n",
      "Ep 1 (Step 000060): Train loss 6.235, Val loss 6.236\n",
      "Ep 1 (Step 000070): Train loss 6.156, Val loss 6.147\n",
      "Ep 1 (Step 000080): Train loss 6.042, Val loss 6.052\n",
      "Ep 1 (Step 000090): Train loss 5.986, Val loss 5.990\n",
      "Ep 1 (Step 000100): Train loss 5.858, Val loss 5.913\n",
      "Ep 1 (Step 000110): Train loss 5.824, Val loss 5.864\n",
      "Ep 1 (Step 000120): Train loss 5.790, Val loss 5.814\n",
      "Ep 1 (Step 000130): Train loss 5.739, Val loss 5.794\n",
      "Ep 1 (Step 000140): Train loss 5.733, Val loss 5.758\n",
      "Ep 1 (Step 000150): Train loss 5.672, Val loss 5.712\n",
      "Ep 1 (Step 000160): Train loss 5.587, Val loss 5.661\n",
      "Ep 1 (Step 000170): Train loss 5.558, Val loss 5.643\n",
      "Ep 1 (Step 000180): Train loss 5.539, Val loss 5.611\n",
      "Ep 1 (Step 000190): Train loss 5.522, Val loss 5.581\n",
      "Ep 1 (Step 000200): Train loss 5.465, Val loss 5.576\n",
      "Ep 1 (Step 000210): Train loss 5.474, Val loss 5.539\n",
      "Ep 1 (Step 000220): Train loss 5.412, Val loss 5.525\n",
      "Ep 1 (Step 000230): Train loss 5.431, Val loss 5.512\n",
      "Ep 1 (Step 000240): Train loss 5.378, Val loss 5.488\n",
      "Ep 1 (Step 000250): Train loss 5.481, Val loss 5.483\n",
      "Ep 1 (Step 000260): Train loss 5.403, Val loss 5.445\n",
      "Ep 1 (Step 000270): Train loss 5.263, Val loss 5.438\n",
      "Ep 1 (Step 000280): Train loss 5.307, Val loss 5.422\n",
      "Ep 1 (Step 000290): Train loss 5.208, Val loss 5.415\n",
      "Ep 1 (Step 000300): Train loss 5.299, Val loss 5.400\n",
      "Ep 1 (Step 000310): Train loss 5.301, Val loss 5.400\n",
      "Ep 1 (Step 000320): Train loss 5.281, Val loss 5.381\n",
      "Ep 1 (Step 000330): Train loss 5.248, Val loss 5.383\n",
      "Ep 1 (Step 000340): Train loss 5.296, Val loss 5.358\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3583\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.939, Val loss 8.916\n",
      "Ep 1 (Step 000010): Train loss 7.429, Val loss 7.387\n",
      "Ep 1 (Step 000020): Train loss 6.783, Val loss 6.765\n",
      "Ep 1 (Step 000030): Train loss 6.511, Val loss 6.457\n",
      "Ep 1 (Step 000040): Train loss 6.366, Val loss 6.374\n",
      "Ep 1 (Step 000050): Train loss 6.334, Val loss 6.333\n",
      "Ep 1 (Step 000060): Train loss 6.299, Val loss 6.238\n",
      "Ep 1 (Step 000070): Train loss 6.158, Val loss 6.142\n",
      "Ep 1 (Step 000080): Train loss 6.047, Val loss 6.033\n",
      "Ep 1 (Step 000090): Train loss 5.925, Val loss 5.959\n",
      "Ep 1 (Step 000100): Train loss 5.866, Val loss 5.908\n",
      "Ep 1 (Step 000110): Train loss 5.836, Val loss 5.865\n",
      "Ep 1 (Step 000120): Train loss 5.736, Val loss 5.806\n",
      "Ep 1 (Step 000130): Train loss 5.727, Val loss 5.769\n",
      "Ep 1 (Step 000140): Train loss 5.737, Val loss 5.726\n",
      "Ep 1 (Step 000150): Train loss 5.651, Val loss 5.703\n",
      "Ep 1 (Step 000160): Train loss 5.611, Val loss 5.656\n",
      "Ep 1 (Step 000170): Train loss 5.597, Val loss 5.619\n",
      "Ep 1 (Step 000180): Train loss 5.461, Val loss 5.591\n",
      "Ep 1 (Step 000190): Train loss 5.526, Val loss 5.579\n",
      "Ep 1 (Step 000200): Train loss 5.540, Val loss 5.552\n",
      "Ep 1 (Step 000210): Train loss 5.489, Val loss 5.557\n",
      "Ep 1 (Step 000220): Train loss 5.448, Val loss 5.529\n",
      "Ep 1 (Step 000230): Train loss 5.356, Val loss 5.511\n",
      "Ep 1 (Step 000240): Train loss 5.403, Val loss 5.490\n",
      "Ep 1 (Step 000250): Train loss 5.386, Val loss 5.465\n",
      "Ep 1 (Step 000260): Train loss 5.340, Val loss 5.455\n",
      "Ep 1 (Step 000270): Train loss 5.266, Val loss 5.455\n",
      "Ep 1 (Step 000280): Train loss 5.373, Val loss 5.434\n",
      "Ep 1 (Step 000290): Train loss 5.332, Val loss 5.454\n",
      "Ep 1 (Step 000300): Train loss 5.337, Val loss 5.402\n",
      "Ep 1 (Step 000310): Train loss 5.300, Val loss 5.380\n",
      "Ep 1 (Step 000320): Train loss 5.252, Val loss 5.379\n",
      "Ep 1 (Step 000330): Train loss 5.274, Val loss 5.365\n",
      "Ep 1 (Step 000340): Train loss 5.262, Val loss 5.354\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3536\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.977, Val loss 8.940\n",
      "Ep 1 (Step 000010): Train loss 7.423, Val loss 7.427\n",
      "Ep 1 (Step 000020): Train loss 6.782, Val loss 6.765\n",
      "Ep 1 (Step 000030): Train loss 6.478, Val loss 6.456\n",
      "Ep 1 (Step 000040): Train loss 6.348, Val loss 6.375\n",
      "Ep 1 (Step 000050): Train loss 6.382, Val loss 6.341\n",
      "Ep 1 (Step 000060): Train loss 6.197, Val loss 6.216\n",
      "Ep 1 (Step 000070): Train loss 6.074, Val loss 6.139\n",
      "Ep 1 (Step 000080): Train loss 5.945, Val loss 6.036\n",
      "Ep 1 (Step 000090): Train loss 5.908, Val loss 5.969\n",
      "Ep 1 (Step 000100): Train loss 5.913, Val loss 5.897\n",
      "Ep 1 (Step 000110): Train loss 5.818, Val loss 5.851\n",
      "Ep 1 (Step 000120): Train loss 5.730, Val loss 5.812\n",
      "Ep 1 (Step 000130): Train loss 5.715, Val loss 5.792\n",
      "Ep 1 (Step 000140): Train loss 5.633, Val loss 5.743\n",
      "Ep 1 (Step 000150): Train loss 5.658, Val loss 5.711\n",
      "Ep 1 (Step 000160): Train loss 5.586, Val loss 5.679\n",
      "Ep 1 (Step 000170): Train loss 5.567, Val loss 5.653\n",
      "Ep 1 (Step 000180): Train loss 5.553, Val loss 5.610\n",
      "Ep 1 (Step 000190): Train loss 5.501, Val loss 5.583\n",
      "Ep 1 (Step 000200): Train loss 5.467, Val loss 5.558\n",
      "Ep 1 (Step 000210): Train loss 5.500, Val loss 5.539\n",
      "Ep 1 (Step 000220): Train loss 5.393, Val loss 5.526\n",
      "Ep 1 (Step 000230): Train loss 5.364, Val loss 5.494\n",
      "Ep 1 (Step 000240): Train loss 5.453, Val loss 5.518\n",
      "Ep 1 (Step 000250): Train loss 5.341, Val loss 5.475\n",
      "Ep 1 (Step 000260): Train loss 5.341, Val loss 5.464\n",
      "Ep 1 (Step 000270): Train loss 5.355, Val loss 5.453\n",
      "Ep 1 (Step 000280): Train loss 5.270, Val loss 5.443\n",
      "Ep 1 (Step 000290): Train loss 5.342, Val loss 5.419\n",
      "Ep 1 (Step 000300): Train loss 5.143, Val loss 5.416\n",
      "Ep 1 (Step 000310): Train loss 5.289, Val loss 5.405\n",
      "Ep 1 (Step 000320): Train loss 5.315, Val loss 5.393\n",
      "Ep 1 (Step 000330): Train loss 5.236, Val loss 5.376\n",
      "Ep 1 (Step 000340): Train loss 5.229, Val loss 5.364\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3640\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.868, Val loss 8.876\n",
      "Ep 1 (Step 000010): Train loss 7.433, Val loss 7.372\n",
      "Ep 1 (Step 000020): Train loss 6.843, Val loss 6.748\n",
      "Ep 1 (Step 000030): Train loss 6.442, Val loss 6.445\n",
      "Ep 1 (Step 000040): Train loss 6.396, Val loss 6.366\n",
      "Ep 1 (Step 000050): Train loss 6.305, Val loss 6.316\n",
      "Ep 1 (Step 000060): Train loss 6.197, Val loss 6.229\n",
      "Ep 1 (Step 000070): Train loss 6.129, Val loss 6.129\n",
      "Ep 1 (Step 000080): Train loss 6.146, Val loss 6.040\n",
      "Ep 1 (Step 000090): Train loss 6.009, Val loss 5.966\n",
      "Ep 1 (Step 000100): Train loss 5.896, Val loss 5.919\n",
      "Ep 1 (Step 000110): Train loss 5.849, Val loss 5.853\n",
      "Ep 1 (Step 000120): Train loss 5.738, Val loss 5.791\n",
      "Ep 1 (Step 000130): Train loss 5.699, Val loss 5.765\n",
      "Ep 1 (Step 000140): Train loss 5.652, Val loss 5.735\n",
      "Ep 1 (Step 000150): Train loss 5.683, Val loss 5.694\n",
      "Ep 1 (Step 000160): Train loss 5.672, Val loss 5.655\n",
      "Ep 1 (Step 000170): Train loss 5.496, Val loss 5.629\n",
      "Ep 1 (Step 000180): Train loss 5.501, Val loss 5.597\n",
      "Ep 1 (Step 000190): Train loss 5.461, Val loss 5.571\n",
      "Ep 1 (Step 000200): Train loss 5.442, Val loss 5.545\n",
      "Ep 1 (Step 000210): Train loss 5.429, Val loss 5.520\n",
      "Ep 1 (Step 000220): Train loss 5.316, Val loss 5.498\n",
      "Ep 1 (Step 000230): Train loss 5.355, Val loss 5.479\n",
      "Ep 1 (Step 000240): Train loss 5.458, Val loss 5.471\n",
      "Ep 1 (Step 000250): Train loss 5.273, Val loss 5.446\n",
      "Ep 1 (Step 000260): Train loss 5.408, Val loss 5.440\n",
      "Ep 1 (Step 000270): Train loss 5.334, Val loss 5.415\n",
      "Ep 1 (Step 000280): Train loss 5.331, Val loss 5.416\n",
      "Ep 1 (Step 000290): Train loss 5.310, Val loss 5.394\n",
      "Ep 1 (Step 000300): Train loss 5.255, Val loss 5.380\n",
      "Ep 1 (Step 000310): Train loss 5.292, Val loss 5.360\n",
      "Ep 1 (Step 000320): Train loss 5.268, Val loss 5.348\n",
      "Ep 1 (Step 000330): Train loss 5.224, Val loss 5.345\n",
      "Ep 1 (Step 000340): Train loss 5.189, Val loss 5.332\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3322\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.986, Val loss 8.970\n",
      "Ep 1 (Step 000010): Train loss 7.463, Val loss 7.450\n",
      "Ep 1 (Step 000020): Train loss 6.787, Val loss 6.773\n",
      "Ep 1 (Step 000030): Train loss 6.424, Val loss 6.456\n",
      "Ep 1 (Step 000040): Train loss 6.394, Val loss 6.379\n",
      "Ep 1 (Step 000050): Train loss 6.316, Val loss 6.328\n",
      "Ep 1 (Step 000060): Train loss 6.196, Val loss 6.244\n",
      "Ep 1 (Step 000070): Train loss 6.070, Val loss 6.131\n",
      "Ep 1 (Step 000080): Train loss 6.043, Val loss 6.046\n",
      "Ep 1 (Step 000090): Train loss 5.998, Val loss 5.979\n",
      "Ep 1 (Step 000100): Train loss 5.914, Val loss 5.923\n",
      "Ep 1 (Step 000110): Train loss 5.874, Val loss 5.861\n",
      "Ep 1 (Step 000120): Train loss 5.808, Val loss 5.816\n",
      "Ep 1 (Step 000130): Train loss 5.767, Val loss 5.776\n",
      "Ep 1 (Step 000140): Train loss 5.733, Val loss 5.738\n",
      "Ep 1 (Step 000150): Train loss 5.647, Val loss 5.700\n",
      "Ep 1 (Step 000160): Train loss 5.651, Val loss 5.663\n",
      "Ep 1 (Step 000170): Train loss 5.551, Val loss 5.640\n",
      "Ep 1 (Step 000180): Train loss 5.628, Val loss 5.609\n",
      "Ep 1 (Step 000190): Train loss 5.548, Val loss 5.582\n",
      "Ep 1 (Step 000200): Train loss 5.452, Val loss 5.552\n",
      "Ep 1 (Step 000210): Train loss 5.460, Val loss 5.534\n",
      "Ep 1 (Step 000220): Train loss 5.389, Val loss 5.510\n",
      "Ep 1 (Step 000230): Train loss 5.375, Val loss 5.487\n",
      "Ep 1 (Step 000240): Train loss 5.345, Val loss 5.466\n",
      "Ep 1 (Step 000250): Train loss 5.322, Val loss 5.459\n",
      "Ep 1 (Step 000260): Train loss 5.383, Val loss 5.441\n",
      "Ep 1 (Step 000270): Train loss 5.361, Val loss 5.415\n",
      "Ep 1 (Step 000280): Train loss 5.287, Val loss 5.410\n",
      "Ep 1 (Step 000290): Train loss 5.267, Val loss 5.392\n",
      "Ep 1 (Step 000300): Train loss 5.338, Val loss 5.380\n",
      "Ep 1 (Step 000310): Train loss 5.283, Val loss 5.360\n",
      "Ep 1 (Step 000320): Train loss 5.273, Val loss 5.357\n",
      "Ep 1 (Step 000330): Train loss 5.250, Val loss 5.352\n",
      "Ep 1 (Step 000340): Train loss 5.194, Val loss 5.327\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3270\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.992, Val loss 8.938\n",
      "Ep 1 (Step 000010): Train loss 7.472, Val loss 7.432\n",
      "Ep 1 (Step 000020): Train loss 6.836, Val loss 6.763\n",
      "Ep 1 (Step 000030): Train loss 6.497, Val loss 6.464\n",
      "Ep 1 (Step 000040): Train loss 6.423, Val loss 6.387\n",
      "Ep 1 (Step 000050): Train loss 6.349, Val loss 6.338\n",
      "Ep 1 (Step 000060): Train loss 6.252, Val loss 6.264\n",
      "Ep 1 (Step 000070): Train loss 6.186, Val loss 6.149\n",
      "Ep 1 (Step 000080): Train loss 6.167, Val loss 6.042\n",
      "Ep 1 (Step 000090): Train loss 6.014, Val loss 5.979\n",
      "Ep 1 (Step 000100): Train loss 5.895, Val loss 5.926\n",
      "Ep 1 (Step 000110): Train loss 5.889, Val loss 5.870\n",
      "Ep 1 (Step 000120): Train loss 5.791, Val loss 5.824\n",
      "Ep 1 (Step 000130): Train loss 5.730, Val loss 5.782\n",
      "Ep 1 (Step 000140): Train loss 5.753, Val loss 5.736\n",
      "Ep 1 (Step 000150): Train loss 5.675, Val loss 5.703\n",
      "Ep 1 (Step 000160): Train loss 5.652, Val loss 5.687\n",
      "Ep 1 (Step 000170): Train loss 5.614, Val loss 5.644\n",
      "Ep 1 (Step 000180): Train loss 5.586, Val loss 5.624\n",
      "Ep 1 (Step 000190): Train loss 5.583, Val loss 5.593\n",
      "Ep 1 (Step 000200): Train loss 5.505, Val loss 5.566\n",
      "Ep 1 (Step 000210): Train loss 5.467, Val loss 5.548\n",
      "Ep 1 (Step 000220): Train loss 5.439, Val loss 5.518\n",
      "Ep 1 (Step 000230): Train loss 5.474, Val loss 5.503\n",
      "Ep 1 (Step 000240): Train loss 5.372, Val loss 5.493\n",
      "Ep 1 (Step 000250): Train loss 5.350, Val loss 5.471\n",
      "Ep 1 (Step 000260): Train loss 5.375, Val loss 5.452\n",
      "Ep 1 (Step 000270): Train loss 5.332, Val loss 5.437\n",
      "Ep 1 (Step 000280): Train loss 5.347, Val loss 5.415\n",
      "Ep 1 (Step 000290): Train loss 5.288, Val loss 5.397\n",
      "Ep 1 (Step 000300): Train loss 5.229, Val loss 5.386\n",
      "Ep 1 (Step 000310): Train loss 5.283, Val loss 5.381\n",
      "Ep 1 (Step 000320): Train loss 5.245, Val loss 5.373\n",
      "Ep 1 (Step 000330): Train loss 5.241, Val loss 5.354\n",
      "Ep 1 (Step 000340): Train loss 5.260, Val loss 5.348\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3483\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.673, Val loss 8.613\n",
      "Ep 1 (Step 000010): Train loss 6.911, Val loss 6.816\n",
      "Ep 1 (Step 000020): Train loss 6.478, Val loss 6.461\n",
      "Ep 1 (Step 000030): Train loss 6.538, Val loss 6.487\n",
      "Ep 1 (Step 000040): Train loss 6.360, Val loss 6.345\n",
      "Ep 1 (Step 000050): Train loss 6.230, Val loss 6.178\n",
      "Ep 1 (Step 000060): Train loss 6.074, Val loss 6.050\n",
      "Ep 1 (Step 000070): Train loss 6.016, Val loss 5.954\n",
      "Ep 1 (Step 000080): Train loss 5.825, Val loss 5.864\n",
      "Ep 1 (Step 000090): Train loss 5.748, Val loss 5.796\n",
      "Ep 1 (Step 000100): Train loss 5.760, Val loss 5.730\n",
      "Ep 1 (Step 000110): Train loss 5.619, Val loss 5.690\n",
      "Ep 1 (Step 000120): Train loss 5.556, Val loss 5.653\n",
      "Ep 1 (Step 000130): Train loss 5.547, Val loss 5.611\n",
      "Ep 1 (Step 000140): Train loss 5.547, Val loss 5.596\n",
      "Ep 1 (Step 000150): Train loss 5.402, Val loss 5.568\n",
      "Ep 1 (Step 000160): Train loss 5.329, Val loss 5.533\n",
      "Ep 1 (Step 000170): Train loss 5.446, Val loss 5.509\n",
      "Ep 1 (Step 000180): Train loss 5.508, Val loss 5.465\n",
      "Ep 1 (Step 000190): Train loss 5.390, Val loss 5.452\n",
      "Ep 1 (Step 000200): Train loss 5.302, Val loss 5.447\n",
      "Ep 1 (Step 000210): Train loss 5.298, Val loss 5.411\n",
      "Ep 1 (Step 000220): Train loss 5.277, Val loss 5.381\n",
      "Ep 1 (Step 000230): Train loss 5.300, Val loss 5.379\n",
      "Ep 1 (Step 000240): Train loss 5.269, Val loss 5.375\n",
      "Ep 1 (Step 000250): Train loss 5.323, Val loss 5.357\n",
      "Ep 1 (Step 000260): Train loss 5.188, Val loss 5.340\n",
      "Ep 1 (Step 000270): Train loss 5.266, Val loss 5.319\n",
      "Ep 1 (Step 000280): Train loss 5.149, Val loss 5.312\n",
      "Ep 1 (Step 000290): Train loss 5.243, Val loss 5.302\n",
      "Ep 1 (Step 000300): Train loss 5.191, Val loss 5.264\n",
      "Ep 1 (Step 000310): Train loss 5.289, Val loss 5.280\n",
      "Ep 1 (Step 000320): Train loss 5.163, Val loss 5.261\n",
      "Ep 1 (Step 000330): Train loss 5.096, Val loss 5.256\n",
      "Ep 1 (Step 000340): Train loss 5.144, Val loss 5.257\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2572\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.616, Val loss 8.601\n",
      "Ep 1 (Step 000010): Train loss 6.815, Val loss 6.787\n",
      "Ep 1 (Step 000020): Train loss 6.475, Val loss 6.451\n",
      "Ep 1 (Step 000030): Train loss 6.464, Val loss 6.410\n",
      "Ep 1 (Step 000040): Train loss 6.333, Val loss 6.310\n",
      "Ep 1 (Step 000050): Train loss 6.168, Val loss 6.179\n",
      "Ep 1 (Step 000060): Train loss 6.048, Val loss 6.027\n",
      "Ep 1 (Step 000070): Train loss 5.932, Val loss 5.950\n",
      "Ep 1 (Step 000080): Train loss 5.834, Val loss 5.871\n",
      "Ep 1 (Step 000090): Train loss 5.734, Val loss 5.784\n",
      "Ep 1 (Step 000100): Train loss 5.685, Val loss 5.731\n",
      "Ep 1 (Step 000110): Train loss 5.646, Val loss 5.687\n",
      "Ep 1 (Step 000120): Train loss 5.597, Val loss 5.635\n",
      "Ep 1 (Step 000130): Train loss 5.539, Val loss 5.601\n",
      "Ep 1 (Step 000140): Train loss 5.550, Val loss 5.589\n",
      "Ep 1 (Step 000150): Train loss 5.483, Val loss 5.548\n",
      "Ep 1 (Step 000160): Train loss 5.390, Val loss 5.534\n",
      "Ep 1 (Step 000170): Train loss 5.490, Val loss 5.493\n",
      "Ep 1 (Step 000180): Train loss 5.387, Val loss 5.501\n",
      "Ep 1 (Step 000190): Train loss 5.377, Val loss 5.467\n",
      "Ep 1 (Step 000200): Train loss 5.350, Val loss 5.452\n",
      "Ep 1 (Step 000210): Train loss 5.406, Val loss 5.437\n",
      "Ep 1 (Step 000220): Train loss 5.236, Val loss 5.418\n",
      "Ep 1 (Step 000230): Train loss 5.301, Val loss 5.382\n",
      "Ep 1 (Step 000240): Train loss 5.311, Val loss 5.390\n",
      "Ep 1 (Step 000250): Train loss 5.256, Val loss 5.377\n",
      "Ep 1 (Step 000260): Train loss 5.202, Val loss 5.361\n",
      "Ep 1 (Step 000270): Train loss 5.190, Val loss 5.350\n",
      "Ep 1 (Step 000280): Train loss 5.211, Val loss 5.358\n",
      "Ep 1 (Step 000290): Train loss 5.192, Val loss 5.351\n",
      "Ep 1 (Step 000300): Train loss 5.143, Val loss 5.345\n",
      "Ep 1 (Step 000310): Train loss 5.122, Val loss 5.333\n",
      "Ep 1 (Step 000320): Train loss 5.227, Val loss 5.330\n",
      "Ep 1 (Step 000330): Train loss 5.089, Val loss 5.295\n",
      "Ep 1 (Step 000340): Train loss 5.158, Val loss 5.272\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2723\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.661, Val loss 8.625\n",
      "Ep 1 (Step 000010): Train loss 6.852, Val loss 6.816\n",
      "Ep 1 (Step 000020): Train loss 6.462, Val loss 6.448\n",
      "Ep 1 (Step 000030): Train loss 6.461, Val loss 6.458\n",
      "Ep 1 (Step 000040): Train loss 6.377, Val loss 6.342\n",
      "Ep 1 (Step 000050): Train loss 6.182, Val loss 6.202\n",
      "Ep 1 (Step 000060): Train loss 6.059, Val loss 6.077\n",
      "Ep 1 (Step 000070): Train loss 5.939, Val loss 5.957\n",
      "Ep 1 (Step 000080): Train loss 5.933, Val loss 5.884\n",
      "Ep 1 (Step 000090): Train loss 5.737, Val loss 5.781\n",
      "Ep 1 (Step 000100): Train loss 5.695, Val loss 5.736\n",
      "Ep 1 (Step 000110): Train loss 5.668, Val loss 5.682\n",
      "Ep 1 (Step 000120): Train loss 5.623, Val loss 5.642\n",
      "Ep 1 (Step 000130): Train loss 5.531, Val loss 5.648\n",
      "Ep 1 (Step 000140): Train loss 5.556, Val loss 5.580\n",
      "Ep 1 (Step 000150): Train loss 5.515, Val loss 5.560\n",
      "Ep 1 (Step 000160): Train loss 5.471, Val loss 5.522\n",
      "Ep 1 (Step 000170): Train loss 5.424, Val loss 5.503\n",
      "Ep 1 (Step 000180): Train loss 5.349, Val loss 5.472\n",
      "Ep 1 (Step 000190): Train loss 5.378, Val loss 5.433\n",
      "Ep 1 (Step 000200): Train loss 5.430, Val loss 5.429\n",
      "Ep 1 (Step 000210): Train loss 5.329, Val loss 5.428\n",
      "Ep 1 (Step 000220): Train loss 5.341, Val loss 5.419\n",
      "Ep 1 (Step 000230): Train loss 5.222, Val loss 5.399\n",
      "Ep 1 (Step 000240): Train loss 5.256, Val loss 5.385\n",
      "Ep 1 (Step 000250): Train loss 5.280, Val loss 5.365\n",
      "Ep 1 (Step 000260): Train loss 5.219, Val loss 5.356\n",
      "Ep 1 (Step 000270): Train loss 5.287, Val loss 5.328\n",
      "Ep 1 (Step 000280): Train loss 5.267, Val loss 5.339\n",
      "Ep 1 (Step 000290): Train loss 5.108, Val loss 5.317\n",
      "Ep 1 (Step 000300): Train loss 5.260, Val loss 5.317\n",
      "Ep 1 (Step 000310): Train loss 5.217, Val loss 5.294\n",
      "Ep 1 (Step 000320): Train loss 5.257, Val loss 5.287\n",
      "Ep 1 (Step 000330): Train loss 5.247, Val loss 5.294\n",
      "Ep 1 (Step 000340): Train loss 5.091, Val loss 5.270\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2704\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.571, Val loss 8.571\n",
      "Ep 1 (Step 000010): Train loss 6.801, Val loss 6.805\n",
      "Ep 1 (Step 000020): Train loss 6.464, Val loss 6.448\n",
      "Ep 1 (Step 000030): Train loss 6.436, Val loss 6.408\n",
      "Ep 1 (Step 000040): Train loss 6.294, Val loss 6.242\n",
      "Ep 1 (Step 000050): Train loss 6.182, Val loss 6.154\n",
      "Ep 1 (Step 000060): Train loss 6.052, Val loss 6.048\n",
      "Ep 1 (Step 000070): Train loss 5.826, Val loss 5.940\n",
      "Ep 1 (Step 000080): Train loss 5.840, Val loss 5.846\n",
      "Ep 1 (Step 000090): Train loss 5.721, Val loss 5.763\n",
      "Ep 1 (Step 000100): Train loss 5.682, Val loss 5.702\n",
      "Ep 1 (Step 000110): Train loss 5.644, Val loss 5.689\n",
      "Ep 1 (Step 000120): Train loss 5.663, Val loss 5.638\n",
      "Ep 1 (Step 000130): Train loss 5.481, Val loss 5.583\n",
      "Ep 1 (Step 000140): Train loss 5.485, Val loss 5.541\n",
      "Ep 1 (Step 000150): Train loss 5.407, Val loss 5.526\n",
      "Ep 1 (Step 000160): Train loss 5.407, Val loss 5.477\n",
      "Ep 1 (Step 000170): Train loss 5.377, Val loss 5.454\n",
      "Ep 1 (Step 000180): Train loss 5.409, Val loss 5.455\n",
      "Ep 1 (Step 000190): Train loss 5.362, Val loss 5.442\n",
      "Ep 1 (Step 000200): Train loss 5.335, Val loss 5.411\n",
      "Ep 1 (Step 000210): Train loss 5.300, Val loss 5.390\n",
      "Ep 1 (Step 000220): Train loss 5.251, Val loss 5.381\n",
      "Ep 1 (Step 000230): Train loss 5.248, Val loss 5.371\n",
      "Ep 1 (Step 000240): Train loss 5.327, Val loss 5.355\n",
      "Ep 1 (Step 000250): Train loss 5.209, Val loss 5.323\n",
      "Ep 1 (Step 000260): Train loss 5.214, Val loss 5.322\n",
      "Ep 1 (Step 000270): Train loss 5.271, Val loss 5.302\n",
      "Ep 1 (Step 000280): Train loss 5.134, Val loss 5.285\n",
      "Ep 1 (Step 000290): Train loss 5.173, Val loss 5.283\n",
      "Ep 1 (Step 000300): Train loss 5.137, Val loss 5.260\n",
      "Ep 1 (Step 000310): Train loss 5.097, Val loss 5.250\n",
      "Ep 1 (Step 000320): Train loss 5.168, Val loss 5.282\n",
      "Ep 1 (Step 000330): Train loss 5.108, Val loss 5.262\n",
      "Ep 1 (Step 000340): Train loss 5.074, Val loss 5.259\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2591\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.621, Val loss 8.600\n",
      "Ep 1 (Step 000010): Train loss 6.828, Val loss 6.812\n",
      "Ep 1 (Step 000020): Train loss 6.450, Val loss 6.471\n",
      "Ep 1 (Step 000030): Train loss 6.486, Val loss 6.450\n",
      "Ep 1 (Step 000040): Train loss 6.454, Val loss 6.361\n",
      "Ep 1 (Step 000050): Train loss 6.283, Val loss 6.248\n",
      "Ep 1 (Step 000060): Train loss 6.094, Val loss 6.157\n",
      "Ep 1 (Step 000070): Train loss 6.025, Val loss 6.027\n",
      "Ep 1 (Step 000080): Train loss 5.878, Val loss 5.906\n",
      "Ep 1 (Step 000090): Train loss 5.710, Val loss 5.823\n",
      "Ep 1 (Step 000100): Train loss 5.702, Val loss 5.777\n",
      "Ep 1 (Step 000110): Train loss 5.638, Val loss 5.717\n",
      "Ep 1 (Step 000120): Train loss 5.608, Val loss 5.679\n",
      "Ep 1 (Step 000130): Train loss 5.530, Val loss 5.621\n",
      "Ep 1 (Step 000140): Train loss 5.429, Val loss 5.592\n",
      "Ep 1 (Step 000150): Train loss 5.526, Val loss 5.544\n",
      "Ep 1 (Step 000160): Train loss 5.376, Val loss 5.521\n",
      "Ep 1 (Step 000170): Train loss 5.430, Val loss 5.481\n",
      "Ep 1 (Step 000180): Train loss 5.420, Val loss 5.467\n",
      "Ep 1 (Step 000190): Train loss 5.429, Val loss 5.436\n",
      "Ep 1 (Step 000200): Train loss 5.361, Val loss 5.436\n",
      "Ep 1 (Step 000210): Train loss 5.380, Val loss 5.415\n",
      "Ep 1 (Step 000220): Train loss 5.313, Val loss 5.391\n",
      "Ep 1 (Step 000230): Train loss 5.337, Val loss 5.361\n",
      "Ep 1 (Step 000240): Train loss 5.198, Val loss 5.370\n",
      "Ep 1 (Step 000250): Train loss 5.254, Val loss 5.367\n",
      "Ep 1 (Step 000260): Train loss 5.237, Val loss 5.344\n",
      "Ep 1 (Step 000270): Train loss 5.241, Val loss 5.325\n",
      "Ep 1 (Step 000280): Train loss 5.195, Val loss 5.319\n",
      "Ep 1 (Step 000290): Train loss 5.104, Val loss 5.296\n",
      "Ep 1 (Step 000300): Train loss 5.173, Val loss 5.288\n",
      "Ep 1 (Step 000310): Train loss 5.155, Val loss 5.265\n",
      "Ep 1 (Step 000320): Train loss 5.155, Val loss 5.254\n",
      "Ep 1 (Step 000330): Train loss 5.128, Val loss 5.231\n",
      "Ep 1 (Step 000340): Train loss 5.191, Val loss 5.230\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2301\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.659, Val loss 8.621\n",
      "Ep 1 (Step 000010): Train loss 6.818, Val loss 6.816\n",
      "Ep 1 (Step 000020): Train loss 6.502, Val loss 6.467\n",
      "Ep 1 (Step 000030): Train loss 6.401, Val loss 6.430\n",
      "Ep 1 (Step 000040): Train loss 6.282, Val loss 6.303\n",
      "Ep 1 (Step 000050): Train loss 6.123, Val loss 6.165\n",
      "Ep 1 (Step 000060): Train loss 6.011, Val loss 6.042\n",
      "Ep 1 (Step 000070): Train loss 5.943, Val loss 5.925\n",
      "Ep 1 (Step 000080): Train loss 5.884, Val loss 5.852\n",
      "Ep 1 (Step 000090): Train loss 5.724, Val loss 5.784\n",
      "Ep 1 (Step 000100): Train loss 5.717, Val loss 5.708\n",
      "Ep 1 (Step 000110): Train loss 5.642, Val loss 5.688\n",
      "Ep 1 (Step 000120): Train loss 5.542, Val loss 5.630\n",
      "Ep 1 (Step 000130): Train loss 5.512, Val loss 5.582\n",
      "Ep 1 (Step 000140): Train loss 5.557, Val loss 5.547\n",
      "Ep 1 (Step 000150): Train loss 5.367, Val loss 5.532\n",
      "Ep 1 (Step 000160): Train loss 5.376, Val loss 5.504\n",
      "Ep 1 (Step 000170): Train loss 5.410, Val loss 5.467\n",
      "Ep 1 (Step 000180): Train loss 5.385, Val loss 5.464\n",
      "Ep 1 (Step 000190): Train loss 5.322, Val loss 5.441\n",
      "Ep 1 (Step 000200): Train loss 5.366, Val loss 5.415\n",
      "Ep 1 (Step 000210): Train loss 5.275, Val loss 5.401\n",
      "Ep 1 (Step 000220): Train loss 5.276, Val loss 5.380\n",
      "Ep 1 (Step 000230): Train loss 5.195, Val loss 5.367\n",
      "Ep 1 (Step 000240): Train loss 5.288, Val loss 5.356\n",
      "Ep 1 (Step 000250): Train loss 5.285, Val loss 5.335\n",
      "Ep 1 (Step 000260): Train loss 5.241, Val loss 5.323\n",
      "Ep 1 (Step 000270): Train loss 5.234, Val loss 5.319\n",
      "Ep 1 (Step 000280): Train loss 5.234, Val loss 5.292\n",
      "Ep 1 (Step 000290): Train loss 5.276, Val loss 5.306\n",
      "Ep 1 (Step 000300): Train loss 5.162, Val loss 5.285\n",
      "Ep 1 (Step 000310): Train loss 5.219, Val loss 5.291\n",
      "Ep 1 (Step 000320): Train loss 5.129, Val loss 5.257\n",
      "Ep 1 (Step 000330): Train loss 5.103, Val loss 5.250\n",
      "Ep 1 (Step 000340): Train loss 5.104, Val loss 5.229\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2295\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.012, Val loss 8.987\n",
      "Ep 1 (Step 000010): Train loss 7.439, Val loss 7.452\n",
      "Ep 1 (Step 000020): Train loss 6.869, Val loss 6.790\n",
      "Ep 1 (Step 000030): Train loss 6.521, Val loss 6.488\n",
      "Ep 1 (Step 000040): Train loss 6.359, Val loss 6.410\n",
      "Ep 1 (Step 000050): Train loss 6.354, Val loss 6.370\n",
      "Ep 1 (Step 000060): Train loss 6.316, Val loss 6.278\n",
      "Ep 1 (Step 000070): Train loss 6.169, Val loss 6.152\n",
      "Ep 1 (Step 000080): Train loss 6.042, Val loss 6.066\n",
      "Ep 1 (Step 000090): Train loss 6.030, Val loss 5.987\n",
      "Ep 1 (Step 000100): Train loss 5.887, Val loss 5.911\n",
      "Ep 1 (Step 000110): Train loss 5.855, Val loss 5.869\n",
      "Ep 1 (Step 000120): Train loss 5.753, Val loss 5.816\n",
      "Ep 1 (Step 000130): Train loss 5.788, Val loss 5.786\n",
      "Ep 1 (Step 000140): Train loss 5.646, Val loss 5.754\n",
      "Ep 1 (Step 000150): Train loss 5.601, Val loss 5.709\n",
      "Ep 1 (Step 000160): Train loss 5.640, Val loss 5.693\n",
      "Ep 1 (Step 000170): Train loss 5.495, Val loss 5.648\n",
      "Ep 1 (Step 000180): Train loss 5.556, Val loss 5.636\n",
      "Ep 1 (Step 000190): Train loss 5.481, Val loss 5.631\n",
      "Ep 1 (Step 000200): Train loss 5.568, Val loss 5.586\n",
      "Ep 1 (Step 000210): Train loss 5.554, Val loss 5.567\n",
      "Ep 1 (Step 000220): Train loss 5.486, Val loss 5.534\n",
      "Ep 1 (Step 000230): Train loss 5.403, Val loss 5.520\n",
      "Ep 1 (Step 000240): Train loss 5.401, Val loss 5.500\n",
      "Ep 1 (Step 000250): Train loss 5.368, Val loss 5.464\n",
      "Ep 1 (Step 000260): Train loss 5.324, Val loss 5.453\n",
      "Ep 1 (Step 000270): Train loss 5.449, Val loss 5.447\n",
      "Ep 1 (Step 000280): Train loss 5.335, Val loss 5.441\n",
      "Ep 1 (Step 000290): Train loss 5.274, Val loss 5.425\n",
      "Ep 1 (Step 000300): Train loss 5.269, Val loss 5.409\n",
      "Ep 1 (Step 000310): Train loss 5.229, Val loss 5.388\n",
      "Ep 1 (Step 000320): Train loss 5.303, Val loss 5.388\n",
      "Ep 1 (Step 000330): Train loss 5.260, Val loss 5.361\n",
      "Ep 1 (Step 000340): Train loss 5.168, Val loss 5.350\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3503\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.884, Val loss 8.884\n",
      "Ep 1 (Step 000010): Train loss 7.408, Val loss 7.363\n",
      "Ep 1 (Step 000020): Train loss 6.800, Val loss 6.741\n",
      "Ep 1 (Step 000030): Train loss 6.489, Val loss 6.455\n",
      "Ep 1 (Step 000040): Train loss 6.401, Val loss 6.371\n",
      "Ep 1 (Step 000050): Train loss 6.382, Val loss 6.333\n",
      "Ep 1 (Step 000060): Train loss 6.277, Val loss 6.254\n",
      "Ep 1 (Step 000070): Train loss 6.095, Val loss 6.131\n",
      "Ep 1 (Step 000080): Train loss 6.027, Val loss 6.043\n",
      "Ep 1 (Step 000090): Train loss 5.939, Val loss 5.968\n",
      "Ep 1 (Step 000100): Train loss 5.884, Val loss 5.916\n",
      "Ep 1 (Step 000110): Train loss 5.814, Val loss 5.861\n",
      "Ep 1 (Step 000120): Train loss 5.705, Val loss 5.802\n",
      "Ep 1 (Step 000130): Train loss 5.694, Val loss 5.789\n",
      "Ep 1 (Step 000140): Train loss 5.587, Val loss 5.730\n",
      "Ep 1 (Step 000150): Train loss 5.622, Val loss 5.697\n",
      "Ep 1 (Step 000160): Train loss 5.653, Val loss 5.665\n",
      "Ep 1 (Step 000170): Train loss 5.595, Val loss 5.641\n",
      "Ep 1 (Step 000180): Train loss 5.513, Val loss 5.611\n",
      "Ep 1 (Step 000190): Train loss 5.499, Val loss 5.575\n",
      "Ep 1 (Step 000200): Train loss 5.410, Val loss 5.563\n",
      "Ep 1 (Step 000210): Train loss 5.432, Val loss 5.547\n",
      "Ep 1 (Step 000220): Train loss 5.483, Val loss 5.530\n",
      "Ep 1 (Step 000230): Train loss 5.434, Val loss 5.500\n",
      "Ep 1 (Step 000240): Train loss 5.344, Val loss 5.501\n",
      "Ep 1 (Step 000250): Train loss 5.353, Val loss 5.470\n",
      "Ep 1 (Step 000260): Train loss 5.362, Val loss 5.466\n",
      "Ep 1 (Step 000270): Train loss 5.345, Val loss 5.442\n",
      "Ep 1 (Step 000280): Train loss 5.277, Val loss 5.428\n",
      "Ep 1 (Step 000290): Train loss 5.309, Val loss 5.416\n",
      "Ep 1 (Step 000300): Train loss 5.256, Val loss 5.414\n",
      "Ep 1 (Step 000310): Train loss 5.259, Val loss 5.398\n",
      "Ep 1 (Step 000320): Train loss 5.304, Val loss 5.386\n",
      "Ep 1 (Step 000330): Train loss 5.273, Val loss 5.383\n",
      "Ep 1 (Step 000340): Train loss 5.188, Val loss 5.361\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3608\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.909, Val loss 8.867\n",
      "Ep 1 (Step 000010): Train loss 7.374, Val loss 7.353\n",
      "Ep 1 (Step 000020): Train loss 6.736, Val loss 6.747\n",
      "Ep 1 (Step 000030): Train loss 6.520, Val loss 6.452\n",
      "Ep 1 (Step 000040): Train loss 6.382, Val loss 6.376\n",
      "Ep 1 (Step 000050): Train loss 6.309, Val loss 6.321\n",
      "Ep 1 (Step 000060): Train loss 6.234, Val loss 6.248\n",
      "Ep 1 (Step 000070): Train loss 6.117, Val loss 6.125\n",
      "Ep 1 (Step 000080): Train loss 6.044, Val loss 6.053\n",
      "Ep 1 (Step 000090): Train loss 5.969, Val loss 6.010\n",
      "Ep 1 (Step 000100): Train loss 5.867, Val loss 5.938\n",
      "Ep 1 (Step 000110): Train loss 5.804, Val loss 5.869\n",
      "Ep 1 (Step 000120): Train loss 5.776, Val loss 5.827\n",
      "Ep 1 (Step 000130): Train loss 5.757, Val loss 5.780\n",
      "Ep 1 (Step 000140): Train loss 5.686, Val loss 5.732\n",
      "Ep 1 (Step 000150): Train loss 5.648, Val loss 5.674\n",
      "Ep 1 (Step 000160): Train loss 5.551, Val loss 5.660\n",
      "Ep 1 (Step 000170): Train loss 5.595, Val loss 5.614\n",
      "Ep 1 (Step 000180): Train loss 5.536, Val loss 5.602\n",
      "Ep 1 (Step 000190): Train loss 5.575, Val loss 5.578\n",
      "Ep 1 (Step 000200): Train loss 5.510, Val loss 5.551\n",
      "Ep 1 (Step 000210): Train loss 5.423, Val loss 5.527\n",
      "Ep 1 (Step 000220): Train loss 5.411, Val loss 5.505\n",
      "Ep 1 (Step 000230): Train loss 5.491, Val loss 5.496\n",
      "Ep 1 (Step 000240): Train loss 5.372, Val loss 5.478\n",
      "Ep 1 (Step 000250): Train loss 5.369, Val loss 5.471\n",
      "Ep 1 (Step 000260): Train loss 5.372, Val loss 5.460\n",
      "Ep 1 (Step 000270): Train loss 5.242, Val loss 5.422\n",
      "Ep 1 (Step 000280): Train loss 5.281, Val loss 5.424\n",
      "Ep 1 (Step 000290): Train loss 5.328, Val loss 5.413\n",
      "Ep 1 (Step 000300): Train loss 5.297, Val loss 5.399\n",
      "Ep 1 (Step 000310): Train loss 5.315, Val loss 5.390\n",
      "Ep 1 (Step 000320): Train loss 5.260, Val loss 5.367\n",
      "Ep 1 (Step 000330): Train loss 5.275, Val loss 5.360\n",
      "Ep 1 (Step 000340): Train loss 5.206, Val loss 5.349\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3485\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.891, Val loss 8.899\n",
      "Ep 1 (Step 000010): Train loss 7.436, Val loss 7.425\n",
      "Ep 1 (Step 000020): Train loss 6.816, Val loss 6.760\n",
      "Ep 1 (Step 000030): Train loss 6.455, Val loss 6.449\n",
      "Ep 1 (Step 000040): Train loss 6.392, Val loss 6.379\n",
      "Ep 1 (Step 000050): Train loss 6.311, Val loss 6.352\n",
      "Ep 1 (Step 000060): Train loss 6.207, Val loss 6.249\n",
      "Ep 1 (Step 000070): Train loss 6.166, Val loss 6.135\n",
      "Ep 1 (Step 000080): Train loss 6.080, Val loss 6.049\n",
      "Ep 1 (Step 000090): Train loss 5.931, Val loss 5.985\n",
      "Ep 1 (Step 000100): Train loss 5.933, Val loss 5.929\n",
      "Ep 1 (Step 000110): Train loss 5.889, Val loss 5.844\n",
      "Ep 1 (Step 000120): Train loss 5.772, Val loss 5.801\n",
      "Ep 1 (Step 000130): Train loss 5.749, Val loss 5.746\n",
      "Ep 1 (Step 000140): Train loss 5.633, Val loss 5.706\n",
      "Ep 1 (Step 000150): Train loss 5.625, Val loss 5.677\n",
      "Ep 1 (Step 000160): Train loss 5.638, Val loss 5.651\n",
      "Ep 1 (Step 000170): Train loss 5.575, Val loss 5.612\n",
      "Ep 1 (Step 000180): Train loss 5.464, Val loss 5.598\n",
      "Ep 1 (Step 000190): Train loss 5.515, Val loss 5.560\n",
      "Ep 1 (Step 000200): Train loss 5.527, Val loss 5.536\n",
      "Ep 1 (Step 000210): Train loss 5.490, Val loss 5.506\n",
      "Ep 1 (Step 000220): Train loss 5.475, Val loss 5.503\n",
      "Ep 1 (Step 000230): Train loss 5.387, Val loss 5.481\n",
      "Ep 1 (Step 000240): Train loss 5.370, Val loss 5.457\n",
      "Ep 1 (Step 000250): Train loss 5.419, Val loss 5.434\n",
      "Ep 1 (Step 000260): Train loss 5.420, Val loss 5.433\n",
      "Ep 1 (Step 000270): Train loss 5.340, Val loss 5.415\n",
      "Ep 1 (Step 000280): Train loss 5.251, Val loss 5.401\n",
      "Ep 1 (Step 000290): Train loss 5.343, Val loss 5.377\n",
      "Ep 1 (Step 000300): Train loss 5.254, Val loss 5.363\n",
      "Ep 1 (Step 000310): Train loss 5.196, Val loss 5.353\n",
      "Ep 1 (Step 000320): Train loss 5.247, Val loss 5.349\n",
      "Ep 1 (Step 000330): Train loss 5.286, Val loss 5.336\n",
      "Ep 1 (Step 000340): Train loss 5.218, Val loss 5.330\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3304\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.951, Val loss 8.933\n",
      "Ep 1 (Step 000010): Train loss 7.474, Val loss 7.457\n",
      "Ep 1 (Step 000020): Train loss 6.824, Val loss 6.813\n",
      "Ep 1 (Step 000030): Train loss 6.475, Val loss 6.485\n",
      "Ep 1 (Step 000040): Train loss 6.393, Val loss 6.371\n",
      "Ep 1 (Step 000050): Train loss 6.296, Val loss 6.325\n",
      "Ep 1 (Step 000060): Train loss 6.209, Val loss 6.226\n",
      "Ep 1 (Step 000070): Train loss 6.113, Val loss 6.126\n",
      "Ep 1 (Step 000080): Train loss 5.957, Val loss 6.035\n",
      "Ep 1 (Step 000090): Train loss 5.902, Val loss 5.953\n",
      "Ep 1 (Step 000100): Train loss 5.879, Val loss 5.894\n",
      "Ep 1 (Step 000110): Train loss 5.823, Val loss 5.851\n",
      "Ep 1 (Step 000120): Train loss 5.771, Val loss 5.818\n",
      "Ep 1 (Step 000130): Train loss 5.754, Val loss 5.778\n",
      "Ep 1 (Step 000140): Train loss 5.700, Val loss 5.720\n",
      "Ep 1 (Step 000150): Train loss 5.588, Val loss 5.684\n",
      "Ep 1 (Step 000160): Train loss 5.583, Val loss 5.644\n",
      "Ep 1 (Step 000170): Train loss 5.622, Val loss 5.617\n",
      "Ep 1 (Step 000180): Train loss 5.567, Val loss 5.595\n",
      "Ep 1 (Step 000190): Train loss 5.419, Val loss 5.581\n",
      "Ep 1 (Step 000200): Train loss 5.479, Val loss 5.537\n",
      "Ep 1 (Step 000210): Train loss 5.420, Val loss 5.524\n",
      "Ep 1 (Step 000220): Train loss 5.450, Val loss 5.520\n",
      "Ep 1 (Step 000230): Train loss 5.346, Val loss 5.479\n",
      "Ep 1 (Step 000240): Train loss 5.389, Val loss 5.463\n",
      "Ep 1 (Step 000250): Train loss 5.358, Val loss 5.467\n",
      "Ep 1 (Step 000260): Train loss 5.311, Val loss 5.436\n",
      "Ep 1 (Step 000270): Train loss 5.420, Val loss 5.411\n",
      "Ep 1 (Step 000280): Train loss 5.276, Val loss 5.395\n",
      "Ep 1 (Step 000290): Train loss 5.368, Val loss 5.389\n",
      "Ep 1 (Step 000300): Train loss 5.254, Val loss 5.391\n",
      "Ep 1 (Step 000310): Train loss 5.311, Val loss 5.380\n",
      "Ep 1 (Step 000320): Train loss 5.259, Val loss 5.364\n",
      "Ep 1 (Step 000330): Train loss 5.196, Val loss 5.359\n",
      "Ep 1 (Step 000340): Train loss 5.252, Val loss 5.345\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3446\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.965, Val loss 8.942\n",
      "Ep 1 (Step 000010): Train loss 7.457, Val loss 7.406\n",
      "Ep 1 (Step 000020): Train loss 6.801, Val loss 6.763\n",
      "Ep 1 (Step 000030): Train loss 6.495, Val loss 6.470\n",
      "Ep 1 (Step 000040): Train loss 6.416, Val loss 6.386\n",
      "Ep 1 (Step 000050): Train loss 6.407, Val loss 6.345\n",
      "Ep 1 (Step 000060): Train loss 6.246, Val loss 6.300\n",
      "Ep 1 (Step 000070): Train loss 6.188, Val loss 6.173\n",
      "Ep 1 (Step 000080): Train loss 6.081, Val loss 6.083\n",
      "Ep 1 (Step 000090): Train loss 6.021, Val loss 6.008\n",
      "Ep 1 (Step 000100): Train loss 5.970, Val loss 5.935\n",
      "Ep 1 (Step 000110): Train loss 5.802, Val loss 5.849\n",
      "Ep 1 (Step 000120): Train loss 5.711, Val loss 5.807\n",
      "Ep 1 (Step 000130): Train loss 5.719, Val loss 5.772\n",
      "Ep 1 (Step 000140): Train loss 5.575, Val loss 5.731\n",
      "Ep 1 (Step 000150): Train loss 5.635, Val loss 5.699\n",
      "Ep 1 (Step 000160): Train loss 5.594, Val loss 5.646\n",
      "Ep 1 (Step 000170): Train loss 5.613, Val loss 5.638\n",
      "Ep 1 (Step 000180): Train loss 5.569, Val loss 5.605\n",
      "Ep 1 (Step 000190): Train loss 5.453, Val loss 5.576\n",
      "Ep 1 (Step 000200): Train loss 5.386, Val loss 5.555\n",
      "Ep 1 (Step 000210): Train loss 5.465, Val loss 5.523\n",
      "Ep 1 (Step 000220): Train loss 5.506, Val loss 5.518\n",
      "Ep 1 (Step 000230): Train loss 5.440, Val loss 5.482\n",
      "Ep 1 (Step 000240): Train loss 5.322, Val loss 5.474\n",
      "Ep 1 (Step 000250): Train loss 5.344, Val loss 5.443\n",
      "Ep 1 (Step 000260): Train loss 5.300, Val loss 5.431\n",
      "Ep 1 (Step 000270): Train loss 5.312, Val loss 5.419\n",
      "Ep 1 (Step 000280): Train loss 5.294, Val loss 5.408\n",
      "Ep 1 (Step 000290): Train loss 5.292, Val loss 5.396\n",
      "Ep 1 (Step 000300): Train loss 5.186, Val loss 5.374\n",
      "Ep 1 (Step 000310): Train loss 5.249, Val loss 5.382\n",
      "Ep 1 (Step 000320): Train loss 5.216, Val loss 5.355\n",
      "Ep 1 (Step 000330): Train loss 5.261, Val loss 5.355\n",
      "Ep 1 (Step 000340): Train loss 5.236, Val loss 5.338\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3382\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.650, Val loss 8.608\n",
      "Ep 1 (Step 000010): Train loss 6.847, Val loss 6.823\n",
      "Ep 1 (Step 000020): Train loss 6.490, Val loss 6.448\n",
      "Ep 1 (Step 000030): Train loss 6.424, Val loss 6.423\n",
      "Ep 1 (Step 000040): Train loss 6.262, Val loss 6.290\n",
      "Ep 1 (Step 000050): Train loss 6.081, Val loss 6.142\n",
      "Ep 1 (Step 000060): Train loss 6.017, Val loss 6.032\n",
      "Ep 1 (Step 000070): Train loss 5.943, Val loss 5.928\n",
      "Ep 1 (Step 000080): Train loss 5.851, Val loss 5.863\n",
      "Ep 1 (Step 000090): Train loss 5.762, Val loss 5.778\n",
      "Ep 1 (Step 000100): Train loss 5.690, Val loss 5.739\n",
      "Ep 1 (Step 000110): Train loss 5.562, Val loss 5.675\n",
      "Ep 1 (Step 000120): Train loss 5.591, Val loss 5.641\n",
      "Ep 1 (Step 000130): Train loss 5.606, Val loss 5.612\n",
      "Ep 1 (Step 000140): Train loss 5.479, Val loss 5.583\n",
      "Ep 1 (Step 000150): Train loss 5.459, Val loss 5.560\n",
      "Ep 1 (Step 000160): Train loss 5.523, Val loss 5.527\n",
      "Ep 1 (Step 000170): Train loss 5.382, Val loss 5.501\n",
      "Ep 1 (Step 000180): Train loss 5.372, Val loss 5.496\n",
      "Ep 1 (Step 000190): Train loss 5.266, Val loss 5.449\n",
      "Ep 1 (Step 000200): Train loss 5.387, Val loss 5.442\n",
      "Ep 1 (Step 000210): Train loss 5.270, Val loss 5.406\n",
      "Ep 1 (Step 000220): Train loss 5.316, Val loss 5.390\n",
      "Ep 1 (Step 000230): Train loss 5.349, Val loss 5.383\n",
      "Ep 1 (Step 000240): Train loss 5.268, Val loss 5.391\n",
      "Ep 1 (Step 000250): Train loss 5.208, Val loss 5.369\n",
      "Ep 1 (Step 000260): Train loss 5.224, Val loss 5.334\n",
      "Ep 1 (Step 000270): Train loss 5.309, Val loss 5.310\n",
      "Ep 1 (Step 000280): Train loss 5.182, Val loss 5.323\n",
      "Ep 1 (Step 000290): Train loss 5.167, Val loss 5.300\n",
      "Ep 1 (Step 000300): Train loss 5.169, Val loss 5.304\n",
      "Ep 1 (Step 000310): Train loss 5.203, Val loss 5.298\n",
      "Ep 1 (Step 000320): Train loss 5.140, Val loss 5.279\n",
      "Ep 1 (Step 000330): Train loss 5.055, Val loss 5.286\n",
      "Ep 1 (Step 000340): Train loss 5.034, Val loss 5.276\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2764\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.631, Val loss 8.637\n",
      "Ep 1 (Step 000010): Train loss 6.833, Val loss 6.807\n",
      "Ep 1 (Step 000020): Train loss 6.417, Val loss 6.500\n",
      "Ep 1 (Step 000030): Train loss 6.419, Val loss 6.447\n",
      "Ep 1 (Step 000040): Train loss 6.311, Val loss 6.326\n",
      "Ep 1 (Step 000050): Train loss 6.124, Val loss 6.163\n",
      "Ep 1 (Step 000060): Train loss 5.971, Val loss 6.048\n",
      "Ep 1 (Step 000070): Train loss 5.959, Val loss 5.953\n",
      "Ep 1 (Step 000080): Train loss 5.831, Val loss 5.856\n",
      "Ep 1 (Step 000090): Train loss 5.729, Val loss 5.787\n",
      "Ep 1 (Step 000100): Train loss 5.653, Val loss 5.752\n",
      "Ep 1 (Step 000110): Train loss 5.673, Val loss 5.697\n",
      "Ep 1 (Step 000120): Train loss 5.505, Val loss 5.632\n",
      "Ep 1 (Step 000130): Train loss 5.500, Val loss 5.621\n",
      "Ep 1 (Step 000140): Train loss 5.496, Val loss 5.584\n",
      "Ep 1 (Step 000150): Train loss 5.530, Val loss 5.558\n",
      "Ep 1 (Step 000160): Train loss 5.547, Val loss 5.514\n",
      "Ep 1 (Step 000170): Train loss 5.384, Val loss 5.495\n",
      "Ep 1 (Step 000180): Train loss 5.416, Val loss 5.459\n",
      "Ep 1 (Step 000190): Train loss 5.405, Val loss 5.460\n",
      "Ep 1 (Step 000200): Train loss 5.388, Val loss 5.453\n",
      "Ep 1 (Step 000210): Train loss 5.184, Val loss 5.427\n",
      "Ep 1 (Step 000220): Train loss 5.216, Val loss 5.414\n",
      "Ep 1 (Step 000230): Train loss 5.236, Val loss 5.379\n",
      "Ep 1 (Step 000240): Train loss 5.280, Val loss 5.379\n",
      "Ep 1 (Step 000250): Train loss 5.263, Val loss 5.373\n",
      "Ep 1 (Step 000260): Train loss 5.292, Val loss 5.369\n",
      "Ep 1 (Step 000270): Train loss 5.249, Val loss 5.365\n",
      "Ep 1 (Step 000280): Train loss 5.174, Val loss 5.326\n",
      "Ep 1 (Step 000290): Train loss 5.228, Val loss 5.306\n",
      "Ep 1 (Step 000300): Train loss 5.158, Val loss 5.298\n",
      "Ep 1 (Step 000310): Train loss 5.172, Val loss 5.305\n",
      "Ep 1 (Step 000320): Train loss 5.199, Val loss 5.300\n",
      "Ep 1 (Step 000330): Train loss 5.139, Val loss 5.281\n",
      "Ep 1 (Step 000340): Train loss 5.222, Val loss 5.298\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2985\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.675, Val loss 8.662\n",
      "Ep 1 (Step 000010): Train loss 6.844, Val loss 6.848\n",
      "Ep 1 (Step 000020): Train loss 6.484, Val loss 6.445\n",
      "Ep 1 (Step 000030): Train loss 6.426, Val loss 6.418\n",
      "Ep 1 (Step 000040): Train loss 6.249, Val loss 6.308\n",
      "Ep 1 (Step 000050): Train loss 6.154, Val loss 6.167\n",
      "Ep 1 (Step 000060): Train loss 6.063, Val loss 6.032\n",
      "Ep 1 (Step 000070): Train loss 5.855, Val loss 5.933\n",
      "Ep 1 (Step 000080): Train loss 5.851, Val loss 5.878\n",
      "Ep 1 (Step 000090): Train loss 5.793, Val loss 5.803\n",
      "Ep 1 (Step 000100): Train loss 5.650, Val loss 5.750\n",
      "Ep 1 (Step 000110): Train loss 5.620, Val loss 5.688\n",
      "Ep 1 (Step 000120): Train loss 5.581, Val loss 5.642\n",
      "Ep 1 (Step 000130): Train loss 5.615, Val loss 5.601\n",
      "Ep 1 (Step 000140): Train loss 5.483, Val loss 5.575\n",
      "Ep 1 (Step 000150): Train loss 5.521, Val loss 5.531\n",
      "Ep 1 (Step 000160): Train loss 5.442, Val loss 5.521\n",
      "Ep 1 (Step 000170): Train loss 5.350, Val loss 5.496\n",
      "Ep 1 (Step 000180): Train loss 5.420, Val loss 5.472\n",
      "Ep 1 (Step 000190): Train loss 5.325, Val loss 5.474\n",
      "Ep 1 (Step 000200): Train loss 5.297, Val loss 5.456\n",
      "Ep 1 (Step 000210): Train loss 5.287, Val loss 5.447\n",
      "Ep 1 (Step 000220): Train loss 5.369, Val loss 5.428\n",
      "Ep 1 (Step 000230): Train loss 5.277, Val loss 5.398\n",
      "Ep 1 (Step 000240): Train loss 5.227, Val loss 5.376\n",
      "Ep 1 (Step 000250): Train loss 5.289, Val loss 5.357\n",
      "Ep 1 (Step 000260): Train loss 5.219, Val loss 5.368\n",
      "Ep 1 (Step 000270): Train loss 5.204, Val loss 5.339\n",
      "Ep 1 (Step 000280): Train loss 5.209, Val loss 5.348\n",
      "Ep 1 (Step 000290): Train loss 5.211, Val loss 5.330\n",
      "Ep 1 (Step 000300): Train loss 5.148, Val loss 5.311\n",
      "Ep 1 (Step 000310): Train loss 5.244, Val loss 5.317\n",
      "Ep 1 (Step 000320): Train loss 5.187, Val loss 5.303\n",
      "Ep 1 (Step 000330): Train loss 5.148, Val loss 5.298\n",
      "Ep 1 (Step 000340): Train loss 5.110, Val loss 5.298\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2985\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.638, Val loss 8.589\n",
      "Ep 1 (Step 000010): Train loss 6.865, Val loss 6.815\n",
      "Ep 1 (Step 000020): Train loss 6.428, Val loss 6.461\n",
      "Ep 1 (Step 000030): Train loss 6.385, Val loss 6.428\n",
      "Ep 1 (Step 000040): Train loss 6.347, Val loss 6.304\n",
      "Ep 1 (Step 000050): Train loss 6.340, Val loss 6.345\n",
      "Ep 1 (Step 000060): Train loss 6.091, Val loss 6.057\n",
      "Ep 1 (Step 000070): Train loss 5.845, Val loss 5.947\n",
      "Ep 1 (Step 000080): Train loss 5.821, Val loss 5.838\n",
      "Ep 1 (Step 000090): Train loss 5.759, Val loss 5.788\n",
      "Ep 1 (Step 000100): Train loss 5.643, Val loss 5.711\n",
      "Ep 1 (Step 000110): Train loss 5.586, Val loss 5.668\n",
      "Ep 1 (Step 000120): Train loss 5.627, Val loss 5.645\n",
      "Ep 1 (Step 000130): Train loss 5.550, Val loss 5.598\n",
      "Ep 1 (Step 000140): Train loss 5.455, Val loss 5.567\n",
      "Ep 1 (Step 000150): Train loss 5.538, Val loss 5.524\n",
      "Ep 1 (Step 000160): Train loss 5.500, Val loss 5.495\n",
      "Ep 1 (Step 000170): Train loss 5.344, Val loss 5.475\n",
      "Ep 1 (Step 000180): Train loss 5.386, Val loss 5.467\n",
      "Ep 1 (Step 000190): Train loss 5.379, Val loss 5.431\n",
      "Ep 1 (Step 000200): Train loss 5.362, Val loss 5.416\n",
      "Ep 1 (Step 000210): Train loss 5.332, Val loss 5.410\n",
      "Ep 1 (Step 000220): Train loss 5.283, Val loss 5.388\n",
      "Ep 1 (Step 000230): Train loss 5.242, Val loss 5.381\n",
      "Ep 1 (Step 000240): Train loss 5.286, Val loss 5.363\n",
      "Ep 1 (Step 000250): Train loss 5.189, Val loss 5.327\n",
      "Ep 1 (Step 000260): Train loss 5.208, Val loss 5.320\n",
      "Ep 1 (Step 000270): Train loss 5.129, Val loss 5.303\n",
      "Ep 1 (Step 000280): Train loss 5.290, Val loss 5.301\n",
      "Ep 1 (Step 000290): Train loss 5.240, Val loss 5.295\n",
      "Ep 1 (Step 000300): Train loss 5.057, Val loss 5.284\n",
      "Ep 1 (Step 000310): Train loss 5.108, Val loss 5.285\n",
      "Ep 1 (Step 000320): Train loss 5.131, Val loss 5.261\n",
      "Ep 1 (Step 000330): Train loss 5.089, Val loss 5.249\n",
      "Ep 1 (Step 000340): Train loss 5.181, Val loss 5.237\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2369\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.681, Val loss 8.642\n",
      "Ep 1 (Step 000010): Train loss 6.814, Val loss 6.787\n",
      "Ep 1 (Step 000020): Train loss 6.508, Val loss 6.455\n",
      "Ep 1 (Step 000030): Train loss 6.401, Val loss 6.447\n",
      "Ep 1 (Step 000040): Train loss 6.341, Val loss 6.305\n",
      "Ep 1 (Step 000050): Train loss 6.141, Val loss 6.161\n",
      "Ep 1 (Step 000060): Train loss 6.065, Val loss 6.042\n",
      "Ep 1 (Step 000070): Train loss 5.866, Val loss 5.943\n",
      "Ep 1 (Step 000080): Train loss 5.855, Val loss 5.862\n",
      "Ep 1 (Step 000090): Train loss 5.788, Val loss 5.796\n",
      "Ep 1 (Step 000100): Train loss 5.657, Val loss 5.744\n",
      "Ep 1 (Step 000110): Train loss 5.601, Val loss 5.699\n",
      "Ep 1 (Step 000120): Train loss 5.569, Val loss 5.637\n",
      "Ep 1 (Step 000130): Train loss 5.567, Val loss 5.598\n",
      "Ep 1 (Step 000140): Train loss 5.615, Val loss 5.567\n",
      "Ep 1 (Step 000150): Train loss 5.509, Val loss 5.526\n",
      "Ep 1 (Step 000160): Train loss 5.434, Val loss 5.491\n",
      "Ep 1 (Step 000170): Train loss 5.457, Val loss 5.475\n",
      "Ep 1 (Step 000180): Train loss 5.381, Val loss 5.452\n",
      "Ep 1 (Step 000190): Train loss 5.364, Val loss 5.428\n",
      "Ep 1 (Step 000200): Train loss 5.334, Val loss 5.405\n",
      "Ep 1 (Step 000210): Train loss 5.291, Val loss 5.396\n",
      "Ep 1 (Step 000220): Train loss 5.364, Val loss 5.387\n",
      "Ep 1 (Step 000230): Train loss 5.252, Val loss 5.363\n",
      "Ep 1 (Step 000240): Train loss 5.241, Val loss 5.376\n",
      "Ep 1 (Step 000250): Train loss 5.196, Val loss 5.338\n",
      "Ep 1 (Step 000260): Train loss 5.282, Val loss 5.343\n",
      "Ep 1 (Step 000270): Train loss 5.212, Val loss 5.302\n",
      "Ep 1 (Step 000280): Train loss 5.130, Val loss 5.294\n",
      "Ep 1 (Step 000290): Train loss 5.186, Val loss 5.270\n",
      "Ep 1 (Step 000300): Train loss 5.223, Val loss 5.278\n",
      "Ep 1 (Step 000310): Train loss 5.178, Val loss 5.276\n",
      "Ep 1 (Step 000320): Train loss 5.218, Val loss 5.255\n",
      "Ep 1 (Step 000330): Train loss 5.149, Val loss 5.236\n",
      "Ep 1 (Step 000340): Train loss 5.151, Val loss 5.247\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2467\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.629, Val loss 8.606\n",
      "Ep 1 (Step 000010): Train loss 6.815, Val loss 6.834\n",
      "Ep 1 (Step 000020): Train loss 6.464, Val loss 6.494\n",
      "Ep 1 (Step 000030): Train loss 6.505, Val loss 6.446\n",
      "Ep 1 (Step 000040): Train loss 6.325, Val loss 6.314\n",
      "Ep 1 (Step 000050): Train loss 6.124, Val loss 6.165\n",
      "Ep 1 (Step 000060): Train loss 6.001, Val loss 6.033\n",
      "Ep 1 (Step 000070): Train loss 5.927, Val loss 5.955\n",
      "Ep 1 (Step 000080): Train loss 5.767, Val loss 5.874\n",
      "Ep 1 (Step 000090): Train loss 5.700, Val loss 5.793\n",
      "Ep 1 (Step 000100): Train loss 5.729, Val loss 5.718\n",
      "Ep 1 (Step 000110): Train loss 5.621, Val loss 5.659\n",
      "Ep 1 (Step 000120): Train loss 5.595, Val loss 5.614\n",
      "Ep 1 (Step 000130): Train loss 5.455, Val loss 5.589\n",
      "Ep 1 (Step 000140): Train loss 5.434, Val loss 5.542\n",
      "Ep 1 (Step 000150): Train loss 5.475, Val loss 5.519\n",
      "Ep 1 (Step 000160): Train loss 5.389, Val loss 5.491\n",
      "Ep 1 (Step 000170): Train loss 5.314, Val loss 5.486\n",
      "Ep 1 (Step 000180): Train loss 5.414, Val loss 5.478\n",
      "Ep 1 (Step 000190): Train loss 5.322, Val loss 5.446\n",
      "Ep 1 (Step 000200): Train loss 5.341, Val loss 5.441\n",
      "Ep 1 (Step 000210): Train loss 5.248, Val loss 5.428\n",
      "Ep 1 (Step 000220): Train loss 5.270, Val loss 5.369\n",
      "Ep 1 (Step 000230): Train loss 5.335, Val loss 5.383\n",
      "Ep 1 (Step 000240): Train loss 5.183, Val loss 5.341\n",
      "Ep 1 (Step 000250): Train loss 5.203, Val loss 5.349\n",
      "Ep 1 (Step 000260): Train loss 5.158, Val loss 5.333\n",
      "Ep 1 (Step 000270): Train loss 5.226, Val loss 5.317\n",
      "Ep 1 (Step 000280): Train loss 5.265, Val loss 5.290\n",
      "Ep 1 (Step 000290): Train loss 5.177, Val loss 5.287\n",
      "Ep 1 (Step 000300): Train loss 5.193, Val loss 5.291\n",
      "Ep 1 (Step 000310): Train loss 5.123, Val loss 5.270\n",
      "Ep 1 (Step 000320): Train loss 5.161, Val loss 5.259\n",
      "Ep 1 (Step 000330): Train loss 5.169, Val loss 5.242\n",
      "Ep 1 (Step 000340): Train loss 5.089, Val loss 5.242\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2419\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.920, Val loss 8.884\n",
      "Ep 1 (Step 000010): Train loss 7.437, Val loss 7.408\n",
      "Ep 1 (Step 000020): Train loss 6.785, Val loss 6.759\n",
      "Ep 1 (Step 000030): Train loss 6.456, Val loss 6.446\n",
      "Ep 1 (Step 000040): Train loss 6.370, Val loss 6.357\n",
      "Ep 1 (Step 000050): Train loss 6.254, Val loss 6.239\n",
      "Ep 1 (Step 000060): Train loss 6.084, Val loss 6.117\n",
      "Ep 1 (Step 000070): Train loss 5.990, Val loss 6.025\n",
      "Ep 1 (Step 000080): Train loss 5.895, Val loss 5.964\n",
      "Ep 1 (Step 000090): Train loss 5.872, Val loss 5.891\n",
      "Ep 1 (Step 000100): Train loss 5.712, Val loss 5.845\n",
      "Ep 1 (Step 000110): Train loss 5.758, Val loss 5.798\n",
      "Ep 1 (Step 000120): Train loss 5.648, Val loss 5.754\n",
      "Ep 1 (Step 000130): Train loss 5.731, Val loss 5.710\n",
      "Ep 1 (Step 000140): Train loss 5.547, Val loss 5.669\n",
      "Ep 1 (Step 000150): Train loss 5.575, Val loss 5.617\n",
      "Ep 1 (Step 000160): Train loss 5.530, Val loss 5.582\n",
      "Ep 1 (Step 000170): Train loss 5.460, Val loss 5.555\n",
      "Ep 1 (Step 000180): Train loss 5.573, Val loss 5.530\n",
      "Ep 1 (Step 000190): Train loss 5.475, Val loss 5.510\n",
      "Ep 1 (Step 000200): Train loss 5.380, Val loss 5.488\n",
      "Ep 1 (Step 000210): Train loss 5.377, Val loss 5.454\n",
      "Ep 1 (Step 000220): Train loss 5.451, Val loss 5.433\n",
      "Ep 1 (Step 000230): Train loss 5.362, Val loss 5.420\n",
      "Ep 1 (Step 000240): Train loss 5.215, Val loss 5.406\n",
      "Ep 1 (Step 000250): Train loss 5.289, Val loss 5.396\n",
      "Ep 1 (Step 000260): Train loss 5.315, Val loss 5.375\n",
      "Ep 1 (Step 000270): Train loss 5.254, Val loss 5.360\n",
      "Ep 1 (Step 000280): Train loss 5.227, Val loss 5.356\n",
      "Ep 1 (Step 000290): Train loss 5.197, Val loss 5.351\n",
      "Ep 1 (Step 000300): Train loss 5.329, Val loss 5.349\n",
      "Ep 1 (Step 000310): Train loss 5.278, Val loss 5.318\n",
      "Ep 1 (Step 000320): Train loss 5.169, Val loss 5.318\n",
      "Ep 1 (Step 000330): Train loss 5.156, Val loss 5.305\n",
      "Ep 1 (Step 000340): Train loss 5.177, Val loss 5.305\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3053\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.970, Val loss 8.953\n",
      "Ep 1 (Step 000010): Train loss 7.541, Val loss 7.516\n",
      "Ep 1 (Step 000020): Train loss 6.867, Val loss 6.853\n",
      "Ep 1 (Step 000030): Train loss 6.514, Val loss 6.480\n",
      "Ep 1 (Step 000040): Train loss 6.367, Val loss 6.351\n",
      "Ep 1 (Step 000050): Train loss 6.286, Val loss 6.268\n",
      "Ep 1 (Step 000060): Train loss 6.129, Val loss 6.146\n",
      "Ep 1 (Step 000070): Train loss 6.019, Val loss 6.063\n",
      "Ep 1 (Step 000080): Train loss 5.956, Val loss 5.981\n",
      "Ep 1 (Step 000090): Train loss 5.905, Val loss 5.899\n",
      "Ep 1 (Step 000100): Train loss 5.818, Val loss 5.837\n",
      "Ep 1 (Step 000110): Train loss 5.758, Val loss 5.778\n",
      "Ep 1 (Step 000120): Train loss 5.799, Val loss 5.736\n",
      "Ep 1 (Step 000130): Train loss 5.602, Val loss 5.696\n",
      "Ep 1 (Step 000140): Train loss 5.622, Val loss 5.653\n",
      "Ep 1 (Step 000150): Train loss 5.540, Val loss 5.626\n",
      "Ep 1 (Step 000160): Train loss 5.537, Val loss 5.594\n",
      "Ep 1 (Step 000170): Train loss 5.499, Val loss 5.562\n",
      "Ep 1 (Step 000180): Train loss 5.494, Val loss 5.535\n",
      "Ep 1 (Step 000190): Train loss 5.450, Val loss 5.516\n",
      "Ep 1 (Step 000200): Train loss 5.433, Val loss 5.505\n",
      "Ep 1 (Step 000210): Train loss 5.365, Val loss 5.471\n",
      "Ep 1 (Step 000220): Train loss 5.414, Val loss 5.463\n",
      "Ep 1 (Step 000230): Train loss 5.349, Val loss 5.433\n",
      "Ep 1 (Step 000240): Train loss 5.393, Val loss 5.444\n",
      "Ep 1 (Step 000250): Train loss 5.378, Val loss 5.404\n",
      "Ep 1 (Step 000260): Train loss 5.311, Val loss 5.396\n",
      "Ep 1 (Step 000270): Train loss 5.193, Val loss 5.374\n",
      "Ep 1 (Step 000280): Train loss 5.214, Val loss 5.358\n",
      "Ep 1 (Step 000290): Train loss 5.281, Val loss 5.350\n",
      "Ep 1 (Step 000300): Train loss 5.229, Val loss 5.351\n",
      "Ep 1 (Step 000310): Train loss 5.199, Val loss 5.334\n",
      "Ep 1 (Step 000320): Train loss 5.146, Val loss 5.327\n",
      "Ep 1 (Step 000330): Train loss 5.131, Val loss 5.310\n",
      "Ep 1 (Step 000340): Train loss 5.112, Val loss 5.298\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2982\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.905, Val loss 8.901\n",
      "Ep 1 (Step 000010): Train loss 7.439, Val loss 7.376\n",
      "Ep 1 (Step 000020): Train loss 6.746, Val loss 6.753\n",
      "Ep 1 (Step 000030): Train loss 6.446, Val loss 6.451\n",
      "Ep 1 (Step 000040): Train loss 6.362, Val loss 6.362\n",
      "Ep 1 (Step 000050): Train loss 6.254, Val loss 6.271\n",
      "Ep 1 (Step 000060): Train loss 6.137, Val loss 6.147\n",
      "Ep 1 (Step 000070): Train loss 5.901, Val loss 6.027\n",
      "Ep 1 (Step 000080): Train loss 5.993, Val loss 5.960\n",
      "Ep 1 (Step 000090): Train loss 5.775, Val loss 5.890\n",
      "Ep 1 (Step 000100): Train loss 5.774, Val loss 5.845\n",
      "Ep 1 (Step 000110): Train loss 5.754, Val loss 5.798\n",
      "Ep 1 (Step 000120): Train loss 5.687, Val loss 5.750\n",
      "Ep 1 (Step 000130): Train loss 5.674, Val loss 5.698\n",
      "Ep 1 (Step 000140): Train loss 5.551, Val loss 5.666\n",
      "Ep 1 (Step 000150): Train loss 5.524, Val loss 5.625\n",
      "Ep 1 (Step 000160): Train loss 5.627, Val loss 5.631\n",
      "Ep 1 (Step 000170): Train loss 5.473, Val loss 5.588\n",
      "Ep 1 (Step 000180): Train loss 5.464, Val loss 5.558\n",
      "Ep 1 (Step 000190): Train loss 5.417, Val loss 5.526\n",
      "Ep 1 (Step 000200): Train loss 5.411, Val loss 5.518\n",
      "Ep 1 (Step 000210): Train loss 5.369, Val loss 5.484\n",
      "Ep 1 (Step 000220): Train loss 5.345, Val loss 5.475\n",
      "Ep 1 (Step 000230): Train loss 5.428, Val loss 5.467\n",
      "Ep 1 (Step 000240): Train loss 5.361, Val loss 5.437\n",
      "Ep 1 (Step 000250): Train loss 5.343, Val loss 5.419\n",
      "Ep 1 (Step 000260): Train loss 5.274, Val loss 5.411\n",
      "Ep 1 (Step 000270): Train loss 5.267, Val loss 5.377\n",
      "Ep 1 (Step 000280): Train loss 5.227, Val loss 5.386\n",
      "Ep 1 (Step 000290): Train loss 5.209, Val loss 5.360\n",
      "Ep 1 (Step 000300): Train loss 5.248, Val loss 5.349\n",
      "Ep 1 (Step 000310): Train loss 5.203, Val loss 5.339\n",
      "Ep 1 (Step 000320): Train loss 5.253, Val loss 5.340\n",
      "Ep 1 (Step 000330): Train loss 5.202, Val loss 5.324\n",
      "Ep 1 (Step 000340): Train loss 5.104, Val loss 5.319\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3185\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.910, Val loss 8.918\n",
      "Ep 1 (Step 000010): Train loss 7.409, Val loss 7.388\n",
      "Ep 1 (Step 000020): Train loss 6.808, Val loss 6.757\n",
      "Ep 1 (Step 000030): Train loss 6.445, Val loss 6.453\n",
      "Ep 1 (Step 000040): Train loss 6.363, Val loss 6.369\n",
      "Ep 1 (Step 000050): Train loss 6.298, Val loss 6.304\n",
      "Ep 1 (Step 000060): Train loss 6.115, Val loss 6.172\n",
      "Ep 1 (Step 000070): Train loss 6.098, Val loss 6.076\n",
      "Ep 1 (Step 000080): Train loss 5.945, Val loss 5.976\n",
      "Ep 1 (Step 000090): Train loss 5.907, Val loss 5.918\n",
      "Ep 1 (Step 000100): Train loss 5.753, Val loss 5.863\n",
      "Ep 1 (Step 000110): Train loss 5.817, Val loss 5.810\n",
      "Ep 1 (Step 000120): Train loss 5.716, Val loss 5.747\n",
      "Ep 1 (Step 000130): Train loss 5.638, Val loss 5.727\n",
      "Ep 1 (Step 000140): Train loss 5.512, Val loss 5.690\n",
      "Ep 1 (Step 000150): Train loss 5.587, Val loss 5.636\n",
      "Ep 1 (Step 000160): Train loss 5.553, Val loss 5.605\n",
      "Ep 1 (Step 000170): Train loss 5.479, Val loss 5.607\n",
      "Ep 1 (Step 000180): Train loss 5.450, Val loss 5.564\n",
      "Ep 1 (Step 000190): Train loss 5.463, Val loss 5.545\n",
      "Ep 1 (Step 000200): Train loss 5.489, Val loss 5.512\n",
      "Ep 1 (Step 000210): Train loss 5.425, Val loss 5.493\n",
      "Ep 1 (Step 000220): Train loss 5.422, Val loss 5.466\n",
      "Ep 1 (Step 000230): Train loss 5.404, Val loss 5.455\n",
      "Ep 1 (Step 000240): Train loss 5.339, Val loss 5.427\n",
      "Ep 1 (Step 000250): Train loss 5.415, Val loss 5.420\n",
      "Ep 1 (Step 000260): Train loss 5.328, Val loss 5.378\n",
      "Ep 1 (Step 000270): Train loss 5.254, Val loss 5.368\n",
      "Ep 1 (Step 000280): Train loss 5.280, Val loss 5.375\n",
      "Ep 1 (Step 000290): Train loss 5.300, Val loss 5.344\n",
      "Ep 1 (Step 000300): Train loss 5.275, Val loss 5.336\n",
      "Ep 1 (Step 000310): Train loss 5.243, Val loss 5.318\n",
      "Ep 1 (Step 000320): Train loss 5.192, Val loss 5.313\n",
      "Ep 1 (Step 000330): Train loss 5.127, Val loss 5.303\n",
      "Ep 1 (Step 000340): Train loss 5.247, Val loss 5.293\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2929\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.902, Val loss 8.892\n",
      "Ep 1 (Step 000010): Train loss 7.411, Val loss 7.385\n",
      "Ep 1 (Step 000020): Train loss 6.762, Val loss 6.752\n",
      "Ep 1 (Step 000030): Train loss 6.504, Val loss 6.454\n",
      "Ep 1 (Step 000040): Train loss 6.389, Val loss 6.361\n",
      "Ep 1 (Step 000050): Train loss 6.319, Val loss 6.277\n",
      "Ep 1 (Step 000060): Train loss 6.123, Val loss 6.147\n",
      "Ep 1 (Step 000070): Train loss 6.108, Val loss 6.051\n",
      "Ep 1 (Step 000080): Train loss 5.967, Val loss 5.968\n",
      "Ep 1 (Step 000090): Train loss 5.818, Val loss 5.879\n",
      "Ep 1 (Step 000100): Train loss 5.959, Val loss 5.831\n",
      "Ep 1 (Step 000110): Train loss 5.803, Val loss 5.776\n",
      "Ep 1 (Step 000120): Train loss 5.709, Val loss 5.730\n",
      "Ep 1 (Step 000130): Train loss 5.614, Val loss 5.695\n",
      "Ep 1 (Step 000140): Train loss 5.625, Val loss 5.656\n",
      "Ep 1 (Step 000150): Train loss 5.596, Val loss 5.633\n",
      "Ep 1 (Step 000160): Train loss 5.603, Val loss 5.598\n",
      "Ep 1 (Step 000170): Train loss 5.514, Val loss 5.564\n",
      "Ep 1 (Step 000180): Train loss 5.440, Val loss 5.541\n",
      "Ep 1 (Step 000190): Train loss 5.430, Val loss 5.511\n",
      "Ep 1 (Step 000200): Train loss 5.408, Val loss 5.491\n",
      "Ep 1 (Step 000210): Train loss 5.401, Val loss 5.471\n",
      "Ep 1 (Step 000220): Train loss 5.351, Val loss 5.445\n",
      "Ep 1 (Step 000230): Train loss 5.383, Val loss 5.424\n",
      "Ep 1 (Step 000240): Train loss 5.358, Val loss 5.411\n",
      "Ep 1 (Step 000250): Train loss 5.307, Val loss 5.409\n",
      "Ep 1 (Step 000260): Train loss 5.302, Val loss 5.400\n",
      "Ep 1 (Step 000270): Train loss 5.210, Val loss 5.391\n",
      "Ep 1 (Step 000280): Train loss 5.289, Val loss 5.364\n",
      "Ep 1 (Step 000290): Train loss 5.218, Val loss 5.364\n",
      "Ep 1 (Step 000300): Train loss 5.222, Val loss 5.344\n",
      "Ep 1 (Step 000310): Train loss 5.196, Val loss 5.321\n",
      "Ep 1 (Step 000320): Train loss 5.158, Val loss 5.310\n",
      "Ep 1 (Step 000330): Train loss 5.247, Val loss 5.286\n",
      "Ep 1 (Step 000340): Train loss 5.168, Val loss 5.273\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2733\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.908, Val loss 8.881\n",
      "Ep 1 (Step 000010): Train loss 7.411, Val loss 7.353\n",
      "Ep 1 (Step 000020): Train loss 6.761, Val loss 6.714\n",
      "Ep 1 (Step 000030): Train loss 6.485, Val loss 6.423\n",
      "Ep 1 (Step 000040): Train loss 6.303, Val loss 6.348\n",
      "Ep 1 (Step 000050): Train loss 6.281, Val loss 6.270\n",
      "Ep 1 (Step 000060): Train loss 6.228, Val loss 6.149\n",
      "Ep 1 (Step 000070): Train loss 6.050, Val loss 6.034\n",
      "Ep 1 (Step 000080): Train loss 5.934, Val loss 5.958\n",
      "Ep 1 (Step 000090): Train loss 5.835, Val loss 5.903\n",
      "Ep 1 (Step 000100): Train loss 5.840, Val loss 5.857\n",
      "Ep 1 (Step 000110): Train loss 5.752, Val loss 5.779\n",
      "Ep 1 (Step 000120): Train loss 5.707, Val loss 5.751\n",
      "Ep 1 (Step 000130): Train loss 5.674, Val loss 5.709\n",
      "Ep 1 (Step 000140): Train loss 5.659, Val loss 5.661\n",
      "Ep 1 (Step 000150): Train loss 5.490, Val loss 5.631\n",
      "Ep 1 (Step 000160): Train loss 5.480, Val loss 5.594\n",
      "Ep 1 (Step 000170): Train loss 5.509, Val loss 5.557\n",
      "Ep 1 (Step 000180): Train loss 5.493, Val loss 5.521\n",
      "Ep 1 (Step 000190): Train loss 5.445, Val loss 5.525\n",
      "Ep 1 (Step 000200): Train loss 5.524, Val loss 5.490\n",
      "Ep 1 (Step 000210): Train loss 5.299, Val loss 5.467\n",
      "Ep 1 (Step 000220): Train loss 5.301, Val loss 5.442\n",
      "Ep 1 (Step 000230): Train loss 5.353, Val loss 5.420\n",
      "Ep 1 (Step 000240): Train loss 5.235, Val loss 5.415\n",
      "Ep 1 (Step 000250): Train loss 5.283, Val loss 5.405\n",
      "Ep 1 (Step 000260): Train loss 5.299, Val loss 5.384\n",
      "Ep 1 (Step 000270): Train loss 5.203, Val loss 5.364\n",
      "Ep 1 (Step 000280): Train loss 5.262, Val loss 5.361\n",
      "Ep 1 (Step 000290): Train loss 5.173, Val loss 5.353\n",
      "Ep 1 (Step 000300): Train loss 5.213, Val loss 5.331\n",
      "Ep 1 (Step 000310): Train loss 5.159, Val loss 5.314\n",
      "Ep 1 (Step 000320): Train loss 5.139, Val loss 5.305\n",
      "Ep 1 (Step 000330): Train loss 5.174, Val loss 5.288\n",
      "Ep 1 (Step 000340): Train loss 5.163, Val loss 5.282\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2818\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.666, Val loss 8.656\n",
      "Ep 1 (Step 000010): Train loss 6.869, Val loss 6.856\n",
      "Ep 1 (Step 000020): Train loss 6.465, Val loss 6.427\n",
      "Ep 1 (Step 000030): Train loss 6.447, Val loss 6.413\n",
      "Ep 1 (Step 000040): Train loss 6.250, Val loss 6.256\n",
      "Ep 1 (Step 000050): Train loss 6.091, Val loss 6.140\n",
      "Ep 1 (Step 000060): Train loss 6.006, Val loss 5.986\n",
      "Ep 1 (Step 000070): Train loss 5.817, Val loss 5.903\n",
      "Ep 1 (Step 000080): Train loss 5.766, Val loss 5.818\n",
      "Ep 1 (Step 000090): Train loss 5.696, Val loss 5.759\n",
      "Ep 1 (Step 000100): Train loss 5.609, Val loss 5.682\n",
      "Ep 1 (Step 000110): Train loss 5.544, Val loss 5.659\n",
      "Ep 1 (Step 000120): Train loss 5.430, Val loss 5.614\n",
      "Ep 1 (Step 000130): Train loss 5.536, Val loss 5.566\n",
      "Ep 1 (Step 000140): Train loss 5.429, Val loss 5.536\n",
      "Ep 1 (Step 000150): Train loss 5.483, Val loss 5.519\n",
      "Ep 1 (Step 000160): Train loss 5.350, Val loss 5.497\n",
      "Ep 1 (Step 000170): Train loss 5.292, Val loss 5.470\n",
      "Ep 1 (Step 000180): Train loss 5.340, Val loss 5.441\n",
      "Ep 1 (Step 000190): Train loss 5.258, Val loss 5.415\n",
      "Ep 1 (Step 000200): Train loss 5.358, Val loss 5.406\n",
      "Ep 1 (Step 000210): Train loss 5.295, Val loss 5.386\n",
      "Ep 1 (Step 000220): Train loss 5.283, Val loss 5.371\n",
      "Ep 1 (Step 000230): Train loss 5.292, Val loss 5.355\n",
      "Ep 1 (Step 000240): Train loss 5.213, Val loss 5.350\n",
      "Ep 1 (Step 000250): Train loss 5.179, Val loss 5.353\n",
      "Ep 1 (Step 000260): Train loss 5.177, Val loss 5.334\n",
      "Ep 1 (Step 000270): Train loss 5.164, Val loss 5.327\n",
      "Ep 1 (Step 000280): Train loss 5.110, Val loss 5.295\n",
      "Ep 1 (Step 000290): Train loss 5.267, Val loss 5.303\n",
      "Ep 1 (Step 000300): Train loss 5.123, Val loss 5.290\n",
      "Ep 1 (Step 000310): Train loss 5.199, Val loss 5.274\n",
      "Ep 1 (Step 000320): Train loss 5.144, Val loss 5.258\n",
      "Ep 1 (Step 000330): Train loss 5.105, Val loss 5.245\n",
      "Ep 1 (Step 000340): Train loss 5.107, Val loss 5.240\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2402\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.683, Val loss 8.633\n",
      "Ep 1 (Step 000010): Train loss 6.861, Val loss 6.823\n",
      "Ep 1 (Step 000020): Train loss 6.450, Val loss 6.473\n",
      "Ep 1 (Step 000030): Train loss 6.416, Val loss 6.414\n",
      "Ep 1 (Step 000040): Train loss 6.274, Val loss 6.236\n",
      "Ep 1 (Step 000050): Train loss 6.031, Val loss 6.087\n",
      "Ep 1 (Step 000060): Train loss 5.962, Val loss 5.986\n",
      "Ep 1 (Step 000070): Train loss 5.844, Val loss 5.892\n",
      "Ep 1 (Step 000080): Train loss 5.827, Val loss 5.833\n",
      "Ep 1 (Step 000090): Train loss 5.730, Val loss 5.757\n",
      "Ep 1 (Step 000100): Train loss 5.656, Val loss 5.685\n",
      "Ep 1 (Step 000110): Train loss 5.616, Val loss 5.672\n",
      "Ep 1 (Step 000120): Train loss 5.485, Val loss 5.600\n",
      "Ep 1 (Step 000130): Train loss 5.497, Val loss 5.566\n",
      "Ep 1 (Step 000140): Train loss 5.416, Val loss 5.553\n",
      "Ep 1 (Step 000150): Train loss 5.426, Val loss 5.515\n",
      "Ep 1 (Step 000160): Train loss 5.340, Val loss 5.494\n",
      "Ep 1 (Step 000170): Train loss 5.370, Val loss 5.462\n",
      "Ep 1 (Step 000180): Train loss 5.310, Val loss 5.465\n",
      "Ep 1 (Step 000190): Train loss 5.321, Val loss 5.422\n",
      "Ep 1 (Step 000200): Train loss 5.285, Val loss 5.407\n",
      "Ep 1 (Step 000210): Train loss 5.277, Val loss 5.398\n",
      "Ep 1 (Step 000220): Train loss 5.308, Val loss 5.396\n",
      "Ep 1 (Step 000230): Train loss 5.319, Val loss 5.377\n",
      "Ep 1 (Step 000240): Train loss 5.215, Val loss 5.359\n",
      "Ep 1 (Step 000250): Train loss 5.278, Val loss 5.343\n",
      "Ep 1 (Step 000260): Train loss 5.194, Val loss 5.316\n",
      "Ep 1 (Step 000270): Train loss 5.170, Val loss 5.287\n",
      "Ep 1 (Step 000280): Train loss 5.090, Val loss 5.276\n",
      "Ep 1 (Step 000290): Train loss 5.209, Val loss 5.288\n",
      "Ep 1 (Step 000300): Train loss 5.196, Val loss 5.264\n",
      "Ep 1 (Step 000310): Train loss 5.124, Val loss 5.252\n",
      "Ep 1 (Step 000320): Train loss 5.089, Val loss 5.262\n",
      "Ep 1 (Step 000330): Train loss 5.106, Val loss 5.258\n",
      "Ep 1 (Step 000340): Train loss 5.143, Val loss 5.244\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2439\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.628, Val loss 8.592\n",
      "Ep 1 (Step 000010): Train loss 6.865, Val loss 6.813\n",
      "Ep 1 (Step 000020): Train loss 6.404, Val loss 6.471\n",
      "Ep 1 (Step 000030): Train loss 6.424, Val loss 6.400\n",
      "Ep 1 (Step 000040): Train loss 6.245, Val loss 6.240\n",
      "Ep 1 (Step 000050): Train loss 6.113, Val loss 6.085\n",
      "Ep 1 (Step 000060): Train loss 5.952, Val loss 5.998\n",
      "Ep 1 (Step 000070): Train loss 5.821, Val loss 5.902\n",
      "Ep 1 (Step 000080): Train loss 5.685, Val loss 5.808\n",
      "Ep 1 (Step 000090): Train loss 5.650, Val loss 5.724\n",
      "Ep 1 (Step 000100): Train loss 5.608, Val loss 5.667\n",
      "Ep 1 (Step 000110): Train loss 5.588, Val loss 5.613\n",
      "Ep 1 (Step 000120): Train loss 5.475, Val loss 5.586\n",
      "Ep 1 (Step 000130): Train loss 5.498, Val loss 5.545\n",
      "Ep 1 (Step 000140): Train loss 5.550, Val loss 5.514\n",
      "Ep 1 (Step 000150): Train loss 5.422, Val loss 5.471\n",
      "Ep 1 (Step 000160): Train loss 5.393, Val loss 5.472\n",
      "Ep 1 (Step 000170): Train loss 5.386, Val loss 5.441\n",
      "Ep 1 (Step 000180): Train loss 5.329, Val loss 5.426\n",
      "Ep 1 (Step 000190): Train loss 5.296, Val loss 5.415\n",
      "Ep 1 (Step 000200): Train loss 5.272, Val loss 5.395\n",
      "Ep 1 (Step 000210): Train loss 5.292, Val loss 5.381\n",
      "Ep 1 (Step 000220): Train loss 5.232, Val loss 5.353\n",
      "Ep 1 (Step 000230): Train loss 5.217, Val loss 5.344\n",
      "Ep 1 (Step 000240): Train loss 5.160, Val loss 5.340\n",
      "Ep 1 (Step 000250): Train loss 5.263, Val loss 5.313\n",
      "Ep 1 (Step 000260): Train loss 5.129, Val loss 5.311\n",
      "Ep 1 (Step 000270): Train loss 5.125, Val loss 5.303\n",
      "Ep 1 (Step 000280): Train loss 5.080, Val loss 5.301\n",
      "Ep 1 (Step 000290): Train loss 5.253, Val loss 5.285\n",
      "Ep 1 (Step 000300): Train loss 5.144, Val loss 5.282\n",
      "Ep 1 (Step 000310): Train loss 5.034, Val loss 5.248\n",
      "Ep 1 (Step 000320): Train loss 5.172, Val loss 5.241\n",
      "Ep 1 (Step 000330): Train loss 5.115, Val loss 5.235\n",
      "Ep 1 (Step 000340): Train loss 5.072, Val loss 5.231\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2313\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.610, Val loss 8.574\n",
      "Ep 1 (Step 000010): Train loss 6.833, Val loss 6.774\n",
      "Ep 1 (Step 000020): Train loss 6.468, Val loss 6.447\n",
      "Ep 1 (Step 000030): Train loss 6.437, Val loss 6.411\n",
      "Ep 1 (Step 000040): Train loss 6.161, Val loss 6.209\n",
      "Ep 1 (Step 000050): Train loss 6.091, Val loss 6.086\n",
      "Ep 1 (Step 000060): Train loss 6.003, Val loss 5.988\n",
      "Ep 1 (Step 000070): Train loss 5.834, Val loss 5.883\n",
      "Ep 1 (Step 000080): Train loss 5.704, Val loss 5.774\n",
      "Ep 1 (Step 000090): Train loss 5.705, Val loss 5.725\n",
      "Ep 1 (Step 000100): Train loss 5.671, Val loss 5.679\n",
      "Ep 1 (Step 000110): Train loss 5.614, Val loss 5.638\n",
      "Ep 1 (Step 000120): Train loss 5.476, Val loss 5.577\n",
      "Ep 1 (Step 000130): Train loss 5.404, Val loss 5.545\n",
      "Ep 1 (Step 000140): Train loss 5.460, Val loss 5.523\n",
      "Ep 1 (Step 000150): Train loss 5.330, Val loss 5.492\n",
      "Ep 1 (Step 000160): Train loss 5.393, Val loss 5.480\n",
      "Ep 1 (Step 000170): Train loss 5.384, Val loss 5.459\n",
      "Ep 1 (Step 000180): Train loss 5.348, Val loss 5.425\n",
      "Ep 1 (Step 000190): Train loss 5.346, Val loss 5.411\n",
      "Ep 1 (Step 000200): Train loss 5.311, Val loss 5.380\n",
      "Ep 1 (Step 000210): Train loss 5.333, Val loss 5.388\n",
      "Ep 1 (Step 000220): Train loss 5.199, Val loss 5.355\n",
      "Ep 1 (Step 000230): Train loss 5.273, Val loss 5.320\n",
      "Ep 1 (Step 000240): Train loss 5.184, Val loss 5.322\n",
      "Ep 1 (Step 000250): Train loss 5.154, Val loss 5.309\n",
      "Ep 1 (Step 000260): Train loss 5.092, Val loss 5.311\n",
      "Ep 1 (Step 000270): Train loss 5.222, Val loss 5.282\n",
      "Ep 1 (Step 000280): Train loss 5.108, Val loss 5.275\n",
      "Ep 1 (Step 000290): Train loss 5.158, Val loss 5.256\n",
      "Ep 1 (Step 000300): Train loss 5.077, Val loss 5.228\n",
      "Ep 1 (Step 000310): Train loss 5.169, Val loss 5.247\n",
      "Ep 1 (Step 000320): Train loss 5.146, Val loss 5.238\n",
      "Ep 1 (Step 000330): Train loss 5.016, Val loss 5.210\n",
      "Ep 1 (Step 000340): Train loss 5.029, Val loss 5.220\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2198\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.660, Val loss 8.602\n",
      "Ep 1 (Step 000010): Train loss 6.828, Val loss 6.837\n",
      "Ep 1 (Step 000020): Train loss 6.462, Val loss 6.463\n",
      "Ep 1 (Step 000030): Train loss 6.355, Val loss 6.363\n",
      "Ep 1 (Step 000040): Train loss 6.238, Val loss 6.240\n",
      "Ep 1 (Step 000050): Train loss 6.033, Val loss 6.082\n",
      "Ep 1 (Step 000060): Train loss 5.905, Val loss 5.957\n",
      "Ep 1 (Step 000070): Train loss 5.861, Val loss 5.865\n",
      "Ep 1 (Step 000080): Train loss 5.819, Val loss 5.789\n",
      "Ep 1 (Step 000090): Train loss 5.661, Val loss 5.723\n",
      "Ep 1 (Step 000100): Train loss 5.608, Val loss 5.690\n",
      "Ep 1 (Step 000110): Train loss 5.588, Val loss 5.629\n",
      "Ep 1 (Step 000120): Train loss 5.459, Val loss 5.608\n",
      "Ep 1 (Step 000130): Train loss 5.513, Val loss 5.546\n",
      "Ep 1 (Step 000140): Train loss 5.518, Val loss 5.513\n",
      "Ep 1 (Step 000150): Train loss 5.470, Val loss 5.484\n",
      "Ep 1 (Step 000160): Train loss 5.337, Val loss 5.463\n",
      "Ep 1 (Step 000170): Train loss 5.385, Val loss 5.427\n",
      "Ep 1 (Step 000180): Train loss 5.301, Val loss 5.400\n",
      "Ep 1 (Step 000190): Train loss 5.282, Val loss 5.392\n",
      "Ep 1 (Step 000200): Train loss 5.263, Val loss 5.387\n",
      "Ep 1 (Step 000210): Train loss 5.218, Val loss 5.375\n",
      "Ep 1 (Step 000220): Train loss 5.173, Val loss 5.356\n",
      "Ep 1 (Step 000230): Train loss 5.196, Val loss 5.339\n",
      "Ep 1 (Step 000240): Train loss 5.223, Val loss 5.329\n",
      "Ep 1 (Step 000250): Train loss 5.225, Val loss 5.300\n",
      "Ep 1 (Step 000260): Train loss 5.087, Val loss 5.300\n",
      "Ep 1 (Step 000270): Train loss 5.193, Val loss 5.289\n",
      "Ep 1 (Step 000280): Train loss 5.158, Val loss 5.276\n",
      "Ep 1 (Step 000290): Train loss 5.189, Val loss 5.267\n",
      "Ep 1 (Step 000300): Train loss 5.068, Val loss 5.256\n",
      "Ep 1 (Step 000310): Train loss 5.058, Val loss 5.227\n",
      "Ep 1 (Step 000320): Train loss 5.064, Val loss 5.210\n",
      "Ep 1 (Step 000330): Train loss 5.053, Val loss 5.193\n",
      "Ep 1 (Step 000340): Train loss 5.042, Val loss 5.205\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2049\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.599, Val loss 8.580\n",
      "Ep 1 (Step 000010): Train loss 6.864, Val loss 6.814\n",
      "Ep 1 (Step 000020): Train loss 6.445, Val loss 6.427\n",
      "Ep 1 (Step 000030): Train loss 6.389, Val loss 6.355\n",
      "Ep 1 (Step 000040): Train loss 6.223, Val loss 6.158\n",
      "Ep 1 (Step 000050): Train loss 6.173, Val loss 6.052\n",
      "Ep 1 (Step 000060): Train loss 5.933, Val loss 5.982\n",
      "Ep 1 (Step 000070): Train loss 5.873, Val loss 5.864\n",
      "Ep 1 (Step 000080): Train loss 5.744, Val loss 5.800\n",
      "Ep 1 (Step 000090): Train loss 5.584, Val loss 5.739\n",
      "Ep 1 (Step 000100): Train loss 5.635, Val loss 5.682\n",
      "Ep 1 (Step 000110): Train loss 5.596, Val loss 5.637\n",
      "Ep 1 (Step 000120): Train loss 5.499, Val loss 5.580\n",
      "Ep 1 (Step 000130): Train loss 5.505, Val loss 5.549\n",
      "Ep 1 (Step 000140): Train loss 5.449, Val loss 5.521\n",
      "Ep 1 (Step 000150): Train loss 5.460, Val loss 5.500\n",
      "Ep 1 (Step 000160): Train loss 5.444, Val loss 5.472\n",
      "Ep 1 (Step 000170): Train loss 5.339, Val loss 5.455\n",
      "Ep 1 (Step 000180): Train loss 5.308, Val loss 5.429\n",
      "Ep 1 (Step 000190): Train loss 5.234, Val loss 5.411\n",
      "Ep 1 (Step 000200): Train loss 5.311, Val loss 5.395\n",
      "Ep 1 (Step 000210): Train loss 5.282, Val loss 5.362\n",
      "Ep 1 (Step 000220): Train loss 5.269, Val loss 5.343\n",
      "Ep 1 (Step 000230): Train loss 5.183, Val loss 5.330\n",
      "Ep 1 (Step 000240): Train loss 5.149, Val loss 5.321\n",
      "Ep 1 (Step 000250): Train loss 5.184, Val loss 5.312\n",
      "Ep 1 (Step 000260): Train loss 5.218, Val loss 5.301\n",
      "Ep 1 (Step 000270): Train loss 5.206, Val loss 5.290\n",
      "Ep 1 (Step 000280): Train loss 5.113, Val loss 5.295\n",
      "Ep 1 (Step 000290): Train loss 5.143, Val loss 5.284\n",
      "Ep 1 (Step 000300): Train loss 5.112, Val loss 5.264\n",
      "Ep 1 (Step 000310): Train loss 5.166, Val loss 5.252\n",
      "Ep 1 (Step 000320): Train loss 5.038, Val loss 5.230\n",
      "Ep 1 (Step 000330): Train loss 5.147, Val loss 5.224\n",
      "Ep 1 (Step 000340): Train loss 5.050, Val loss 5.224\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2243\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.911, Val loss 8.872\n",
      "Ep 1 (Step 000010): Train loss 7.393, Val loss 7.411\n",
      "Ep 1 (Step 000020): Train loss 6.780, Val loss 6.766\n",
      "Ep 1 (Step 000030): Train loss 6.501, Val loss 6.445\n",
      "Ep 1 (Step 000040): Train loss 6.357, Val loss 6.344\n",
      "Ep 1 (Step 000050): Train loss 6.283, Val loss 6.258\n",
      "Ep 1 (Step 000060): Train loss 6.130, Val loss 6.135\n",
      "Ep 1 (Step 000070): Train loss 6.008, Val loss 6.026\n",
      "Ep 1 (Step 000080): Train loss 5.889, Val loss 5.944\n",
      "Ep 1 (Step 000090): Train loss 5.842, Val loss 5.888\n",
      "Ep 1 (Step 000100): Train loss 5.810, Val loss 5.835\n",
      "Ep 1 (Step 000110): Train loss 5.757, Val loss 5.792\n",
      "Ep 1 (Step 000120): Train loss 5.707, Val loss 5.758\n",
      "Ep 1 (Step 000130): Train loss 5.730, Val loss 5.701\n",
      "Ep 1 (Step 000140): Train loss 5.656, Val loss 5.668\n",
      "Ep 1 (Step 000150): Train loss 5.644, Val loss 5.632\n",
      "Ep 1 (Step 000160): Train loss 5.506, Val loss 5.615\n",
      "Ep 1 (Step 000170): Train loss 5.523, Val loss 5.581\n",
      "Ep 1 (Step 000180): Train loss 5.494, Val loss 5.554\n",
      "Ep 1 (Step 000190): Train loss 5.472, Val loss 5.537\n",
      "Ep 1 (Step 000200): Train loss 5.373, Val loss 5.509\n",
      "Ep 1 (Step 000210): Train loss 5.323, Val loss 5.476\n",
      "Ep 1 (Step 000220): Train loss 5.315, Val loss 5.474\n",
      "Ep 1 (Step 000230): Train loss 5.410, Val loss 5.443\n",
      "Ep 1 (Step 000240): Train loss 5.340, Val loss 5.444\n",
      "Ep 1 (Step 000250): Train loss 5.339, Val loss 5.422\n",
      "Ep 1 (Step 000260): Train loss 5.287, Val loss 5.406\n",
      "Ep 1 (Step 000270): Train loss 5.335, Val loss 5.396\n",
      "Ep 1 (Step 000280): Train loss 5.254, Val loss 5.370\n",
      "Ep 1 (Step 000290): Train loss 5.247, Val loss 5.361\n",
      "Ep 1 (Step 000300): Train loss 5.233, Val loss 5.360\n",
      "Ep 1 (Step 000310): Train loss 5.126, Val loss 5.344\n",
      "Ep 1 (Step 000320): Train loss 5.192, Val loss 5.336\n",
      "Ep 1 (Step 000330): Train loss 5.189, Val loss 5.319\n",
      "Ep 1 (Step 000340): Train loss 5.205, Val loss 5.302\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3018\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.900, Val loss 8.869\n",
      "Ep 1 (Step 000010): Train loss 7.407, Val loss 7.400\n",
      "Ep 1 (Step 000020): Train loss 6.807, Val loss 6.754\n",
      "Ep 1 (Step 000030): Train loss 6.488, Val loss 6.449\n",
      "Ep 1 (Step 000040): Train loss 6.340, Val loss 6.363\n",
      "Ep 1 (Step 000050): Train loss 6.317, Val loss 6.279\n",
      "Ep 1 (Step 000060): Train loss 6.127, Val loss 6.153\n",
      "Ep 1 (Step 000070): Train loss 6.060, Val loss 6.033\n",
      "Ep 1 (Step 000080): Train loss 5.946, Val loss 5.966\n",
      "Ep 1 (Step 000090): Train loss 5.828, Val loss 5.900\n",
      "Ep 1 (Step 000100): Train loss 5.820, Val loss 5.844\n",
      "Ep 1 (Step 000110): Train loss 5.665, Val loss 5.806\n",
      "Ep 1 (Step 000120): Train loss 5.735, Val loss 5.758\n",
      "Ep 1 (Step 000130): Train loss 5.713, Val loss 5.734\n",
      "Ep 1 (Step 000140): Train loss 5.648, Val loss 5.685\n",
      "Ep 1 (Step 000150): Train loss 5.545, Val loss 5.647\n",
      "Ep 1 (Step 000160): Train loss 5.478, Val loss 5.626\n",
      "Ep 1 (Step 000170): Train loss 5.520, Val loss 5.576\n",
      "Ep 1 (Step 000180): Train loss 5.438, Val loss 5.559\n",
      "Ep 1 (Step 000190): Train loss 5.395, Val loss 5.531\n",
      "Ep 1 (Step 000200): Train loss 5.393, Val loss 5.540\n",
      "Ep 1 (Step 000210): Train loss 5.486, Val loss 5.498\n",
      "Ep 1 (Step 000220): Train loss 5.348, Val loss 5.477\n",
      "Ep 1 (Step 000230): Train loss 5.375, Val loss 5.459\n",
      "Ep 1 (Step 000240): Train loss 5.307, Val loss 5.445\n",
      "Ep 1 (Step 000250): Train loss 5.256, Val loss 5.444\n",
      "Ep 1 (Step 000260): Train loss 5.319, Val loss 5.410\n",
      "Ep 1 (Step 000270): Train loss 5.272, Val loss 5.401\n",
      "Ep 1 (Step 000280): Train loss 5.255, Val loss 5.379\n",
      "Ep 1 (Step 000290): Train loss 5.252, Val loss 5.358\n",
      "Ep 1 (Step 000300): Train loss 5.209, Val loss 5.359\n",
      "Ep 1 (Step 000310): Train loss 5.222, Val loss 5.352\n",
      "Ep 1 (Step 000320): Train loss 5.147, Val loss 5.333\n",
      "Ep 1 (Step 000330): Train loss 5.253, Val loss 5.316\n",
      "Ep 1 (Step 000340): Train loss 5.139, Val loss 5.307\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3066\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.862, Val loss 8.827\n",
      "Ep 1 (Step 000010): Train loss 7.391, Val loss 7.377\n",
      "Ep 1 (Step 000020): Train loss 6.798, Val loss 6.736\n",
      "Ep 1 (Step 000030): Train loss 6.463, Val loss 6.450\n",
      "Ep 1 (Step 000040): Train loss 6.339, Val loss 6.351\n",
      "Ep 1 (Step 000050): Train loss 6.308, Val loss 6.270\n",
      "Ep 1 (Step 000060): Train loss 6.126, Val loss 6.140\n",
      "Ep 1 (Step 000070): Train loss 6.105, Val loss 6.048\n",
      "Ep 1 (Step 000080): Train loss 5.956, Val loss 5.980\n",
      "Ep 1 (Step 000090): Train loss 5.848, Val loss 5.910\n",
      "Ep 1 (Step 000100): Train loss 5.837, Val loss 5.848\n",
      "Ep 1 (Step 000110): Train loss 5.720, Val loss 5.776\n",
      "Ep 1 (Step 000120): Train loss 5.647, Val loss 5.753\n",
      "Ep 1 (Step 000130): Train loss 5.692, Val loss 5.689\n",
      "Ep 1 (Step 000140): Train loss 5.627, Val loss 5.666\n",
      "Ep 1 (Step 000150): Train loss 5.545, Val loss 5.622\n",
      "Ep 1 (Step 000160): Train loss 5.553, Val loss 5.585\n",
      "Ep 1 (Step 000170): Train loss 5.482, Val loss 5.564\n",
      "Ep 1 (Step 000180): Train loss 5.517, Val loss 5.552\n",
      "Ep 1 (Step 000190): Train loss 5.371, Val loss 5.514\n",
      "Ep 1 (Step 000200): Train loss 5.433, Val loss 5.496\n",
      "Ep 1 (Step 000210): Train loss 5.394, Val loss 5.466\n",
      "Ep 1 (Step 000220): Train loss 5.443, Val loss 5.460\n",
      "Ep 1 (Step 000230): Train loss 5.354, Val loss 5.438\n",
      "Ep 1 (Step 000240): Train loss 5.339, Val loss 5.435\n",
      "Ep 1 (Step 000250): Train loss 5.274, Val loss 5.421\n",
      "Ep 1 (Step 000260): Train loss 5.253, Val loss 5.415\n",
      "Ep 1 (Step 000270): Train loss 5.374, Val loss 5.391\n",
      "Ep 1 (Step 000280): Train loss 5.300, Val loss 5.373\n",
      "Ep 1 (Step 000290): Train loss 5.169, Val loss 5.369\n",
      "Ep 1 (Step 000300): Train loss 5.212, Val loss 5.359\n",
      "Ep 1 (Step 000310): Train loss 5.269, Val loss 5.347\n",
      "Ep 1 (Step 000320): Train loss 5.236, Val loss 5.352\n",
      "Ep 1 (Step 000330): Train loss 5.212, Val loss 5.340\n",
      "Ep 1 (Step 000340): Train loss 5.194, Val loss 5.332\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3323\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.900, Val loss 8.873\n",
      "Ep 1 (Step 000010): Train loss 7.419, Val loss 7.375\n",
      "Ep 1 (Step 000020): Train loss 6.774, Val loss 6.736\n",
      "Ep 1 (Step 000030): Train loss 6.481, Val loss 6.447\n",
      "Ep 1 (Step 000040): Train loss 6.402, Val loss 6.374\n",
      "Ep 1 (Step 000050): Train loss 6.309, Val loss 6.320\n",
      "Ep 1 (Step 000060): Train loss 6.165, Val loss 6.187\n",
      "Ep 1 (Step 000070): Train loss 6.047, Val loss 6.083\n",
      "Ep 1 (Step 000080): Train loss 5.993, Val loss 6.029\n",
      "Ep 1 (Step 000090): Train loss 5.855, Val loss 5.934\n",
      "Ep 1 (Step 000100): Train loss 5.761, Val loss 5.869\n",
      "Ep 1 (Step 000110): Train loss 5.813, Val loss 5.806\n",
      "Ep 1 (Step 000120): Train loss 5.791, Val loss 5.772\n",
      "Ep 1 (Step 000130): Train loss 5.629, Val loss 5.734\n",
      "Ep 1 (Step 000140): Train loss 5.636, Val loss 5.676\n",
      "Ep 1 (Step 000150): Train loss 5.587, Val loss 5.641\n",
      "Ep 1 (Step 000160): Train loss 5.589, Val loss 5.626\n",
      "Ep 1 (Step 000170): Train loss 5.543, Val loss 5.586\n",
      "Ep 1 (Step 000180): Train loss 5.526, Val loss 5.566\n",
      "Ep 1 (Step 000190): Train loss 5.471, Val loss 5.533\n",
      "Ep 1 (Step 000200): Train loss 5.425, Val loss 5.510\n",
      "Ep 1 (Step 000210): Train loss 5.418, Val loss 5.488\n",
      "Ep 1 (Step 000220): Train loss 5.353, Val loss 5.472\n",
      "Ep 1 (Step 000230): Train loss 5.439, Val loss 5.440\n",
      "Ep 1 (Step 000240): Train loss 5.346, Val loss 5.420\n",
      "Ep 1 (Step 000250): Train loss 5.313, Val loss 5.400\n",
      "Ep 1 (Step 000260): Train loss 5.304, Val loss 5.391\n",
      "Ep 1 (Step 000270): Train loss 5.288, Val loss 5.362\n",
      "Ep 1 (Step 000280): Train loss 5.207, Val loss 5.364\n",
      "Ep 1 (Step 000290): Train loss 5.196, Val loss 5.340\n",
      "Ep 1 (Step 000300): Train loss 5.266, Val loss 5.330\n",
      "Ep 1 (Step 000310): Train loss 5.228, Val loss 5.334\n",
      "Ep 1 (Step 000320): Train loss 5.177, Val loss 5.329\n",
      "Ep 1 (Step 000330): Train loss 5.216, Val loss 5.310\n",
      "Ep 1 (Step 000340): Train loss 5.215, Val loss 5.294\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2938\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.885, Val loss 8.879\n",
      "Ep 1 (Step 000010): Train loss 7.476, Val loss 7.413\n",
      "Ep 1 (Step 000020): Train loss 6.796, Val loss 6.764\n",
      "Ep 1 (Step 000030): Train loss 6.473, Val loss 6.437\n",
      "Ep 1 (Step 000040): Train loss 6.360, Val loss 6.341\n",
      "Ep 1 (Step 000050): Train loss 6.297, Val loss 6.262\n",
      "Ep 1 (Step 000060): Train loss 6.176, Val loss 6.148\n",
      "Ep 1 (Step 000070): Train loss 6.029, Val loss 6.036\n",
      "Ep 1 (Step 000080): Train loss 5.995, Val loss 5.952\n",
      "Ep 1 (Step 000090): Train loss 5.879, Val loss 5.896\n",
      "Ep 1 (Step 000100): Train loss 5.795, Val loss 5.836\n",
      "Ep 1 (Step 000110): Train loss 5.731, Val loss 5.766\n",
      "Ep 1 (Step 000120): Train loss 5.727, Val loss 5.739\n",
      "Ep 1 (Step 000130): Train loss 5.676, Val loss 5.695\n",
      "Ep 1 (Step 000140): Train loss 5.606, Val loss 5.649\n",
      "Ep 1 (Step 000150): Train loss 5.622, Val loss 5.623\n",
      "Ep 1 (Step 000160): Train loss 5.552, Val loss 5.601\n",
      "Ep 1 (Step 000170): Train loss 5.491, Val loss 5.562\n",
      "Ep 1 (Step 000180): Train loss 5.492, Val loss 5.549\n",
      "Ep 1 (Step 000190): Train loss 5.421, Val loss 5.526\n",
      "Ep 1 (Step 000200): Train loss 5.357, Val loss 5.500\n",
      "Ep 1 (Step 000210): Train loss 5.364, Val loss 5.476\n",
      "Ep 1 (Step 000220): Train loss 5.350, Val loss 5.453\n",
      "Ep 1 (Step 000230): Train loss 5.353, Val loss 5.440\n",
      "Ep 1 (Step 000240): Train loss 5.324, Val loss 5.429\n",
      "Ep 1 (Step 000250): Train loss 5.292, Val loss 5.401\n",
      "Ep 1 (Step 000260): Train loss 5.196, Val loss 5.385\n",
      "Ep 1 (Step 000270): Train loss 5.228, Val loss 5.382\n",
      "Ep 1 (Step 000280): Train loss 5.210, Val loss 5.373\n",
      "Ep 1 (Step 000290): Train loss 5.249, Val loss 5.355\n",
      "Ep 1 (Step 000300): Train loss 5.173, Val loss 5.353\n",
      "Ep 1 (Step 000310): Train loss 5.199, Val loss 5.348\n",
      "Ep 1 (Step 000320): Train loss 5.218, Val loss 5.335\n",
      "Ep 1 (Step 000330): Train loss 5.198, Val loss 5.303\n",
      "Ep 1 (Step 000340): Train loss 5.123, Val loss 5.287\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2869\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.942, Val loss 8.926\n",
      "Ep 1 (Step 000010): Train loss 7.414, Val loss 7.436\n",
      "Ep 1 (Step 000020): Train loss 6.826, Val loss 6.775\n",
      "Ep 1 (Step 000030): Train loss 6.480, Val loss 6.463\n",
      "Ep 1 (Step 000040): Train loss 6.384, Val loss 6.365\n",
      "Ep 1 (Step 000050): Train loss 6.244, Val loss 6.278\n",
      "Ep 1 (Step 000060): Train loss 6.201, Val loss 6.166\n",
      "Ep 1 (Step 000070): Train loss 5.968, Val loss 6.037\n",
      "Ep 1 (Step 000080): Train loss 5.871, Val loss 5.957\n",
      "Ep 1 (Step 000090): Train loss 5.837, Val loss 5.894\n",
      "Ep 1 (Step 000100): Train loss 5.768, Val loss 5.823\n",
      "Ep 1 (Step 000110): Train loss 5.713, Val loss 5.791\n",
      "Ep 1 (Step 000120): Train loss 5.706, Val loss 5.730\n",
      "Ep 1 (Step 000130): Train loss 5.641, Val loss 5.684\n",
      "Ep 1 (Step 000140): Train loss 5.653, Val loss 5.649\n",
      "Ep 1 (Step 000150): Train loss 5.515, Val loss 5.626\n",
      "Ep 1 (Step 000160): Train loss 5.474, Val loss 5.587\n",
      "Ep 1 (Step 000170): Train loss 5.476, Val loss 5.564\n",
      "Ep 1 (Step 000180): Train loss 5.496, Val loss 5.539\n",
      "Ep 1 (Step 000190): Train loss 5.474, Val loss 5.509\n",
      "Ep 1 (Step 000200): Train loss 5.421, Val loss 5.507\n",
      "Ep 1 (Step 000210): Train loss 5.367, Val loss 5.470\n",
      "Ep 1 (Step 000220): Train loss 5.352, Val loss 5.456\n",
      "Ep 1 (Step 000230): Train loss 5.380, Val loss 5.431\n",
      "Ep 1 (Step 000240): Train loss 5.316, Val loss 5.415\n",
      "Ep 1 (Step 000250): Train loss 5.310, Val loss 5.401\n",
      "Ep 1 (Step 000260): Train loss 5.299, Val loss 5.376\n",
      "Ep 1 (Step 000270): Train loss 5.211, Val loss 5.370\n",
      "Ep 1 (Step 000280): Train loss 5.235, Val loss 5.353\n",
      "Ep 1 (Step 000290): Train loss 5.236, Val loss 5.327\n",
      "Ep 1 (Step 000300): Train loss 5.242, Val loss 5.330\n",
      "Ep 1 (Step 000310): Train loss 5.137, Val loss 5.306\n",
      "Ep 1 (Step 000320): Train loss 5.244, Val loss 5.298\n",
      "Ep 1 (Step 000330): Train loss 5.163, Val loss 5.284\n",
      "Ep 1 (Step 000340): Train loss 5.143, Val loss 5.275\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2755\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.575, Val loss 8.534\n",
      "Ep 1 (Step 000010): Train loss 6.773, Val loss 6.758\n",
      "Ep 1 (Step 000020): Train loss 6.448, Val loss 6.436\n",
      "Ep 1 (Step 000030): Train loss 6.359, Val loss 6.365\n",
      "Ep 1 (Step 000040): Train loss 6.715, Val loss 6.640\n",
      "Ep 1 (Step 000050): Train loss 6.092, Val loss 6.090\n",
      "Ep 1 (Step 000060): Train loss 5.952, Val loss 5.990\n",
      "Ep 1 (Step 000070): Train loss 5.891, Val loss 5.873\n",
      "Ep 1 (Step 000080): Train loss 5.767, Val loss 5.808\n",
      "Ep 1 (Step 000090): Train loss 5.700, Val loss 5.744\n",
      "Ep 1 (Step 000100): Train loss 5.606, Val loss 5.710\n",
      "Ep 1 (Step 000110): Train loss 5.508, Val loss 5.649\n",
      "Ep 1 (Step 000120): Train loss 5.506, Val loss 5.617\n",
      "Ep 1 (Step 000130): Train loss 5.549, Val loss 5.597\n",
      "Ep 1 (Step 000140): Train loss 5.471, Val loss 5.567\n",
      "Ep 1 (Step 000150): Train loss 5.488, Val loss 5.524\n",
      "Ep 1 (Step 000160): Train loss 5.417, Val loss 5.495\n",
      "Ep 1 (Step 000170): Train loss 5.412, Val loss 5.481\n",
      "Ep 1 (Step 000180): Train loss 5.274, Val loss 5.461\n",
      "Ep 1 (Step 000190): Train loss 5.344, Val loss 5.437\n",
      "Ep 1 (Step 000200): Train loss 5.302, Val loss 5.418\n",
      "Ep 1 (Step 000210): Train loss 5.239, Val loss 5.397\n",
      "Ep 1 (Step 000220): Train loss 5.286, Val loss 5.396\n",
      "Ep 1 (Step 000230): Train loss 5.231, Val loss 5.379\n",
      "Ep 1 (Step 000240): Train loss 5.199, Val loss 5.343\n",
      "Ep 1 (Step 000250): Train loss 5.212, Val loss 5.319\n",
      "Ep 1 (Step 000260): Train loss 5.319, Val loss 5.332\n",
      "Ep 1 (Step 000270): Train loss 5.149, Val loss 5.326\n",
      "Ep 1 (Step 000280): Train loss 5.085, Val loss 5.298\n",
      "Ep 1 (Step 000290): Train loss 5.170, Val loss 5.295\n",
      "Ep 1 (Step 000300): Train loss 5.096, Val loss 5.295\n",
      "Ep 1 (Step 000310): Train loss 5.089, Val loss 5.294\n",
      "Ep 1 (Step 000320): Train loss 5.155, Val loss 5.280\n",
      "Ep 1 (Step 000330): Train loss 5.040, Val loss 5.254\n",
      "Ep 1 (Step 000340): Train loss 5.042, Val loss 5.245\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2445\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.591, Val loss 8.597\n",
      "Ep 1 (Step 000010): Train loss 6.879, Val loss 6.836\n",
      "Ep 1 (Step 000020): Train loss 6.497, Val loss 6.449\n",
      "Ep 1 (Step 000030): Train loss 6.375, Val loss 6.430\n",
      "Ep 1 (Step 000040): Train loss 6.388, Val loss 6.295\n",
      "Ep 1 (Step 000050): Train loss 6.152, Val loss 6.175\n",
      "Ep 1 (Step 000060): Train loss 5.967, Val loss 6.061\n",
      "Ep 1 (Step 000070): Train loss 6.022, Val loss 5.957\n",
      "Ep 1 (Step 000080): Train loss 5.799, Val loss 5.866\n",
      "Ep 1 (Step 000090): Train loss 5.788, Val loss 5.789\n",
      "Ep 1 (Step 000100): Train loss 5.724, Val loss 5.738\n",
      "Ep 1 (Step 000110): Train loss 5.653, Val loss 5.674\n",
      "Ep 1 (Step 000120): Train loss 5.605, Val loss 5.643\n",
      "Ep 1 (Step 000130): Train loss 5.581, Val loss 5.610\n",
      "Ep 1 (Step 000140): Train loss 5.455, Val loss 5.569\n",
      "Ep 1 (Step 000150): Train loss 5.486, Val loss 5.533\n",
      "Ep 1 (Step 000160): Train loss 5.518, Val loss 5.512\n",
      "Ep 1 (Step 000170): Train loss 5.365, Val loss 5.499\n",
      "Ep 1 (Step 000180): Train loss 5.362, Val loss 5.478\n",
      "Ep 1 (Step 000190): Train loss 5.396, Val loss 5.435\n",
      "Ep 1 (Step 000200): Train loss 5.340, Val loss 5.426\n",
      "Ep 1 (Step 000210): Train loss 5.294, Val loss 5.406\n",
      "Ep 1 (Step 000220): Train loss 5.268, Val loss 5.380\n",
      "Ep 1 (Step 000230): Train loss 5.246, Val loss 5.364\n",
      "Ep 1 (Step 000240): Train loss 5.243, Val loss 5.353\n",
      "Ep 1 (Step 000250): Train loss 5.299, Val loss 5.344\n",
      "Ep 1 (Step 000260): Train loss 5.212, Val loss 5.334\n",
      "Ep 1 (Step 000270): Train loss 5.275, Val loss 5.315\n",
      "Ep 1 (Step 000280): Train loss 5.228, Val loss 5.309\n",
      "Ep 1 (Step 000290): Train loss 5.122, Val loss 5.284\n",
      "Ep 1 (Step 000300): Train loss 5.127, Val loss 5.263\n",
      "Ep 1 (Step 000310): Train loss 5.125, Val loss 5.261\n",
      "Ep 1 (Step 000320): Train loss 5.155, Val loss 5.252\n",
      "Ep 1 (Step 000330): Train loss 5.119, Val loss 5.236\n",
      "Ep 1 (Step 000340): Train loss 5.163, Val loss 5.242\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2424\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.657, Val loss 8.645\n",
      "Ep 1 (Step 000010): Train loss 6.828, Val loss 6.851\n",
      "Ep 1 (Step 000020): Train loss 6.453, Val loss 6.440\n",
      "Ep 1 (Step 000030): Train loss 6.360, Val loss 6.396\n",
      "Ep 1 (Step 000040): Train loss 6.168, Val loss 6.180\n",
      "Ep 1 (Step 000050): Train loss 6.008, Val loss 6.043\n",
      "Ep 1 (Step 000060): Train loss 5.902, Val loss 5.928\n",
      "Ep 1 (Step 000070): Train loss 5.770, Val loss 5.853\n",
      "Ep 1 (Step 000080): Train loss 5.709, Val loss 5.798\n",
      "Ep 1 (Step 000090): Train loss 5.648, Val loss 5.740\n",
      "Ep 1 (Step 000100): Train loss 5.706, Val loss 5.683\n",
      "Ep 1 (Step 000110): Train loss 5.613, Val loss 5.626\n",
      "Ep 1 (Step 000120): Train loss 5.441, Val loss 5.583\n",
      "Ep 1 (Step 000130): Train loss 5.423, Val loss 5.556\n",
      "Ep 1 (Step 000140): Train loss 5.419, Val loss 5.508\n",
      "Ep 1 (Step 000150): Train loss 5.473, Val loss 5.497\n",
      "Ep 1 (Step 000160): Train loss 5.346, Val loss 5.474\n",
      "Ep 1 (Step 000170): Train loss 5.395, Val loss 5.458\n",
      "Ep 1 (Step 000180): Train loss 5.256, Val loss 5.417\n",
      "Ep 1 (Step 000190): Train loss 5.240, Val loss 5.410\n",
      "Ep 1 (Step 000200): Train loss 5.393, Val loss 5.409\n",
      "Ep 1 (Step 000210): Train loss 5.286, Val loss 5.371\n",
      "Ep 1 (Step 000220): Train loss 5.263, Val loss 5.378\n",
      "Ep 1 (Step 000230): Train loss 5.236, Val loss 5.354\n",
      "Ep 1 (Step 000240): Train loss 5.255, Val loss 5.353\n",
      "Ep 1 (Step 000250): Train loss 5.159, Val loss 5.346\n",
      "Ep 1 (Step 000260): Train loss 5.177, Val loss 5.317\n",
      "Ep 1 (Step 000270): Train loss 5.147, Val loss 5.310\n",
      "Ep 1 (Step 000280): Train loss 5.145, Val loss 5.287\n",
      "Ep 1 (Step 000290): Train loss 5.122, Val loss 5.278\n",
      "Ep 1 (Step 000300): Train loss 5.228, Val loss 5.273\n",
      "Ep 1 (Step 000310): Train loss 5.134, Val loss 5.255\n",
      "Ep 1 (Step 000320): Train loss 5.042, Val loss 5.223\n",
      "Ep 1 (Step 000330): Train loss 5.040, Val loss 5.213\n",
      "Ep 1 (Step 000340): Train loss 5.188, Val loss 5.232\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2325\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.665, Val loss 8.665\n",
      "Ep 1 (Step 000010): Train loss 6.855, Val loss 6.822\n",
      "Ep 1 (Step 000020): Train loss 6.490, Val loss 6.457\n",
      "Ep 1 (Step 000030): Train loss 6.393, Val loss 6.382\n",
      "Ep 1 (Step 000040): Train loss 6.202, Val loss 6.211\n",
      "Ep 1 (Step 000050): Train loss 6.073, Val loss 6.084\n",
      "Ep 1 (Step 000060): Train loss 5.928, Val loss 5.958\n",
      "Ep 1 (Step 000070): Train loss 5.804, Val loss 5.872\n",
      "Ep 1 (Step 000080): Train loss 5.774, Val loss 5.796\n",
      "Ep 1 (Step 000090): Train loss 5.590, Val loss 5.733\n",
      "Ep 1 (Step 000100): Train loss 5.654, Val loss 5.693\n",
      "Ep 1 (Step 000110): Train loss 5.553, Val loss 5.639\n",
      "Ep 1 (Step 000120): Train loss 5.505, Val loss 5.592\n",
      "Ep 1 (Step 000130): Train loss 5.466, Val loss 5.560\n",
      "Ep 1 (Step 000140): Train loss 5.414, Val loss 5.518\n",
      "Ep 1 (Step 000150): Train loss 5.399, Val loss 5.501\n",
      "Ep 1 (Step 000160): Train loss 5.431, Val loss 5.469\n",
      "Ep 1 (Step 000170): Train loss 5.328, Val loss 5.448\n",
      "Ep 1 (Step 000180): Train loss 5.325, Val loss 5.429\n",
      "Ep 1 (Step 000190): Train loss 5.308, Val loss 5.410\n",
      "Ep 1 (Step 000200): Train loss 5.261, Val loss 5.384\n",
      "Ep 1 (Step 000210): Train loss 5.252, Val loss 5.344\n",
      "Ep 1 (Step 000220): Train loss 5.210, Val loss 5.337\n",
      "Ep 1 (Step 000230): Train loss 5.232, Val loss 5.328\n",
      "Ep 1 (Step 000240): Train loss 5.289, Val loss 5.310\n",
      "Ep 1 (Step 000250): Train loss 5.166, Val loss 5.295\n",
      "Ep 1 (Step 000260): Train loss 5.235, Val loss 5.310\n",
      "Ep 1 (Step 000270): Train loss 5.087, Val loss 5.279\n",
      "Ep 1 (Step 000280): Train loss 5.187, Val loss 5.281\n",
      "Ep 1 (Step 000290): Train loss 5.094, Val loss 5.264\n",
      "Ep 1 (Step 000300): Train loss 5.171, Val loss 5.258\n",
      "Ep 1 (Step 000310): Train loss 5.116, Val loss 5.255\n",
      "Ep 1 (Step 000320): Train loss 5.207, Val loss 5.250\n",
      "Ep 1 (Step 000330): Train loss 5.032, Val loss 5.224\n",
      "Ep 1 (Step 000340): Train loss 5.020, Val loss 5.206\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2061\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.571, Val loss 8.533\n",
      "Ep 1 (Step 000010): Train loss 6.848, Val loss 6.809\n",
      "Ep 1 (Step 000020): Train loss 6.459, Val loss 6.457\n",
      "Ep 1 (Step 000030): Train loss 6.417, Val loss 6.379\n",
      "Ep 1 (Step 000040): Train loss 6.204, Val loss 6.235\n",
      "Ep 1 (Step 000050): Train loss 6.077, Val loss 6.075\n",
      "Ep 1 (Step 000060): Train loss 5.924, Val loss 5.939\n",
      "Ep 1 (Step 000070): Train loss 5.835, Val loss 5.869\n",
      "Ep 1 (Step 000080): Train loss 5.755, Val loss 5.807\n",
      "Ep 1 (Step 000090): Train loss 5.645, Val loss 5.748\n",
      "Ep 1 (Step 000100): Train loss 5.589, Val loss 5.689\n",
      "Ep 1 (Step 000110): Train loss 5.496, Val loss 5.616\n",
      "Ep 1 (Step 000120): Train loss 5.486, Val loss 5.560\n",
      "Ep 1 (Step 000130): Train loss 5.519, Val loss 5.535\n",
      "Ep 1 (Step 000140): Train loss 5.494, Val loss 5.493\n",
      "Ep 1 (Step 000150): Train loss 5.436, Val loss 5.462\n",
      "Ep 1 (Step 000160): Train loss 5.398, Val loss 5.446\n",
      "Ep 1 (Step 000170): Train loss 5.408, Val loss 5.421\n",
      "Ep 1 (Step 000180): Train loss 5.396, Val loss 5.411\n",
      "Ep 1 (Step 000190): Train loss 5.318, Val loss 5.386\n",
      "Ep 1 (Step 000200): Train loss 5.166, Val loss 5.375\n",
      "Ep 1 (Step 000210): Train loss 5.261, Val loss 5.363\n",
      "Ep 1 (Step 000220): Train loss 5.221, Val loss 5.350\n",
      "Ep 1 (Step 000230): Train loss 5.210, Val loss 5.333\n",
      "Ep 1 (Step 000240): Train loss 5.200, Val loss 5.314\n",
      "Ep 1 (Step 000250): Train loss 5.203, Val loss 5.301\n",
      "Ep 1 (Step 000260): Train loss 5.166, Val loss 5.283\n",
      "Ep 1 (Step 000270): Train loss 5.207, Val loss 5.280\n",
      "Ep 1 (Step 000280): Train loss 5.144, Val loss 5.260\n",
      "Ep 1 (Step 000290): Train loss 5.075, Val loss 5.256\n",
      "Ep 1 (Step 000300): Train loss 5.072, Val loss 5.236\n",
      "Ep 1 (Step 000310): Train loss 5.151, Val loss 5.215\n",
      "Ep 1 (Step 000320): Train loss 5.035, Val loss 5.200\n",
      "Ep 1 (Step 000330): Train loss 5.093, Val loss 5.200\n",
      "Ep 1 (Step 000340): Train loss 5.042, Val loss 5.187\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1874\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.645, Val loss 8.632\n",
      "Ep 1 (Step 000010): Train loss 6.855, Val loss 6.814\n",
      "Ep 1 (Step 000020): Train loss 6.480, Val loss 6.474\n",
      "Ep 1 (Step 000030): Train loss 6.389, Val loss 6.370\n",
      "Ep 1 (Step 000040): Train loss 6.143, Val loss 6.180\n",
      "Ep 1 (Step 000050): Train loss 5.976, Val loss 6.045\n",
      "Ep 1 (Step 000060): Train loss 5.874, Val loss 5.927\n",
      "Ep 1 (Step 000070): Train loss 5.736, Val loss 5.854\n",
      "Ep 1 (Step 000080): Train loss 5.741, Val loss 5.763\n",
      "Ep 1 (Step 000090): Train loss 5.797, Val loss 5.695\n",
      "Ep 1 (Step 000100): Train loss 5.604, Val loss 5.653\n",
      "Ep 1 (Step 000110): Train loss 5.609, Val loss 5.596\n",
      "Ep 1 (Step 000120): Train loss 5.489, Val loss 5.559\n",
      "Ep 1 (Step 000130): Train loss 5.444, Val loss 5.531\n",
      "Ep 1 (Step 000140): Train loss 5.403, Val loss 5.505\n",
      "Ep 1 (Step 000150): Train loss 5.408, Val loss 5.481\n",
      "Ep 1 (Step 000160): Train loss 5.399, Val loss 5.434\n",
      "Ep 1 (Step 000170): Train loss 5.400, Val loss 5.421\n",
      "Ep 1 (Step 000180): Train loss 5.343, Val loss 5.389\n",
      "Ep 1 (Step 000190): Train loss 5.402, Val loss 5.384\n",
      "Ep 1 (Step 000200): Train loss 5.316, Val loss 5.348\n",
      "Ep 1 (Step 000210): Train loss 5.263, Val loss 5.328\n",
      "Ep 1 (Step 000220): Train loss 5.158, Val loss 5.308\n",
      "Ep 1 (Step 000230): Train loss 5.208, Val loss 5.298\n",
      "Ep 1 (Step 000240): Train loss 5.234, Val loss 5.291\n",
      "Ep 1 (Step 000250): Train loss 5.229, Val loss 5.283\n",
      "Ep 1 (Step 000260): Train loss 5.202, Val loss 5.250\n",
      "Ep 1 (Step 000270): Train loss 5.140, Val loss 5.263\n",
      "Ep 1 (Step 000280): Train loss 5.115, Val loss 5.232\n",
      "Ep 1 (Step 000290): Train loss 5.142, Val loss 5.224\n",
      "Ep 1 (Step 000300): Train loss 5.079, Val loss 5.206\n",
      "Ep 1 (Step 000310): Train loss 5.024, Val loss 5.210\n",
      "Ep 1 (Step 000320): Train loss 5.091, Val loss 5.210\n",
      "Ep 1 (Step 000330): Train loss 5.021, Val loss 5.179\n",
      "Ep 1 (Step 000340): Train loss 5.100, Val loss 5.175\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1753\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.834, Val loss 8.810\n",
      "Ep 1 (Step 000010): Train loss 7.409, Val loss 7.369\n",
      "Ep 1 (Step 000020): Train loss 6.767, Val loss 6.734\n",
      "Ep 1 (Step 000030): Train loss 6.458, Val loss 6.436\n",
      "Ep 1 (Step 000040): Train loss 6.389, Val loss 6.366\n",
      "Ep 1 (Step 000050): Train loss 6.288, Val loss 6.302\n",
      "Ep 1 (Step 000060): Train loss 6.257, Val loss 6.209\n",
      "Ep 1 (Step 000070): Train loss 6.090, Val loss 6.109\n",
      "Ep 1 (Step 000080): Train loss 5.987, Val loss 5.989\n",
      "Ep 1 (Step 000090): Train loss 5.903, Val loss 5.929\n",
      "Ep 1 (Step 000100): Train loss 5.849, Val loss 5.890\n",
      "Ep 1 (Step 000110): Train loss 5.792, Val loss 5.837\n",
      "Ep 1 (Step 000120): Train loss 5.679, Val loss 5.786\n",
      "Ep 1 (Step 000130): Train loss 5.771, Val loss 5.734\n",
      "Ep 1 (Step 000140): Train loss 5.643, Val loss 5.711\n",
      "Ep 1 (Step 000150): Train loss 5.676, Val loss 5.672\n",
      "Ep 1 (Step 000160): Train loss 5.657, Val loss 5.647\n",
      "Ep 1 (Step 000170): Train loss 5.527, Val loss 5.613\n",
      "Ep 1 (Step 000180): Train loss 5.557, Val loss 5.584\n",
      "Ep 1 (Step 000190): Train loss 5.514, Val loss 5.566\n",
      "Ep 1 (Step 000200): Train loss 5.443, Val loss 5.526\n",
      "Ep 1 (Step 000210): Train loss 5.453, Val loss 5.524\n",
      "Ep 1 (Step 000220): Train loss 5.418, Val loss 5.491\n",
      "Ep 1 (Step 000230): Train loss 5.386, Val loss 5.464\n",
      "Ep 1 (Step 000240): Train loss 5.311, Val loss 5.458\n",
      "Ep 1 (Step 000250): Train loss 5.412, Val loss 5.442\n",
      "Ep 1 (Step 000260): Train loss 5.316, Val loss 5.431\n",
      "Ep 1 (Step 000270): Train loss 5.388, Val loss 5.419\n",
      "Ep 1 (Step 000280): Train loss 5.372, Val loss 5.408\n",
      "Ep 1 (Step 000290): Train loss 5.358, Val loss 5.392\n",
      "Ep 1 (Step 000300): Train loss 5.256, Val loss 5.389\n",
      "Ep 1 (Step 000310): Train loss 5.351, Val loss 5.372\n",
      "Ep 1 (Step 000320): Train loss 5.256, Val loss 5.356\n",
      "Ep 1 (Step 000330): Train loss 5.259, Val loss 5.350\n",
      "Ep 1 (Step 000340): Train loss 5.239, Val loss 5.340\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3400\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.738, Val loss 8.743\n",
      "Ep 1 (Step 000010): Train loss 7.376, Val loss 7.317\n",
      "Ep 1 (Step 000020): Train loss 6.729, Val loss 6.703\n",
      "Ep 1 (Step 000030): Train loss 6.431, Val loss 6.425\n",
      "Ep 1 (Step 000040): Train loss 6.416, Val loss 6.351\n",
      "Ep 1 (Step 000050): Train loss 6.331, Val loss 6.261\n",
      "Ep 1 (Step 000060): Train loss 6.150, Val loss 6.137\n",
      "Ep 1 (Step 000070): Train loss 6.049, Val loss 6.057\n",
      "Ep 1 (Step 000080): Train loss 6.003, Val loss 5.989\n",
      "Ep 1 (Step 000090): Train loss 5.943, Val loss 5.934\n",
      "Ep 1 (Step 000100): Train loss 5.823, Val loss 5.877\n",
      "Ep 1 (Step 000110): Train loss 5.834, Val loss 5.838\n",
      "Ep 1 (Step 000120): Train loss 5.691, Val loss 5.787\n",
      "Ep 1 (Step 000130): Train loss 5.671, Val loss 5.750\n",
      "Ep 1 (Step 000140): Train loss 5.668, Val loss 5.701\n",
      "Ep 1 (Step 000150): Train loss 5.623, Val loss 5.670\n",
      "Ep 1 (Step 000160): Train loss 5.605, Val loss 5.648\n",
      "Ep 1 (Step 000170): Train loss 5.533, Val loss 5.633\n",
      "Ep 1 (Step 000180): Train loss 5.556, Val loss 5.588\n",
      "Ep 1 (Step 000190): Train loss 5.408, Val loss 5.566\n",
      "Ep 1 (Step 000200): Train loss 5.438, Val loss 5.537\n",
      "Ep 1 (Step 000210): Train loss 5.469, Val loss 5.527\n",
      "Ep 1 (Step 000220): Train loss 5.341, Val loss 5.504\n",
      "Ep 1 (Step 000230): Train loss 5.397, Val loss 5.485\n",
      "Ep 1 (Step 000240): Train loss 5.398, Val loss 5.467\n",
      "Ep 1 (Step 000250): Train loss 5.313, Val loss 5.461\n",
      "Ep 1 (Step 000260): Train loss 5.428, Val loss 5.434\n",
      "Ep 1 (Step 000270): Train loss 5.273, Val loss 5.419\n",
      "Ep 1 (Step 000280): Train loss 5.287, Val loss 5.414\n",
      "Ep 1 (Step 000290): Train loss 5.317, Val loss 5.408\n",
      "Ep 1 (Step 000300): Train loss 5.280, Val loss 5.386\n",
      "Ep 1 (Step 000310): Train loss 5.230, Val loss 5.379\n",
      "Ep 1 (Step 000320): Train loss 5.243, Val loss 5.362\n",
      "Ep 1 (Step 000330): Train loss 5.239, Val loss 5.353\n",
      "Ep 1 (Step 000340): Train loss 5.229, Val loss 5.342\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3420\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.806, Val loss 8.796\n",
      "Ep 1 (Step 000010): Train loss 7.397, Val loss 7.372\n",
      "Ep 1 (Step 000020): Train loss 6.748, Val loss 6.735\n",
      "Ep 1 (Step 000030): Train loss 6.407, Val loss 6.452\n",
      "Ep 1 (Step 000040): Train loss 6.397, Val loss 6.366\n",
      "Ep 1 (Step 000050): Train loss 6.270, Val loss 6.316\n",
      "Ep 1 (Step 000060): Train loss 6.164, Val loss 6.189\n",
      "Ep 1 (Step 000070): Train loss 6.109, Val loss 6.078\n",
      "Ep 1 (Step 000080): Train loss 5.967, Val loss 5.995\n",
      "Ep 1 (Step 000090): Train loss 5.964, Val loss 5.976\n",
      "Ep 1 (Step 000100): Train loss 5.892, Val loss 5.875\n",
      "Ep 1 (Step 000110): Train loss 5.765, Val loss 5.820\n",
      "Ep 1 (Step 000120): Train loss 5.683, Val loss 5.783\n",
      "Ep 1 (Step 000130): Train loss 5.673, Val loss 5.727\n",
      "Ep 1 (Step 000140): Train loss 5.649, Val loss 5.703\n",
      "Ep 1 (Step 000150): Train loss 5.667, Val loss 5.686\n",
      "Ep 1 (Step 000160): Train loss 5.591, Val loss 5.644\n",
      "Ep 1 (Step 000170): Train loss 5.606, Val loss 5.612\n",
      "Ep 1 (Step 000180): Train loss 5.546, Val loss 5.589\n",
      "Ep 1 (Step 000190): Train loss 5.443, Val loss 5.569\n",
      "Ep 1 (Step 000200): Train loss 5.483, Val loss 5.556\n",
      "Ep 1 (Step 000210): Train loss 5.473, Val loss 5.525\n",
      "Ep 1 (Step 000220): Train loss 5.428, Val loss 5.492\n",
      "Ep 1 (Step 000230): Train loss 5.407, Val loss 5.495\n",
      "Ep 1 (Step 000240): Train loss 5.374, Val loss 5.496\n",
      "Ep 1 (Step 000250): Train loss 5.343, Val loss 5.473\n",
      "Ep 1 (Step 000260): Train loss 5.327, Val loss 5.477\n",
      "Ep 1 (Step 000270): Train loss 5.384, Val loss 5.463\n",
      "Ep 1 (Step 000280): Train loss 5.354, Val loss 5.427\n",
      "Ep 1 (Step 000290): Train loss 5.230, Val loss 5.407\n",
      "Ep 1 (Step 000300): Train loss 5.198, Val loss 5.393\n",
      "Ep 1 (Step 000310): Train loss 5.262, Val loss 5.383\n",
      "Ep 1 (Step 000320): Train loss 5.276, Val loss 5.373\n",
      "Ep 1 (Step 000330): Train loss 5.240, Val loss 5.358\n",
      "Ep 1 (Step 000340): Train loss 5.281, Val loss 5.374\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3740\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.828, Val loss 8.805\n",
      "Ep 1 (Step 000010): Train loss 7.427, Val loss 7.406\n",
      "Ep 1 (Step 000020): Train loss 6.781, Val loss 6.737\n",
      "Ep 1 (Step 000030): Train loss 6.485, Val loss 6.435\n",
      "Ep 1 (Step 000040): Train loss 6.399, Val loss 6.374\n",
      "Ep 1 (Step 000050): Train loss 6.349, Val loss 6.325\n",
      "Ep 1 (Step 000060): Train loss 6.260, Val loss 6.241\n",
      "Ep 1 (Step 000070): Train loss 6.097, Val loss 6.096\n",
      "Ep 1 (Step 000080): Train loss 6.014, Val loss 6.035\n",
      "Ep 1 (Step 000090): Train loss 5.907, Val loss 5.953\n",
      "Ep 1 (Step 000100): Train loss 5.850, Val loss 5.891\n",
      "Ep 1 (Step 000110): Train loss 5.788, Val loss 5.826\n",
      "Ep 1 (Step 000120): Train loss 5.667, Val loss 5.784\n",
      "Ep 1 (Step 000130): Train loss 5.714, Val loss 5.741\n",
      "Ep 1 (Step 000140): Train loss 5.672, Val loss 5.705\n",
      "Ep 1 (Step 000150): Train loss 5.587, Val loss 5.666\n",
      "Ep 1 (Step 000160): Train loss 5.531, Val loss 5.631\n",
      "Ep 1 (Step 000170): Train loss 5.525, Val loss 5.605\n",
      "Ep 1 (Step 000180): Train loss 5.556, Val loss 5.574\n",
      "Ep 1 (Step 000190): Train loss 5.494, Val loss 5.549\n",
      "Ep 1 (Step 000200): Train loss 5.441, Val loss 5.530\n",
      "Ep 1 (Step 000210): Train loss 5.355, Val loss 5.510\n",
      "Ep 1 (Step 000220): Train loss 5.358, Val loss 5.487\n",
      "Ep 1 (Step 000230): Train loss 5.429, Val loss 5.459\n",
      "Ep 1 (Step 000240): Train loss 5.427, Val loss 5.453\n",
      "Ep 1 (Step 000250): Train loss 5.301, Val loss 5.435\n",
      "Ep 1 (Step 000260): Train loss 5.299, Val loss 5.410\n",
      "Ep 1 (Step 000270): Train loss 5.272, Val loss 5.402\n",
      "Ep 1 (Step 000280): Train loss 5.334, Val loss 5.403\n",
      "Ep 1 (Step 000290): Train loss 5.285, Val loss 5.384\n",
      "Ep 1 (Step 000300): Train loss 5.255, Val loss 5.378\n",
      "Ep 1 (Step 000310): Train loss 5.212, Val loss 5.371\n",
      "Ep 1 (Step 000320): Train loss 5.175, Val loss 5.348\n",
      "Ep 1 (Step 000330): Train loss 5.221, Val loss 5.338\n",
      "Ep 1 (Step 000340): Train loss 5.209, Val loss 5.316\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3157\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.814, Val loss 8.776\n",
      "Ep 1 (Step 000010): Train loss 7.471, Val loss 7.417\n",
      "Ep 1 (Step 000020): Train loss 6.826, Val loss 6.763\n",
      "Ep 1 (Step 000030): Train loss 6.488, Val loss 6.451\n",
      "Ep 1 (Step 000040): Train loss 6.358, Val loss 6.369\n",
      "Ep 1 (Step 000050): Train loss 6.355, Val loss 6.328\n",
      "Ep 1 (Step 000060): Train loss 6.206, Val loss 6.211\n",
      "Ep 1 (Step 000070): Train loss 6.059, Val loss 6.129\n",
      "Ep 1 (Step 000080): Train loss 6.000, Val loss 6.037\n",
      "Ep 1 (Step 000090): Train loss 5.926, Val loss 5.946\n",
      "Ep 1 (Step 000100): Train loss 5.821, Val loss 5.904\n",
      "Ep 1 (Step 000110): Train loss 5.864, Val loss 5.858\n",
      "Ep 1 (Step 000120): Train loss 5.750, Val loss 5.784\n",
      "Ep 1 (Step 000130): Train loss 5.762, Val loss 5.751\n",
      "Ep 1 (Step 000140): Train loss 5.611, Val loss 5.702\n",
      "Ep 1 (Step 000150): Train loss 5.566, Val loss 5.676\n",
      "Ep 1 (Step 000160): Train loss 5.635, Val loss 5.658\n",
      "Ep 1 (Step 000170): Train loss 5.585, Val loss 5.607\n",
      "Ep 1 (Step 000180): Train loss 5.521, Val loss 5.583\n",
      "Ep 1 (Step 000190): Train loss 5.547, Val loss 5.552\n",
      "Ep 1 (Step 000200): Train loss 5.396, Val loss 5.531\n",
      "Ep 1 (Step 000210): Train loss 5.408, Val loss 5.510\n",
      "Ep 1 (Step 000220): Train loss 5.396, Val loss 5.477\n",
      "Ep 1 (Step 000230): Train loss 5.429, Val loss 5.462\n",
      "Ep 1 (Step 000240): Train loss 5.356, Val loss 5.453\n",
      "Ep 1 (Step 000250): Train loss 5.397, Val loss 5.438\n",
      "Ep 1 (Step 000260): Train loss 5.279, Val loss 5.427\n",
      "Ep 1 (Step 000270): Train loss 5.273, Val loss 5.401\n",
      "Ep 1 (Step 000280): Train loss 5.294, Val loss 5.386\n",
      "Ep 1 (Step 000290): Train loss 5.315, Val loss 5.381\n",
      "Ep 1 (Step 000300): Train loss 5.228, Val loss 5.362\n",
      "Ep 1 (Step 000310): Train loss 5.266, Val loss 5.352\n",
      "Ep 1 (Step 000320): Train loss 5.307, Val loss 5.349\n",
      "Ep 1 (Step 000330): Train loss 5.276, Val loss 5.339\n",
      "Ep 1 (Step 000340): Train loss 5.279, Val loss 5.338\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3376\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.746, Val loss 8.721\n",
      "Ep 1 (Step 000010): Train loss 7.370, Val loss 7.360\n",
      "Ep 1 (Step 000020): Train loss 6.783, Val loss 6.745\n",
      "Ep 1 (Step 000030): Train loss 6.444, Val loss 6.464\n",
      "Ep 1 (Step 000040): Train loss 6.381, Val loss 6.397\n",
      "Ep 1 (Step 000050): Train loss 6.318, Val loss 6.342\n",
      "Ep 1 (Step 000060): Train loss 6.248, Val loss 6.217\n",
      "Ep 1 (Step 000070): Train loss 6.129, Val loss 6.134\n",
      "Ep 1 (Step 000080): Train loss 6.040, Val loss 6.026\n",
      "Ep 1 (Step 000090): Train loss 5.895, Val loss 5.950\n",
      "Ep 1 (Step 000100): Train loss 5.852, Val loss 5.898\n",
      "Ep 1 (Step 000110): Train loss 5.751, Val loss 5.849\n",
      "Ep 1 (Step 000120): Train loss 5.765, Val loss 5.795\n",
      "Ep 1 (Step 000130): Train loss 5.701, Val loss 5.744\n",
      "Ep 1 (Step 000140): Train loss 5.651, Val loss 5.701\n",
      "Ep 1 (Step 000150): Train loss 5.616, Val loss 5.665\n",
      "Ep 1 (Step 000160): Train loss 5.529, Val loss 5.646\n",
      "Ep 1 (Step 000170): Train loss 5.541, Val loss 5.597\n",
      "Ep 1 (Step 000180): Train loss 5.635, Val loss 5.573\n",
      "Ep 1 (Step 000190): Train loss 5.466, Val loss 5.550\n",
      "Ep 1 (Step 000200): Train loss 5.408, Val loss 5.523\n",
      "Ep 1 (Step 000210): Train loss 5.609, Val loss 5.508\n",
      "Ep 1 (Step 000220): Train loss 5.334, Val loss 5.491\n",
      "Ep 1 (Step 000230): Train loss 5.357, Val loss 5.473\n",
      "Ep 1 (Step 000240): Train loss 5.404, Val loss 5.465\n",
      "Ep 1 (Step 000250): Train loss 5.378, Val loss 5.454\n",
      "Ep 1 (Step 000260): Train loss 5.337, Val loss 5.428\n",
      "Ep 1 (Step 000270): Train loss 5.380, Val loss 5.416\n",
      "Ep 1 (Step 000280): Train loss 5.334, Val loss 5.392\n",
      "Ep 1 (Step 000290): Train loss 5.286, Val loss 5.384\n",
      "Ep 1 (Step 000300): Train loss 5.270, Val loss 5.379\n",
      "Ep 1 (Step 000310): Train loss 5.302, Val loss 5.363\n",
      "Ep 1 (Step 000320): Train loss 5.353, Val loss 5.358\n",
      "Ep 1 (Step 000330): Train loss 5.229, Val loss 5.342\n",
      "Ep 1 (Step 000340): Train loss 5.235, Val loss 5.333\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3333\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.479, Val loss 8.472\n",
      "Ep 1 (Step 000010): Train loss 6.827, Val loss 6.792\n",
      "Ep 1 (Step 000020): Train loss 6.467, Val loss 6.441\n",
      "Ep 1 (Step 000030): Train loss 6.420, Val loss 6.418\n",
      "Ep 1 (Step 000040): Train loss 6.296, Val loss 6.286\n",
      "Ep 1 (Step 000050): Train loss 6.187, Val loss 6.205\n",
      "Ep 1 (Step 000060): Train loss 6.046, Val loss 6.078\n",
      "Ep 1 (Step 000070): Train loss 5.998, Val loss 5.985\n",
      "Ep 1 (Step 000080): Train loss 5.895, Val loss 5.911\n",
      "Ep 1 (Step 000090): Train loss 5.744, Val loss 5.853\n",
      "Ep 1 (Step 000100): Train loss 5.763, Val loss 5.779\n",
      "Ep 1 (Step 000110): Train loss 5.667, Val loss 5.748\n",
      "Ep 1 (Step 000120): Train loss 5.555, Val loss 5.678\n",
      "Ep 1 (Step 000130): Train loss 5.634, Val loss 5.659\n",
      "Ep 1 (Step 000140): Train loss 5.586, Val loss 5.615\n",
      "Ep 1 (Step 000150): Train loss 5.508, Val loss 5.597\n",
      "Ep 1 (Step 000160): Train loss 5.455, Val loss 5.543\n",
      "Ep 1 (Step 000170): Train loss 5.412, Val loss 5.525\n",
      "Ep 1 (Step 000180): Train loss 5.440, Val loss 5.518\n",
      "Ep 1 (Step 000190): Train loss 5.446, Val loss 5.495\n",
      "Ep 1 (Step 000200): Train loss 5.359, Val loss 5.472\n",
      "Ep 1 (Step 000210): Train loss 5.380, Val loss 5.450\n",
      "Ep 1 (Step 000220): Train loss 5.440, Val loss 5.439\n",
      "Ep 1 (Step 000230): Train loss 5.384, Val loss 5.421\n",
      "Ep 1 (Step 000240): Train loss 5.270, Val loss 5.405\n",
      "Ep 1 (Step 000250): Train loss 5.289, Val loss 5.393\n",
      "Ep 1 (Step 000260): Train loss 5.239, Val loss 5.375\n",
      "Ep 1 (Step 000270): Train loss 5.225, Val loss 5.361\n",
      "Ep 1 (Step 000280): Train loss 5.257, Val loss 5.360\n",
      "Ep 1 (Step 000290): Train loss 5.304, Val loss 5.340\n",
      "Ep 1 (Step 000300): Train loss 5.184, Val loss 5.344\n",
      "Ep 1 (Step 000310): Train loss 5.178, Val loss 5.321\n",
      "Ep 1 (Step 000320): Train loss 5.119, Val loss 5.329\n",
      "Ep 1 (Step 000330): Train loss 5.200, Val loss 5.316\n",
      "Ep 1 (Step 000340): Train loss 5.149, Val loss 5.307\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3075\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.504, Val loss 8.469\n",
      "Ep 1 (Step 000010): Train loss 6.798, Val loss 6.816\n",
      "Ep 1 (Step 000020): Train loss 6.507, Val loss 6.466\n",
      "Ep 1 (Step 000030): Train loss 6.452, Val loss 6.426\n",
      "Ep 1 (Step 000040): Train loss 6.357, Val loss 6.287\n",
      "Ep 1 (Step 000050): Train loss 6.128, Val loss 6.173\n",
      "Ep 1 (Step 000060): Train loss 6.058, Val loss 6.063\n",
      "Ep 1 (Step 000070): Train loss 5.957, Val loss 5.964\n",
      "Ep 1 (Step 000080): Train loss 5.847, Val loss 5.871\n",
      "Ep 1 (Step 000090): Train loss 5.785, Val loss 5.785\n",
      "Ep 1 (Step 000100): Train loss 5.728, Val loss 5.758\n",
      "Ep 1 (Step 000110): Train loss 5.581, Val loss 5.679\n",
      "Ep 1 (Step 000120): Train loss 5.656, Val loss 5.642\n",
      "Ep 1 (Step 000130): Train loss 5.495, Val loss 5.609\n",
      "Ep 1 (Step 000140): Train loss 5.571, Val loss 5.575\n",
      "Ep 1 (Step 000150): Train loss 5.503, Val loss 5.549\n",
      "Ep 1 (Step 000160): Train loss 5.461, Val loss 5.517\n",
      "Ep 1 (Step 000170): Train loss 5.444, Val loss 5.508\n",
      "Ep 1 (Step 000180): Train loss 5.425, Val loss 5.478\n",
      "Ep 1 (Step 000190): Train loss 5.460, Val loss 5.448\n",
      "Ep 1 (Step 000200): Train loss 5.364, Val loss 5.442\n",
      "Ep 1 (Step 000210): Train loss 5.378, Val loss 5.432\n",
      "Ep 1 (Step 000220): Train loss 5.392, Val loss 5.427\n",
      "Ep 1 (Step 000230): Train loss 5.229, Val loss 5.404\n",
      "Ep 1 (Step 000240): Train loss 5.220, Val loss 5.398\n",
      "Ep 1 (Step 000250): Train loss 5.340, Val loss 5.370\n",
      "Ep 1 (Step 000260): Train loss 5.249, Val loss 5.365\n",
      "Ep 1 (Step 000270): Train loss 5.221, Val loss 5.346\n",
      "Ep 1 (Step 000280): Train loss 5.208, Val loss 5.330\n",
      "Ep 1 (Step 000290): Train loss 5.231, Val loss 5.326\n",
      "Ep 1 (Step 000300): Train loss 5.178, Val loss 5.302\n",
      "Ep 1 (Step 000310): Train loss 5.141, Val loss 5.301\n",
      "Ep 1 (Step 000320): Train loss 5.183, Val loss 5.305\n",
      "Ep 1 (Step 000330): Train loss 5.212, Val loss 5.290\n",
      "Ep 1 (Step 000340): Train loss 5.194, Val loss 5.307\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3073\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.609, Val loss 8.570\n",
      "Ep 1 (Step 000010): Train loss 6.908, Val loss 6.846\n",
      "Ep 1 (Step 000020): Train loss 6.469, Val loss 6.460\n",
      "Ep 1 (Step 000030): Train loss 6.439, Val loss 6.419\n",
      "Ep 1 (Step 000040): Train loss 8.477, Val loss 8.501\n",
      "Ep 1 (Step 000050): Train loss 6.148, Val loss 6.171\n",
      "Ep 1 (Step 000060): Train loss 6.050, Val loss 6.044\n",
      "Ep 1 (Step 000070): Train loss 5.963, Val loss 5.949\n",
      "Ep 1 (Step 000080): Train loss 5.846, Val loss 5.872\n",
      "Ep 1 (Step 000090): Train loss 5.867, Val loss 5.796\n",
      "Ep 1 (Step 000100): Train loss 5.719, Val loss 5.729\n",
      "Ep 1 (Step 000110): Train loss 5.642, Val loss 5.707\n",
      "Ep 1 (Step 000120): Train loss 5.538, Val loss 5.646\n",
      "Ep 1 (Step 000130): Train loss 5.594, Val loss 5.623\n",
      "Ep 1 (Step 000140): Train loss 5.500, Val loss 5.585\n",
      "Ep 1 (Step 000150): Train loss 5.464, Val loss 5.562\n",
      "Ep 1 (Step 000160): Train loss 5.382, Val loss 5.538\n",
      "Ep 1 (Step 000170): Train loss 5.400, Val loss 5.531\n",
      "Ep 1 (Step 000180): Train loss 5.377, Val loss 5.508\n",
      "Ep 1 (Step 000190): Train loss 5.406, Val loss 5.484\n",
      "Ep 1 (Step 000200): Train loss 5.425, Val loss 5.469\n",
      "Ep 1 (Step 000210): Train loss 5.318, Val loss 5.447\n",
      "Ep 1 (Step 000220): Train loss 5.344, Val loss 5.458\n",
      "Ep 1 (Step 000230): Train loss 5.364, Val loss 5.431\n",
      "Ep 1 (Step 000240): Train loss 5.291, Val loss 5.391\n",
      "Ep 1 (Step 000250): Train loss 5.370, Val loss 5.388\n",
      "Ep 1 (Step 000260): Train loss 5.339, Val loss 5.390\n",
      "Ep 1 (Step 000270): Train loss 5.250, Val loss 5.373\n",
      "Ep 1 (Step 000280): Train loss 5.251, Val loss 5.356\n",
      "Ep 1 (Step 000290): Train loss 5.241, Val loss 5.344\n",
      "Ep 1 (Step 000300): Train loss 5.238, Val loss 5.327\n",
      "Ep 1 (Step 000310): Train loss 5.226, Val loss 5.331\n",
      "Ep 1 (Step 000320): Train loss 5.129, Val loss 5.319\n",
      "Ep 1 (Step 000330): Train loss 5.163, Val loss 5.298\n",
      "Ep 1 (Step 000340): Train loss 5.204, Val loss 5.313\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3131\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.554, Val loss 8.517\n",
      "Ep 1 (Step 000010): Train loss 6.860, Val loss 6.822\n",
      "Ep 1 (Step 000020): Train loss 6.428, Val loss 6.445\n",
      "Ep 1 (Step 000030): Train loss 6.374, Val loss 6.395\n",
      "Ep 1 (Step 000040): Train loss 6.264, Val loss 6.266\n",
      "Ep 1 (Step 000050): Train loss 6.142, Val loss 6.191\n",
      "Ep 1 (Step 000060): Train loss 5.991, Val loss 6.022\n",
      "Ep 1 (Step 000070): Train loss 5.941, Val loss 5.940\n",
      "Ep 1 (Step 000080): Train loss 5.841, Val loss 5.854\n",
      "Ep 1 (Step 000090): Train loss 5.711, Val loss 5.787\n",
      "Ep 1 (Step 000100): Train loss 5.603, Val loss 5.731\n",
      "Ep 1 (Step 000110): Train loss 5.586, Val loss 5.735\n",
      "Ep 1 (Step 000120): Train loss 5.567, Val loss 5.662\n",
      "Ep 1 (Step 000130): Train loss 5.501, Val loss 5.617\n",
      "Ep 1 (Step 000140): Train loss 5.513, Val loss 5.610\n",
      "Ep 1 (Step 000150): Train loss 5.475, Val loss 5.577\n",
      "Ep 1 (Step 000160): Train loss 5.465, Val loss 5.553\n",
      "Ep 1 (Step 000170): Train loss 5.415, Val loss 5.507\n",
      "Ep 1 (Step 000180): Train loss 5.373, Val loss 5.488\n",
      "Ep 1 (Step 000190): Train loss 5.414, Val loss 5.464\n",
      "Ep 1 (Step 000200): Train loss 5.355, Val loss 5.429\n",
      "Ep 1 (Step 000210): Train loss 5.341, Val loss 5.427\n",
      "Ep 1 (Step 000220): Train loss 5.272, Val loss 5.433\n",
      "Ep 1 (Step 000230): Train loss 5.185, Val loss 5.404\n",
      "Ep 1 (Step 000240): Train loss 5.225, Val loss 5.394\n",
      "Ep 1 (Step 000250): Train loss 5.261, Val loss 5.356\n",
      "Ep 1 (Step 000260): Train loss 5.185, Val loss 5.337\n",
      "Ep 1 (Step 000270): Train loss 5.192, Val loss 5.329\n",
      "Ep 1 (Step 000280): Train loss 5.123, Val loss 5.330\n",
      "Ep 1 (Step 000290): Train loss 5.297, Val loss 5.308\n",
      "Ep 1 (Step 000300): Train loss 5.147, Val loss 5.294\n",
      "Ep 1 (Step 000310): Train loss 5.279, Val loss 5.302\n",
      "Ep 1 (Step 000320): Train loss 5.146, Val loss 5.286\n",
      "Ep 1 (Step 000330): Train loss 5.091, Val loss 5.259\n",
      "Ep 1 (Step 000340): Train loss 5.167, Val loss 5.243\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2425\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.548, Val loss 8.519\n",
      "Ep 1 (Step 000010): Train loss 6.872, Val loss 6.833\n",
      "Ep 1 (Step 000020): Train loss 6.468, Val loss 6.448\n",
      "Ep 1 (Step 000030): Train loss 6.423, Val loss 6.400\n",
      "Ep 1 (Step 000040): Train loss 6.282, Val loss 6.228\n",
      "Ep 1 (Step 000050): Train loss 6.153, Val loss 6.139\n",
      "Ep 1 (Step 000060): Train loss 5.972, Val loss 6.017\n",
      "Ep 1 (Step 000070): Train loss 5.902, Val loss 5.923\n",
      "Ep 1 (Step 000080): Train loss 5.793, Val loss 5.837\n",
      "Ep 1 (Step 000090): Train loss 5.681, Val loss 5.779\n",
      "Ep 1 (Step 000100): Train loss 5.645, Val loss 5.734\n",
      "Ep 1 (Step 000110): Train loss 5.697, Val loss 5.690\n",
      "Ep 1 (Step 000120): Train loss 5.594, Val loss 5.642\n",
      "Ep 1 (Step 000130): Train loss 5.593, Val loss 5.606\n",
      "Ep 1 (Step 000140): Train loss 5.591, Val loss 5.566\n",
      "Ep 1 (Step 000150): Train loss 5.559, Val loss 5.558\n",
      "Ep 1 (Step 000160): Train loss 5.424, Val loss 5.520\n",
      "Ep 1 (Step 000170): Train loss 5.442, Val loss 5.483\n",
      "Ep 1 (Step 000180): Train loss 5.373, Val loss 5.470\n",
      "Ep 1 (Step 000190): Train loss 5.423, Val loss 5.465\n",
      "Ep 1 (Step 000200): Train loss 5.297, Val loss 5.434\n",
      "Ep 1 (Step 000210): Train loss 5.256, Val loss 5.435\n",
      "Ep 1 (Step 000220): Train loss 5.250, Val loss 5.407\n",
      "Ep 1 (Step 000230): Train loss 5.282, Val loss 5.388\n",
      "Ep 1 (Step 000240): Train loss 5.297, Val loss 5.374\n",
      "Ep 1 (Step 000250): Train loss 5.290, Val loss 5.357\n",
      "Ep 1 (Step 000260): Train loss 5.152, Val loss 5.360\n",
      "Ep 1 (Step 000270): Train loss 5.239, Val loss 5.329\n",
      "Ep 1 (Step 000280): Train loss 5.188, Val loss 5.314\n",
      "Ep 1 (Step 000290): Train loss 5.247, Val loss 5.285\n",
      "Ep 1 (Step 000300): Train loss 5.196, Val loss 5.283\n",
      "Ep 1 (Step 000310): Train loss 5.190, Val loss 5.294\n",
      "Ep 1 (Step 000320): Train loss 5.240, Val loss 5.275\n",
      "Ep 1 (Step 000330): Train loss 5.180, Val loss 5.277\n",
      "Ep 1 (Step 000340): Train loss 5.087, Val loss 5.261\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2607\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.467, Val loss 8.447\n",
      "Ep 1 (Step 000010): Train loss 6.812, Val loss 6.749\n",
      "Ep 1 (Step 000020): Train loss 6.522, Val loss 6.447\n",
      "Ep 1 (Step 000030): Train loss 6.465, Val loss 6.445\n",
      "Ep 1 (Step 000040): Train loss 6.326, Val loss 6.328\n",
      "Ep 1 (Step 000050): Train loss 6.179, Val loss 6.179\n",
      "Ep 1 (Step 000060): Train loss 5.975, Val loss 6.058\n",
      "Ep 1 (Step 000070): Train loss 5.969, Val loss 5.976\n",
      "Ep 1 (Step 000080): Train loss 5.830, Val loss 5.895\n",
      "Ep 1 (Step 000090): Train loss 5.773, Val loss 5.830\n",
      "Ep 1 (Step 000100): Train loss 5.825, Val loss 5.769\n",
      "Ep 1 (Step 000110): Train loss 5.666, Val loss 5.698\n",
      "Ep 1 (Step 000120): Train loss 5.664, Val loss 5.687\n",
      "Ep 1 (Step 000130): Train loss 5.565, Val loss 5.639\n",
      "Ep 1 (Step 000140): Train loss 5.500, Val loss 5.579\n",
      "Ep 1 (Step 000150): Train loss 5.606, Val loss 5.561\n",
      "Ep 1 (Step 000160): Train loss 5.560, Val loss 5.534\n",
      "Ep 1 (Step 000170): Train loss 5.486, Val loss 5.488\n",
      "Ep 1 (Step 000180): Train loss 5.388, Val loss 5.474\n",
      "Ep 1 (Step 000190): Train loss 5.370, Val loss 5.464\n",
      "Ep 1 (Step 000200): Train loss 5.343, Val loss 5.437\n",
      "Ep 1 (Step 000210): Train loss 5.306, Val loss 5.432\n",
      "Ep 1 (Step 000220): Train loss 5.284, Val loss 5.414\n",
      "Ep 1 (Step 000230): Train loss 5.289, Val loss 5.406\n",
      "Ep 1 (Step 000240): Train loss 5.253, Val loss 5.393\n",
      "Ep 1 (Step 000250): Train loss 5.205, Val loss 5.375\n",
      "Ep 1 (Step 000260): Train loss 5.239, Val loss 5.370\n",
      "Ep 1 (Step 000270): Train loss 5.260, Val loss 5.338\n",
      "Ep 1 (Step 000280): Train loss 5.211, Val loss 5.340\n",
      "Ep 1 (Step 000290): Train loss 5.194, Val loss 5.337\n",
      "Ep 1 (Step 000300): Train loss 5.155, Val loss 5.316\n",
      "Ep 1 (Step 000310): Train loss 5.226, Val loss 5.302\n",
      "Ep 1 (Step 000320): Train loss 5.135, Val loss 5.314\n",
      "Ep 1 (Step 000330): Train loss 5.130, Val loss 5.282\n",
      "Ep 1 (Step 000340): Train loss 5.135, Val loss 5.284\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2838\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.748, Val loss 8.732\n",
      "Ep 1 (Step 000010): Train loss 7.384, Val loss 7.344\n",
      "Ep 1 (Step 000020): Train loss 6.744, Val loss 6.725\n",
      "Ep 1 (Step 000030): Train loss 6.498, Val loss 6.447\n",
      "Ep 1 (Step 000040): Train loss 6.412, Val loss 6.365\n",
      "Ep 1 (Step 000050): Train loss 6.285, Val loss 6.285\n",
      "Ep 1 (Step 000060): Train loss 6.191, Val loss 6.191\n",
      "Ep 1 (Step 000070): Train loss 6.062, Val loss 6.093\n",
      "Ep 1 (Step 000080): Train loss 5.978, Val loss 6.030\n",
      "Ep 1 (Step 000090): Train loss 5.869, Val loss 5.928\n",
      "Ep 1 (Step 000100): Train loss 5.817, Val loss 5.881\n",
      "Ep 1 (Step 000110): Train loss 5.791, Val loss 5.833\n",
      "Ep 1 (Step 000120): Train loss 5.711, Val loss 5.765\n",
      "Ep 1 (Step 000130): Train loss 5.775, Val loss 5.729\n",
      "Ep 1 (Step 000140): Train loss 5.617, Val loss 5.694\n",
      "Ep 1 (Step 000150): Train loss 5.594, Val loss 5.671\n",
      "Ep 1 (Step 000160): Train loss 5.530, Val loss 5.645\n",
      "Ep 1 (Step 000170): Train loss 5.492, Val loss 5.604\n",
      "Ep 1 (Step 000180): Train loss 5.480, Val loss 5.587\n",
      "Ep 1 (Step 000190): Train loss 5.426, Val loss 5.558\n",
      "Ep 1 (Step 000200): Train loss 5.481, Val loss 5.532\n",
      "Ep 1 (Step 000210): Train loss 5.412, Val loss 5.514\n",
      "Ep 1 (Step 000220): Train loss 5.407, Val loss 5.484\n",
      "Ep 1 (Step 000230): Train loss 5.387, Val loss 5.485\n",
      "Ep 1 (Step 000240): Train loss 5.372, Val loss 5.471\n",
      "Ep 1 (Step 000250): Train loss 5.339, Val loss 5.449\n",
      "Ep 1 (Step 000260): Train loss 5.381, Val loss 5.431\n",
      "Ep 1 (Step 000270): Train loss 5.340, Val loss 5.404\n",
      "Ep 1 (Step 000280): Train loss 5.279, Val loss 5.391\n",
      "Ep 1 (Step 000290): Train loss 5.323, Val loss 5.379\n",
      "Ep 1 (Step 000300): Train loss 5.299, Val loss 5.365\n",
      "Ep 1 (Step 000310): Train loss 5.219, Val loss 5.345\n",
      "Ep 1 (Step 000320): Train loss 5.208, Val loss 5.360\n",
      "Ep 1 (Step 000330): Train loss 5.207, Val loss 5.344\n",
      "Ep 1 (Step 000340): Train loss 5.204, Val loss 5.345\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3453\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.749, Val loss 8.715\n",
      "Ep 1 (Step 000010): Train loss 7.399, Val loss 7.354\n",
      "Ep 1 (Step 000020): Train loss 6.741, Val loss 6.753\n",
      "Ep 1 (Step 000030): Train loss 6.434, Val loss 6.460\n",
      "Ep 1 (Step 000040): Train loss 6.397, Val loss 6.377\n",
      "Ep 1 (Step 000050): Train loss 6.370, Val loss 6.335\n",
      "Ep 1 (Step 000060): Train loss 6.191, Val loss 6.219\n",
      "Ep 1 (Step 000070): Train loss 6.150, Val loss 6.111\n",
      "Ep 1 (Step 000080): Train loss 6.007, Val loss 6.031\n",
      "Ep 1 (Step 000090): Train loss 5.941, Val loss 5.946\n",
      "Ep 1 (Step 000100): Train loss 5.849, Val loss 5.869\n",
      "Ep 1 (Step 000110): Train loss 5.833, Val loss 5.830\n",
      "Ep 1 (Step 000120): Train loss 5.735, Val loss 5.767\n",
      "Ep 1 (Step 000130): Train loss 5.725, Val loss 5.737\n",
      "Ep 1 (Step 000140): Train loss 5.680, Val loss 5.703\n",
      "Ep 1 (Step 000150): Train loss 5.667, Val loss 5.682\n",
      "Ep 1 (Step 000160): Train loss 5.517, Val loss 5.655\n",
      "Ep 1 (Step 000170): Train loss 5.533, Val loss 5.625\n",
      "Ep 1 (Step 000180): Train loss 5.559, Val loss 5.587\n",
      "Ep 1 (Step 000190): Train loss 5.468, Val loss 5.569\n",
      "Ep 1 (Step 000200): Train loss 5.490, Val loss 5.556\n",
      "Ep 1 (Step 000210): Train loss 5.536, Val loss 5.528\n",
      "Ep 1 (Step 000220): Train loss 5.362, Val loss 5.509\n",
      "Ep 1 (Step 000230): Train loss 5.374, Val loss 5.495\n",
      "Ep 1 (Step 000240): Train loss 5.428, Val loss 5.471\n",
      "Ep 1 (Step 000250): Train loss 5.335, Val loss 5.454\n",
      "Ep 1 (Step 000260): Train loss 5.358, Val loss 5.437\n",
      "Ep 1 (Step 000270): Train loss 5.246, Val loss 5.434\n",
      "Ep 1 (Step 000280): Train loss 5.299, Val loss 5.416\n",
      "Ep 1 (Step 000290): Train loss 5.320, Val loss 5.411\n",
      "Ep 1 (Step 000300): Train loss 5.285, Val loss 5.389\n",
      "Ep 1 (Step 000310): Train loss 5.298, Val loss 5.377\n",
      "Ep 1 (Step 000320): Train loss 5.285, Val loss 5.351\n",
      "Ep 1 (Step 000330): Train loss 5.236, Val loss 5.338\n",
      "Ep 1 (Step 000340): Train loss 5.252, Val loss 5.345\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3455\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.840, Val loss 8.823\n",
      "Ep 1 (Step 000010): Train loss 7.444, Val loss 7.449\n",
      "Ep 1 (Step 000020): Train loss 6.790, Val loss 6.808\n",
      "Ep 1 (Step 000030): Train loss 6.472, Val loss 6.465\n",
      "Ep 1 (Step 000040): Train loss 6.403, Val loss 6.385\n",
      "Ep 1 (Step 000050): Train loss 6.293, Val loss 6.289\n",
      "Ep 1 (Step 000060): Train loss 6.181, Val loss 6.217\n",
      "Ep 1 (Step 000070): Train loss 6.058, Val loss 6.081\n",
      "Ep 1 (Step 000080): Train loss 5.973, Val loss 6.003\n",
      "Ep 1 (Step 000090): Train loss 5.939, Val loss 5.940\n",
      "Ep 1 (Step 000100): Train loss 5.807, Val loss 5.877\n",
      "Ep 1 (Step 000110): Train loss 5.727, Val loss 5.825\n",
      "Ep 1 (Step 000120): Train loss 5.714, Val loss 5.784\n",
      "Ep 1 (Step 000130): Train loss 5.705, Val loss 5.743\n",
      "Ep 1 (Step 000140): Train loss 5.672, Val loss 5.706\n",
      "Ep 1 (Step 000150): Train loss 5.556, Val loss 5.678\n",
      "Ep 1 (Step 000160): Train loss 5.605, Val loss 5.636\n",
      "Ep 1 (Step 000170): Train loss 5.589, Val loss 5.604\n",
      "Ep 1 (Step 000180): Train loss 5.561, Val loss 5.583\n",
      "Ep 1 (Step 000190): Train loss 5.520, Val loss 5.561\n",
      "Ep 1 (Step 000200): Train loss 5.467, Val loss 5.556\n",
      "Ep 1 (Step 000210): Train loss 5.427, Val loss 5.525\n",
      "Ep 1 (Step 000220): Train loss 5.401, Val loss 5.497\n",
      "Ep 1 (Step 000230): Train loss 5.421, Val loss 5.494\n",
      "Ep 1 (Step 000240): Train loss 5.324, Val loss 5.484\n",
      "Ep 1 (Step 000250): Train loss 5.335, Val loss 5.464\n",
      "Ep 1 (Step 000260): Train loss 5.358, Val loss 5.446\n",
      "Ep 1 (Step 000270): Train loss 5.296, Val loss 5.431\n",
      "Ep 1 (Step 000280): Train loss 5.377, Val loss 5.400\n",
      "Ep 1 (Step 000290): Train loss 5.410, Val loss 5.403\n",
      "Ep 1 (Step 000300): Train loss 5.325, Val loss 5.389\n",
      "Ep 1 (Step 000310): Train loss 5.323, Val loss 5.383\n",
      "Ep 1 (Step 000320): Train loss 5.286, Val loss 5.366\n",
      "Ep 1 (Step 000330): Train loss 5.305, Val loss 5.355\n",
      "Ep 1 (Step 000340): Train loss 5.218, Val loss 5.355\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3554\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.744, Val loss 8.704\n",
      "Ep 1 (Step 000010): Train loss 7.402, Val loss 7.373\n",
      "Ep 1 (Step 000020): Train loss 6.770, Val loss 6.742\n",
      "Ep 1 (Step 000030): Train loss 6.471, Val loss 6.455\n",
      "Ep 1 (Step 000040): Train loss 6.439, Val loss 6.372\n",
      "Ep 1 (Step 000050): Train loss 6.248, Val loss 6.316\n",
      "Ep 1 (Step 000060): Train loss 6.194, Val loss 6.198\n",
      "Ep 1 (Step 000070): Train loss 6.109, Val loss 6.103\n",
      "Ep 1 (Step 000080): Train loss 6.024, Val loss 5.978\n",
      "Ep 1 (Step 000090): Train loss 5.934, Val loss 5.903\n",
      "Ep 1 (Step 000100): Train loss 5.778, Val loss 5.851\n",
      "Ep 1 (Step 000110): Train loss 5.807, Val loss 5.801\n",
      "Ep 1 (Step 000120): Train loss 5.699, Val loss 5.761\n",
      "Ep 1 (Step 000130): Train loss 5.708, Val loss 5.722\n",
      "Ep 1 (Step 000140): Train loss 5.638, Val loss 5.681\n",
      "Ep 1 (Step 000150): Train loss 5.593, Val loss 5.661\n",
      "Ep 1 (Step 000160): Train loss 5.517, Val loss 5.643\n",
      "Ep 1 (Step 000170): Train loss 5.443, Val loss 5.593\n",
      "Ep 1 (Step 000180): Train loss 5.539, Val loss 5.567\n",
      "Ep 1 (Step 000190): Train loss 5.549, Val loss 5.543\n",
      "Ep 1 (Step 000200): Train loss 5.469, Val loss 5.524\n",
      "Ep 1 (Step 000210): Train loss 5.451, Val loss 5.500\n",
      "Ep 1 (Step 000220): Train loss 5.400, Val loss 5.474\n",
      "Ep 1 (Step 000230): Train loss 5.404, Val loss 5.466\n",
      "Ep 1 (Step 000240): Train loss 5.369, Val loss 5.436\n",
      "Ep 1 (Step 000250): Train loss 5.401, Val loss 5.428\n",
      "Ep 1 (Step 000260): Train loss 5.308, Val loss 5.408\n",
      "Ep 1 (Step 000270): Train loss 5.300, Val loss 5.422\n",
      "Ep 1 (Step 000280): Train loss 5.226, Val loss 5.403\n",
      "Ep 1 (Step 000290): Train loss 5.280, Val loss 5.378\n",
      "Ep 1 (Step 000300): Train loss 5.339, Val loss 5.377\n",
      "Ep 1 (Step 000310): Train loss 5.259, Val loss 5.357\n",
      "Ep 1 (Step 000320): Train loss 5.182, Val loss 5.345\n",
      "Ep 1 (Step 000330): Train loss 5.226, Val loss 5.352\n",
      "Ep 1 (Step 000340): Train loss 5.220, Val loss 5.331\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3307\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.811, Val loss 8.760\n",
      "Ep 1 (Step 000010): Train loss 7.393, Val loss 7.359\n",
      "Ep 1 (Step 000020): Train loss 6.775, Val loss 6.739\n",
      "Ep 1 (Step 000030): Train loss 6.429, Val loss 6.454\n",
      "Ep 1 (Step 000040): Train loss 6.382, Val loss 6.356\n",
      "Ep 1 (Step 000050): Train loss 6.318, Val loss 6.292\n",
      "Ep 1 (Step 000060): Train loss 6.236, Val loss 6.216\n",
      "Ep 1 (Step 000070): Train loss 6.068, Val loss 6.083\n",
      "Ep 1 (Step 000080): Train loss 6.034, Val loss 6.009\n",
      "Ep 1 (Step 000090): Train loss 5.942, Val loss 5.935\n",
      "Ep 1 (Step 000100): Train loss 5.859, Val loss 5.898\n",
      "Ep 1 (Step 000110): Train loss 5.806, Val loss 5.819\n",
      "Ep 1 (Step 000120): Train loss 5.758, Val loss 5.787\n",
      "Ep 1 (Step 000130): Train loss 5.666, Val loss 5.754\n",
      "Ep 1 (Step 000140): Train loss 5.609, Val loss 5.705\n",
      "Ep 1 (Step 000150): Train loss 5.565, Val loss 5.672\n",
      "Ep 1 (Step 000160): Train loss 5.619, Val loss 5.659\n",
      "Ep 1 (Step 000170): Train loss 5.522, Val loss 5.610\n",
      "Ep 1 (Step 000180): Train loss 5.514, Val loss 5.578\n",
      "Ep 1 (Step 000190): Train loss 5.491, Val loss 5.555\n",
      "Ep 1 (Step 000200): Train loss 5.484, Val loss 5.546\n",
      "Ep 1 (Step 000210): Train loss 5.446, Val loss 5.512\n",
      "Ep 1 (Step 000220): Train loss 5.356, Val loss 5.499\n",
      "Ep 1 (Step 000230): Train loss 5.368, Val loss 5.485\n",
      "Ep 1 (Step 000240): Train loss 5.357, Val loss 5.473\n",
      "Ep 1 (Step 000250): Train loss 5.436, Val loss 5.440\n",
      "Ep 1 (Step 000260): Train loss 5.312, Val loss 5.414\n",
      "Ep 1 (Step 000270): Train loss 5.361, Val loss 5.397\n",
      "Ep 1 (Step 000280): Train loss 5.239, Val loss 5.397\n",
      "Ep 1 (Step 000290): Train loss 5.305, Val loss 5.389\n",
      "Ep 1 (Step 000300): Train loss 5.295, Val loss 5.375\n",
      "Ep 1 (Step 000310): Train loss 5.229, Val loss 5.356\n",
      "Ep 1 (Step 000320): Train loss 5.271, Val loss 5.356\n",
      "Ep 1 (Step 000330): Train loss 5.227, Val loss 5.326\n",
      "Ep 1 (Step 000340): Train loss 5.209, Val loss 5.325\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3252\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.880, Val loss 8.865\n",
      "Ep 1 (Step 000010): Train loss 7.462, Val loss 7.435\n",
      "Ep 1 (Step 000020): Train loss 6.809, Val loss 6.777\n",
      "Ep 1 (Step 000030): Train loss 6.503, Val loss 6.447\n",
      "Ep 1 (Step 000040): Train loss 6.388, Val loss 6.364\n",
      "Ep 1 (Step 000050): Train loss 6.324, Val loss 6.311\n",
      "Ep 1 (Step 000060): Train loss 6.184, Val loss 6.186\n",
      "Ep 1 (Step 000070): Train loss 6.128, Val loss 6.119\n",
      "Ep 1 (Step 000080): Train loss 5.931, Val loss 6.010\n",
      "Ep 1 (Step 000090): Train loss 5.913, Val loss 5.959\n",
      "Ep 1 (Step 000100): Train loss 5.917, Val loss 5.910\n",
      "Ep 1 (Step 000110): Train loss 5.820, Val loss 5.843\n",
      "Ep 1 (Step 000120): Train loss 5.690, Val loss 5.768\n",
      "Ep 1 (Step 000130): Train loss 5.644, Val loss 5.712\n",
      "Ep 1 (Step 000140): Train loss 5.697, Val loss 5.701\n",
      "Ep 1 (Step 000150): Train loss 5.532, Val loss 5.654\n",
      "Ep 1 (Step 000160): Train loss 5.617, Val loss 5.614\n",
      "Ep 1 (Step 000170): Train loss 5.510, Val loss 5.593\n",
      "Ep 1 (Step 000180): Train loss 5.505, Val loss 5.587\n",
      "Ep 1 (Step 000190): Train loss 5.491, Val loss 5.540\n",
      "Ep 1 (Step 000200): Train loss 5.440, Val loss 5.524\n",
      "Ep 1 (Step 000210): Train loss 5.454, Val loss 5.512\n",
      "Ep 1 (Step 000220): Train loss 5.393, Val loss 5.490\n",
      "Ep 1 (Step 000230): Train loss 5.397, Val loss 5.468\n",
      "Ep 1 (Step 000240): Train loss 5.369, Val loss 5.459\n",
      "Ep 1 (Step 000250): Train loss 5.365, Val loss 5.459\n",
      "Ep 1 (Step 000260): Train loss 5.441, Val loss 5.439\n",
      "Ep 1 (Step 000270): Train loss 5.341, Val loss 5.418\n",
      "Ep 1 (Step 000280): Train loss 5.308, Val loss 5.409\n",
      "Ep 1 (Step 000290): Train loss 5.324, Val loss 5.397\n",
      "Ep 1 (Step 000300): Train loss 5.288, Val loss 5.369\n",
      "Ep 1 (Step 000310): Train loss 5.220, Val loss 5.369\n",
      "Ep 1 (Step 000320): Train loss 5.200, Val loss 5.359\n",
      "Ep 1 (Step 000330): Train loss 5.216, Val loss 5.351\n",
      "Ep 1 (Step 000340): Train loss 5.266, Val loss 5.344\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3443\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.554, Val loss 8.529\n",
      "Ep 1 (Step 000010): Train loss 6.856, Val loss 6.833\n",
      "Ep 1 (Step 000020): Train loss 6.443, Val loss 6.457\n",
      "Ep 1 (Step 000030): Train loss 6.432, Val loss 6.410\n",
      "Ep 1 (Step 000040): Train loss 6.335, Val loss 6.281\n",
      "Ep 1 (Step 000050): Train loss 6.101, Val loss 6.138\n",
      "Ep 1 (Step 000060): Train loss 5.992, Val loss 6.019\n",
      "Ep 1 (Step 000070): Train loss 5.934, Val loss 5.934\n",
      "Ep 1 (Step 000080): Train loss 5.842, Val loss 5.871\n",
      "Ep 1 (Step 000090): Train loss 5.684, Val loss 5.805\n",
      "Ep 1 (Step 000100): Train loss 5.737, Val loss 5.755\n",
      "Ep 1 (Step 000110): Train loss 5.543, Val loss 5.702\n",
      "Ep 1 (Step 000120): Train loss 5.615, Val loss 5.664\n",
      "Ep 1 (Step 000130): Train loss 5.551, Val loss 5.607\n",
      "Ep 1 (Step 000140): Train loss 5.496, Val loss 5.597\n",
      "Ep 1 (Step 000150): Train loss 5.510, Val loss 5.579\n",
      "Ep 1 (Step 000160): Train loss 5.493, Val loss 5.538\n",
      "Ep 1 (Step 000170): Train loss 5.360, Val loss 5.513\n",
      "Ep 1 (Step 000180): Train loss 5.332, Val loss 5.500\n",
      "Ep 1 (Step 000190): Train loss 5.372, Val loss 5.472\n",
      "Ep 1 (Step 000200): Train loss 5.292, Val loss 5.462\n",
      "Ep 1 (Step 000210): Train loss 5.335, Val loss 5.441\n",
      "Ep 1 (Step 000220): Train loss 5.285, Val loss 5.423\n",
      "Ep 1 (Step 000230): Train loss 5.225, Val loss 5.424\n",
      "Ep 1 (Step 000240): Train loss 5.378, Val loss 5.390\n",
      "Ep 1 (Step 000250): Train loss 5.251, Val loss 5.392\n",
      "Ep 1 (Step 000260): Train loss 5.213, Val loss 5.366\n",
      "Ep 1 (Step 000270): Train loss 5.177, Val loss 5.356\n",
      "Ep 1 (Step 000280): Train loss 5.261, Val loss 5.325\n",
      "Ep 1 (Step 000290): Train loss 5.169, Val loss 5.321\n",
      "Ep 1 (Step 000300): Train loss 5.158, Val loss 5.315\n",
      "Ep 1 (Step 000310): Train loss 5.188, Val loss 5.314\n",
      "Ep 1 (Step 000320): Train loss 5.134, Val loss 5.290\n",
      "Ep 1 (Step 000330): Train loss 5.193, Val loss 5.283\n",
      "Ep 1 (Step 000340): Train loss 5.247, Val loss 5.291\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2906\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.487, Val loss 8.470\n",
      "Ep 1 (Step 000010): Train loss 6.868, Val loss 6.806\n",
      "Ep 1 (Step 000020): Train loss 6.467, Val loss 6.422\n",
      "Ep 1 (Step 000030): Train loss 6.434, Val loss 6.390\n",
      "Ep 1 (Step 000040): Train loss 6.279, Val loss 6.245\n",
      "Ep 1 (Step 000050): Train loss 6.091, Val loss 6.137\n",
      "Ep 1 (Step 000060): Train loss 6.002, Val loss 6.004\n",
      "Ep 1 (Step 000070): Train loss 5.837, Val loss 5.897\n",
      "Ep 1 (Step 000080): Train loss 5.750, Val loss 5.843\n",
      "Ep 1 (Step 000090): Train loss 5.750, Val loss 5.789\n",
      "Ep 1 (Step 000100): Train loss 5.800, Val loss 5.754\n",
      "Ep 1 (Step 000110): Train loss 5.571, Val loss 5.694\n",
      "Ep 1 (Step 000120): Train loss 5.590, Val loss 5.645\n",
      "Ep 1 (Step 000130): Train loss 5.557, Val loss 5.625\n",
      "Ep 1 (Step 000140): Train loss 5.500, Val loss 5.594\n",
      "Ep 1 (Step 000150): Train loss 5.523, Val loss 5.577\n",
      "Ep 1 (Step 000160): Train loss 5.396, Val loss 5.529\n",
      "Ep 1 (Step 000170): Train loss 5.424, Val loss 5.535\n",
      "Ep 1 (Step 000180): Train loss 5.398, Val loss 5.494\n",
      "Ep 1 (Step 000190): Train loss 5.470, Val loss 5.501\n",
      "Ep 1 (Step 000200): Train loss 5.395, Val loss 5.478\n",
      "Ep 1 (Step 000210): Train loss 5.313, Val loss 5.440\n",
      "Ep 1 (Step 000220): Train loss 5.337, Val loss 5.431\n",
      "Ep 1 (Step 000230): Train loss 5.294, Val loss 5.411\n",
      "Ep 1 (Step 000240): Train loss 5.224, Val loss 5.390\n",
      "Ep 1 (Step 000250): Train loss 5.204, Val loss 5.372\n",
      "Ep 1 (Step 000260): Train loss 5.193, Val loss 5.377\n",
      "Ep 1 (Step 000270): Train loss 5.271, Val loss 5.365\n",
      "Ep 1 (Step 000280): Train loss 5.187, Val loss 5.356\n",
      "Ep 1 (Step 000290): Train loss 5.264, Val loss 5.353\n",
      "Ep 1 (Step 000300): Train loss 5.161, Val loss 5.353\n",
      "Ep 1 (Step 000310): Train loss 5.219, Val loss 5.351\n",
      "Ep 1 (Step 000320): Train loss 5.282, Val loss 5.333\n",
      "Ep 1 (Step 000330): Train loss 5.147, Val loss 5.319\n",
      "Ep 1 (Step 000340): Train loss 5.169, Val loss 5.306\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3064\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.587, Val loss 8.564\n",
      "Ep 1 (Step 000010): Train loss 6.825, Val loss 6.809\n",
      "Ep 1 (Step 000020): Train loss 6.508, Val loss 6.473\n",
      "Ep 1 (Step 000030): Train loss 6.428, Val loss 6.432\n",
      "Ep 1 (Step 000040): Train loss 6.330, Val loss 6.279\n",
      "Ep 1 (Step 000050): Train loss 6.129, Val loss 6.122\n",
      "Ep 1 (Step 000060): Train loss 6.019, Val loss 6.023\n",
      "Ep 1 (Step 000070): Train loss 5.872, Val loss 5.953\n",
      "Ep 1 (Step 000080): Train loss 5.889, Val loss 5.873\n",
      "Ep 1 (Step 000090): Train loss 5.792, Val loss 5.807\n",
      "Ep 1 (Step 000100): Train loss 5.624, Val loss 5.745\n",
      "Ep 1 (Step 000110): Train loss 5.664, Val loss 5.689\n",
      "Ep 1 (Step 000120): Train loss 5.621, Val loss 5.660\n",
      "Ep 1 (Step 000130): Train loss 5.524, Val loss 5.623\n",
      "Ep 1 (Step 000140): Train loss 5.470, Val loss 5.587\n",
      "Ep 1 (Step 000150): Train loss 5.535, Val loss 5.554\n",
      "Ep 1 (Step 000160): Train loss 5.431, Val loss 5.562\n",
      "Ep 1 (Step 000170): Train loss 5.466, Val loss 5.524\n",
      "Ep 1 (Step 000180): Train loss 5.322, Val loss 5.495\n",
      "Ep 1 (Step 000190): Train loss 5.385, Val loss 5.482\n",
      "Ep 1 (Step 000200): Train loss 5.377, Val loss 5.468\n",
      "Ep 1 (Step 000210): Train loss 5.310, Val loss 5.455\n",
      "Ep 1 (Step 000220): Train loss 5.304, Val loss 5.436\n",
      "Ep 1 (Step 000230): Train loss 5.323, Val loss 5.406\n",
      "Ep 1 (Step 000240): Train loss 5.304, Val loss 5.375\n",
      "Ep 1 (Step 000250): Train loss 5.268, Val loss 5.377\n",
      "Ep 1 (Step 000260): Train loss 5.357, Val loss 5.360\n",
      "Ep 1 (Step 000270): Train loss 5.277, Val loss 5.366\n",
      "Ep 1 (Step 000280): Train loss 5.261, Val loss 5.328\n",
      "Ep 1 (Step 000290): Train loss 5.193, Val loss 5.321\n",
      "Ep 1 (Step 000300): Train loss 5.164, Val loss 5.320\n",
      "Ep 1 (Step 000310): Train loss 5.280, Val loss 5.317\n",
      "Ep 1 (Step 000320): Train loss 5.232, Val loss 5.295\n",
      "Ep 1 (Step 000330): Train loss 5.108, Val loss 5.302\n",
      "Ep 1 (Step 000340): Train loss 5.176, Val loss 5.297\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2967\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.643, Val loss 8.564\n",
      "Ep 1 (Step 000010): Train loss 6.916, Val loss 6.859\n",
      "Ep 1 (Step 000020): Train loss 6.455, Val loss 6.449\n",
      "Ep 1 (Step 000030): Train loss 6.416, Val loss 6.437\n",
      "Ep 1 (Step 000040): Train loss 6.328, Val loss 6.318\n",
      "Ep 1 (Step 000050): Train loss 6.216, Val loss 6.166\n",
      "Ep 1 (Step 000060): Train loss 6.019, Val loss 6.034\n",
      "Ep 1 (Step 000070): Train loss 5.889, Val loss 5.937\n",
      "Ep 1 (Step 000080): Train loss 5.869, Val loss 5.864\n",
      "Ep 1 (Step 000090): Train loss 5.812, Val loss 5.789\n",
      "Ep 1 (Step 000100): Train loss 5.693, Val loss 5.739\n",
      "Ep 1 (Step 000110): Train loss 5.657, Val loss 5.685\n",
      "Ep 1 (Step 000120): Train loss 5.544, Val loss 5.628\n",
      "Ep 1 (Step 000130): Train loss 5.567, Val loss 5.589\n",
      "Ep 1 (Step 000140): Train loss 5.563, Val loss 5.556\n",
      "Ep 1 (Step 000150): Train loss 5.439, Val loss 5.528\n",
      "Ep 1 (Step 000160): Train loss 5.461, Val loss 5.511\n",
      "Ep 1 (Step 000170): Train loss 5.352, Val loss 5.508\n",
      "Ep 1 (Step 000180): Train loss 5.442, Val loss 5.487\n",
      "Ep 1 (Step 000190): Train loss 5.385, Val loss 5.501\n",
      "Ep 1 (Step 000200): Train loss 5.409, Val loss 5.435\n",
      "Ep 1 (Step 000210): Train loss 5.405, Val loss 5.409\n",
      "Ep 1 (Step 000220): Train loss 5.325, Val loss 5.412\n",
      "Ep 1 (Step 000230): Train loss 5.324, Val loss 5.391\n",
      "Ep 1 (Step 000240): Train loss 5.307, Val loss 5.392\n",
      "Ep 1 (Step 000250): Train loss 5.342, Val loss 5.358\n",
      "Ep 1 (Step 000260): Train loss 5.199, Val loss 5.350\n",
      "Ep 1 (Step 000270): Train loss 5.251, Val loss 5.352\n",
      "Ep 1 (Step 000280): Train loss 5.226, Val loss 5.334\n",
      "Ep 1 (Step 000290): Train loss 5.192, Val loss 5.326\n",
      "Ep 1 (Step 000300): Train loss 5.149, Val loss 5.315\n",
      "Ep 1 (Step 000310): Train loss 5.201, Val loss 5.296\n",
      "Ep 1 (Step 000320): Train loss 5.151, Val loss 5.255\n",
      "Ep 1 (Step 000330): Train loss 5.150, Val loss 5.257\n",
      "Ep 1 (Step 000340): Train loss 5.109, Val loss 5.237\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2369\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.543, Val loss 8.512\n",
      "Ep 1 (Step 000010): Train loss 6.844, Val loss 6.806\n",
      "Ep 1 (Step 000020): Train loss 6.473, Val loss 6.474\n",
      "Ep 1 (Step 000030): Train loss 6.424, Val loss 6.408\n",
      "Ep 1 (Step 000040): Train loss 6.237, Val loss 6.289\n",
      "Ep 1 (Step 000050): Train loss 6.191, Val loss 6.176\n",
      "Ep 1 (Step 000060): Train loss 6.056, Val loss 6.048\n",
      "Ep 1 (Step 000070): Train loss 5.892, Val loss 5.954\n",
      "Ep 1 (Step 000080): Train loss 5.823, Val loss 5.866\n",
      "Ep 1 (Step 000090): Train loss 5.727, Val loss 5.788\n",
      "Ep 1 (Step 000100): Train loss 5.683, Val loss 5.772\n",
      "Ep 1 (Step 000110): Train loss 5.565, Val loss 5.736\n",
      "Ep 1 (Step 000120): Train loss 5.572, Val loss 5.668\n",
      "Ep 1 (Step 000130): Train loss 5.503, Val loss 5.637\n",
      "Ep 1 (Step 000140): Train loss 5.448, Val loss 5.596\n",
      "Ep 1 (Step 000150): Train loss 5.479, Val loss 5.585\n",
      "Ep 1 (Step 000160): Train loss 5.407, Val loss 5.531\n",
      "Ep 1 (Step 000170): Train loss 5.465, Val loss 5.508\n",
      "Ep 1 (Step 000180): Train loss 5.464, Val loss 5.487\n",
      "Ep 1 (Step 000190): Train loss 5.395, Val loss 5.469\n",
      "Ep 1 (Step 000200): Train loss 5.502, Val loss 5.453\n",
      "Ep 1 (Step 000210): Train loss 5.365, Val loss 5.441\n",
      "Ep 1 (Step 000220): Train loss 5.321, Val loss 5.417\n",
      "Ep 1 (Step 000230): Train loss 5.251, Val loss 5.400\n",
      "Ep 1 (Step 000240): Train loss 5.286, Val loss 5.382\n",
      "Ep 1 (Step 000250): Train loss 5.257, Val loss 5.385\n",
      "Ep 1 (Step 000260): Train loss 5.261, Val loss 5.345\n",
      "Ep 1 (Step 000270): Train loss 5.238, Val loss 5.330\n",
      "Ep 1 (Step 000280): Train loss 5.303, Val loss 5.322\n",
      "Ep 1 (Step 000290): Train loss 5.270, Val loss 5.324\n",
      "Ep 1 (Step 000300): Train loss 5.214, Val loss 5.321\n",
      "Ep 1 (Step 000310): Train loss 5.158, Val loss 5.312\n",
      "Ep 1 (Step 000320): Train loss 5.214, Val loss 5.291\n",
      "Ep 1 (Step 000330): Train loss 5.116, Val loss 5.290\n",
      "Ep 1 (Step 000340): Train loss 5.224, Val loss 5.278\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2778\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.568, Val loss 8.563\n",
      "Ep 1 (Step 000010): Train loss 6.879, Val loss 6.851\n",
      "Ep 1 (Step 000020): Train loss 6.425, Val loss 6.447\n",
      "Ep 1 (Step 000030): Train loss 6.387, Val loss 6.388\n",
      "Ep 1 (Step 000040): Train loss 6.287, Val loss 6.283\n",
      "Ep 1 (Step 000050): Train loss 6.109, Val loss 6.133\n",
      "Ep 1 (Step 000060): Train loss 5.948, Val loss 6.009\n",
      "Ep 1 (Step 000070): Train loss 5.874, Val loss 5.908\n",
      "Ep 1 (Step 000080): Train loss 5.869, Val loss 5.861\n",
      "Ep 1 (Step 000090): Train loss 5.726, Val loss 5.802\n",
      "Ep 1 (Step 000100): Train loss 5.582, Val loss 5.737\n",
      "Ep 1 (Step 000110): Train loss 5.591, Val loss 5.666\n",
      "Ep 1 (Step 000120): Train loss 5.494, Val loss 5.637\n",
      "Ep 1 (Step 000130): Train loss 5.561, Val loss 5.590\n",
      "Ep 1 (Step 000140): Train loss 5.558, Val loss 5.571\n",
      "Ep 1 (Step 000150): Train loss 5.505, Val loss 5.530\n",
      "Ep 1 (Step 000160): Train loss 5.388, Val loss 5.519\n",
      "Ep 1 (Step 000170): Train loss 5.421, Val loss 5.497\n",
      "Ep 1 (Step 000180): Train loss 5.406, Val loss 5.486\n",
      "Ep 1 (Step 000190): Train loss 5.286, Val loss 5.455\n",
      "Ep 1 (Step 000200): Train loss 5.386, Val loss 5.418\n",
      "Ep 1 (Step 000210): Train loss 5.382, Val loss 5.416\n",
      "Ep 1 (Step 000220): Train loss 5.278, Val loss 5.406\n",
      "Ep 1 (Step 000230): Train loss 5.300, Val loss 5.381\n",
      "Ep 1 (Step 000240): Train loss 5.264, Val loss 5.381\n",
      "Ep 1 (Step 000250): Train loss 5.258, Val loss 5.358\n",
      "Ep 1 (Step 000260): Train loss 5.209, Val loss 5.337\n",
      "Ep 1 (Step 000270): Train loss 5.241, Val loss 5.327\n",
      "Ep 1 (Step 000280): Train loss 5.183, Val loss 5.330\n",
      "Ep 1 (Step 000290): Train loss 5.160, Val loss 5.323\n",
      "Ep 1 (Step 000300): Train loss 5.075, Val loss 5.281\n",
      "Ep 1 (Step 000310): Train loss 5.251, Val loss 5.287\n",
      "Ep 1 (Step 000320): Train loss 5.078, Val loss 5.271\n",
      "Ep 1 (Step 000330): Train loss 5.074, Val loss 5.254\n",
      "Ep 1 (Step 000340): Train loss 5.173, Val loss 5.275\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2747\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.774, Val loss 8.746\n",
      "Ep 1 (Step 000010): Train loss 7.425, Val loss 7.375\n",
      "Ep 1 (Step 000020): Train loss 6.766, Val loss 6.738\n",
      "Ep 1 (Step 000030): Train loss 6.408, Val loss 6.430\n",
      "Ep 1 (Step 000040): Train loss 6.311, Val loss 6.313\n",
      "Ep 1 (Step 000050): Train loss 6.256, Val loss 6.186\n",
      "Ep 1 (Step 000060): Train loss 6.032, Val loss 6.059\n",
      "Ep 1 (Step 000070): Train loss 5.942, Val loss 5.978\n",
      "Ep 1 (Step 000080): Train loss 5.995, Val loss 5.908\n",
      "Ep 1 (Step 000090): Train loss 5.767, Val loss 5.838\n",
      "Ep 1 (Step 000100): Train loss 5.764, Val loss 5.792\n",
      "Ep 1 (Step 000110): Train loss 5.628, Val loss 5.764\n",
      "Ep 1 (Step 000120): Train loss 5.558, Val loss 5.734\n",
      "Ep 1 (Step 000130): Train loss 5.646, Val loss 5.673\n",
      "Ep 1 (Step 000140): Train loss 5.564, Val loss 5.641\n",
      "Ep 1 (Step 000150): Train loss 5.485, Val loss 5.601\n",
      "Ep 1 (Step 000160): Train loss 5.455, Val loss 5.582\n",
      "Ep 1 (Step 000170): Train loss 5.416, Val loss 5.534\n",
      "Ep 1 (Step 000180): Train loss 5.444, Val loss 5.515\n",
      "Ep 1 (Step 000190): Train loss 5.336, Val loss 5.501\n",
      "Ep 1 (Step 000200): Train loss 5.426, Val loss 5.477\n",
      "Ep 1 (Step 000210): Train loss 5.426, Val loss 5.461\n",
      "Ep 1 (Step 000220): Train loss 5.347, Val loss 5.447\n",
      "Ep 1 (Step 000230): Train loss 5.362, Val loss 5.422\n",
      "Ep 1 (Step 000240): Train loss 5.311, Val loss 5.399\n",
      "Ep 1 (Step 000250): Train loss 5.299, Val loss 5.388\n",
      "Ep 1 (Step 000260): Train loss 5.284, Val loss 5.372\n",
      "Ep 1 (Step 000270): Train loss 5.305, Val loss 5.366\n",
      "Ep 1 (Step 000280): Train loss 5.285, Val loss 5.344\n",
      "Ep 1 (Step 000290): Train loss 5.304, Val loss 5.342\n",
      "Ep 1 (Step 000300): Train loss 5.176, Val loss 5.328\n",
      "Ep 1 (Step 000310): Train loss 5.212, Val loss 5.316\n",
      "Ep 1 (Step 000320): Train loss 5.203, Val loss 5.306\n",
      "Ep 1 (Step 000330): Train loss 5.195, Val loss 5.299\n",
      "Ep 1 (Step 000340): Train loss 5.187, Val loss 5.297\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2967\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.749, Val loss 8.723\n",
      "Ep 1 (Step 000010): Train loss 7.375, Val loss 7.346\n",
      "Ep 1 (Step 000020): Train loss 6.802, Val loss 6.719\n",
      "Ep 1 (Step 000030): Train loss 6.492, Val loss 6.442\n",
      "Ep 1 (Step 000040): Train loss 6.293, Val loss 6.331\n",
      "Ep 1 (Step 000050): Train loss 6.185, Val loss 6.204\n",
      "Ep 1 (Step 000060): Train loss 6.011, Val loss 6.083\n",
      "Ep 1 (Step 000070): Train loss 6.055, Val loss 6.023\n",
      "Ep 1 (Step 000080): Train loss 5.959, Val loss 5.937\n",
      "Ep 1 (Step 000090): Train loss 5.843, Val loss 5.877\n",
      "Ep 1 (Step 000100): Train loss 5.738, Val loss 5.824\n",
      "Ep 1 (Step 000110): Train loss 5.798, Val loss 5.775\n",
      "Ep 1 (Step 000120): Train loss 5.798, Val loss 5.722\n",
      "Ep 1 (Step 000130): Train loss 5.640, Val loss 5.686\n",
      "Ep 1 (Step 000140): Train loss 5.611, Val loss 5.639\n",
      "Ep 1 (Step 000150): Train loss 5.510, Val loss 5.615\n",
      "Ep 1 (Step 000160): Train loss 5.596, Val loss 5.579\n",
      "Ep 1 (Step 000170): Train loss 5.493, Val loss 5.553\n",
      "Ep 1 (Step 000180): Train loss 5.425, Val loss 5.523\n",
      "Ep 1 (Step 000190): Train loss 5.428, Val loss 5.501\n",
      "Ep 1 (Step 000200): Train loss 5.446, Val loss 5.488\n",
      "Ep 1 (Step 000210): Train loss 5.417, Val loss 5.480\n",
      "Ep 1 (Step 000220): Train loss 5.363, Val loss 5.442\n",
      "Ep 1 (Step 000230): Train loss 5.324, Val loss 5.434\n",
      "Ep 1 (Step 000240): Train loss 5.313, Val loss 5.424\n",
      "Ep 1 (Step 000250): Train loss 5.267, Val loss 5.409\n",
      "Ep 1 (Step 000260): Train loss 5.346, Val loss 5.395\n",
      "Ep 1 (Step 000270): Train loss 5.276, Val loss 5.383\n",
      "Ep 1 (Step 000280): Train loss 5.189, Val loss 5.368\n",
      "Ep 1 (Step 000290): Train loss 5.249, Val loss 5.365\n",
      "Ep 1 (Step 000300): Train loss 5.131, Val loss 5.343\n",
      "Ep 1 (Step 000310): Train loss 5.283, Val loss 5.349\n",
      "Ep 1 (Step 000320): Train loss 5.152, Val loss 5.331\n",
      "Ep 1 (Step 000330): Train loss 5.215, Val loss 5.320\n",
      "Ep 1 (Step 000340): Train loss 5.228, Val loss 5.302\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3025\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.789, Val loss 8.772\n",
      "Ep 1 (Step 000010): Train loss 7.403, Val loss 7.415\n",
      "Ep 1 (Step 000020): Train loss 6.753, Val loss 6.764\n",
      "Ep 1 (Step 000030): Train loss 6.430, Val loss 6.466\n",
      "Ep 1 (Step 000040): Train loss 6.357, Val loss 6.362\n",
      "Ep 1 (Step 000050): Train loss 6.245, Val loss 6.265\n",
      "Ep 1 (Step 000060): Train loss 6.214, Val loss 6.234\n",
      "Ep 1 (Step 000070): Train loss 6.042, Val loss 6.031\n",
      "Ep 1 (Step 000080): Train loss 5.960, Val loss 5.965\n",
      "Ep 1 (Step 000090): Train loss 5.884, Val loss 5.894\n",
      "Ep 1 (Step 000100): Train loss 5.793, Val loss 5.846\n",
      "Ep 1 (Step 000110): Train loss 5.731, Val loss 5.795\n",
      "Ep 1 (Step 000120): Train loss 5.701, Val loss 5.741\n",
      "Ep 1 (Step 000130): Train loss 5.652, Val loss 5.703\n",
      "Ep 1 (Step 000140): Train loss 5.497, Val loss 5.673\n",
      "Ep 1 (Step 000150): Train loss 5.539, Val loss 5.643\n",
      "Ep 1 (Step 000160): Train loss 5.525, Val loss 5.600\n",
      "Ep 1 (Step 000170): Train loss 5.485, Val loss 5.572\n",
      "Ep 1 (Step 000180): Train loss 5.521, Val loss 5.557\n",
      "Ep 1 (Step 000190): Train loss 5.484, Val loss 5.522\n",
      "Ep 1 (Step 000200): Train loss 5.334, Val loss 5.494\n",
      "Ep 1 (Step 000210): Train loss 5.403, Val loss 5.470\n",
      "Ep 1 (Step 000220): Train loss 5.316, Val loss 5.464\n",
      "Ep 1 (Step 000230): Train loss 5.477, Val loss 5.446\n",
      "Ep 1 (Step 000240): Train loss 5.358, Val loss 5.415\n",
      "Ep 1 (Step 000250): Train loss 5.343, Val loss 5.401\n",
      "Ep 1 (Step 000260): Train loss 5.371, Val loss 5.377\n",
      "Ep 1 (Step 000270): Train loss 5.212, Val loss 5.372\n",
      "Ep 1 (Step 000280): Train loss 5.363, Val loss 5.356\n",
      "Ep 1 (Step 000290): Train loss 5.302, Val loss 5.351\n",
      "Ep 1 (Step 000300): Train loss 5.222, Val loss 5.342\n",
      "Ep 1 (Step 000310): Train loss 5.283, Val loss 5.334\n",
      "Ep 1 (Step 000320): Train loss 5.217, Val loss 5.317\n",
      "Ep 1 (Step 000330): Train loss 5.247, Val loss 5.297\n",
      "Ep 1 (Step 000340): Train loss 5.132, Val loss 5.291\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2909\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.802, Val loss 8.815\n",
      "Ep 1 (Step 000010): Train loss 7.462, Val loss 7.415\n",
      "Ep 1 (Step 000020): Train loss 6.746, Val loss 6.739\n",
      "Ep 1 (Step 000030): Train loss 6.449, Val loss 6.417\n",
      "Ep 1 (Step 000040): Train loss 6.318, Val loss 6.317\n",
      "Ep 1 (Step 000050): Train loss 6.258, Val loss 6.215\n",
      "Ep 1 (Step 000060): Train loss 6.090, Val loss 6.117\n",
      "Ep 1 (Step 000070): Train loss 6.061, Val loss 6.007\n",
      "Ep 1 (Step 000080): Train loss 5.859, Val loss 5.943\n",
      "Ep 1 (Step 000090): Train loss 5.888, Val loss 5.865\n",
      "Ep 1 (Step 000100): Train loss 5.806, Val loss 5.802\n",
      "Ep 1 (Step 000110): Train loss 5.687, Val loss 5.758\n",
      "Ep 1 (Step 000120): Train loss 5.656, Val loss 5.731\n",
      "Ep 1 (Step 000130): Train loss 5.649, Val loss 5.681\n",
      "Ep 1 (Step 000140): Train loss 5.602, Val loss 5.656\n",
      "Ep 1 (Step 000150): Train loss 5.486, Val loss 5.633\n",
      "Ep 1 (Step 000160): Train loss 5.428, Val loss 5.577\n",
      "Ep 1 (Step 000170): Train loss 5.380, Val loss 5.555\n",
      "Ep 1 (Step 000180): Train loss 5.433, Val loss 5.522\n",
      "Ep 1 (Step 000190): Train loss 5.291, Val loss 5.493\n",
      "Ep 1 (Step 000200): Train loss 5.457, Val loss 5.470\n",
      "Ep 1 (Step 000210): Train loss 5.378, Val loss 5.459\n",
      "Ep 1 (Step 000220): Train loss 5.292, Val loss 5.451\n",
      "Ep 1 (Step 000230): Train loss 5.351, Val loss 5.437\n",
      "Ep 1 (Step 000240): Train loss 5.267, Val loss 5.413\n",
      "Ep 1 (Step 000250): Train loss 5.314, Val loss 5.392\n",
      "Ep 1 (Step 000260): Train loss 5.232, Val loss 5.374\n",
      "Ep 1 (Step 000270): Train loss 5.315, Val loss 5.367\n",
      "Ep 1 (Step 000280): Train loss 5.268, Val loss 5.332\n",
      "Ep 1 (Step 000290): Train loss 5.217, Val loss 5.333\n",
      "Ep 1 (Step 000300): Train loss 5.231, Val loss 5.333\n",
      "Ep 1 (Step 000310): Train loss 5.192, Val loss 5.317\n",
      "Ep 1 (Step 000320): Train loss 5.202, Val loss 5.305\n",
      "Ep 1 (Step 000330): Train loss 5.139, Val loss 5.298\n",
      "Ep 1 (Step 000340): Train loss 5.155, Val loss 5.273\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2726\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.748, Val loss 8.728\n",
      "Ep 1 (Step 000010): Train loss 7.374, Val loss 7.354\n",
      "Ep 1 (Step 000020): Train loss 6.736, Val loss 6.726\n",
      "Ep 1 (Step 000030): Train loss 6.459, Val loss 6.441\n",
      "Ep 1 (Step 000040): Train loss 6.390, Val loss 6.334\n",
      "Ep 1 (Step 000050): Train loss 6.218, Val loss 6.232\n",
      "Ep 1 (Step 000060): Train loss 6.098, Val loss 6.105\n",
      "Ep 1 (Step 000070): Train loss 5.975, Val loss 6.013\n",
      "Ep 1 (Step 000080): Train loss 5.985, Val loss 5.925\n",
      "Ep 1 (Step 000090): Train loss 5.859, Val loss 5.869\n",
      "Ep 1 (Step 000100): Train loss 5.744, Val loss 5.817\n",
      "Ep 1 (Step 000110): Train loss 5.771, Val loss 5.775\n",
      "Ep 1 (Step 000120): Train loss 5.612, Val loss 5.714\n",
      "Ep 1 (Step 000130): Train loss 5.614, Val loss 5.682\n",
      "Ep 1 (Step 000140): Train loss 5.580, Val loss 5.646\n",
      "Ep 1 (Step 000150): Train loss 5.570, Val loss 5.609\n",
      "Ep 1 (Step 000160): Train loss 5.503, Val loss 5.591\n",
      "Ep 1 (Step 000170): Train loss 5.594, Val loss 5.554\n",
      "Ep 1 (Step 000180): Train loss 5.424, Val loss 5.521\n",
      "Ep 1 (Step 000190): Train loss 5.409, Val loss 5.503\n",
      "Ep 1 (Step 000200): Train loss 5.350, Val loss 5.468\n",
      "Ep 1 (Step 000210): Train loss 5.409, Val loss 5.447\n",
      "Ep 1 (Step 000220): Train loss 5.308, Val loss 5.430\n",
      "Ep 1 (Step 000230): Train loss 5.274, Val loss 5.412\n",
      "Ep 1 (Step 000240): Train loss 5.295, Val loss 5.397\n",
      "Ep 1 (Step 000250): Train loss 5.269, Val loss 5.397\n",
      "Ep 1 (Step 000260): Train loss 5.287, Val loss 5.369\n",
      "Ep 1 (Step 000270): Train loss 5.221, Val loss 5.354\n",
      "Ep 1 (Step 000280): Train loss 5.225, Val loss 5.349\n",
      "Ep 1 (Step 000290): Train loss 5.151, Val loss 5.337\n",
      "Ep 1 (Step 000300): Train loss 5.197, Val loss 5.310\n",
      "Ep 1 (Step 000310): Train loss 5.251, Val loss 5.311\n",
      "Ep 1 (Step 000320): Train loss 5.173, Val loss 5.304\n",
      "Ep 1 (Step 000330): Train loss 5.192, Val loss 5.278\n",
      "Ep 1 (Step 000340): Train loss 5.177, Val loss 5.264\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2645\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.767, Val loss 8.760\n",
      "Ep 1 (Step 000010): Train loss 7.375, Val loss 7.348\n",
      "Ep 1 (Step 000020): Train loss 6.765, Val loss 6.701\n",
      "Ep 1 (Step 000030): Train loss 6.485, Val loss 6.418\n",
      "Ep 1 (Step 000040): Train loss 6.329, Val loss 6.335\n",
      "Ep 1 (Step 000050): Train loss 6.259, Val loss 6.222\n",
      "Ep 1 (Step 000060): Train loss 6.059, Val loss 6.107\n",
      "Ep 1 (Step 000070): Train loss 6.037, Val loss 6.005\n",
      "Ep 1 (Step 000080): Train loss 5.948, Val loss 5.933\n",
      "Ep 1 (Step 000090): Train loss 5.818, Val loss 5.871\n",
      "Ep 1 (Step 000100): Train loss 5.807, Val loss 5.835\n",
      "Ep 1 (Step 000110): Train loss 5.687, Val loss 5.771\n",
      "Ep 1 (Step 000120): Train loss 5.667, Val loss 5.731\n",
      "Ep 1 (Step 000130): Train loss 5.652, Val loss 5.684\n",
      "Ep 1 (Step 000140): Train loss 5.581, Val loss 5.646\n",
      "Ep 1 (Step 000150): Train loss 5.514, Val loss 5.627\n",
      "Ep 1 (Step 000160): Train loss 5.457, Val loss 5.585\n",
      "Ep 1 (Step 000170): Train loss 5.429, Val loss 5.558\n",
      "Ep 1 (Step 000180): Train loss 5.449, Val loss 5.539\n",
      "Ep 1 (Step 000190): Train loss 5.422, Val loss 5.504\n",
      "Ep 1 (Step 000200): Train loss 5.369, Val loss 5.474\n",
      "Ep 1 (Step 000210): Train loss 5.417, Val loss 5.458\n",
      "Ep 1 (Step 000220): Train loss 5.292, Val loss 5.436\n",
      "Ep 1 (Step 000230): Train loss 5.429, Val loss 5.438\n",
      "Ep 1 (Step 000240): Train loss 5.334, Val loss 5.411\n",
      "Ep 1 (Step 000250): Train loss 5.262, Val loss 5.402\n",
      "Ep 1 (Step 000260): Train loss 5.294, Val loss 5.391\n",
      "Ep 1 (Step 000270): Train loss 5.301, Val loss 5.359\n",
      "Ep 1 (Step 000280): Train loss 5.258, Val loss 5.351\n",
      "Ep 1 (Step 000290): Train loss 5.276, Val loss 5.328\n",
      "Ep 1 (Step 000300): Train loss 5.175, Val loss 5.318\n",
      "Ep 1 (Step 000310): Train loss 5.221, Val loss 5.300\n",
      "Ep 1 (Step 000320): Train loss 5.216, Val loss 5.283\n",
      "Ep 1 (Step 000330): Train loss 5.173, Val loss 5.273\n",
      "Ep 1 (Step 000340): Train loss 5.143, Val loss 5.274\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2743\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.559, Val loss 8.565\n",
      "Ep 1 (Step 000010): Train loss 6.828, Val loss 6.798\n",
      "Ep 1 (Step 000020): Train loss 6.502, Val loss 6.426\n",
      "Ep 1 (Step 000030): Train loss 6.346, Val loss 6.379\n",
      "Ep 1 (Step 000040): Train loss 6.259, Val loss 6.235\n",
      "Ep 1 (Step 000050): Train loss 6.020, Val loss 6.073\n",
      "Ep 1 (Step 000060): Train loss 5.972, Val loss 5.984\n",
      "Ep 1 (Step 000070): Train loss 5.856, Val loss 5.918\n",
      "Ep 1 (Step 000080): Train loss 5.780, Val loss 5.824\n",
      "Ep 1 (Step 000090): Train loss 5.702, Val loss 5.738\n",
      "Ep 1 (Step 000100): Train loss 5.572, Val loss 5.715\n",
      "Ep 1 (Step 000110): Train loss 5.479, Val loss 5.647\n",
      "Ep 1 (Step 000120): Train loss 5.647, Val loss 5.609\n",
      "Ep 1 (Step 000130): Train loss 5.539, Val loss 5.584\n",
      "Ep 1 (Step 000140): Train loss 5.482, Val loss 5.554\n",
      "Ep 1 (Step 000150): Train loss 5.425, Val loss 5.521\n",
      "Ep 1 (Step 000160): Train loss 5.433, Val loss 5.510\n",
      "Ep 1 (Step 000170): Train loss 5.342, Val loss 5.486\n",
      "Ep 1 (Step 000180): Train loss 5.352, Val loss 5.463\n",
      "Ep 1 (Step 000190): Train loss 5.356, Val loss 5.439\n",
      "Ep 1 (Step 000200): Train loss 5.317, Val loss 5.423\n",
      "Ep 1 (Step 000210): Train loss 5.334, Val loss 5.396\n",
      "Ep 1 (Step 000220): Train loss 5.222, Val loss 5.377\n",
      "Ep 1 (Step 000230): Train loss 5.292, Val loss 5.379\n",
      "Ep 1 (Step 000240): Train loss 5.185, Val loss 5.360\n",
      "Ep 1 (Step 000250): Train loss 5.244, Val loss 5.339\n",
      "Ep 1 (Step 000260): Train loss 5.267, Val loss 5.325\n",
      "Ep 1 (Step 000270): Train loss 5.143, Val loss 5.324\n",
      "Ep 1 (Step 000280): Train loss 5.185, Val loss 5.320\n",
      "Ep 1 (Step 000290): Train loss 5.150, Val loss 5.317\n",
      "Ep 1 (Step 000300): Train loss 5.186, Val loss 5.273\n",
      "Ep 1 (Step 000310): Train loss 5.237, Val loss 5.275\n",
      "Ep 1 (Step 000320): Train loss 5.083, Val loss 5.267\n",
      "Ep 1 (Step 000330): Train loss 5.125, Val loss 5.271\n",
      "Ep 1 (Step 000340): Train loss 5.024, Val loss 5.266\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2662\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.524, Val loss 8.504\n",
      "Ep 1 (Step 000010): Train loss 6.881, Val loss 6.816\n",
      "Ep 1 (Step 000020): Train loss 6.456, Val loss 6.424\n",
      "Ep 1 (Step 000030): Train loss 6.321, Val loss 6.344\n",
      "Ep 1 (Step 000040): Train loss 6.218, Val loss 6.226\n",
      "Ep 1 (Step 000050): Train loss 6.095, Val loss 6.083\n",
      "Ep 1 (Step 000060): Train loss 5.967, Val loss 5.977\n",
      "Ep 1 (Step 000070): Train loss 5.939, Val loss 5.890\n",
      "Ep 1 (Step 000080): Train loss 5.817, Val loss 5.819\n",
      "Ep 1 (Step 000090): Train loss 5.715, Val loss 5.753\n",
      "Ep 1 (Step 000100): Train loss 5.605, Val loss 5.688\n",
      "Ep 1 (Step 000110): Train loss 5.588, Val loss 5.639\n",
      "Ep 1 (Step 000120): Train loss 5.581, Val loss 5.594\n",
      "Ep 1 (Step 000130): Train loss 5.436, Val loss 5.560\n",
      "Ep 1 (Step 000140): Train loss 5.499, Val loss 5.536\n",
      "Ep 1 (Step 000150): Train loss 5.471, Val loss 5.526\n",
      "Ep 1 (Step 000160): Train loss 5.461, Val loss 5.477\n",
      "Ep 1 (Step 000170): Train loss 5.328, Val loss 5.470\n",
      "Ep 1 (Step 000180): Train loss 5.305, Val loss 5.471\n",
      "Ep 1 (Step 000190): Train loss 5.337, Val loss 5.454\n",
      "Ep 1 (Step 000200): Train loss 5.298, Val loss 5.421\n",
      "Ep 1 (Step 000210): Train loss 5.217, Val loss 5.406\n",
      "Ep 1 (Step 000220): Train loss 5.295, Val loss 5.402\n",
      "Ep 1 (Step 000230): Train loss 5.195, Val loss 5.392\n",
      "Ep 1 (Step 000240): Train loss 5.204, Val loss 5.370\n",
      "Ep 1 (Step 000250): Train loss 5.198, Val loss 5.352\n",
      "Ep 1 (Step 000260): Train loss 5.190, Val loss 5.316\n",
      "Ep 1 (Step 000270): Train loss 5.176, Val loss 5.301\n",
      "Ep 1 (Step 000280): Train loss 5.195, Val loss 5.307\n",
      "Ep 1 (Step 000290): Train loss 5.214, Val loss 5.293\n",
      "Ep 1 (Step 000300): Train loss 5.122, Val loss 5.275\n",
      "Ep 1 (Step 000310): Train loss 5.250, Val loss 5.258\n",
      "Ep 1 (Step 000320): Train loss 5.090, Val loss 5.265\n",
      "Ep 1 (Step 000330): Train loss 5.110, Val loss 5.257\n",
      "Ep 1 (Step 000340): Train loss 5.190, Val loss 5.245\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2450\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.506, Val loss 8.495\n",
      "Ep 1 (Step 000010): Train loss 6.818, Val loss 6.830\n",
      "Ep 1 (Step 000020): Train loss 6.442, Val loss 6.444\n",
      "Ep 1 (Step 000030): Train loss 6.392, Val loss 6.382\n",
      "Ep 1 (Step 000040): Train loss 6.206, Val loss 6.191\n",
      "Ep 1 (Step 000050): Train loss 6.046, Val loss 6.081\n",
      "Ep 1 (Step 000060): Train loss 5.994, Val loss 5.959\n",
      "Ep 1 (Step 000070): Train loss 5.795, Val loss 5.865\n",
      "Ep 1 (Step 000080): Train loss 5.787, Val loss 5.799\n",
      "Ep 1 (Step 000090): Train loss 5.714, Val loss 5.739\n",
      "Ep 1 (Step 000100): Train loss 5.689, Val loss 5.718\n",
      "Ep 1 (Step 000110): Train loss 5.601, Val loss 5.633\n",
      "Ep 1 (Step 000120): Train loss 5.519, Val loss 5.594\n",
      "Ep 1 (Step 000130): Train loss 5.577, Val loss 5.565\n",
      "Ep 1 (Step 000140): Train loss 5.460, Val loss 5.554\n",
      "Ep 1 (Step 000150): Train loss 5.478, Val loss 5.509\n",
      "Ep 1 (Step 000160): Train loss 5.413, Val loss 5.490\n",
      "Ep 1 (Step 000170): Train loss 5.337, Val loss 5.467\n",
      "Ep 1 (Step 000180): Train loss 5.266, Val loss 5.435\n",
      "Ep 1 (Step 000190): Train loss 5.261, Val loss 5.415\n",
      "Ep 1 (Step 000200): Train loss 5.329, Val loss 5.407\n",
      "Ep 1 (Step 000210): Train loss 5.277, Val loss 5.393\n",
      "Ep 1 (Step 000220): Train loss 5.209, Val loss 5.401\n",
      "Ep 1 (Step 000230): Train loss 5.348, Val loss 5.375\n",
      "Ep 1 (Step 000240): Train loss 5.159, Val loss 5.369\n",
      "Ep 1 (Step 000250): Train loss 5.198, Val loss 5.356\n",
      "Ep 1 (Step 000260): Train loss 5.153, Val loss 5.352\n",
      "Ep 1 (Step 000270): Train loss 5.201, Val loss 5.339\n",
      "Ep 1 (Step 000280): Train loss 5.246, Val loss 5.317\n",
      "Ep 1 (Step 000290): Train loss 5.255, Val loss 5.286\n",
      "Ep 1 (Step 000300): Train loss 5.177, Val loss 5.310\n",
      "Ep 1 (Step 000310): Train loss 5.170, Val loss 5.287\n",
      "Ep 1 (Step 000320): Train loss 5.108, Val loss 5.267\n",
      "Ep 1 (Step 000330): Train loss 5.034, Val loss 5.255\n",
      "Ep 1 (Step 000340): Train loss 5.135, Val loss 5.237\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2372\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.575, Val loss 8.540\n",
      "Ep 1 (Step 000010): Train loss 6.875, Val loss 6.855\n",
      "Ep 1 (Step 000020): Train loss 6.474, Val loss 6.436\n",
      "Ep 1 (Step 000030): Train loss 6.356, Val loss 6.342\n",
      "Ep 1 (Step 000040): Train loss 6.225, Val loss 6.172\n",
      "Ep 1 (Step 000050): Train loss 6.097, Val loss 6.062\n",
      "Ep 1 (Step 000060): Train loss 5.903, Val loss 5.926\n",
      "Ep 1 (Step 000070): Train loss 5.834, Val loss 5.838\n",
      "Ep 1 (Step 000080): Train loss 5.762, Val loss 5.787\n",
      "Ep 1 (Step 000090): Train loss 5.733, Val loss 5.755\n",
      "Ep 1 (Step 000100): Train loss 5.595, Val loss 5.677\n",
      "Ep 1 (Step 000110): Train loss 5.557, Val loss 5.629\n",
      "Ep 1 (Step 000120): Train loss 5.597, Val loss 5.587\n",
      "Ep 1 (Step 000130): Train loss 5.458, Val loss 5.554\n",
      "Ep 1 (Step 000140): Train loss 5.458, Val loss 5.517\n",
      "Ep 1 (Step 000150): Train loss 5.434, Val loss 5.473\n",
      "Ep 1 (Step 000160): Train loss 5.331, Val loss 5.454\n",
      "Ep 1 (Step 000170): Train loss 5.429, Val loss 5.433\n",
      "Ep 1 (Step 000180): Train loss 5.329, Val loss 5.415\n",
      "Ep 1 (Step 000190): Train loss 5.404, Val loss 5.404\n",
      "Ep 1 (Step 000200): Train loss 5.328, Val loss 5.382\n",
      "Ep 1 (Step 000210): Train loss 5.262, Val loss 5.358\n",
      "Ep 1 (Step 000220): Train loss 5.258, Val loss 5.367\n",
      "Ep 1 (Step 000230): Train loss 5.207, Val loss 5.343\n",
      "Ep 1 (Step 000240): Train loss 5.146, Val loss 5.330\n",
      "Ep 1 (Step 000250): Train loss 5.251, Val loss 5.293\n",
      "Ep 1 (Step 000260): Train loss 5.265, Val loss 5.302\n",
      "Ep 1 (Step 000270): Train loss 5.198, Val loss 5.289\n",
      "Ep 1 (Step 000280): Train loss 5.142, Val loss 5.276\n",
      "Ep 1 (Step 000290): Train loss 5.201, Val loss 5.273\n",
      "Ep 1 (Step 000300): Train loss 5.213, Val loss 5.267\n",
      "Ep 1 (Step 000310): Train loss 5.107, Val loss 5.252\n",
      "Ep 1 (Step 000320): Train loss 5.150, Val loss 5.239\n",
      "Ep 1 (Step 000330): Train loss 5.077, Val loss 5.240\n",
      "Ep 1 (Step 000340): Train loss 5.118, Val loss 5.237\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2370\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.552, Val loss 8.516\n",
      "Ep 1 (Step 000010): Train loss 6.854, Val loss 6.847\n",
      "Ep 1 (Step 000020): Train loss 6.464, Val loss 6.440\n",
      "Ep 1 (Step 000030): Train loss 6.325, Val loss 6.351\n",
      "Ep 1 (Step 000040): Train loss 6.226, Val loss 6.223\n",
      "Ep 1 (Step 000050): Train loss 6.049, Val loss 6.072\n",
      "Ep 1 (Step 000060): Train loss 5.962, Val loss 5.970\n",
      "Ep 1 (Step 000070): Train loss 5.836, Val loss 5.890\n",
      "Ep 1 (Step 000080): Train loss 5.744, Val loss 5.812\n",
      "Ep 1 (Step 000090): Train loss 5.668, Val loss 5.738\n",
      "Ep 1 (Step 000100): Train loss 5.613, Val loss 5.684\n",
      "Ep 1 (Step 000110): Train loss 5.607, Val loss 5.650\n",
      "Ep 1 (Step 000120): Train loss 5.463, Val loss 5.617\n",
      "Ep 1 (Step 000130): Train loss 5.543, Val loss 5.578\n",
      "Ep 1 (Step 000140): Train loss 5.463, Val loss 5.551\n",
      "Ep 1 (Step 000150): Train loss 5.422, Val loss 5.498\n",
      "Ep 1 (Step 000160): Train loss 5.410, Val loss 5.485\n",
      "Ep 1 (Step 000170): Train loss 5.377, Val loss 5.468\n",
      "Ep 1 (Step 000180): Train loss 5.350, Val loss 5.458\n",
      "Ep 1 (Step 000190): Train loss 5.356, Val loss 5.433\n",
      "Ep 1 (Step 000200): Train loss 5.274, Val loss 5.395\n",
      "Ep 1 (Step 000210): Train loss 5.250, Val loss 5.382\n",
      "Ep 1 (Step 000220): Train loss 5.313, Val loss 5.372\n",
      "Ep 1 (Step 000230): Train loss 5.218, Val loss 5.349\n",
      "Ep 1 (Step 000240): Train loss 5.285, Val loss 5.342\n",
      "Ep 1 (Step 000250): Train loss 5.162, Val loss 5.318\n",
      "Ep 1 (Step 000260): Train loss 5.166, Val loss 5.322\n",
      "Ep 1 (Step 000270): Train loss 5.218, Val loss 5.306\n",
      "Ep 1 (Step 000280): Train loss 5.198, Val loss 5.276\n",
      "Ep 1 (Step 000290): Train loss 5.197, Val loss 5.268\n",
      "Ep 1 (Step 000300): Train loss 5.050, Val loss 5.256\n",
      "Ep 1 (Step 000310): Train loss 5.118, Val loss 5.248\n",
      "Ep 1 (Step 000320): Train loss 5.190, Val loss 5.235\n",
      "Ep 1 (Step 000330): Train loss 5.127, Val loss 5.240\n",
      "Ep 1 (Step 000340): Train loss 5.096, Val loss 5.243\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2428\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.547, Val loss 8.549\n",
      "Ep 1 (Step 000010): Train loss 6.788, Val loss 6.837\n",
      "Ep 1 (Step 000020): Train loss 6.433, Val loss 6.446\n",
      "Ep 1 (Step 000030): Train loss 6.317, Val loss 6.350\n",
      "Ep 1 (Step 000040): Train loss 6.137, Val loss 6.172\n",
      "Ep 1 (Step 000050): Train loss 5.968, Val loss 6.042\n",
      "Ep 1 (Step 000060): Train loss 5.925, Val loss 5.939\n",
      "Ep 1 (Step 000070): Train loss 5.830, Val loss 5.847\n",
      "Ep 1 (Step 000080): Train loss 5.769, Val loss 5.794\n",
      "Ep 1 (Step 000090): Train loss 5.643, Val loss 5.695\n",
      "Ep 1 (Step 000100): Train loss 5.600, Val loss 5.657\n",
      "Ep 1 (Step 000110): Train loss 5.539, Val loss 5.645\n",
      "Ep 1 (Step 000120): Train loss 5.525, Val loss 5.588\n",
      "Ep 1 (Step 000130): Train loss 5.543, Val loss 5.572\n",
      "Ep 1 (Step 000140): Train loss 5.554, Val loss 5.540\n",
      "Ep 1 (Step 000150): Train loss 5.444, Val loss 5.499\n",
      "Ep 1 (Step 000160): Train loss 5.300, Val loss 5.467\n",
      "Ep 1 (Step 000170): Train loss 5.333, Val loss 5.416\n",
      "Ep 1 (Step 000180): Train loss 5.432, Val loss 5.411\n",
      "Ep 1 (Step 000190): Train loss 5.263, Val loss 5.408\n",
      "Ep 1 (Step 000200): Train loss 5.323, Val loss 5.413\n",
      "Ep 1 (Step 000210): Train loss 5.224, Val loss 5.371\n",
      "Ep 1 (Step 000220): Train loss 5.260, Val loss 5.361\n",
      "Ep 1 (Step 000230): Train loss 5.287, Val loss 5.348\n",
      "Ep 1 (Step 000240): Train loss 5.240, Val loss 5.344\n",
      "Ep 1 (Step 000250): Train loss 5.127, Val loss 5.331\n",
      "Ep 1 (Step 000260): Train loss 5.137, Val loss 5.318\n",
      "Ep 1 (Step 000270): Train loss 5.113, Val loss 5.294\n",
      "Ep 1 (Step 000280): Train loss 5.108, Val loss 5.289\n",
      "Ep 1 (Step 000290): Train loss 5.200, Val loss 5.268\n",
      "Ep 1 (Step 000300): Train loss 5.164, Val loss 5.254\n",
      "Ep 1 (Step 000310): Train loss 5.053, Val loss 5.241\n",
      "Ep 1 (Step 000320): Train loss 5.135, Val loss 5.242\n",
      "Ep 1 (Step 000330): Train loss 5.051, Val loss 5.239\n",
      "Ep 1 (Step 000340): Train loss 5.084, Val loss 5.226\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2264\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.862, Val loss 8.847\n",
      "Ep 1 (Step 000010): Train loss 7.432, Val loss 7.427\n",
      "Ep 1 (Step 000020): Train loss 6.781, Val loss 6.748\n",
      "Ep 1 (Step 000030): Train loss 6.525, Val loss 6.429\n",
      "Ep 1 (Step 000040): Train loss 6.318, Val loss 6.304\n",
      "Ep 1 (Step 000050): Train loss 6.218, Val loss 6.190\n",
      "Ep 1 (Step 000060): Train loss 6.169, Val loss 6.083\n",
      "Ep 1 (Step 000070): Train loss 6.008, Val loss 6.006\n",
      "Ep 1 (Step 000080): Train loss 5.843, Val loss 5.929\n",
      "Ep 1 (Step 000090): Train loss 5.894, Val loss 5.876\n",
      "Ep 1 (Step 000100): Train loss 5.722, Val loss 5.805\n",
      "Ep 1 (Step 000110): Train loss 5.745, Val loss 5.759\n",
      "Ep 1 (Step 000120): Train loss 5.691, Val loss 5.710\n",
      "Ep 1 (Step 000130): Train loss 5.607, Val loss 5.680\n",
      "Ep 1 (Step 000140): Train loss 5.574, Val loss 5.640\n",
      "Ep 1 (Step 000150): Train loss 5.538, Val loss 5.626\n",
      "Ep 1 (Step 000160): Train loss 5.479, Val loss 5.590\n",
      "Ep 1 (Step 000170): Train loss 5.435, Val loss 5.565\n",
      "Ep 1 (Step 000180): Train loss 5.441, Val loss 5.532\n",
      "Ep 1 (Step 000190): Train loss 5.416, Val loss 5.503\n",
      "Ep 1 (Step 000200): Train loss 5.410, Val loss 5.490\n",
      "Ep 1 (Step 000210): Train loss 5.349, Val loss 5.468\n",
      "Ep 1 (Step 000220): Train loss 5.384, Val loss 5.452\n",
      "Ep 1 (Step 000230): Train loss 5.312, Val loss 5.443\n",
      "Ep 1 (Step 000240): Train loss 5.262, Val loss 5.435\n",
      "Ep 1 (Step 000250): Train loss 5.412, Val loss 5.420\n",
      "Ep 1 (Step 000260): Train loss 5.251, Val loss 5.410\n",
      "Ep 1 (Step 000270): Train loss 5.327, Val loss 5.392\n",
      "Ep 1 (Step 000280): Train loss 5.228, Val loss 5.380\n",
      "Ep 1 (Step 000290): Train loss 5.238, Val loss 5.371\n",
      "Ep 1 (Step 000300): Train loss 5.245, Val loss 5.356\n",
      "Ep 1 (Step 000310): Train loss 5.185, Val loss 5.324\n",
      "Ep 1 (Step 000320): Train loss 5.134, Val loss 5.331\n",
      "Ep 1 (Step 000330): Train loss 5.184, Val loss 5.323\n",
      "Ep 1 (Step 000340): Train loss 5.205, Val loss 5.312\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3121\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.783, Val loss 8.775\n",
      "Ep 1 (Step 000010): Train loss 7.381, Val loss 7.405\n",
      "Ep 1 (Step 000020): Train loss 6.724, Val loss 6.742\n",
      "Ep 1 (Step 000030): Train loss 6.417, Val loss 6.425\n",
      "Ep 1 (Step 000040): Train loss 6.363, Val loss 6.331\n",
      "Ep 1 (Step 000050): Train loss 6.287, Val loss 6.255\n",
      "Ep 1 (Step 000060): Train loss 6.155, Val loss 6.116\n",
      "Ep 1 (Step 000070): Train loss 5.976, Val loss 6.021\n",
      "Ep 1 (Step 000080): Train loss 5.961, Val loss 5.950\n",
      "Ep 1 (Step 000090): Train loss 5.809, Val loss 5.878\n",
      "Ep 1 (Step 000100): Train loss 5.762, Val loss 5.845\n",
      "Ep 1 (Step 000110): Train loss 5.682, Val loss 5.798\n",
      "Ep 1 (Step 000120): Train loss 5.655, Val loss 5.753\n",
      "Ep 1 (Step 000130): Train loss 5.676, Val loss 5.720\n",
      "Ep 1 (Step 000140): Train loss 5.515, Val loss 5.679\n",
      "Ep 1 (Step 000150): Train loss 5.513, Val loss 5.638\n",
      "Ep 1 (Step 000160): Train loss 5.505, Val loss 5.601\n",
      "Ep 1 (Step 000170): Train loss 5.506, Val loss 5.583\n",
      "Ep 1 (Step 000180): Train loss 5.444, Val loss 5.560\n",
      "Ep 1 (Step 000190): Train loss 5.409, Val loss 5.533\n",
      "Ep 1 (Step 000200): Train loss 5.385, Val loss 5.503\n",
      "Ep 1 (Step 000210): Train loss 5.373, Val loss 5.469\n",
      "Ep 1 (Step 000220): Train loss 5.332, Val loss 5.454\n",
      "Ep 1 (Step 000230): Train loss 5.404, Val loss 5.447\n",
      "Ep 1 (Step 000240): Train loss 5.362, Val loss 5.422\n",
      "Ep 1 (Step 000250): Train loss 5.323, Val loss 5.419\n",
      "Ep 1 (Step 000260): Train loss 5.233, Val loss 5.393\n",
      "Ep 1 (Step 000270): Train loss 5.270, Val loss 5.393\n",
      "Ep 1 (Step 000280): Train loss 5.304, Val loss 5.382\n",
      "Ep 1 (Step 000290): Train loss 5.212, Val loss 5.354\n",
      "Ep 1 (Step 000300): Train loss 5.169, Val loss 5.352\n",
      "Ep 1 (Step 000310): Train loss 5.242, Val loss 5.346\n",
      "Ep 1 (Step 000320): Train loss 5.321, Val loss 5.337\n",
      "Ep 1 (Step 000330): Train loss 5.190, Val loss 5.332\n",
      "Ep 1 (Step 000340): Train loss 5.161, Val loss 5.321\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3207\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.834, Val loss 8.805\n",
      "Ep 1 (Step 000010): Train loss 7.417, Val loss 7.401\n",
      "Ep 1 (Step 000020): Train loss 6.830, Val loss 6.767\n",
      "Ep 1 (Step 000030): Train loss 6.483, Val loss 6.445\n",
      "Ep 1 (Step 000040): Train loss 6.331, Val loss 6.319\n",
      "Ep 1 (Step 000050): Train loss 6.165, Val loss 6.202\n",
      "Ep 1 (Step 000060): Train loss 6.142, Val loss 6.095\n",
      "Ep 1 (Step 000070): Train loss 5.993, Val loss 6.006\n",
      "Ep 1 (Step 000080): Train loss 5.821, Val loss 5.915\n",
      "Ep 1 (Step 000090): Train loss 5.826, Val loss 5.860\n",
      "Ep 1 (Step 000100): Train loss 5.751, Val loss 5.807\n",
      "Ep 1 (Step 000110): Train loss 5.743, Val loss 5.755\n",
      "Ep 1 (Step 000120): Train loss 5.618, Val loss 5.737\n",
      "Ep 1 (Step 000130): Train loss 5.649, Val loss 5.697\n",
      "Ep 1 (Step 000140): Train loss 5.587, Val loss 5.655\n",
      "Ep 1 (Step 000150): Train loss 5.483, Val loss 5.621\n",
      "Ep 1 (Step 000160): Train loss 5.435, Val loss 5.588\n",
      "Ep 1 (Step 000170): Train loss 5.455, Val loss 5.575\n",
      "Ep 1 (Step 000180): Train loss 5.545, Val loss 5.550\n",
      "Ep 1 (Step 000190): Train loss 5.468, Val loss 5.519\n",
      "Ep 1 (Step 000200): Train loss 5.328, Val loss 5.500\n",
      "Ep 1 (Step 000210): Train loss 5.419, Val loss 5.485\n",
      "Ep 1 (Step 000220): Train loss 5.386, Val loss 5.481\n",
      "Ep 1 (Step 000230): Train loss 5.398, Val loss 5.457\n",
      "Ep 1 (Step 000240): Train loss 5.305, Val loss 5.429\n",
      "Ep 1 (Step 000250): Train loss 5.306, Val loss 5.412\n",
      "Ep 1 (Step 000260): Train loss 5.248, Val loss 5.402\n",
      "Ep 1 (Step 000270): Train loss 5.248, Val loss 5.394\n",
      "Ep 1 (Step 000280): Train loss 5.310, Val loss 5.382\n",
      "Ep 1 (Step 000290): Train loss 5.207, Val loss 5.367\n",
      "Ep 1 (Step 000300): Train loss 5.205, Val loss 5.361\n",
      "Ep 1 (Step 000310): Train loss 5.172, Val loss 5.358\n",
      "Ep 1 (Step 000320): Train loss 5.193, Val loss 5.330\n",
      "Ep 1 (Step 000330): Train loss 5.261, Val loss 5.309\n",
      "Ep 1 (Step 000340): Train loss 5.094, Val loss 5.296\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2960\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.765, Val loss 8.744\n",
      "Ep 1 (Step 000010): Train loss 7.394, Val loss 7.356\n",
      "Ep 1 (Step 000020): Train loss 6.727, Val loss 6.730\n",
      "Ep 1 (Step 000030): Train loss 6.466, Val loss 6.418\n",
      "Ep 1 (Step 000040): Train loss 6.365, Val loss 6.340\n",
      "Ep 1 (Step 000050): Train loss 6.209, Val loss 6.249\n",
      "Ep 1 (Step 000060): Train loss 6.183, Val loss 6.092\n",
      "Ep 1 (Step 000070): Train loss 6.073, Val loss 6.012\n",
      "Ep 1 (Step 000080): Train loss 5.915, Val loss 5.945\n",
      "Ep 1 (Step 000090): Train loss 5.811, Val loss 5.879\n",
      "Ep 1 (Step 000100): Train loss 5.826, Val loss 5.825\n",
      "Ep 1 (Step 000110): Train loss 5.705, Val loss 5.770\n",
      "Ep 1 (Step 000120): Train loss 5.695, Val loss 5.746\n",
      "Ep 1 (Step 000130): Train loss 5.600, Val loss 5.693\n",
      "Ep 1 (Step 000140): Train loss 5.571, Val loss 5.657\n",
      "Ep 1 (Step 000150): Train loss 5.542, Val loss 5.608\n",
      "Ep 1 (Step 000160): Train loss 5.606, Val loss 5.590\n",
      "Ep 1 (Step 000170): Train loss 5.503, Val loss 5.549\n",
      "Ep 1 (Step 000180): Train loss 5.504, Val loss 5.509\n",
      "Ep 1 (Step 000190): Train loss 5.383, Val loss 5.504\n",
      "Ep 1 (Step 000200): Train loss 5.375, Val loss 5.481\n",
      "Ep 1 (Step 000210): Train loss 5.326, Val loss 5.467\n",
      "Ep 1 (Step 000220): Train loss 5.341, Val loss 5.433\n",
      "Ep 1 (Step 000230): Train loss 5.292, Val loss 5.430\n",
      "Ep 1 (Step 000240): Train loss 5.293, Val loss 5.414\n",
      "Ep 1 (Step 000250): Train loss 5.199, Val loss 5.384\n",
      "Ep 1 (Step 000260): Train loss 5.316, Val loss 5.374\n",
      "Ep 1 (Step 000270): Train loss 5.252, Val loss 5.368\n",
      "Ep 1 (Step 000280): Train loss 5.276, Val loss 5.362\n",
      "Ep 1 (Step 000290): Train loss 5.142, Val loss 5.356\n",
      "Ep 1 (Step 000300): Train loss 5.225, Val loss 5.333\n",
      "Ep 1 (Step 000310): Train loss 5.149, Val loss 5.338\n",
      "Ep 1 (Step 000320): Train loss 5.237, Val loss 5.320\n",
      "Ep 1 (Step 000330): Train loss 5.210, Val loss 5.307\n",
      "Ep 1 (Step 000340): Train loss 5.112, Val loss 5.283\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2832\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.797, Val loss 8.791\n",
      "Ep 1 (Step 000010): Train loss 7.434, Val loss 7.419\n",
      "Ep 1 (Step 000020): Train loss 6.798, Val loss 6.767\n",
      "Ep 1 (Step 000030): Train loss 6.451, Val loss 6.446\n",
      "Ep 1 (Step 000040): Train loss 6.283, Val loss 6.329\n",
      "Ep 1 (Step 000050): Train loss 6.209, Val loss 6.239\n",
      "Ep 1 (Step 000060): Train loss 6.055, Val loss 6.079\n",
      "Ep 1 (Step 000070): Train loss 5.969, Val loss 5.992\n",
      "Ep 1 (Step 000080): Train loss 5.916, Val loss 5.919\n",
      "Ep 1 (Step 000090): Train loss 5.860, Val loss 5.843\n",
      "Ep 1 (Step 000100): Train loss 5.846, Val loss 5.798\n",
      "Ep 1 (Step 000110): Train loss 5.731, Val loss 5.749\n",
      "Ep 1 (Step 000120): Train loss 5.698, Val loss 5.711\n",
      "Ep 1 (Step 000130): Train loss 5.617, Val loss 5.661\n",
      "Ep 1 (Step 000140): Train loss 5.603, Val loss 5.617\n",
      "Ep 1 (Step 000150): Train loss 5.514, Val loss 5.589\n",
      "Ep 1 (Step 000160): Train loss 5.478, Val loss 5.566\n",
      "Ep 1 (Step 000170): Train loss 5.440, Val loss 5.523\n",
      "Ep 1 (Step 000180): Train loss 5.416, Val loss 5.505\n",
      "Ep 1 (Step 000190): Train loss 5.416, Val loss 5.489\n",
      "Ep 1 (Step 000200): Train loss 5.313, Val loss 5.476\n",
      "Ep 1 (Step 000210): Train loss 5.378, Val loss 5.452\n",
      "Ep 1 (Step 000220): Train loss 5.305, Val loss 5.435\n",
      "Ep 1 (Step 000230): Train loss 5.293, Val loss 5.422\n",
      "Ep 1 (Step 000240): Train loss 5.314, Val loss 5.382\n",
      "Ep 1 (Step 000250): Train loss 5.291, Val loss 5.378\n",
      "Ep 1 (Step 000260): Train loss 5.239, Val loss 5.369\n",
      "Ep 1 (Step 000270): Train loss 5.205, Val loss 5.347\n",
      "Ep 1 (Step 000280): Train loss 5.212, Val loss 5.337\n",
      "Ep 1 (Step 000290): Train loss 5.184, Val loss 5.323\n",
      "Ep 1 (Step 000300): Train loss 5.233, Val loss 5.317\n",
      "Ep 1 (Step 000310): Train loss 5.216, Val loss 5.302\n",
      "Ep 1 (Step 000320): Train loss 5.142, Val loss 5.296\n",
      "Ep 1 (Step 000330): Train loss 5.186, Val loss 5.278\n",
      "Ep 1 (Step 000340): Train loss 5.128, Val loss 5.270\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2700\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.792, Val loss 8.778\n",
      "Ep 1 (Step 000010): Train loss 7.357, Val loss 7.408\n",
      "Ep 1 (Step 000020): Train loss 6.733, Val loss 6.738\n",
      "Ep 1 (Step 000030): Train loss 6.425, Val loss 6.437\n",
      "Ep 1 (Step 000040): Train loss 6.347, Val loss 6.343\n",
      "Ep 1 (Step 000050): Train loss 6.263, Val loss 6.266\n",
      "Ep 1 (Step 000060): Train loss 6.161, Val loss 6.133\n",
      "Ep 1 (Step 000070): Train loss 5.974, Val loss 6.037\n",
      "Ep 1 (Step 000080): Train loss 5.958, Val loss 5.956\n",
      "Ep 1 (Step 000090): Train loss 5.890, Val loss 5.874\n",
      "Ep 1 (Step 000100): Train loss 5.716, Val loss 5.813\n",
      "Ep 1 (Step 000110): Train loss 5.733, Val loss 5.783\n",
      "Ep 1 (Step 000120): Train loss 5.711, Val loss 5.722\n",
      "Ep 1 (Step 000130): Train loss 5.732, Val loss 5.673\n",
      "Ep 1 (Step 000140): Train loss 5.601, Val loss 5.650\n",
      "Ep 1 (Step 000150): Train loss 5.489, Val loss 5.622\n",
      "Ep 1 (Step 000160): Train loss 5.523, Val loss 5.581\n",
      "Ep 1 (Step 000170): Train loss 5.513, Val loss 5.550\n",
      "Ep 1 (Step 000180): Train loss 5.502, Val loss 5.541\n",
      "Ep 1 (Step 000190): Train loss 5.415, Val loss 5.504\n",
      "Ep 1 (Step 000200): Train loss 5.435, Val loss 5.495\n",
      "Ep 1 (Step 000210): Train loss 5.318, Val loss 5.457\n",
      "Ep 1 (Step 000220): Train loss 5.358, Val loss 5.440\n",
      "Ep 1 (Step 000230): Train loss 5.310, Val loss 5.433\n",
      "Ep 1 (Step 000240): Train loss 5.316, Val loss 5.412\n",
      "Ep 1 (Step 000250): Train loss 5.333, Val loss 5.381\n",
      "Ep 1 (Step 000260): Train loss 5.293, Val loss 5.382\n",
      "Ep 1 (Step 000270): Train loss 5.211, Val loss 5.357\n",
      "Ep 1 (Step 000280): Train loss 5.216, Val loss 5.339\n",
      "Ep 1 (Step 000290): Train loss 5.214, Val loss 5.332\n",
      "Ep 1 (Step 000300): Train loss 5.273, Val loss 5.304\n",
      "Ep 1 (Step 000310): Train loss 5.221, Val loss 5.304\n",
      "Ep 1 (Step 000320): Train loss 5.137, Val loss 5.300\n",
      "Ep 1 (Step 000330): Train loss 5.063, Val loss 5.289\n",
      "Ep 1 (Step 000340): Train loss 5.192, Val loss 5.287\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2869\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.524, Val loss 8.506\n",
      "Ep 1 (Step 000010): Train loss 6.852, Val loss 6.816\n",
      "Ep 1 (Step 000020): Train loss 6.483, Val loss 6.457\n",
      "Ep 1 (Step 000030): Train loss 6.387, Val loss 6.387\n",
      "Ep 1 (Step 000040): Train loss 6.217, Val loss 6.234\n",
      "Ep 1 (Step 000050): Train loss 6.106, Val loss 6.125\n",
      "Ep 1 (Step 000060): Train loss 5.976, Val loss 6.005\n",
      "Ep 1 (Step 000070): Train loss 5.856, Val loss 5.896\n",
      "Ep 1 (Step 000080): Train loss 5.812, Val loss 5.832\n",
      "Ep 1 (Step 000090): Train loss 5.730, Val loss 5.796\n",
      "Ep 1 (Step 000100): Train loss 5.644, Val loss 5.714\n",
      "Ep 1 (Step 000110): Train loss 5.625, Val loss 5.675\n",
      "Ep 1 (Step 000120): Train loss 5.556, Val loss 5.635\n",
      "Ep 1 (Step 000130): Train loss 5.551, Val loss 5.584\n",
      "Ep 1 (Step 000140): Train loss 5.444, Val loss 5.564\n",
      "Ep 1 (Step 000150): Train loss 5.375, Val loss 5.544\n",
      "Ep 1 (Step 000160): Train loss 5.387, Val loss 5.502\n",
      "Ep 1 (Step 000170): Train loss 5.446, Val loss 5.489\n",
      "Ep 1 (Step 000180): Train loss 5.431, Val loss 5.479\n",
      "Ep 1 (Step 000190): Train loss 5.309, Val loss 5.440\n",
      "Ep 1 (Step 000200): Train loss 5.382, Val loss 5.419\n",
      "Ep 1 (Step 000210): Train loss 5.373, Val loss 5.420\n",
      "Ep 1 (Step 000220): Train loss 5.297, Val loss 5.377\n",
      "Ep 1 (Step 000230): Train loss 5.287, Val loss 5.365\n",
      "Ep 1 (Step 000240): Train loss 5.321, Val loss 5.352\n",
      "Ep 1 (Step 000250): Train loss 5.279, Val loss 5.353\n",
      "Ep 1 (Step 000260): Train loss 5.175, Val loss 5.328\n",
      "Ep 1 (Step 000270): Train loss 5.224, Val loss 5.320\n",
      "Ep 1 (Step 000280): Train loss 5.186, Val loss 5.309\n",
      "Ep 1 (Step 000290): Train loss 5.146, Val loss 5.300\n",
      "Ep 1 (Step 000300): Train loss 5.165, Val loss 5.292\n",
      "Ep 1 (Step 000310): Train loss 5.199, Val loss 5.281\n",
      "Ep 1 (Step 000320): Train loss 5.143, Val loss 5.259\n",
      "Ep 1 (Step 000330): Train loss 5.087, Val loss 5.264\n",
      "Ep 1 (Step 000340): Train loss 5.079, Val loss 5.257\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2574\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.464, Val loss 8.450\n",
      "Ep 1 (Step 000010): Train loss 6.809, Val loss 6.790\n",
      "Ep 1 (Step 000020): Train loss 6.472, Val loss 6.436\n",
      "Ep 1 (Step 000030): Train loss 6.393, Val loss 6.362\n",
      "Ep 1 (Step 000040): Train loss 6.216, Val loss 6.197\n",
      "Ep 1 (Step 000050): Train loss 6.062, Val loss 6.098\n",
      "Ep 1 (Step 000060): Train loss 5.896, Val loss 5.948\n",
      "Ep 1 (Step 000070): Train loss 5.872, Val loss 5.879\n",
      "Ep 1 (Step 000080): Train loss 5.765, Val loss 5.805\n",
      "Ep 1 (Step 000090): Train loss 5.672, Val loss 5.735\n",
      "Ep 1 (Step 000100): Train loss 5.645, Val loss 5.698\n",
      "Ep 1 (Step 000110): Train loss 5.635, Val loss 5.640\n",
      "Ep 1 (Step 000120): Train loss 5.567, Val loss 5.601\n",
      "Ep 1 (Step 000130): Train loss 5.416, Val loss 5.583\n",
      "Ep 1 (Step 000140): Train loss 5.445, Val loss 5.551\n",
      "Ep 1 (Step 000150): Train loss 5.493, Val loss 5.537\n",
      "Ep 1 (Step 000160): Train loss 5.394, Val loss 5.508\n",
      "Ep 1 (Step 000170): Train loss 5.469, Val loss 5.515\n",
      "Ep 1 (Step 000180): Train loss 5.402, Val loss 5.476\n",
      "Ep 1 (Step 000190): Train loss 5.381, Val loss 5.457\n",
      "Ep 1 (Step 000200): Train loss 5.267, Val loss 5.423\n",
      "Ep 1 (Step 000210): Train loss 5.220, Val loss 5.409\n",
      "Ep 1 (Step 000220): Train loss 5.295, Val loss 5.404\n",
      "Ep 1 (Step 000230): Train loss 5.301, Val loss 5.372\n",
      "Ep 1 (Step 000240): Train loss 5.165, Val loss 5.371\n",
      "Ep 1 (Step 000250): Train loss 5.270, Val loss 5.356\n",
      "Ep 1 (Step 000260): Train loss 5.243, Val loss 5.342\n",
      "Ep 1 (Step 000270): Train loss 5.286, Val loss 5.325\n",
      "Ep 1 (Step 000280): Train loss 5.185, Val loss 5.318\n",
      "Ep 1 (Step 000290): Train loss 5.226, Val loss 5.311\n",
      "Ep 1 (Step 000300): Train loss 5.111, Val loss 5.280\n",
      "Ep 1 (Step 000310): Train loss 5.180, Val loss 5.277\n",
      "Ep 1 (Step 000320): Train loss 5.166, Val loss 5.262\n",
      "Ep 1 (Step 000330): Train loss 5.151, Val loss 5.251\n",
      "Ep 1 (Step 000340): Train loss 5.084, Val loss 5.250\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2502\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.507, Val loss 8.485\n",
      "Ep 1 (Step 000010): Train loss 6.883, Val loss 6.800\n",
      "Ep 1 (Step 000020): Train loss 6.476, Val loss 6.436\n",
      "Ep 1 (Step 000030): Train loss 6.340, Val loss 6.380\n",
      "Ep 1 (Step 000040): Train loss 6.214, Val loss 6.239\n",
      "Ep 1 (Step 000050): Train loss 6.060, Val loss 6.074\n",
      "Ep 1 (Step 000060): Train loss 5.948, Val loss 5.958\n",
      "Ep 1 (Step 000070): Train loss 5.851, Val loss 5.858\n",
      "Ep 1 (Step 000080): Train loss 5.753, Val loss 5.804\n",
      "Ep 1 (Step 000090): Train loss 5.710, Val loss 5.755\n",
      "Ep 1 (Step 000100): Train loss 5.525, Val loss 5.700\n",
      "Ep 1 (Step 000110): Train loss 5.600, Val loss 5.655\n",
      "Ep 1 (Step 000120): Train loss 5.452, Val loss 5.607\n",
      "Ep 1 (Step 000130): Train loss 5.469, Val loss 5.586\n",
      "Ep 1 (Step 000140): Train loss 5.491, Val loss 5.532\n",
      "Ep 1 (Step 000150): Train loss 5.392, Val loss 5.497\n",
      "Ep 1 (Step 000160): Train loss 5.383, Val loss 5.485\n",
      "Ep 1 (Step 000170): Train loss 5.333, Val loss 5.462\n",
      "Ep 1 (Step 000180): Train loss 5.302, Val loss 5.432\n",
      "Ep 1 (Step 000190): Train loss 5.318, Val loss 5.426\n",
      "Ep 1 (Step 000200): Train loss 5.248, Val loss 5.411\n",
      "Ep 1 (Step 000210): Train loss 5.334, Val loss 5.389\n",
      "Ep 1 (Step 000220): Train loss 5.277, Val loss 5.401\n",
      "Ep 1 (Step 000230): Train loss 5.269, Val loss 5.392\n",
      "Ep 1 (Step 000240): Train loss 5.247, Val loss 5.376\n",
      "Ep 1 (Step 000250): Train loss 5.270, Val loss 5.362\n",
      "Ep 1 (Step 000260): Train loss 5.310, Val loss 5.340\n",
      "Ep 1 (Step 000270): Train loss 5.220, Val loss 5.326\n",
      "Ep 1 (Step 000280): Train loss 5.135, Val loss 5.311\n",
      "Ep 1 (Step 000290): Train loss 5.174, Val loss 5.307\n",
      "Ep 1 (Step 000300): Train loss 5.204, Val loss 5.304\n",
      "Ep 1 (Step 000310): Train loss 5.151, Val loss 5.288\n",
      "Ep 1 (Step 000320): Train loss 5.172, Val loss 5.277\n",
      "Ep 1 (Step 000330): Train loss 5.111, Val loss 5.284\n",
      "Ep 1 (Step 000340): Train loss 5.147, Val loss 5.271\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2708\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.605, Val loss 8.619\n",
      "Ep 1 (Step 000010): Train loss 6.842, Val loss 6.828\n",
      "Ep 1 (Step 000020): Train loss 6.447, Val loss 6.409\n",
      "Ep 1 (Step 000030): Train loss 6.380, Val loss 6.327\n",
      "Ep 1 (Step 000040): Train loss 6.151, Val loss 6.161\n",
      "Ep 1 (Step 000050): Train loss 5.987, Val loss 6.040\n",
      "Ep 1 (Step 000060): Train loss 5.893, Val loss 5.934\n",
      "Ep 1 (Step 000070): Train loss 5.788, Val loss 5.856\n",
      "Ep 1 (Step 000080): Train loss 5.699, Val loss 5.774\n",
      "Ep 1 (Step 000090): Train loss 5.673, Val loss 5.712\n",
      "Ep 1 (Step 000100): Train loss 5.644, Val loss 5.657\n",
      "Ep 1 (Step 000110): Train loss 5.625, Val loss 5.633\n",
      "Ep 1 (Step 000120): Train loss 5.452, Val loss 5.595\n",
      "Ep 1 (Step 000130): Train loss 5.566, Val loss 5.544\n",
      "Ep 1 (Step 000140): Train loss 5.338, Val loss 5.526\n",
      "Ep 1 (Step 000150): Train loss 5.404, Val loss 5.505\n",
      "Ep 1 (Step 000160): Train loss 5.373, Val loss 5.487\n",
      "Ep 1 (Step 000170): Train loss 5.327, Val loss 5.435\n",
      "Ep 1 (Step 000180): Train loss 5.322, Val loss 5.409\n",
      "Ep 1 (Step 000190): Train loss 5.352, Val loss 5.391\n",
      "Ep 1 (Step 000200): Train loss 5.281, Val loss 5.378\n",
      "Ep 1 (Step 000210): Train loss 5.245, Val loss 5.354\n",
      "Ep 1 (Step 000220): Train loss 5.220, Val loss 5.354\n",
      "Ep 1 (Step 000230): Train loss 5.281, Val loss 5.347\n",
      "Ep 1 (Step 000240): Train loss 5.156, Val loss 5.316\n",
      "Ep 1 (Step 000250): Train loss 5.238, Val loss 5.314\n",
      "Ep 1 (Step 000260): Train loss 5.151, Val loss 5.301\n",
      "Ep 1 (Step 000270): Train loss 5.095, Val loss 5.289\n",
      "Ep 1 (Step 000280): Train loss 5.153, Val loss 5.291\n",
      "Ep 1 (Step 000290): Train loss 5.142, Val loss 5.275\n",
      "Ep 1 (Step 000300): Train loss 5.223, Val loss 5.265\n",
      "Ep 1 (Step 000310): Train loss 5.172, Val loss 5.252\n",
      "Ep 1 (Step 000320): Train loss 5.075, Val loss 5.245\n",
      "Ep 1 (Step 000330): Train loss 5.130, Val loss 5.245\n",
      "Ep 1 (Step 000340): Train loss 5.127, Val loss 5.236\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2359\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.409, Val loss 8.397\n",
      "Ep 1 (Step 000010): Train loss 6.815, Val loss 6.816\n",
      "Ep 1 (Step 000020): Train loss 6.436, Val loss 6.450\n",
      "Ep 1 (Step 000030): Train loss 6.401, Val loss 6.340\n",
      "Ep 1 (Step 000040): Train loss 6.240, Val loss 6.157\n",
      "Ep 1 (Step 000050): Train loss 6.016, Val loss 6.060\n",
      "Ep 1 (Step 000060): Train loss 5.907, Val loss 5.937\n",
      "Ep 1 (Step 000070): Train loss 5.806, Val loss 5.837\n",
      "Ep 1 (Step 000080): Train loss 5.788, Val loss 5.776\n",
      "Ep 1 (Step 000090): Train loss 5.661, Val loss 5.731\n",
      "Ep 1 (Step 000100): Train loss 5.664, Val loss 5.701\n",
      "Ep 1 (Step 000110): Train loss 5.582, Val loss 5.646\n",
      "Ep 1 (Step 000120): Train loss 5.490, Val loss 5.601\n",
      "Ep 1 (Step 000130): Train loss 5.432, Val loss 5.548\n",
      "Ep 1 (Step 000140): Train loss 5.388, Val loss 5.502\n",
      "Ep 1 (Step 000150): Train loss 5.489, Val loss 5.484\n",
      "Ep 1 (Step 000160): Train loss 5.389, Val loss 5.473\n",
      "Ep 1 (Step 000170): Train loss 5.357, Val loss 5.440\n",
      "Ep 1 (Step 000180): Train loss 5.326, Val loss 5.403\n",
      "Ep 1 (Step 000190): Train loss 5.254, Val loss 5.400\n",
      "Ep 1 (Step 000200): Train loss 5.232, Val loss 5.393\n",
      "Ep 1 (Step 000210): Train loss 5.315, Val loss 5.379\n",
      "Ep 1 (Step 000220): Train loss 5.206, Val loss 5.370\n",
      "Ep 1 (Step 000230): Train loss 5.243, Val loss 5.356\n",
      "Ep 1 (Step 000240): Train loss 5.297, Val loss 5.323\n",
      "Ep 1 (Step 000250): Train loss 5.154, Val loss 5.289\n",
      "Ep 1 (Step 000260): Train loss 5.194, Val loss 5.296\n",
      "Ep 1 (Step 000270): Train loss 5.113, Val loss 5.277\n",
      "Ep 1 (Step 000280): Train loss 5.203, Val loss 5.277\n",
      "Ep 1 (Step 000290): Train loss 5.177, Val loss 5.272\n",
      "Ep 1 (Step 000300): Train loss 5.112, Val loss 5.250\n",
      "Ep 1 (Step 000310): Train loss 5.206, Val loss 5.242\n",
      "Ep 1 (Step 000320): Train loss 5.142, Val loss 5.224\n",
      "Ep 1 (Step 000330): Train loss 5.108, Val loss 5.213\n",
      "Ep 1 (Step 000340): Train loss 5.106, Val loss 5.197\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1972\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.548, Val loss 8.508\n",
      "Ep 1 (Step 000010): Train loss 6.811, Val loss 6.774\n",
      "Ep 1 (Step 000020): Train loss 6.425, Val loss 6.415\n",
      "Ep 1 (Step 000030): Train loss 6.366, Val loss 6.342\n",
      "Ep 1 (Step 000040): Train loss 6.193, Val loss 6.225\n",
      "Ep 1 (Step 000050): Train loss 6.054, Val loss 6.092\n",
      "Ep 1 (Step 000060): Train loss 5.957, Val loss 5.937\n",
      "Ep 1 (Step 000070): Train loss 5.841, Val loss 5.854\n",
      "Ep 1 (Step 000080): Train loss 5.761, Val loss 5.788\n",
      "Ep 1 (Step 000090): Train loss 5.734, Val loss 5.726\n",
      "Ep 1 (Step 000100): Train loss 5.645, Val loss 5.698\n",
      "Ep 1 (Step 000110): Train loss 5.578, Val loss 5.645\n",
      "Ep 1 (Step 000120): Train loss 5.537, Val loss 5.565\n",
      "Ep 1 (Step 000130): Train loss 5.499, Val loss 5.557\n",
      "Ep 1 (Step 000140): Train loss 5.480, Val loss 5.539\n",
      "Ep 1 (Step 000150): Train loss 5.391, Val loss 5.508\n",
      "Ep 1 (Step 000160): Train loss 5.414, Val loss 5.485\n",
      "Ep 1 (Step 000170): Train loss 5.350, Val loss 5.451\n",
      "Ep 1 (Step 000180): Train loss 5.363, Val loss 5.452\n",
      "Ep 1 (Step 000190): Train loss 5.324, Val loss 5.416\n",
      "Ep 1 (Step 000200): Train loss 5.242, Val loss 5.412\n",
      "Ep 1 (Step 000210): Train loss 5.202, Val loss 5.399\n",
      "Ep 1 (Step 000220): Train loss 5.297, Val loss 5.393\n",
      "Ep 1 (Step 000230): Train loss 5.129, Val loss 5.356\n",
      "Ep 1 (Step 000240): Train loss 5.271, Val loss 5.321\n",
      "Ep 1 (Step 000250): Train loss 5.179, Val loss 5.308\n",
      "Ep 1 (Step 000260): Train loss 5.185, Val loss 5.299\n",
      "Ep 1 (Step 000270): Train loss 5.232, Val loss 5.307\n",
      "Ep 1 (Step 000280): Train loss 5.213, Val loss 5.289\n",
      "Ep 1 (Step 000290): Train loss 5.149, Val loss 5.265\n",
      "Ep 1 (Step 000300): Train loss 5.118, Val loss 5.252\n",
      "Ep 1 (Step 000310): Train loss 5.095, Val loss 5.227\n",
      "Ep 1 (Step 000320): Train loss 5.143, Val loss 5.223\n",
      "Ep 1 (Step 000330): Train loss 5.083, Val loss 5.233\n",
      "Ep 1 (Step 000340): Train loss 5.071, Val loss 5.227\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 8, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2272\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.140, Val loss 9.131\n",
      "Ep 1 (Step 000010): Train loss 7.533, Val loss 7.509\n",
      "Ep 1 (Step 000020): Train loss 6.877, Val loss 6.835\n",
      "Ep 1 (Step 000030): Train loss 6.501, Val loss 6.496\n",
      "Ep 1 (Step 000040): Train loss 6.409, Val loss 6.404\n",
      "Ep 1 (Step 000050): Train loss 6.445, Val loss 6.393\n",
      "Ep 1 (Step 000060): Train loss 6.372, Val loss 6.358\n",
      "Ep 1 (Step 000070): Train loss 6.251, Val loss 6.289\n",
      "Ep 1 (Step 000080): Train loss 6.123, Val loss 6.188\n",
      "Ep 1 (Step 000090): Train loss 6.041, Val loss 6.090\n",
      "Ep 1 (Step 000100): Train loss 6.077, Val loss 6.027\n",
      "Ep 1 (Step 000110): Train loss 5.910, Val loss 5.980\n",
      "Ep 1 (Step 000120): Train loss 5.937, Val loss 5.920\n",
      "Ep 1 (Step 000130): Train loss 5.805, Val loss 5.860\n",
      "Ep 1 (Step 000140): Train loss 5.831, Val loss 5.824\n",
      "Ep 1 (Step 000150): Train loss 5.784, Val loss 5.777\n",
      "Ep 1 (Step 000160): Train loss 5.608, Val loss 5.764\n",
      "Ep 1 (Step 000170): Train loss 5.662, Val loss 5.724\n",
      "Ep 1 (Step 000180): Train loss 5.578, Val loss 5.695\n",
      "Ep 1 (Step 000190): Train loss 5.507, Val loss 5.656\n",
      "Ep 1 (Step 000200): Train loss 5.516, Val loss 5.636\n",
      "Ep 1 (Step 000210): Train loss 5.566, Val loss 5.623\n",
      "Ep 1 (Step 000220): Train loss 5.528, Val loss 5.590\n",
      "Ep 1 (Step 000230): Train loss 5.451, Val loss 5.578\n",
      "Ep 1 (Step 000240): Train loss 5.398, Val loss 5.555\n",
      "Ep 1 (Step 000250): Train loss 5.550, Val loss 5.529\n",
      "Ep 1 (Step 000260): Train loss 5.486, Val loss 5.523\n",
      "Ep 1 (Step 000270): Train loss 5.421, Val loss 5.495\n",
      "Ep 1 (Step 000280): Train loss 5.315, Val loss 5.484\n",
      "Ep 1 (Step 000290): Train loss 5.324, Val loss 5.475\n",
      "Ep 1 (Step 000300): Train loss 5.365, Val loss 5.447\n",
      "Ep 1 (Step 000310): Train loss 5.372, Val loss 5.446\n",
      "Ep 1 (Step 000320): Train loss 5.325, Val loss 5.426\n",
      "Ep 1 (Step 000330): Train loss 5.272, Val loss 5.402\n",
      "Ep 1 (Step 000340): Train loss 5.233, Val loss 5.398\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3975\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.136, Val loss 9.129\n",
      "Ep 1 (Step 000010): Train loss 7.504, Val loss 7.481\n",
      "Ep 1 (Step 000020): Train loss 6.845, Val loss 6.829\n",
      "Ep 1 (Step 000030): Train loss 6.533, Val loss 6.512\n",
      "Ep 1 (Step 000040): Train loss 6.441, Val loss 6.419\n",
      "Ep 1 (Step 000050): Train loss 6.404, Val loss 6.388\n",
      "Ep 1 (Step 000060): Train loss 6.293, Val loss 6.364\n",
      "Ep 1 (Step 000070): Train loss 6.274, Val loss 6.285\n",
      "Ep 1 (Step 000080): Train loss 6.091, Val loss 6.177\n",
      "Ep 1 (Step 000090): Train loss 6.099, Val loss 6.092\n",
      "Ep 1 (Step 000100): Train loss 5.999, Val loss 6.029\n",
      "Ep 1 (Step 000110): Train loss 5.955, Val loss 5.966\n",
      "Ep 1 (Step 000120): Train loss 5.952, Val loss 5.925\n",
      "Ep 1 (Step 000130): Train loss 5.815, Val loss 5.886\n",
      "Ep 1 (Step 000140): Train loss 5.780, Val loss 5.847\n",
      "Ep 1 (Step 000150): Train loss 5.788, Val loss 5.797\n",
      "Ep 1 (Step 000160): Train loss 5.727, Val loss 5.753\n",
      "Ep 1 (Step 000170): Train loss 5.644, Val loss 5.721\n",
      "Ep 1 (Step 000180): Train loss 5.570, Val loss 5.695\n",
      "Ep 1 (Step 000190): Train loss 5.636, Val loss 5.678\n",
      "Ep 1 (Step 000200): Train loss 5.548, Val loss 5.643\n",
      "Ep 1 (Step 000210): Train loss 5.460, Val loss 5.624\n",
      "Ep 1 (Step 000220): Train loss 5.537, Val loss 5.591\n",
      "Ep 1 (Step 000230): Train loss 5.426, Val loss 5.567\n",
      "Ep 1 (Step 000240): Train loss 5.465, Val loss 5.541\n",
      "Ep 1 (Step 000250): Train loss 5.443, Val loss 5.535\n",
      "Ep 1 (Step 000260): Train loss 5.351, Val loss 5.521\n",
      "Ep 1 (Step 000270): Train loss 5.404, Val loss 5.501\n",
      "Ep 1 (Step 000280): Train loss 5.384, Val loss 5.484\n",
      "Ep 1 (Step 000290): Train loss 5.400, Val loss 5.466\n",
      "Ep 1 (Step 000300): Train loss 5.318, Val loss 5.451\n",
      "Ep 1 (Step 000310): Train loss 5.349, Val loss 5.454\n",
      "Ep 1 (Step 000320): Train loss 5.369, Val loss 5.424\n",
      "Ep 1 (Step 000330): Train loss 5.403, Val loss 5.425\n",
      "Ep 1 (Step 000340): Train loss 5.245, Val loss 5.412\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.4117\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.137, Val loss 9.141\n",
      "Ep 1 (Step 000010): Train loss 7.515, Val loss 7.484\n",
      "Ep 1 (Step 000020): Train loss 6.844, Val loss 6.816\n",
      "Ep 1 (Step 000030): Train loss 6.524, Val loss 6.496\n",
      "Ep 1 (Step 000040): Train loss 6.436, Val loss 6.399\n",
      "Ep 1 (Step 000050): Train loss 6.466, Val loss 6.376\n",
      "Ep 1 (Step 000060): Train loss 6.344, Val loss 6.349\n",
      "Ep 1 (Step 000070): Train loss 6.242, Val loss 6.271\n",
      "Ep 1 (Step 000080): Train loss 6.174, Val loss 6.162\n",
      "Ep 1 (Step 000090): Train loss 6.063, Val loss 6.075\n",
      "Ep 1 (Step 000100): Train loss 6.065, Val loss 6.003\n",
      "Ep 1 (Step 000110): Train loss 5.941, Val loss 5.957\n",
      "Ep 1 (Step 000120): Train loss 5.887, Val loss 5.901\n",
      "Ep 1 (Step 000130): Train loss 5.893, Val loss 5.861\n",
      "Ep 1 (Step 000140): Train loss 5.819, Val loss 5.821\n",
      "Ep 1 (Step 000150): Train loss 5.720, Val loss 5.782\n",
      "Ep 1 (Step 000160): Train loss 5.697, Val loss 5.753\n",
      "Ep 1 (Step 000170): Train loss 5.706, Val loss 5.715\n",
      "Ep 1 (Step 000180): Train loss 5.675, Val loss 5.681\n",
      "Ep 1 (Step 000190): Train loss 5.659, Val loss 5.656\n",
      "Ep 1 (Step 000200): Train loss 5.551, Val loss 5.646\n",
      "Ep 1 (Step 000210): Train loss 5.535, Val loss 5.617\n",
      "Ep 1 (Step 000220): Train loss 5.515, Val loss 5.592\n",
      "Ep 1 (Step 000230): Train loss 5.439, Val loss 5.560\n",
      "Ep 1 (Step 000240): Train loss 5.485, Val loss 5.548\n",
      "Ep 1 (Step 000250): Train loss 5.441, Val loss 5.527\n",
      "Ep 1 (Step 000260): Train loss 5.491, Val loss 5.508\n",
      "Ep 1 (Step 000270): Train loss 5.428, Val loss 5.487\n",
      "Ep 1 (Step 000280): Train loss 5.341, Val loss 5.487\n",
      "Ep 1 (Step 000290): Train loss 5.298, Val loss 5.476\n",
      "Ep 1 (Step 000300): Train loss 5.382, Val loss 5.453\n",
      "Ep 1 (Step 000310): Train loss 5.372, Val loss 5.426\n",
      "Ep 1 (Step 000320): Train loss 5.396, Val loss 5.432\n",
      "Ep 1 (Step 000330): Train loss 5.344, Val loss 5.410\n",
      "Ep 1 (Step 000340): Train loss 5.291, Val loss 5.426\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.4260\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.162, Val loss 9.165\n",
      "Ep 1 (Step 000010): Train loss 7.551, Val loss 7.541\n",
      "Ep 1 (Step 000020): Train loss 6.847, Val loss 6.844\n",
      "Ep 1 (Step 000030): Train loss 6.540, Val loss 6.500\n",
      "Ep 1 (Step 000040): Train loss 6.438, Val loss 6.405\n",
      "Ep 1 (Step 000050): Train loss 6.373, Val loss 6.379\n",
      "Ep 1 (Step 000060): Train loss 6.378, Val loss 6.343\n",
      "Ep 1 (Step 000070): Train loss 6.186, Val loss 6.275\n",
      "Ep 1 (Step 000080): Train loss 6.100, Val loss 6.167\n",
      "Ep 1 (Step 000090): Train loss 6.035, Val loss 6.086\n",
      "Ep 1 (Step 000100): Train loss 5.965, Val loss 6.026\n",
      "Ep 1 (Step 000110): Train loss 5.977, Val loss 5.957\n",
      "Ep 1 (Step 000120): Train loss 5.867, Val loss 5.907\n",
      "Ep 1 (Step 000130): Train loss 5.799, Val loss 5.867\n",
      "Ep 1 (Step 000140): Train loss 5.793, Val loss 5.822\n",
      "Ep 1 (Step 000150): Train loss 5.776, Val loss 5.773\n",
      "Ep 1 (Step 000160): Train loss 5.684, Val loss 5.739\n",
      "Ep 1 (Step 000170): Train loss 5.681, Val loss 5.705\n",
      "Ep 1 (Step 000180): Train loss 5.637, Val loss 5.688\n",
      "Ep 1 (Step 000190): Train loss 5.602, Val loss 5.646\n",
      "Ep 1 (Step 000200): Train loss 5.633, Val loss 5.620\n",
      "Ep 1 (Step 000210): Train loss 5.537, Val loss 5.602\n",
      "Ep 1 (Step 000220): Train loss 5.546, Val loss 5.562\n",
      "Ep 1 (Step 000230): Train loss 5.505, Val loss 5.544\n",
      "Ep 1 (Step 000240): Train loss 5.406, Val loss 5.534\n",
      "Ep 1 (Step 000250): Train loss 5.446, Val loss 5.506\n",
      "Ep 1 (Step 000260): Train loss 5.451, Val loss 5.495\n",
      "Ep 1 (Step 000270): Train loss 5.430, Val loss 5.471\n",
      "Ep 1 (Step 000280): Train loss 5.363, Val loss 5.464\n",
      "Ep 1 (Step 000290): Train loss 5.380, Val loss 5.442\n",
      "Ep 1 (Step 000300): Train loss 5.297, Val loss 5.436\n",
      "Ep 1 (Step 000310): Train loss 5.309, Val loss 5.420\n",
      "Ep 1 (Step 000320): Train loss 5.312, Val loss 5.407\n",
      "Ep 1 (Step 000330): Train loss 5.302, Val loss 5.409\n",
      "Ep 1 (Step 000340): Train loss 5.303, Val loss 5.391\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3910\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.152, Val loss 9.135\n",
      "Ep 1 (Step 000010): Train loss 7.535, Val loss 7.469\n",
      "Ep 1 (Step 000020): Train loss 6.829, Val loss 6.810\n",
      "Ep 1 (Step 000030): Train loss 6.461, Val loss 6.482\n",
      "Ep 1 (Step 000040): Train loss 6.388, Val loss 6.380\n",
      "Ep 1 (Step 000050): Train loss 6.335, Val loss 6.370\n",
      "Ep 1 (Step 000060): Train loss 6.338, Val loss 6.332\n",
      "Ep 1 (Step 000070): Train loss 6.268, Val loss 6.263\n",
      "Ep 1 (Step 000080): Train loss 6.156, Val loss 6.163\n",
      "Ep 1 (Step 000090): Train loss 6.006, Val loss 6.068\n",
      "Ep 1 (Step 000100): Train loss 6.106, Val loss 6.005\n",
      "Ep 1 (Step 000110): Train loss 5.951, Val loss 5.964\n",
      "Ep 1 (Step 000120): Train loss 5.926, Val loss 5.921\n",
      "Ep 1 (Step 000130): Train loss 5.814, Val loss 5.886\n",
      "Ep 1 (Step 000140): Train loss 5.664, Val loss 5.822\n",
      "Ep 1 (Step 000150): Train loss 5.761, Val loss 5.774\n",
      "Ep 1 (Step 000160): Train loss 5.669, Val loss 5.752\n",
      "Ep 1 (Step 000170): Train loss 5.616, Val loss 5.719\n",
      "Ep 1 (Step 000180): Train loss 5.633, Val loss 5.686\n",
      "Ep 1 (Step 000190): Train loss 5.539, Val loss 5.654\n",
      "Ep 1 (Step 000200): Train loss 5.496, Val loss 5.628\n",
      "Ep 1 (Step 000210): Train loss 5.462, Val loss 5.609\n",
      "Ep 1 (Step 000220): Train loss 5.457, Val loss 5.565\n",
      "Ep 1 (Step 000230): Train loss 5.458, Val loss 5.546\n",
      "Ep 1 (Step 000240): Train loss 5.497, Val loss 5.540\n",
      "Ep 1 (Step 000250): Train loss 5.531, Val loss 5.518\n",
      "Ep 1 (Step 000260): Train loss 5.409, Val loss 5.492\n",
      "Ep 1 (Step 000270): Train loss 5.442, Val loss 5.480\n",
      "Ep 1 (Step 000280): Train loss 5.287, Val loss 5.466\n",
      "Ep 1 (Step 000290): Train loss 5.356, Val loss 5.454\n",
      "Ep 1 (Step 000300): Train loss 5.321, Val loss 5.440\n",
      "Ep 1 (Step 000310): Train loss 5.361, Val loss 5.434\n",
      "Ep 1 (Step 000320): Train loss 5.348, Val loss 5.428\n",
      "Ep 1 (Step 000330): Train loss 5.223, Val loss 5.410\n",
      "Ep 1 (Step 000340): Train loss 5.325, Val loss 5.399\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3990\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.166, Val loss 9.169\n",
      "Ep 1 (Step 000010): Train loss 7.578, Val loss 7.499\n",
      "Ep 1 (Step 000020): Train loss 6.859, Val loss 6.820\n",
      "Ep 1 (Step 000030): Train loss 6.514, Val loss 6.479\n",
      "Ep 1 (Step 000040): Train loss 6.452, Val loss 6.400\n",
      "Ep 1 (Step 000050): Train loss 6.340, Val loss 6.386\n",
      "Ep 1 (Step 000060): Train loss 6.349, Val loss 6.354\n",
      "Ep 1 (Step 000070): Train loss 6.317, Val loss 6.294\n",
      "Ep 1 (Step 000080): Train loss 6.176, Val loss 6.193\n",
      "Ep 1 (Step 000090): Train loss 6.095, Val loss 6.116\n",
      "Ep 1 (Step 000100): Train loss 6.047, Val loss 6.041\n",
      "Ep 1 (Step 000110): Train loss 5.943, Val loss 5.985\n",
      "Ep 1 (Step 000120): Train loss 5.888, Val loss 5.948\n",
      "Ep 1 (Step 000130): Train loss 5.822, Val loss 5.882\n",
      "Ep 1 (Step 000140): Train loss 5.803, Val loss 5.841\n",
      "Ep 1 (Step 000150): Train loss 5.691, Val loss 5.790\n",
      "Ep 1 (Step 000160): Train loss 5.715, Val loss 5.767\n",
      "Ep 1 (Step 000170): Train loss 5.730, Val loss 5.711\n",
      "Ep 1 (Step 000180): Train loss 5.663, Val loss 5.701\n",
      "Ep 1 (Step 000190): Train loss 5.654, Val loss 5.665\n",
      "Ep 1 (Step 000200): Train loss 5.515, Val loss 5.632\n",
      "Ep 1 (Step 000210): Train loss 5.566, Val loss 5.620\n",
      "Ep 1 (Step 000220): Train loss 5.558, Val loss 5.583\n",
      "Ep 1 (Step 000230): Train loss 5.427, Val loss 5.566\n",
      "Ep 1 (Step 000240): Train loss 5.442, Val loss 5.550\n",
      "Ep 1 (Step 000250): Train loss 5.423, Val loss 5.524\n",
      "Ep 1 (Step 000260): Train loss 5.459, Val loss 5.502\n",
      "Ep 1 (Step 000270): Train loss 5.342, Val loss 5.488\n",
      "Ep 1 (Step 000280): Train loss 5.405, Val loss 5.460\n",
      "Ep 1 (Step 000290): Train loss 5.344, Val loss 5.446\n",
      "Ep 1 (Step 000300): Train loss 5.293, Val loss 5.424\n",
      "Ep 1 (Step 000310): Train loss 5.305, Val loss 5.412\n",
      "Ep 1 (Step 000320): Train loss 5.268, Val loss 5.399\n",
      "Ep 1 (Step 000330): Train loss 5.265, Val loss 5.409\n",
      "Ep 1 (Step 000340): Train loss 5.311, Val loss 5.396\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3965\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.983, Val loss 8.971\n",
      "Ep 1 (Step 000010): Train loss 6.946, Val loss 6.908\n",
      "Ep 1 (Step 000020): Train loss 6.502, Val loss 6.483\n",
      "Ep 1 (Step 000030): Train loss 6.459, Val loss 6.480\n",
      "Ep 1 (Step 000040): Train loss 6.394, Val loss 6.409\n",
      "Ep 1 (Step 000050): Train loss 6.298, Val loss 6.289\n",
      "Ep 1 (Step 000060): Train loss 6.138, Val loss 6.162\n",
      "Ep 1 (Step 000070): Train loss 6.012, Val loss 6.072\n",
      "Ep 1 (Step 000080): Train loss 5.990, Val loss 5.960\n",
      "Ep 1 (Step 000090): Train loss 5.870, Val loss 5.894\n",
      "Ep 1 (Step 000100): Train loss 5.744, Val loss 5.827\n",
      "Ep 1 (Step 000110): Train loss 5.741, Val loss 5.765\n",
      "Ep 1 (Step 000120): Train loss 5.572, Val loss 5.731\n",
      "Ep 1 (Step 000130): Train loss 5.605, Val loss 5.666\n",
      "Ep 1 (Step 000140): Train loss 5.554, Val loss 5.638\n",
      "Ep 1 (Step 000150): Train loss 5.482, Val loss 5.594\n",
      "Ep 1 (Step 000160): Train loss 5.537, Val loss 5.584\n",
      "Ep 1 (Step 000170): Train loss 5.432, Val loss 5.548\n",
      "Ep 1 (Step 000180): Train loss 5.466, Val loss 5.520\n",
      "Ep 1 (Step 000190): Train loss 5.285, Val loss 5.497\n",
      "Ep 1 (Step 000200): Train loss 5.462, Val loss 5.472\n",
      "Ep 1 (Step 000210): Train loss 5.248, Val loss 5.439\n",
      "Ep 1 (Step 000220): Train loss 5.338, Val loss 5.443\n",
      "Ep 1 (Step 000230): Train loss 5.304, Val loss 5.425\n",
      "Ep 1 (Step 000240): Train loss 5.211, Val loss 5.391\n",
      "Ep 1 (Step 000250): Train loss 5.339, Val loss 5.390\n",
      "Ep 1 (Step 000260): Train loss 5.270, Val loss 5.376\n",
      "Ep 1 (Step 000270): Train loss 5.202, Val loss 5.382\n",
      "Ep 1 (Step 000280): Train loss 5.177, Val loss 5.346\n",
      "Ep 1 (Step 000290): Train loss 5.182, Val loss 5.343\n",
      "Ep 1 (Step 000300): Train loss 5.131, Val loss 5.322\n",
      "Ep 1 (Step 000310): Train loss 5.216, Val loss 5.321\n",
      "Ep 1 (Step 000320): Train loss 5.231, Val loss 5.294\n",
      "Ep 1 (Step 000330): Train loss 5.137, Val loss 5.310\n",
      "Ep 1 (Step 000340): Train loss 5.174, Val loss 5.305\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3053\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.917, Val loss 8.889\n",
      "Ep 1 (Step 000010): Train loss 6.858, Val loss 6.819\n",
      "Ep 1 (Step 000020): Train loss 6.466, Val loss 6.471\n",
      "Ep 1 (Step 000030): Train loss 6.523, Val loss 6.470\n",
      "Ep 1 (Step 000040): Train loss 6.429, Val loss 6.427\n",
      "Ep 1 (Step 000050): Train loss 6.343, Val loss 6.322\n",
      "Ep 1 (Step 000060): Train loss 6.180, Val loss 6.154\n",
      "Ep 1 (Step 000070): Train loss 5.983, Val loss 6.051\n",
      "Ep 1 (Step 000080): Train loss 5.918, Val loss 5.955\n",
      "Ep 1 (Step 000090): Train loss 5.869, Val loss 5.879\n",
      "Ep 1 (Step 000100): Train loss 5.804, Val loss 5.820\n",
      "Ep 1 (Step 000110): Train loss 5.748, Val loss 5.762\n",
      "Ep 1 (Step 000120): Train loss 5.585, Val loss 5.701\n",
      "Ep 1 (Step 000130): Train loss 5.669, Val loss 5.683\n",
      "Ep 1 (Step 000140): Train loss 5.495, Val loss 5.617\n",
      "Ep 1 (Step 000150): Train loss 5.528, Val loss 5.579\n",
      "Ep 1 (Step 000160): Train loss 5.467, Val loss 5.574\n",
      "Ep 1 (Step 000170): Train loss 5.401, Val loss 5.525\n",
      "Ep 1 (Step 000180): Train loss 5.397, Val loss 5.523\n",
      "Ep 1 (Step 000190): Train loss 5.324, Val loss 5.469\n",
      "Ep 1 (Step 000200): Train loss 5.412, Val loss 5.503\n",
      "Ep 1 (Step 000210): Train loss 5.300, Val loss 5.479\n",
      "Ep 1 (Step 000220): Train loss 5.344, Val loss 5.499\n",
      "Ep 1 (Step 000230): Train loss 5.350, Val loss 5.426\n",
      "Ep 1 (Step 000240): Train loss 5.233, Val loss 5.409\n",
      "Ep 1 (Step 000250): Train loss 5.348, Val loss 5.403\n",
      "Ep 1 (Step 000260): Train loss 5.260, Val loss 5.387\n",
      "Ep 1 (Step 000270): Train loss 5.186, Val loss 5.362\n",
      "Ep 1 (Step 000280): Train loss 5.177, Val loss 5.351\n",
      "Ep 1 (Step 000290): Train loss 5.225, Val loss 5.330\n",
      "Ep 1 (Step 000300): Train loss 5.169, Val loss 5.319\n",
      "Ep 1 (Step 000310): Train loss 5.198, Val loss 5.295\n",
      "Ep 1 (Step 000320): Train loss 5.123, Val loss 5.285\n",
      "Ep 1 (Step 000330): Train loss 5.215, Val loss 5.276\n",
      "Ep 1 (Step 000340): Train loss 5.131, Val loss 5.273\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2731\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.929, Val loss 8.921\n",
      "Ep 1 (Step 000010): Train loss 6.893, Val loss 6.887\n",
      "Ep 1 (Step 000020): Train loss 6.522, Val loss 6.466\n",
      "Ep 1 (Step 000030): Train loss 6.511, Val loss 6.483\n",
      "Ep 1 (Step 000040): Train loss 6.469, Val loss 6.405\n",
      "Ep 1 (Step 000050): Train loss 6.264, Val loss 6.284\n",
      "Ep 1 (Step 000060): Train loss 6.148, Val loss 6.141\n",
      "Ep 1 (Step 000070): Train loss 6.020, Val loss 6.047\n",
      "Ep 1 (Step 000080): Train loss 5.919, Val loss 5.960\n",
      "Ep 1 (Step 000090): Train loss 5.732, Val loss 5.894\n",
      "Ep 1 (Step 000100): Train loss 5.734, Val loss 5.827\n",
      "Ep 1 (Step 000110): Train loss 5.697, Val loss 5.761\n",
      "Ep 1 (Step 000120): Train loss 5.576, Val loss 5.699\n",
      "Ep 1 (Step 000130): Train loss 5.557, Val loss 5.667\n",
      "Ep 1 (Step 000140): Train loss 5.564, Val loss 5.637\n",
      "Ep 1 (Step 000150): Train loss 5.492, Val loss 5.592\n",
      "Ep 1 (Step 000160): Train loss 5.489, Val loss 5.550\n",
      "Ep 1 (Step 000170): Train loss 5.489, Val loss 5.545\n",
      "Ep 1 (Step 000180): Train loss 5.382, Val loss 5.485\n",
      "Ep 1 (Step 000190): Train loss 5.401, Val loss 5.490\n",
      "Ep 1 (Step 000200): Train loss 5.426, Val loss 5.486\n",
      "Ep 1 (Step 000210): Train loss 5.341, Val loss 5.448\n",
      "Ep 1 (Step 000220): Train loss 5.359, Val loss 5.435\n",
      "Ep 1 (Step 000230): Train loss 5.375, Val loss 5.410\n",
      "Ep 1 (Step 000240): Train loss 5.247, Val loss 5.395\n",
      "Ep 1 (Step 000250): Train loss 5.168, Val loss 5.387\n",
      "Ep 1 (Step 000260): Train loss 5.250, Val loss 5.383\n",
      "Ep 1 (Step 000270): Train loss 5.198, Val loss 5.358\n",
      "Ep 1 (Step 000280): Train loss 5.200, Val loss 5.343\n",
      "Ep 1 (Step 000290): Train loss 5.221, Val loss 5.332\n",
      "Ep 1 (Step 000300): Train loss 5.223, Val loss 5.336\n",
      "Ep 1 (Step 000310): Train loss 5.168, Val loss 5.308\n",
      "Ep 1 (Step 000320): Train loss 5.171, Val loss 5.310\n",
      "Ep 1 (Step 000330): Train loss 5.154, Val loss 5.290\n",
      "Ep 1 (Step 000340): Train loss 5.171, Val loss 5.291\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2913\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.947, Val loss 8.964\n",
      "Ep 1 (Step 000010): Train loss 6.953, Val loss 6.879\n",
      "Ep 1 (Step 000020): Train loss 6.452, Val loss 6.459\n",
      "Ep 1 (Step 000030): Train loss 6.485, Val loss 6.485\n",
      "Ep 1 (Step 000040): Train loss 6.381, Val loss 6.422\n",
      "Ep 1 (Step 000050): Train loss 6.309, Val loss 6.293\n",
      "Ep 1 (Step 000060): Train loss 6.122, Val loss 6.123\n",
      "Ep 1 (Step 000070): Train loss 6.023, Val loss 6.042\n",
      "Ep 1 (Step 000080): Train loss 6.004, Val loss 5.949\n",
      "Ep 1 (Step 000090): Train loss 5.805, Val loss 5.878\n",
      "Ep 1 (Step 000100): Train loss 5.764, Val loss 5.805\n",
      "Ep 1 (Step 000110): Train loss 5.714, Val loss 5.736\n",
      "Ep 1 (Step 000120): Train loss 5.675, Val loss 5.673\n",
      "Ep 1 (Step 000130): Train loss 5.590, Val loss 5.613\n",
      "Ep 1 (Step 000140): Train loss 5.581, Val loss 5.588\n",
      "Ep 1 (Step 000150): Train loss 5.476, Val loss 5.553\n",
      "Ep 1 (Step 000160): Train loss 5.463, Val loss 5.546\n",
      "Ep 1 (Step 000170): Train loss 5.359, Val loss 5.513\n",
      "Ep 1 (Step 000180): Train loss 5.331, Val loss 5.487\n",
      "Ep 1 (Step 000190): Train loss 5.380, Val loss 5.471\n",
      "Ep 1 (Step 000200): Train loss 5.356, Val loss 5.440\n",
      "Ep 1 (Step 000210): Train loss 5.287, Val loss 5.419\n",
      "Ep 1 (Step 000220): Train loss 5.380, Val loss 5.399\n",
      "Ep 1 (Step 000230): Train loss 5.174, Val loss 5.394\n",
      "Ep 1 (Step 000240): Train loss 5.281, Val loss 5.373\n",
      "Ep 1 (Step 000250): Train loss 5.222, Val loss 5.369\n",
      "Ep 1 (Step 000260): Train loss 5.276, Val loss 5.351\n",
      "Ep 1 (Step 000270): Train loss 5.248, Val loss 5.339\n",
      "Ep 1 (Step 000280): Train loss 5.118, Val loss 5.335\n",
      "Ep 1 (Step 000290): Train loss 5.167, Val loss 5.330\n",
      "Ep 1 (Step 000300): Train loss 5.123, Val loss 5.326\n",
      "Ep 1 (Step 000310): Train loss 5.127, Val loss 5.286\n",
      "Ep 1 (Step 000320): Train loss 5.033, Val loss 5.258\n",
      "Ep 1 (Step 000330): Train loss 5.072, Val loss 5.246\n",
      "Ep 1 (Step 000340): Train loss 5.058, Val loss 5.245\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2449\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.960, Val loss 8.950\n",
      "Ep 1 (Step 000010): Train loss 6.850, Val loss 6.869\n",
      "Ep 1 (Step 000020): Train loss 6.563, Val loss 6.495\n",
      "Ep 1 (Step 000030): Train loss 6.525, Val loss 6.486\n",
      "Ep 1 (Step 000040): Train loss 6.422, Val loss 6.404\n",
      "Ep 1 (Step 000050): Train loss 6.320, Val loss 6.284\n",
      "Ep 1 (Step 000060): Train loss 6.136, Val loss 6.143\n",
      "Ep 1 (Step 000070): Train loss 6.024, Val loss 6.045\n",
      "Ep 1 (Step 000080): Train loss 5.924, Val loss 5.957\n",
      "Ep 1 (Step 000090): Train loss 5.818, Val loss 5.882\n",
      "Ep 1 (Step 000100): Train loss 5.761, Val loss 5.823\n",
      "Ep 1 (Step 000110): Train loss 5.661, Val loss 5.769\n",
      "Ep 1 (Step 000120): Train loss 5.620, Val loss 5.724\n",
      "Ep 1 (Step 000130): Train loss 5.555, Val loss 5.651\n",
      "Ep 1 (Step 000140): Train loss 5.475, Val loss 5.611\n",
      "Ep 1 (Step 000150): Train loss 5.554, Val loss 5.585\n",
      "Ep 1 (Step 000160): Train loss 5.469, Val loss 5.523\n",
      "Ep 1 (Step 000170): Train loss 5.424, Val loss 5.519\n",
      "Ep 1 (Step 000180): Train loss 5.384, Val loss 5.492\n",
      "Ep 1 (Step 000190): Train loss 5.405, Val loss 5.470\n",
      "Ep 1 (Step 000200): Train loss 5.327, Val loss 5.460\n",
      "Ep 1 (Step 000210): Train loss 5.362, Val loss 5.432\n",
      "Ep 1 (Step 000220): Train loss 5.364, Val loss 5.414\n",
      "Ep 1 (Step 000230): Train loss 5.285, Val loss 5.380\n",
      "Ep 1 (Step 000240): Train loss 5.227, Val loss 5.369\n",
      "Ep 1 (Step 000250): Train loss 5.186, Val loss 5.349\n",
      "Ep 1 (Step 000260): Train loss 5.224, Val loss 5.337\n",
      "Ep 1 (Step 000270): Train loss 5.191, Val loss 5.321\n",
      "Ep 1 (Step 000280): Train loss 5.185, Val loss 5.316\n",
      "Ep 1 (Step 000290): Train loss 5.209, Val loss 5.293\n",
      "Ep 1 (Step 000300): Train loss 5.228, Val loss 5.255\n",
      "Ep 1 (Step 000310): Train loss 5.148, Val loss 5.271\n",
      "Ep 1 (Step 000320): Train loss 5.096, Val loss 5.270\n",
      "Ep 1 (Step 000330): Train loss 5.191, Val loss 5.258\n",
      "Ep 1 (Step 000340): Train loss 5.109, Val loss 5.244\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2440\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.970, Val loss 8.946\n",
      "Ep 1 (Step 000010): Train loss 6.915, Val loss 6.872\n",
      "Ep 1 (Step 000020): Train loss 6.506, Val loss 6.450\n",
      "Ep 1 (Step 000030): Train loss 6.408, Val loss 6.473\n",
      "Ep 1 (Step 000040): Train loss 6.394, Val loss 6.409\n",
      "Ep 1 (Step 000050): Train loss 6.344, Val loss 6.288\n",
      "Ep 1 (Step 000060): Train loss 6.181, Val loss 6.159\n",
      "Ep 1 (Step 000070): Train loss 6.049, Val loss 6.051\n",
      "Ep 1 (Step 000080): Train loss 5.929, Val loss 5.957\n",
      "Ep 1 (Step 000090): Train loss 5.802, Val loss 5.872\n",
      "Ep 1 (Step 000100): Train loss 5.693, Val loss 5.807\n",
      "Ep 1 (Step 000110): Train loss 5.698, Val loss 5.740\n",
      "Ep 1 (Step 000120): Train loss 5.629, Val loss 5.681\n",
      "Ep 1 (Step 000130): Train loss 5.553, Val loss 5.642\n",
      "Ep 1 (Step 000140): Train loss 5.629, Val loss 5.594\n",
      "Ep 1 (Step 000150): Train loss 5.425, Val loss 5.561\n",
      "Ep 1 (Step 000160): Train loss 5.480, Val loss 5.536\n",
      "Ep 1 (Step 000170): Train loss 5.449, Val loss 5.488\n",
      "Ep 1 (Step 000180): Train loss 5.422, Val loss 5.478\n",
      "Ep 1 (Step 000190): Train loss 5.343, Val loss 5.468\n",
      "Ep 1 (Step 000200): Train loss 5.311, Val loss 5.451\n",
      "Ep 1 (Step 000210): Train loss 5.307, Val loss 5.415\n",
      "Ep 1 (Step 000220): Train loss 5.338, Val loss 5.407\n",
      "Ep 1 (Step 000230): Train loss 5.251, Val loss 5.385\n",
      "Ep 1 (Step 000240): Train loss 5.240, Val loss 5.368\n",
      "Ep 1 (Step 000250): Train loss 5.262, Val loss 5.344\n",
      "Ep 1 (Step 000260): Train loss 5.176, Val loss 5.337\n",
      "Ep 1 (Step 000270): Train loss 5.250, Val loss 5.326\n",
      "Ep 1 (Step 000280): Train loss 5.194, Val loss 5.290\n",
      "Ep 1 (Step 000290): Train loss 5.069, Val loss 5.304\n",
      "Ep 1 (Step 000300): Train loss 5.236, Val loss 5.276\n",
      "Ep 1 (Step 000310): Train loss 5.052, Val loss 5.278\n",
      "Ep 1 (Step 000320): Train loss 5.168, Val loss 5.278\n",
      "Ep 1 (Step 000330): Train loss 5.135, Val loss 5.254\n",
      "Ep 1 (Step 000340): Train loss 5.036, Val loss 5.252\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2521\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.143, Val loss 9.127\n",
      "Ep 1 (Step 000010): Train loss 7.543, Val loss 7.516\n",
      "Ep 1 (Step 000020): Train loss 6.853, Val loss 6.835\n",
      "Ep 1 (Step 000030): Train loss 6.574, Val loss 6.504\n",
      "Ep 1 (Step 000040): Train loss 6.379, Val loss 6.414\n",
      "Ep 1 (Step 000050): Train loss 6.406, Val loss 6.379\n",
      "Ep 1 (Step 000060): Train loss 6.385, Val loss 6.346\n",
      "Ep 1 (Step 000070): Train loss 6.364, Val loss 6.272\n",
      "Ep 1 (Step 000080): Train loss 6.141, Val loss 6.157\n",
      "Ep 1 (Step 000090): Train loss 6.062, Val loss 6.060\n",
      "Ep 1 (Step 000100): Train loss 6.042, Val loss 6.009\n",
      "Ep 1 (Step 000110): Train loss 5.891, Val loss 5.960\n",
      "Ep 1 (Step 000120): Train loss 5.801, Val loss 5.918\n",
      "Ep 1 (Step 000130): Train loss 5.827, Val loss 5.869\n",
      "Ep 1 (Step 000140): Train loss 5.780, Val loss 5.834\n",
      "Ep 1 (Step 000150): Train loss 5.747, Val loss 5.780\n",
      "Ep 1 (Step 000160): Train loss 5.646, Val loss 5.744\n",
      "Ep 1 (Step 000170): Train loss 5.624, Val loss 5.725\n",
      "Ep 1 (Step 000180): Train loss 5.644, Val loss 5.699\n",
      "Ep 1 (Step 000190): Train loss 5.619, Val loss 5.654\n",
      "Ep 1 (Step 000200): Train loss 5.570, Val loss 5.638\n",
      "Ep 1 (Step 000210): Train loss 5.512, Val loss 5.609\n",
      "Ep 1 (Step 000220): Train loss 5.474, Val loss 5.592\n",
      "Ep 1 (Step 000230): Train loss 5.440, Val loss 5.568\n",
      "Ep 1 (Step 000240): Train loss 5.457, Val loss 5.550\n",
      "Ep 1 (Step 000250): Train loss 5.511, Val loss 5.537\n",
      "Ep 1 (Step 000260): Train loss 5.426, Val loss 5.520\n",
      "Ep 1 (Step 000270): Train loss 5.335, Val loss 5.500\n",
      "Ep 1 (Step 000280): Train loss 5.288, Val loss 5.486\n",
      "Ep 1 (Step 000290): Train loss 5.359, Val loss 5.472\n",
      "Ep 1 (Step 000300): Train loss 5.416, Val loss 5.465\n",
      "Ep 1 (Step 000310): Train loss 5.444, Val loss 5.448\n",
      "Ep 1 (Step 000320): Train loss 5.358, Val loss 5.439\n",
      "Ep 1 (Step 000330): Train loss 5.338, Val loss 5.423\n",
      "Ep 1 (Step 000340): Train loss 5.316, Val loss 5.419\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.4193\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.149, Val loss 9.131\n",
      "Ep 1 (Step 000010): Train loss 7.505, Val loss 7.509\n",
      "Ep 1 (Step 000020): Train loss 6.866, Val loss 6.850\n",
      "Ep 1 (Step 000030): Train loss 6.498, Val loss 6.519\n",
      "Ep 1 (Step 000040): Train loss 6.463, Val loss 6.439\n",
      "Ep 1 (Step 000050): Train loss 6.353, Val loss 6.390\n",
      "Ep 1 (Step 000060): Train loss 6.287, Val loss 6.355\n",
      "Ep 1 (Step 000070): Train loss 6.311, Val loss 6.282\n",
      "Ep 1 (Step 000080): Train loss 6.173, Val loss 6.190\n",
      "Ep 1 (Step 000090): Train loss 6.076, Val loss 6.103\n",
      "Ep 1 (Step 000100): Train loss 5.977, Val loss 6.027\n",
      "Ep 1 (Step 000110): Train loss 5.972, Val loss 5.957\n",
      "Ep 1 (Step 000120): Train loss 5.985, Val loss 5.934\n",
      "Ep 1 (Step 000130): Train loss 5.852, Val loss 5.890\n",
      "Ep 1 (Step 000140): Train loss 5.788, Val loss 5.831\n",
      "Ep 1 (Step 000150): Train loss 5.726, Val loss 5.794\n",
      "Ep 1 (Step 000160): Train loss 5.650, Val loss 5.763\n",
      "Ep 1 (Step 000170): Train loss 5.652, Val loss 5.744\n",
      "Ep 1 (Step 000180): Train loss 5.662, Val loss 5.701\n",
      "Ep 1 (Step 000190): Train loss 5.558, Val loss 5.670\n",
      "Ep 1 (Step 000200): Train loss 5.562, Val loss 5.657\n",
      "Ep 1 (Step 000210): Train loss 5.489, Val loss 5.618\n",
      "Ep 1 (Step 000220): Train loss 5.446, Val loss 5.603\n",
      "Ep 1 (Step 000230): Train loss 5.513, Val loss 5.580\n",
      "Ep 1 (Step 000240): Train loss 5.583, Val loss 5.575\n",
      "Ep 1 (Step 000250): Train loss 5.398, Val loss 5.531\n",
      "Ep 1 (Step 000260): Train loss 5.374, Val loss 5.506\n",
      "Ep 1 (Step 000270): Train loss 5.417, Val loss 5.488\n",
      "Ep 1 (Step 000280): Train loss 5.431, Val loss 5.471\n",
      "Ep 1 (Step 000290): Train loss 5.406, Val loss 5.462\n",
      "Ep 1 (Step 000300): Train loss 5.414, Val loss 5.457\n",
      "Ep 1 (Step 000310): Train loss 5.371, Val loss 5.432\n",
      "Ep 1 (Step 000320): Train loss 5.275, Val loss 5.420\n",
      "Ep 1 (Step 000330): Train loss 5.353, Val loss 5.423\n",
      "Ep 1 (Step 000340): Train loss 5.280, Val loss 5.418\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.4181\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.112, Val loss 9.123\n",
      "Ep 1 (Step 000010): Train loss 7.500, Val loss 7.473\n",
      "Ep 1 (Step 000020): Train loss 6.836, Val loss 6.815\n",
      "Ep 1 (Step 000030): Train loss 6.522, Val loss 6.497\n",
      "Ep 1 (Step 000040): Train loss 6.425, Val loss 6.397\n",
      "Ep 1 (Step 000050): Train loss 6.370, Val loss 6.383\n",
      "Ep 1 (Step 000060): Train loss 6.295, Val loss 6.342\n",
      "Ep 1 (Step 000070): Train loss 6.222, Val loss 6.257\n",
      "Ep 1 (Step 000080): Train loss 6.107, Val loss 6.143\n",
      "Ep 1 (Step 000090): Train loss 6.110, Val loss 6.086\n",
      "Ep 1 (Step 000100): Train loss 5.942, Val loss 5.996\n",
      "Ep 1 (Step 000110): Train loss 5.876, Val loss 5.943\n",
      "Ep 1 (Step 000120): Train loss 5.911, Val loss 5.914\n",
      "Ep 1 (Step 000130): Train loss 5.845, Val loss 5.856\n",
      "Ep 1 (Step 000140): Train loss 5.799, Val loss 5.811\n",
      "Ep 1 (Step 000150): Train loss 5.752, Val loss 5.773\n",
      "Ep 1 (Step 000160): Train loss 5.679, Val loss 5.746\n",
      "Ep 1 (Step 000170): Train loss 5.678, Val loss 5.704\n",
      "Ep 1 (Step 000180): Train loss 5.665, Val loss 5.683\n",
      "Ep 1 (Step 000190): Train loss 5.538, Val loss 5.650\n",
      "Ep 1 (Step 000200): Train loss 5.656, Val loss 5.619\n",
      "Ep 1 (Step 000210): Train loss 5.619, Val loss 5.605\n",
      "Ep 1 (Step 000220): Train loss 5.562, Val loss 5.582\n",
      "Ep 1 (Step 000230): Train loss 5.486, Val loss 5.569\n",
      "Ep 1 (Step 000240): Train loss 5.466, Val loss 5.548\n",
      "Ep 1 (Step 000250): Train loss 5.448, Val loss 5.518\n",
      "Ep 1 (Step 000260): Train loss 5.485, Val loss 5.501\n",
      "Ep 1 (Step 000270): Train loss 5.409, Val loss 5.483\n",
      "Ep 1 (Step 000280): Train loss 5.421, Val loss 5.481\n",
      "Ep 1 (Step 000290): Train loss 5.337, Val loss 5.454\n",
      "Ep 1 (Step 000300): Train loss 5.325, Val loss 5.448\n",
      "Ep 1 (Step 000310): Train loss 5.321, Val loss 5.447\n",
      "Ep 1 (Step 000320): Train loss 5.328, Val loss 5.434\n",
      "Ep 1 (Step 000330): Train loss 5.299, Val loss 5.440\n",
      "Ep 1 (Step 000340): Train loss 5.284, Val loss 5.405\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.4050\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.120, Val loss 9.110\n",
      "Ep 1 (Step 000010): Train loss 7.473, Val loss 7.448\n",
      "Ep 1 (Step 000020): Train loss 6.852, Val loss 6.804\n",
      "Ep 1 (Step 000030): Train loss 6.536, Val loss 6.483\n",
      "Ep 1 (Step 000040): Train loss 6.376, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.407, Val loss 6.368\n",
      "Ep 1 (Step 000060): Train loss 6.357, Val loss 6.348\n",
      "Ep 1 (Step 000070): Train loss 6.219, Val loss 6.276\n",
      "Ep 1 (Step 000080): Train loss 6.192, Val loss 6.201\n",
      "Ep 1 (Step 000090): Train loss 6.082, Val loss 6.109\n",
      "Ep 1 (Step 000100): Train loss 5.986, Val loss 6.037\n",
      "Ep 1 (Step 000110): Train loss 5.912, Val loss 5.974\n",
      "Ep 1 (Step 000120): Train loss 5.887, Val loss 5.929\n",
      "Ep 1 (Step 000130): Train loss 5.835, Val loss 5.881\n",
      "Ep 1 (Step 000140): Train loss 5.873, Val loss 5.835\n",
      "Ep 1 (Step 000150): Train loss 5.816, Val loss 5.805\n",
      "Ep 1 (Step 000160): Train loss 5.741, Val loss 5.766\n",
      "Ep 1 (Step 000170): Train loss 5.716, Val loss 5.742\n",
      "Ep 1 (Step 000180): Train loss 5.648, Val loss 5.697\n",
      "Ep 1 (Step 000190): Train loss 5.539, Val loss 5.675\n",
      "Ep 1 (Step 000200): Train loss 5.573, Val loss 5.643\n",
      "Ep 1 (Step 000210): Train loss 5.502, Val loss 5.611\n",
      "Ep 1 (Step 000220): Train loss 5.564, Val loss 5.599\n",
      "Ep 1 (Step 000230): Train loss 5.491, Val loss 5.578\n",
      "Ep 1 (Step 000240): Train loss 5.480, Val loss 5.556\n",
      "Ep 1 (Step 000250): Train loss 5.433, Val loss 5.535\n",
      "Ep 1 (Step 000260): Train loss 5.440, Val loss 5.507\n",
      "Ep 1 (Step 000270): Train loss 5.372, Val loss 5.491\n",
      "Ep 1 (Step 000280): Train loss 5.327, Val loss 5.481\n",
      "Ep 1 (Step 000290): Train loss 5.323, Val loss 5.461\n",
      "Ep 1 (Step 000300): Train loss 5.422, Val loss 5.448\n",
      "Ep 1 (Step 000310): Train loss 5.319, Val loss 5.437\n",
      "Ep 1 (Step 000320): Train loss 5.337, Val loss 5.435\n",
      "Ep 1 (Step 000330): Train loss 5.263, Val loss 5.410\n",
      "Ep 1 (Step 000340): Train loss 5.190, Val loss 5.393\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3931\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.170, Val loss 9.144\n",
      "Ep 1 (Step 000010): Train loss 7.520, Val loss 7.492\n",
      "Ep 1 (Step 000020): Train loss 6.888, Val loss 6.823\n",
      "Ep 1 (Step 000030): Train loss 6.523, Val loss 6.490\n",
      "Ep 1 (Step 000040): Train loss 6.447, Val loss 6.402\n",
      "Ep 1 (Step 000050): Train loss 6.353, Val loss 6.386\n",
      "Ep 1 (Step 000060): Train loss 6.387, Val loss 6.365\n",
      "Ep 1 (Step 000070): Train loss 6.275, Val loss 6.300\n",
      "Ep 1 (Step 000080): Train loss 6.173, Val loss 6.190\n",
      "Ep 1 (Step 000090): Train loss 6.086, Val loss 6.099\n",
      "Ep 1 (Step 000100): Train loss 5.968, Val loss 6.041\n",
      "Ep 1 (Step 000110): Train loss 5.893, Val loss 5.974\n",
      "Ep 1 (Step 000120): Train loss 5.842, Val loss 5.926\n",
      "Ep 1 (Step 000130): Train loss 5.847, Val loss 5.874\n",
      "Ep 1 (Step 000140): Train loss 5.794, Val loss 5.821\n",
      "Ep 1 (Step 000150): Train loss 5.801, Val loss 5.804\n",
      "Ep 1 (Step 000160): Train loss 5.678, Val loss 5.754\n",
      "Ep 1 (Step 000170): Train loss 5.602, Val loss 5.720\n",
      "Ep 1 (Step 000180): Train loss 5.609, Val loss 5.684\n",
      "Ep 1 (Step 000190): Train loss 5.563, Val loss 5.670\n",
      "Ep 1 (Step 000200): Train loss 5.594, Val loss 5.636\n",
      "Ep 1 (Step 000210): Train loss 5.487, Val loss 5.611\n",
      "Ep 1 (Step 000220): Train loss 5.472, Val loss 5.595\n",
      "Ep 1 (Step 000230): Train loss 5.516, Val loss 5.559\n",
      "Ep 1 (Step 000240): Train loss 5.477, Val loss 5.552\n",
      "Ep 1 (Step 000250): Train loss 5.432, Val loss 5.514\n",
      "Ep 1 (Step 000260): Train loss 5.371, Val loss 5.502\n",
      "Ep 1 (Step 000270): Train loss 5.437, Val loss 5.489\n",
      "Ep 1 (Step 000280): Train loss 5.382, Val loss 5.470\n",
      "Ep 1 (Step 000290): Train loss 5.407, Val loss 5.454\n",
      "Ep 1 (Step 000300): Train loss 5.312, Val loss 5.453\n",
      "Ep 1 (Step 000310): Train loss 5.334, Val loss 5.435\n",
      "Ep 1 (Step 000320): Train loss 5.320, Val loss 5.414\n",
      "Ep 1 (Step 000330): Train loss 5.348, Val loss 5.384\n",
      "Ep 1 (Step 000340): Train loss 5.280, Val loss 5.376\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3758\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.168, Val loss 9.146\n",
      "Ep 1 (Step 000010): Train loss 7.577, Val loss 7.526\n",
      "Ep 1 (Step 000020): Train loss 6.846, Val loss 6.838\n",
      "Ep 1 (Step 000030): Train loss 6.515, Val loss 6.493\n",
      "Ep 1 (Step 000040): Train loss 6.390, Val loss 6.394\n",
      "Ep 1 (Step 000050): Train loss 6.375, Val loss 6.399\n",
      "Ep 1 (Step 000060): Train loss 6.391, Val loss 6.361\n",
      "Ep 1 (Step 000070): Train loss 6.269, Val loss 6.296\n",
      "Ep 1 (Step 000080): Train loss 6.142, Val loss 6.196\n",
      "Ep 1 (Step 000090): Train loss 6.111, Val loss 6.100\n",
      "Ep 1 (Step 000100): Train loss 6.049, Val loss 6.033\n",
      "Ep 1 (Step 000110): Train loss 5.898, Val loss 5.963\n",
      "Ep 1 (Step 000120): Train loss 5.858, Val loss 5.933\n",
      "Ep 1 (Step 000130): Train loss 5.847, Val loss 5.860\n",
      "Ep 1 (Step 000140): Train loss 5.800, Val loss 5.804\n",
      "Ep 1 (Step 000150): Train loss 5.750, Val loss 5.779\n",
      "Ep 1 (Step 000160): Train loss 5.723, Val loss 5.749\n",
      "Ep 1 (Step 000170): Train loss 5.662, Val loss 5.708\n",
      "Ep 1 (Step 000180): Train loss 5.616, Val loss 5.681\n",
      "Ep 1 (Step 000190): Train loss 5.572, Val loss 5.643\n",
      "Ep 1 (Step 000200): Train loss 5.521, Val loss 5.626\n",
      "Ep 1 (Step 000210): Train loss 5.555, Val loss 5.599\n",
      "Ep 1 (Step 000220): Train loss 5.460, Val loss 5.579\n",
      "Ep 1 (Step 000230): Train loss 5.446, Val loss 5.548\n",
      "Ep 1 (Step 000240): Train loss 5.502, Val loss 5.522\n",
      "Ep 1 (Step 000250): Train loss 5.412, Val loss 5.525\n",
      "Ep 1 (Step 000260): Train loss 5.341, Val loss 5.497\n",
      "Ep 1 (Step 000270): Train loss 5.452, Val loss 5.492\n",
      "Ep 1 (Step 000280): Train loss 5.429, Val loss 5.462\n",
      "Ep 1 (Step 000290): Train loss 5.424, Val loss 5.456\n",
      "Ep 1 (Step 000300): Train loss 5.338, Val loss 5.442\n",
      "Ep 1 (Step 000310): Train loss 5.318, Val loss 5.429\n",
      "Ep 1 (Step 000320): Train loss 5.396, Val loss 5.426\n",
      "Ep 1 (Step 000330): Train loss 5.310, Val loss 5.404\n",
      "Ep 1 (Step 000340): Train loss 5.351, Val loss 5.400\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.4002\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.912, Val loss 8.915\n",
      "Ep 1 (Step 000010): Train loss 6.864, Val loss 6.876\n",
      "Ep 1 (Step 000020): Train loss 6.522, Val loss 6.463\n",
      "Ep 1 (Step 000030): Train loss 6.480, Val loss 6.479\n",
      "Ep 1 (Step 000040): Train loss 6.397, Val loss 6.416\n",
      "Ep 1 (Step 000050): Train loss 6.302, Val loss 6.281\n",
      "Ep 1 (Step 000060): Train loss 6.142, Val loss 6.142\n",
      "Ep 1 (Step 000070): Train loss 6.090, Val loss 6.032\n",
      "Ep 1 (Step 000080): Train loss 5.915, Val loss 5.943\n",
      "Ep 1 (Step 000090): Train loss 5.791, Val loss 5.863\n",
      "Ep 1 (Step 000100): Train loss 5.766, Val loss 5.798\n",
      "Ep 1 (Step 000110): Train loss 5.668, Val loss 5.728\n",
      "Ep 1 (Step 000120): Train loss 5.724, Val loss 5.683\n",
      "Ep 1 (Step 000130): Train loss 5.512, Val loss 5.649\n",
      "Ep 1 (Step 000140): Train loss 5.479, Val loss 5.591\n",
      "Ep 1 (Step 000150): Train loss 5.430, Val loss 5.562\n",
      "Ep 1 (Step 000160): Train loss 5.356, Val loss 5.543\n",
      "Ep 1 (Step 000170): Train loss 5.381, Val loss 5.530\n",
      "Ep 1 (Step 000180): Train loss 5.410, Val loss 5.491\n",
      "Ep 1 (Step 000190): Train loss 5.263, Val loss 5.483\n",
      "Ep 1 (Step 000200): Train loss 5.370, Val loss 5.446\n",
      "Ep 1 (Step 000210): Train loss 5.315, Val loss 5.438\n",
      "Ep 1 (Step 000220): Train loss 5.159, Val loss 5.440\n",
      "Ep 1 (Step 000230): Train loss 5.217, Val loss 5.417\n",
      "Ep 1 (Step 000240): Train loss 5.196, Val loss 5.387\n",
      "Ep 1 (Step 000250): Train loss 5.200, Val loss 5.381\n",
      "Ep 1 (Step 000260): Train loss 5.180, Val loss 5.367\n",
      "Ep 1 (Step 000270): Train loss 5.183, Val loss 5.351\n",
      "Ep 1 (Step 000280): Train loss 5.242, Val loss 5.332\n",
      "Ep 1 (Step 000290): Train loss 5.210, Val loss 5.336\n",
      "Ep 1 (Step 000300): Train loss 5.253, Val loss 5.332\n",
      "Ep 1 (Step 000310): Train loss 5.179, Val loss 5.319\n",
      "Ep 1 (Step 000320): Train loss 5.091, Val loss 5.307\n",
      "Ep 1 (Step 000330): Train loss 5.139, Val loss 5.307\n",
      "Ep 1 (Step 000340): Train loss 5.221, Val loss 5.290\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2900\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.903, Val loss 8.898\n",
      "Ep 1 (Step 000010): Train loss 6.917, Val loss 6.844\n",
      "Ep 1 (Step 000020): Train loss 6.542, Val loss 6.488\n",
      "Ep 1 (Step 000030): Train loss 6.543, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.469, Val loss 6.394\n",
      "Ep 1 (Step 000050): Train loss 6.332, Val loss 6.304\n",
      "Ep 1 (Step 000060): Train loss 6.092, Val loss 6.149\n",
      "Ep 1 (Step 000070): Train loss 5.998, Val loss 6.036\n",
      "Ep 1 (Step 000080): Train loss 5.882, Val loss 5.971\n",
      "Ep 1 (Step 000090): Train loss 5.767, Val loss 5.881\n",
      "Ep 1 (Step 000100): Train loss 5.829, Val loss 5.815\n",
      "Ep 1 (Step 000110): Train loss 5.692, Val loss 5.764\n",
      "Ep 1 (Step 000120): Train loss 5.694, Val loss 5.728\n",
      "Ep 1 (Step 000130): Train loss 5.564, Val loss 5.680\n",
      "Ep 1 (Step 000140): Train loss 5.468, Val loss 5.652\n",
      "Ep 1 (Step 000150): Train loss 5.546, Val loss 5.609\n",
      "Ep 1 (Step 000160): Train loss 5.380, Val loss 5.565\n",
      "Ep 1 (Step 000170): Train loss 5.520, Val loss 5.524\n",
      "Ep 1 (Step 000180): Train loss 5.361, Val loss 5.494\n",
      "Ep 1 (Step 000190): Train loss 5.332, Val loss 5.483\n",
      "Ep 1 (Step 000200): Train loss 5.364, Val loss 5.478\n",
      "Ep 1 (Step 000210): Train loss 5.321, Val loss 5.453\n",
      "Ep 1 (Step 000220): Train loss 5.390, Val loss 5.434\n",
      "Ep 1 (Step 000230): Train loss 5.231, Val loss 5.404\n",
      "Ep 1 (Step 000240): Train loss 5.308, Val loss 5.401\n",
      "Ep 1 (Step 000250): Train loss 5.241, Val loss 5.388\n",
      "Ep 1 (Step 000260): Train loss 5.200, Val loss 5.385\n",
      "Ep 1 (Step 000270): Train loss 5.251, Val loss 5.356\n",
      "Ep 1 (Step 000280): Train loss 5.234, Val loss 5.369\n",
      "Ep 1 (Step 000290): Train loss 5.191, Val loss 5.362\n",
      "Ep 1 (Step 000300): Train loss 5.224, Val loss 5.329\n",
      "Ep 1 (Step 000310): Train loss 5.220, Val loss 5.315\n",
      "Ep 1 (Step 000320): Train loss 5.261, Val loss 5.309\n",
      "Ep 1 (Step 000330): Train loss 5.197, Val loss 5.325\n",
      "Ep 1 (Step 000340): Train loss 5.104, Val loss 5.299\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2989\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.951, Val loss 8.927\n",
      "Ep 1 (Step 000010): Train loss 6.902, Val loss 6.856\n",
      "Ep 1 (Step 000020): Train loss 6.513, Val loss 6.450\n",
      "Ep 1 (Step 000030): Train loss 6.532, Val loss 6.460\n",
      "Ep 1 (Step 000040): Train loss 6.403, Val loss 6.425\n",
      "Ep 1 (Step 000050): Train loss 6.278, Val loss 6.260\n",
      "Ep 1 (Step 000060): Train loss 6.146, Val loss 6.119\n",
      "Ep 1 (Step 000070): Train loss 5.946, Val loss 6.025\n",
      "Ep 1 (Step 000080): Train loss 5.829, Val loss 5.937\n",
      "Ep 1 (Step 000090): Train loss 5.752, Val loss 5.818\n",
      "Ep 1 (Step 000100): Train loss 5.734, Val loss 5.778\n",
      "Ep 1 (Step 000110): Train loss 5.640, Val loss 5.742\n",
      "Ep 1 (Step 000120): Train loss 5.690, Val loss 5.686\n",
      "Ep 1 (Step 000130): Train loss 5.617, Val loss 5.645\n",
      "Ep 1 (Step 000140): Train loss 5.553, Val loss 5.610\n",
      "Ep 1 (Step 000150): Train loss 5.537, Val loss 5.593\n",
      "Ep 1 (Step 000160): Train loss 5.534, Val loss 5.566\n",
      "Ep 1 (Step 000170): Train loss 5.552, Val loss 5.546\n",
      "Ep 1 (Step 000180): Train loss 5.435, Val loss 5.491\n",
      "Ep 1 (Step 000190): Train loss 5.425, Val loss 5.461\n",
      "Ep 1 (Step 000200): Train loss 5.332, Val loss 5.462\n",
      "Ep 1 (Step 000210): Train loss 5.306, Val loss 5.431\n",
      "Ep 1 (Step 000220): Train loss 5.268, Val loss 5.422\n",
      "Ep 1 (Step 000230): Train loss 5.353, Val loss 5.414\n",
      "Ep 1 (Step 000240): Train loss 5.300, Val loss 5.367\n",
      "Ep 1 (Step 000250): Train loss 5.209, Val loss 5.375\n",
      "Ep 1 (Step 000260): Train loss 5.206, Val loss 5.338\n",
      "Ep 1 (Step 000270): Train loss 5.290, Val loss 5.341\n",
      "Ep 1 (Step 000280): Train loss 5.298, Val loss 5.335\n",
      "Ep 1 (Step 000290): Train loss 5.106, Val loss 5.312\n",
      "Ep 1 (Step 000300): Train loss 5.120, Val loss 5.305\n",
      "Ep 1 (Step 000310): Train loss 5.155, Val loss 5.310\n",
      "Ep 1 (Step 000320): Train loss 5.169, Val loss 5.304\n",
      "Ep 1 (Step 000330): Train loss 5.090, Val loss 5.325\n",
      "Ep 1 (Step 000340): Train loss 5.068, Val loss 5.295\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2951\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.957, Val loss 8.942\n",
      "Ep 1 (Step 000010): Train loss 6.875, Val loss 6.845\n",
      "Ep 1 (Step 000020): Train loss 6.501, Val loss 6.491\n",
      "Ep 1 (Step 000030): Train loss 6.460, Val loss 6.498\n",
      "Ep 1 (Step 000040): Train loss 6.390, Val loss 6.417\n",
      "Ep 1 (Step 000050): Train loss 6.317, Val loss 6.284\n",
      "Ep 1 (Step 000060): Train loss 6.142, Val loss 6.120\n",
      "Ep 1 (Step 000070): Train loss 6.081, Val loss 6.030\n",
      "Ep 1 (Step 000080): Train loss 5.850, Val loss 5.940\n",
      "Ep 1 (Step 000090): Train loss 5.736, Val loss 5.885\n",
      "Ep 1 (Step 000100): Train loss 5.685, Val loss 5.809\n",
      "Ep 1 (Step 000110): Train loss 5.677, Val loss 5.758\n",
      "Ep 1 (Step 000120): Train loss 5.583, Val loss 5.678\n",
      "Ep 1 (Step 000130): Train loss 5.536, Val loss 5.649\n",
      "Ep 1 (Step 000140): Train loss 5.524, Val loss 5.600\n",
      "Ep 1 (Step 000150): Train loss 5.443, Val loss 5.568\n",
      "Ep 1 (Step 000160): Train loss 5.390, Val loss 5.528\n",
      "Ep 1 (Step 000170): Train loss 5.343, Val loss 5.488\n",
      "Ep 1 (Step 000180): Train loss 5.364, Val loss 5.472\n",
      "Ep 1 (Step 000190): Train loss 5.383, Val loss 5.445\n",
      "Ep 1 (Step 000200): Train loss 5.414, Val loss 5.432\n",
      "Ep 1 (Step 000210): Train loss 5.249, Val loss 5.427\n",
      "Ep 1 (Step 000220): Train loss 5.209, Val loss 5.404\n",
      "Ep 1 (Step 000230): Train loss 5.309, Val loss 5.384\n",
      "Ep 1 (Step 000240): Train loss 5.307, Val loss 5.375\n",
      "Ep 1 (Step 000250): Train loss 5.259, Val loss 5.369\n",
      "Ep 1 (Step 000260): Train loss 5.228, Val loss 5.335\n",
      "Ep 1 (Step 000270): Train loss 5.200, Val loss 5.332\n",
      "Ep 1 (Step 000280): Train loss 5.185, Val loss 5.311\n",
      "Ep 1 (Step 000290): Train loss 5.170, Val loss 5.293\n",
      "Ep 1 (Step 000300): Train loss 5.127, Val loss 5.285\n",
      "Ep 1 (Step 000310): Train loss 5.145, Val loss 5.274\n",
      "Ep 1 (Step 000320): Train loss 5.112, Val loss 5.267\n",
      "Ep 1 (Step 000330): Train loss 5.105, Val loss 5.273\n",
      "Ep 1 (Step 000340): Train loss 5.074, Val loss 5.250\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2500\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.937, Val loss 8.921\n",
      "Ep 1 (Step 000010): Train loss 6.935, Val loss 6.872\n",
      "Ep 1 (Step 000020): Train loss 6.523, Val loss 6.457\n",
      "Ep 1 (Step 000030): Train loss 6.512, Val loss 6.467\n",
      "Ep 1 (Step 000040): Train loss 6.431, Val loss 6.387\n",
      "Ep 1 (Step 000050): Train loss 6.228, Val loss 6.245\n",
      "Ep 1 (Step 000060): Train loss 6.096, Val loss 6.105\n",
      "Ep 1 (Step 000070): Train loss 5.973, Val loss 5.991\n",
      "Ep 1 (Step 000080): Train loss 5.813, Val loss 5.901\n",
      "Ep 1 (Step 000090): Train loss 5.756, Val loss 5.813\n",
      "Ep 1 (Step 000100): Train loss 5.702, Val loss 5.766\n",
      "Ep 1 (Step 000110): Train loss 5.633, Val loss 5.709\n",
      "Ep 1 (Step 000120): Train loss 5.638, Val loss 5.667\n",
      "Ep 1 (Step 000130): Train loss 5.576, Val loss 5.617\n",
      "Ep 1 (Step 000140): Train loss 5.481, Val loss 5.570\n",
      "Ep 1 (Step 000150): Train loss 5.386, Val loss 5.554\n",
      "Ep 1 (Step 000160): Train loss 5.473, Val loss 5.529\n",
      "Ep 1 (Step 000170): Train loss 5.428, Val loss 5.487\n",
      "Ep 1 (Step 000180): Train loss 5.404, Val loss 5.476\n",
      "Ep 1 (Step 000190): Train loss 5.446, Val loss 5.488\n",
      "Ep 1 (Step 000200): Train loss 5.439, Val loss 5.457\n",
      "Ep 1 (Step 000210): Train loss 5.291, Val loss 5.424\n",
      "Ep 1 (Step 000220): Train loss 5.318, Val loss 5.405\n",
      "Ep 1 (Step 000230): Train loss 5.245, Val loss 5.370\n",
      "Ep 1 (Step 000240): Train loss 5.310, Val loss 5.360\n",
      "Ep 1 (Step 000250): Train loss 5.157, Val loss 5.345\n",
      "Ep 1 (Step 000260): Train loss 5.204, Val loss 5.321\n",
      "Ep 1 (Step 000270): Train loss 5.101, Val loss 5.316\n",
      "Ep 1 (Step 000280): Train loss 5.192, Val loss 5.311\n",
      "Ep 1 (Step 000290): Train loss 5.153, Val loss 5.290\n",
      "Ep 1 (Step 000300): Train loss 5.150, Val loss 5.283\n",
      "Ep 1 (Step 000310): Train loss 5.113, Val loss 5.283\n",
      "Ep 1 (Step 000320): Train loss 5.113, Val loss 5.264\n",
      "Ep 1 (Step 000330): Train loss 5.128, Val loss 5.253\n",
      "Ep 1 (Step 000340): Train loss 5.049, Val loss 5.239\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2393\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.983, Val loss 8.929\n",
      "Ep 1 (Step 000010): Train loss 6.913, Val loss 6.849\n",
      "Ep 1 (Step 000020): Train loss 6.504, Val loss 6.481\n",
      "Ep 1 (Step 000030): Train loss 6.508, Val loss 6.494\n",
      "Ep 1 (Step 000040): Train loss 6.452, Val loss 6.426\n",
      "Ep 1 (Step 000050): Train loss 6.360, Val loss 6.338\n",
      "Ep 1 (Step 000060): Train loss 6.179, Val loss 6.229\n",
      "Ep 1 (Step 000070): Train loss 6.042, Val loss 6.075\n",
      "Ep 1 (Step 000080): Train loss 6.038, Val loss 5.981\n",
      "Ep 1 (Step 000090): Train loss 5.860, Val loss 5.887\n",
      "Ep 1 (Step 000100): Train loss 5.692, Val loss 5.808\n",
      "Ep 1 (Step 000110): Train loss 5.689, Val loss 5.747\n",
      "Ep 1 (Step 000120): Train loss 5.655, Val loss 5.717\n",
      "Ep 1 (Step 000130): Train loss 5.566, Val loss 5.660\n",
      "Ep 1 (Step 000140): Train loss 5.488, Val loss 5.619\n",
      "Ep 1 (Step 000150): Train loss 5.546, Val loss 5.590\n",
      "Ep 1 (Step 000160): Train loss 5.421, Val loss 5.555\n",
      "Ep 1 (Step 000170): Train loss 5.520, Val loss 5.524\n",
      "Ep 1 (Step 000180): Train loss 5.513, Val loss 5.483\n",
      "Ep 1 (Step 000190): Train loss 5.367, Val loss 5.474\n",
      "Ep 1 (Step 000200): Train loss 5.399, Val loss 5.445\n",
      "Ep 1 (Step 000210): Train loss 5.335, Val loss 5.418\n",
      "Ep 1 (Step 000220): Train loss 5.284, Val loss 5.431\n",
      "Ep 1 (Step 000230): Train loss 5.328, Val loss 5.411\n",
      "Ep 1 (Step 000240): Train loss 5.358, Val loss 5.387\n",
      "Ep 1 (Step 000250): Train loss 5.292, Val loss 5.374\n",
      "Ep 1 (Step 000260): Train loss 5.277, Val loss 5.342\n",
      "Ep 1 (Step 000270): Train loss 5.192, Val loss 5.348\n",
      "Ep 1 (Step 000280): Train loss 5.238, Val loss 5.324\n",
      "Ep 1 (Step 000290): Train loss 5.076, Val loss 5.320\n",
      "Ep 1 (Step 000300): Train loss 5.068, Val loss 5.286\n",
      "Ep 1 (Step 000310): Train loss 5.118, Val loss 5.271\n",
      "Ep 1 (Step 000320): Train loss 5.148, Val loss 5.259\n",
      "Ep 1 (Step 000330): Train loss 5.115, Val loss 5.257\n",
      "Ep 1 (Step 000340): Train loss 5.143, Val loss 5.246\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2457\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.101, Val loss 9.104\n",
      "Ep 1 (Step 000010): Train loss 7.486, Val loss 7.457\n",
      "Ep 1 (Step 000020): Train loss 6.838, Val loss 6.785\n",
      "Ep 1 (Step 000030): Train loss 6.497, Val loss 6.472\n",
      "Ep 1 (Step 000040): Train loss 6.435, Val loss 6.405\n",
      "Ep 1 (Step 000050): Train loss 6.400, Val loss 6.371\n",
      "Ep 1 (Step 000060): Train loss 6.259, Val loss 6.297\n",
      "Ep 1 (Step 000070): Train loss 6.190, Val loss 6.180\n",
      "Ep 1 (Step 000080): Train loss 6.063, Val loss 6.073\n",
      "Ep 1 (Step 000090): Train loss 5.971, Val loss 5.996\n",
      "Ep 1 (Step 000100): Train loss 5.878, Val loss 5.954\n",
      "Ep 1 (Step 000110): Train loss 5.818, Val loss 5.873\n",
      "Ep 1 (Step 000120): Train loss 5.819, Val loss 5.826\n",
      "Ep 1 (Step 000130): Train loss 5.728, Val loss 5.787\n",
      "Ep 1 (Step 000140): Train loss 5.775, Val loss 5.746\n",
      "Ep 1 (Step 000150): Train loss 5.632, Val loss 5.720\n",
      "Ep 1 (Step 000160): Train loss 5.679, Val loss 5.691\n",
      "Ep 1 (Step 000170): Train loss 5.677, Val loss 5.635\n",
      "Ep 1 (Step 000180): Train loss 5.562, Val loss 5.633\n",
      "Ep 1 (Step 000190): Train loss 5.593, Val loss 5.606\n",
      "Ep 1 (Step 000200): Train loss 5.498, Val loss 5.565\n",
      "Ep 1 (Step 000210): Train loss 5.454, Val loss 5.540\n",
      "Ep 1 (Step 000220): Train loss 5.512, Val loss 5.516\n",
      "Ep 1 (Step 000230): Train loss 5.405, Val loss 5.505\n",
      "Ep 1 (Step 000240): Train loss 5.408, Val loss 5.480\n",
      "Ep 1 (Step 000250): Train loss 5.379, Val loss 5.483\n",
      "Ep 1 (Step 000260): Train loss 5.307, Val loss 5.459\n",
      "Ep 1 (Step 000270): Train loss 5.393, Val loss 5.449\n",
      "Ep 1 (Step 000280): Train loss 5.378, Val loss 5.437\n",
      "Ep 1 (Step 000290): Train loss 5.289, Val loss 5.418\n",
      "Ep 1 (Step 000300): Train loss 5.267, Val loss 5.403\n",
      "Ep 1 (Step 000310): Train loss 5.268, Val loss 5.388\n",
      "Ep 1 (Step 000320): Train loss 5.286, Val loss 5.374\n",
      "Ep 1 (Step 000330): Train loss 5.222, Val loss 5.370\n",
      "Ep 1 (Step 000340): Train loss 5.253, Val loss 5.359\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3592\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.153, Val loss 9.137\n",
      "Ep 1 (Step 000010): Train loss 7.544, Val loss 7.506\n",
      "Ep 1 (Step 000020): Train loss 6.889, Val loss 6.846\n",
      "Ep 1 (Step 000030): Train loss 6.527, Val loss 6.506\n",
      "Ep 1 (Step 000040): Train loss 6.415, Val loss 6.393\n",
      "Ep 1 (Step 000050): Train loss 6.354, Val loss 6.340\n",
      "Ep 1 (Step 000060): Train loss 6.259, Val loss 6.283\n",
      "Ep 1 (Step 000070): Train loss 6.254, Val loss 6.188\n",
      "Ep 1 (Step 000080): Train loss 6.061, Val loss 6.096\n",
      "Ep 1 (Step 000090): Train loss 5.987, Val loss 6.023\n",
      "Ep 1 (Step 000100): Train loss 5.892, Val loss 5.968\n",
      "Ep 1 (Step 000110): Train loss 5.867, Val loss 5.918\n",
      "Ep 1 (Step 000120): Train loss 5.856, Val loss 5.862\n",
      "Ep 1 (Step 000130): Train loss 5.792, Val loss 5.809\n",
      "Ep 1 (Step 000140): Train loss 5.718, Val loss 5.755\n",
      "Ep 1 (Step 000150): Train loss 5.685, Val loss 5.733\n",
      "Ep 1 (Step 000160): Train loss 5.642, Val loss 5.710\n",
      "Ep 1 (Step 000170): Train loss 5.627, Val loss 5.668\n",
      "Ep 1 (Step 000180): Train loss 5.547, Val loss 5.646\n",
      "Ep 1 (Step 000190): Train loss 5.613, Val loss 5.616\n",
      "Ep 1 (Step 000200): Train loss 5.422, Val loss 5.574\n",
      "Ep 1 (Step 000210): Train loss 5.452, Val loss 5.547\n",
      "Ep 1 (Step 000220): Train loss 5.368, Val loss 5.528\n",
      "Ep 1 (Step 000230): Train loss 5.381, Val loss 5.501\n",
      "Ep 1 (Step 000240): Train loss 5.398, Val loss 5.486\n",
      "Ep 1 (Step 000250): Train loss 5.358, Val loss 5.464\n",
      "Ep 1 (Step 000260): Train loss 5.321, Val loss 5.449\n",
      "Ep 1 (Step 000270): Train loss 5.378, Val loss 5.439\n",
      "Ep 1 (Step 000280): Train loss 5.260, Val loss 5.411\n",
      "Ep 1 (Step 000290): Train loss 5.237, Val loss 5.400\n",
      "Ep 1 (Step 000300): Train loss 5.217, Val loss 5.391\n",
      "Ep 1 (Step 000310): Train loss 5.271, Val loss 5.394\n",
      "Ep 1 (Step 000320): Train loss 5.240, Val loss 5.373\n",
      "Ep 1 (Step 000330): Train loss 5.262, Val loss 5.365\n",
      "Ep 1 (Step 000340): Train loss 5.219, Val loss 5.361\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3607\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.120, Val loss 9.109\n",
      "Ep 1 (Step 000010): Train loss 7.488, Val loss 7.417\n",
      "Ep 1 (Step 000020): Train loss 6.760, Val loss 6.753\n",
      "Ep 1 (Step 000030): Train loss 6.497, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.458, Val loss 6.396\n",
      "Ep 1 (Step 000050): Train loss 6.379, Val loss 6.361\n",
      "Ep 1 (Step 000060): Train loss 6.293, Val loss 6.297\n",
      "Ep 1 (Step 000070): Train loss 6.119, Val loss 6.172\n",
      "Ep 1 (Step 000080): Train loss 6.101, Val loss 6.057\n",
      "Ep 1 (Step 000090): Train loss 6.000, Val loss 6.006\n",
      "Ep 1 (Step 000100): Train loss 5.923, Val loss 5.948\n",
      "Ep 1 (Step 000110): Train loss 5.834, Val loss 5.893\n",
      "Ep 1 (Step 000120): Train loss 5.802, Val loss 5.838\n",
      "Ep 1 (Step 000130): Train loss 5.712, Val loss 5.804\n",
      "Ep 1 (Step 000140): Train loss 5.709, Val loss 5.765\n",
      "Ep 1 (Step 000150): Train loss 5.700, Val loss 5.717\n",
      "Ep 1 (Step 000160): Train loss 5.585, Val loss 5.686\n",
      "Ep 1 (Step 000170): Train loss 5.545, Val loss 5.667\n",
      "Ep 1 (Step 000180): Train loss 5.597, Val loss 5.627\n",
      "Ep 1 (Step 000190): Train loss 5.487, Val loss 5.609\n",
      "Ep 1 (Step 000200): Train loss 5.451, Val loss 5.583\n",
      "Ep 1 (Step 000210): Train loss 5.401, Val loss 5.557\n",
      "Ep 1 (Step 000220): Train loss 5.393, Val loss 5.541\n",
      "Ep 1 (Step 000230): Train loss 5.416, Val loss 5.520\n",
      "Ep 1 (Step 000240): Train loss 5.356, Val loss 5.501\n",
      "Ep 1 (Step 000250): Train loss 5.370, Val loss 5.489\n",
      "Ep 1 (Step 000260): Train loss 5.402, Val loss 5.462\n",
      "Ep 1 (Step 000270): Train loss 5.275, Val loss 5.444\n",
      "Ep 1 (Step 000280): Train loss 5.286, Val loss 5.428\n",
      "Ep 1 (Step 000290): Train loss 5.327, Val loss 5.425\n",
      "Ep 1 (Step 000300): Train loss 5.312, Val loss 5.394\n",
      "Ep 1 (Step 000310): Train loss 5.234, Val loss 5.383\n",
      "Ep 1 (Step 000320): Train loss 5.287, Val loss 5.373\n",
      "Ep 1 (Step 000330): Train loss 5.248, Val loss 5.360\n",
      "Ep 1 (Step 000340): Train loss 5.139, Val loss 5.357\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3574\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.155, Val loss 9.157\n",
      "Ep 1 (Step 000010): Train loss 7.493, Val loss 7.476\n",
      "Ep 1 (Step 000020): Train loss 6.840, Val loss 6.799\n",
      "Ep 1 (Step 000030): Train loss 6.494, Val loss 6.475\n",
      "Ep 1 (Step 000040): Train loss 6.372, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.337, Val loss 6.344\n",
      "Ep 1 (Step 000060): Train loss 6.267, Val loss 6.287\n",
      "Ep 1 (Step 000070): Train loss 6.198, Val loss 6.167\n",
      "Ep 1 (Step 000080): Train loss 6.119, Val loss 6.069\n",
      "Ep 1 (Step 000090): Train loss 5.921, Val loss 5.985\n",
      "Ep 1 (Step 000100): Train loss 5.897, Val loss 5.919\n",
      "Ep 1 (Step 000110): Train loss 5.795, Val loss 5.857\n",
      "Ep 1 (Step 000120): Train loss 5.775, Val loss 5.803\n",
      "Ep 1 (Step 000130): Train loss 5.720, Val loss 5.749\n",
      "Ep 1 (Step 000140): Train loss 5.730, Val loss 5.721\n",
      "Ep 1 (Step 000150): Train loss 5.554, Val loss 5.696\n",
      "Ep 1 (Step 000160): Train loss 5.616, Val loss 5.659\n",
      "Ep 1 (Step 000170): Train loss 5.540, Val loss 5.628\n",
      "Ep 1 (Step 000180): Train loss 5.524, Val loss 5.602\n",
      "Ep 1 (Step 000190): Train loss 5.502, Val loss 5.574\n",
      "Ep 1 (Step 000200): Train loss 5.450, Val loss 5.542\n",
      "Ep 1 (Step 000210): Train loss 5.382, Val loss 5.526\n",
      "Ep 1 (Step 000220): Train loss 5.413, Val loss 5.507\n",
      "Ep 1 (Step 000230): Train loss 5.427, Val loss 5.491\n",
      "Ep 1 (Step 000240): Train loss 5.354, Val loss 5.475\n",
      "Ep 1 (Step 000250): Train loss 5.410, Val loss 5.438\n",
      "Ep 1 (Step 000260): Train loss 5.369, Val loss 5.426\n",
      "Ep 1 (Step 000270): Train loss 5.343, Val loss 5.413\n",
      "Ep 1 (Step 000280): Train loss 5.297, Val loss 5.396\n",
      "Ep 1 (Step 000290): Train loss 5.274, Val loss 5.388\n",
      "Ep 1 (Step 000300): Train loss 5.243, Val loss 5.385\n",
      "Ep 1 (Step 000310): Train loss 5.272, Val loss 5.361\n",
      "Ep 1 (Step 000320): Train loss 5.187, Val loss 5.360\n",
      "Ep 1 (Step 000330): Train loss 5.167, Val loss 5.344\n",
      "Ep 1 (Step 000340): Train loss 5.176, Val loss 5.327\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3272\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.165, Val loss 9.153\n",
      "Ep 1 (Step 000010): Train loss 7.495, Val loss 7.452\n",
      "Ep 1 (Step 000020): Train loss 6.836, Val loss 6.791\n",
      "Ep 1 (Step 000030): Train loss 6.457, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.402, Val loss 6.389\n",
      "Ep 1 (Step 000050): Train loss 6.383, Val loss 6.366\n",
      "Ep 1 (Step 000060): Train loss 6.345, Val loss 6.324\n",
      "Ep 1 (Step 000070): Train loss 6.222, Val loss 6.195\n",
      "Ep 1 (Step 000080): Train loss 6.139, Val loss 6.095\n",
      "Ep 1 (Step 000090): Train loss 5.956, Val loss 6.024\n",
      "Ep 1 (Step 000100): Train loss 6.001, Val loss 5.950\n",
      "Ep 1 (Step 000110): Train loss 5.942, Val loss 5.901\n",
      "Ep 1 (Step 000120): Train loss 5.821, Val loss 5.857\n",
      "Ep 1 (Step 000130): Train loss 5.784, Val loss 5.805\n",
      "Ep 1 (Step 000140): Train loss 5.686, Val loss 5.763\n",
      "Ep 1 (Step 000150): Train loss 5.667, Val loss 5.720\n",
      "Ep 1 (Step 000160): Train loss 5.666, Val loss 5.700\n",
      "Ep 1 (Step 000170): Train loss 5.653, Val loss 5.655\n",
      "Ep 1 (Step 000180): Train loss 5.521, Val loss 5.620\n",
      "Ep 1 (Step 000190): Train loss 5.547, Val loss 5.597\n",
      "Ep 1 (Step 000200): Train loss 5.460, Val loss 5.570\n",
      "Ep 1 (Step 000210): Train loss 5.488, Val loss 5.545\n",
      "Ep 1 (Step 000220): Train loss 5.367, Val loss 5.519\n",
      "Ep 1 (Step 000230): Train loss 5.397, Val loss 5.496\n",
      "Ep 1 (Step 000240): Train loss 5.337, Val loss 5.477\n",
      "Ep 1 (Step 000250): Train loss 5.356, Val loss 5.454\n",
      "Ep 1 (Step 000260): Train loss 5.361, Val loss 5.442\n",
      "Ep 1 (Step 000270): Train loss 5.364, Val loss 5.422\n",
      "Ep 1 (Step 000280): Train loss 5.353, Val loss 5.416\n",
      "Ep 1 (Step 000290): Train loss 5.404, Val loss 5.397\n",
      "Ep 1 (Step 000300): Train loss 5.314, Val loss 5.386\n",
      "Ep 1 (Step 000310): Train loss 5.281, Val loss 5.384\n",
      "Ep 1 (Step 000320): Train loss 5.260, Val loss 5.355\n",
      "Ep 1 (Step 000330): Train loss 5.265, Val loss 5.346\n",
      "Ep 1 (Step 000340): Train loss 5.216, Val loss 5.322\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3220\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.151, Val loss 9.121\n",
      "Ep 1 (Step 000010): Train loss 7.458, Val loss 7.440\n",
      "Ep 1 (Step 000020): Train loss 6.797, Val loss 6.778\n",
      "Ep 1 (Step 000030): Train loss 6.470, Val loss 6.483\n",
      "Ep 1 (Step 000040): Train loss 6.408, Val loss 6.399\n",
      "Ep 1 (Step 000050): Train loss 6.389, Val loss 6.369\n",
      "Ep 1 (Step 000060): Train loss 6.226, Val loss 6.321\n",
      "Ep 1 (Step 000070): Train loss 6.237, Val loss 6.217\n",
      "Ep 1 (Step 000080): Train loss 6.084, Val loss 6.109\n",
      "Ep 1 (Step 000090): Train loss 6.012, Val loss 6.039\n",
      "Ep 1 (Step 000100): Train loss 5.948, Val loss 5.980\n",
      "Ep 1 (Step 000110): Train loss 5.811, Val loss 5.905\n",
      "Ep 1 (Step 000120): Train loss 5.776, Val loss 5.858\n",
      "Ep 1 (Step 000130): Train loss 5.759, Val loss 5.818\n",
      "Ep 1 (Step 000140): Train loss 5.722, Val loss 5.748\n",
      "Ep 1 (Step 000150): Train loss 5.686, Val loss 5.714\n",
      "Ep 1 (Step 000160): Train loss 5.658, Val loss 5.687\n",
      "Ep 1 (Step 000170): Train loss 5.545, Val loss 5.643\n",
      "Ep 1 (Step 000180): Train loss 5.588, Val loss 5.612\n",
      "Ep 1 (Step 000190): Train loss 5.506, Val loss 5.582\n",
      "Ep 1 (Step 000200): Train loss 5.498, Val loss 5.556\n",
      "Ep 1 (Step 000210): Train loss 5.419, Val loss 5.529\n",
      "Ep 1 (Step 000220): Train loss 5.480, Val loss 5.513\n",
      "Ep 1 (Step 000230): Train loss 5.361, Val loss 5.491\n",
      "Ep 1 (Step 000240): Train loss 5.409, Val loss 5.465\n",
      "Ep 1 (Step 000250): Train loss 5.378, Val loss 5.452\n",
      "Ep 1 (Step 000260): Train loss 5.324, Val loss 5.429\n",
      "Ep 1 (Step 000270): Train loss 5.385, Val loss 5.405\n",
      "Ep 1 (Step 000280): Train loss 5.219, Val loss 5.403\n",
      "Ep 1 (Step 000290): Train loss 5.249, Val loss 5.376\n",
      "Ep 1 (Step 000300): Train loss 5.363, Val loss 5.369\n",
      "Ep 1 (Step 000310): Train loss 5.246, Val loss 5.374\n",
      "Ep 1 (Step 000320): Train loss 5.195, Val loss 5.361\n",
      "Ep 1 (Step 000330): Train loss 5.210, Val loss 5.358\n",
      "Ep 1 (Step 000340): Train loss 5.201, Val loss 5.336\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3360\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.923, Val loss 8.883\n",
      "Ep 1 (Step 000010): Train loss 6.876, Val loss 6.828\n",
      "Ep 1 (Step 000020): Train loss 6.487, Val loss 6.468\n",
      "Ep 1 (Step 000030): Train loss 6.476, Val loss 6.468\n",
      "Ep 1 (Step 000040): Train loss 6.310, Val loss 6.347\n",
      "Ep 1 (Step 000050): Train loss 6.099, Val loss 6.161\n",
      "Ep 1 (Step 000060): Train loss 5.978, Val loss 6.068\n",
      "Ep 1 (Step 000070): Train loss 5.943, Val loss 5.934\n",
      "Ep 1 (Step 000080): Train loss 5.747, Val loss 5.835\n",
      "Ep 1 (Step 000090): Train loss 5.775, Val loss 5.756\n",
      "Ep 1 (Step 000100): Train loss 5.569, Val loss 5.705\n",
      "Ep 1 (Step 000110): Train loss 5.562, Val loss 5.664\n",
      "Ep 1 (Step 000120): Train loss 5.543, Val loss 5.642\n",
      "Ep 1 (Step 000130): Train loss 5.492, Val loss 5.579\n",
      "Ep 1 (Step 000140): Train loss 5.465, Val loss 5.558\n",
      "Ep 1 (Step 000150): Train loss 5.421, Val loss 5.517\n",
      "Ep 1 (Step 000160): Train loss 5.407, Val loss 5.491\n",
      "Ep 1 (Step 000170): Train loss 5.366, Val loss 5.457\n",
      "Ep 1 (Step 000180): Train loss 5.319, Val loss 5.436\n",
      "Ep 1 (Step 000190): Train loss 5.253, Val loss 5.435\n",
      "Ep 1 (Step 000200): Train loss 5.321, Val loss 5.402\n",
      "Ep 1 (Step 000210): Train loss 5.244, Val loss 5.403\n",
      "Ep 1 (Step 000220): Train loss 5.275, Val loss 5.373\n",
      "Ep 1 (Step 000230): Train loss 5.218, Val loss 5.365\n",
      "Ep 1 (Step 000240): Train loss 5.259, Val loss 5.347\n",
      "Ep 1 (Step 000250): Train loss 5.242, Val loss 5.334\n",
      "Ep 1 (Step 000260): Train loss 5.206, Val loss 5.314\n",
      "Ep 1 (Step 000270): Train loss 5.108, Val loss 5.287\n",
      "Ep 1 (Step 000280): Train loss 5.203, Val loss 5.288\n",
      "Ep 1 (Step 000290): Train loss 5.223, Val loss 5.278\n",
      "Ep 1 (Step 000300): Train loss 5.116, Val loss 5.264\n",
      "Ep 1 (Step 000310): Train loss 5.154, Val loss 5.260\n",
      "Ep 1 (Step 000320): Train loss 5.041, Val loss 5.266\n",
      "Ep 1 (Step 000330): Train loss 5.072, Val loss 5.242\n",
      "Ep 1 (Step 000340): Train loss 5.060, Val loss 5.237\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2371\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.958, Val loss 8.938\n",
      "Ep 1 (Step 000010): Train loss 6.872, Val loss 6.813\n",
      "Ep 1 (Step 000020): Train loss 6.428, Val loss 6.445\n",
      "Ep 1 (Step 000030): Train loss 6.419, Val loss 6.451\n",
      "Ep 1 (Step 000040): Train loss 6.380, Val loss 6.319\n",
      "Ep 1 (Step 000050): Train loss 6.289, Val loss 6.153\n",
      "Ep 1 (Step 000060): Train loss 6.037, Val loss 6.028\n",
      "Ep 1 (Step 000070): Train loss 5.971, Val loss 5.932\n",
      "Ep 1 (Step 000080): Train loss 5.903, Val loss 5.839\n",
      "Ep 1 (Step 000090): Train loss 5.677, Val loss 5.782\n",
      "Ep 1 (Step 000100): Train loss 5.670, Val loss 5.717\n",
      "Ep 1 (Step 000110): Train loss 5.600, Val loss 5.649\n",
      "Ep 1 (Step 000120): Train loss 5.587, Val loss 5.617\n",
      "Ep 1 (Step 000130): Train loss 5.563, Val loss 5.559\n",
      "Ep 1 (Step 000140): Train loss 5.449, Val loss 5.516\n",
      "Ep 1 (Step 000150): Train loss 5.355, Val loss 5.487\n",
      "Ep 1 (Step 000160): Train loss 5.372, Val loss 5.467\n",
      "Ep 1 (Step 000170): Train loss 5.400, Val loss 5.443\n",
      "Ep 1 (Step 000180): Train loss 5.349, Val loss 5.425\n",
      "Ep 1 (Step 000190): Train loss 5.323, Val loss 5.415\n",
      "Ep 1 (Step 000200): Train loss 5.329, Val loss 5.392\n",
      "Ep 1 (Step 000210): Train loss 5.313, Val loss 5.377\n",
      "Ep 1 (Step 000220): Train loss 5.206, Val loss 5.376\n",
      "Ep 1 (Step 000230): Train loss 5.240, Val loss 5.363\n",
      "Ep 1 (Step 000240): Train loss 5.273, Val loss 5.327\n",
      "Ep 1 (Step 000250): Train loss 5.214, Val loss 5.322\n",
      "Ep 1 (Step 000260): Train loss 5.144, Val loss 5.307\n",
      "Ep 1 (Step 000270): Train loss 5.156, Val loss 5.297\n",
      "Ep 1 (Step 000280): Train loss 5.142, Val loss 5.275\n",
      "Ep 1 (Step 000290): Train loss 5.190, Val loss 5.276\n",
      "Ep 1 (Step 000300): Train loss 5.123, Val loss 5.256\n",
      "Ep 1 (Step 000310): Train loss 5.141, Val loss 5.242\n",
      "Ep 1 (Step 000320): Train loss 5.092, Val loss 5.234\n",
      "Ep 1 (Step 000330): Train loss 5.095, Val loss 5.238\n",
      "Ep 1 (Step 000340): Train loss 5.044, Val loss 5.241\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2405\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.919, Val loss 8.920\n",
      "Ep 1 (Step 000010): Train loss 6.879, Val loss 6.876\n",
      "Ep 1 (Step 000020): Train loss 6.493, Val loss 6.481\n",
      "Ep 1 (Step 000030): Train loss 6.480, Val loss 6.451\n",
      "Ep 1 (Step 000040): Train loss 6.247, Val loss 6.358\n",
      "Ep 1 (Step 000050): Train loss 6.259, Val loss 6.237\n",
      "Ep 1 (Step 000060): Train loss 6.094, Val loss 6.085\n",
      "Ep 1 (Step 000070): Train loss 5.916, Val loss 5.983\n",
      "Ep 1 (Step 000080): Train loss 5.935, Val loss 5.883\n",
      "Ep 1 (Step 000090): Train loss 5.691, Val loss 5.796\n",
      "Ep 1 (Step 000100): Train loss 5.731, Val loss 5.731\n",
      "Ep 1 (Step 000110): Train loss 5.636, Val loss 5.689\n",
      "Ep 1 (Step 000120): Train loss 5.651, Val loss 5.630\n",
      "Ep 1 (Step 000130): Train loss 5.503, Val loss 5.598\n",
      "Ep 1 (Step 000140): Train loss 5.528, Val loss 5.542\n",
      "Ep 1 (Step 000150): Train loss 5.530, Val loss 5.525\n",
      "Ep 1 (Step 000160): Train loss 5.381, Val loss 5.503\n",
      "Ep 1 (Step 000170): Train loss 5.424, Val loss 5.481\n",
      "Ep 1 (Step 000180): Train loss 5.365, Val loss 5.461\n",
      "Ep 1 (Step 000190): Train loss 5.359, Val loss 5.429\n",
      "Ep 1 (Step 000200): Train loss 5.189, Val loss 5.407\n",
      "Ep 1 (Step 000210): Train loss 5.257, Val loss 5.393\n",
      "Ep 1 (Step 000220): Train loss 5.326, Val loss 5.401\n",
      "Ep 1 (Step 000230): Train loss 5.307, Val loss 5.380\n",
      "Ep 1 (Step 000240): Train loss 5.239, Val loss 5.362\n",
      "Ep 1 (Step 000250): Train loss 5.263, Val loss 5.345\n",
      "Ep 1 (Step 000260): Train loss 5.217, Val loss 5.333\n",
      "Ep 1 (Step 000270): Train loss 5.182, Val loss 5.322\n",
      "Ep 1 (Step 000280): Train loss 5.164, Val loss 5.301\n",
      "Ep 1 (Step 000290): Train loss 5.181, Val loss 5.288\n",
      "Ep 1 (Step 000300): Train loss 5.075, Val loss 5.264\n",
      "Ep 1 (Step 000310): Train loss 5.116, Val loss 5.264\n",
      "Ep 1 (Step 000320): Train loss 5.117, Val loss 5.240\n",
      "Ep 1 (Step 000330): Train loss 5.111, Val loss 5.250\n",
      "Ep 1 (Step 000340): Train loss 5.114, Val loss 5.244\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2438\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.989, Val loss 8.980\n",
      "Ep 1 (Step 000010): Train loss 6.868, Val loss 6.874\n",
      "Ep 1 (Step 000020): Train loss 6.568, Val loss 6.477\n",
      "Ep 1 (Step 000030): Train loss 6.501, Val loss 6.491\n",
      "Ep 1 (Step 000040): Train loss 6.323, Val loss 6.369\n",
      "Ep 1 (Step 000050): Train loss 6.148, Val loss 6.170\n",
      "Ep 1 (Step 000060): Train loss 6.044, Val loss 6.063\n",
      "Ep 1 (Step 000070): Train loss 5.918, Val loss 5.942\n",
      "Ep 1 (Step 000080): Train loss 5.809, Val loss 5.858\n",
      "Ep 1 (Step 000090): Train loss 5.700, Val loss 5.799\n",
      "Ep 1 (Step 000100): Train loss 5.637, Val loss 5.731\n",
      "Ep 1 (Step 000110): Train loss 5.601, Val loss 5.646\n",
      "Ep 1 (Step 000120): Train loss 5.525, Val loss 5.606\n",
      "Ep 1 (Step 000130): Train loss 5.456, Val loss 5.573\n",
      "Ep 1 (Step 000140): Train loss 5.411, Val loss 5.551\n",
      "Ep 1 (Step 000150): Train loss 5.476, Val loss 5.529\n",
      "Ep 1 (Step 000160): Train loss 5.416, Val loss 5.491\n",
      "Ep 1 (Step 000170): Train loss 5.316, Val loss 5.464\n",
      "Ep 1 (Step 000180): Train loss 5.301, Val loss 5.444\n",
      "Ep 1 (Step 000190): Train loss 5.270, Val loss 5.412\n",
      "Ep 1 (Step 000200): Train loss 5.261, Val loss 5.393\n",
      "Ep 1 (Step 000210): Train loss 5.309, Val loss 5.390\n",
      "Ep 1 (Step 000220): Train loss 5.251, Val loss 5.392\n",
      "Ep 1 (Step 000230): Train loss 5.211, Val loss 5.357\n",
      "Ep 1 (Step 000240): Train loss 5.202, Val loss 5.358\n",
      "Ep 1 (Step 000250): Train loss 5.155, Val loss 5.336\n",
      "Ep 1 (Step 000260): Train loss 5.106, Val loss 5.315\n",
      "Ep 1 (Step 000270): Train loss 5.179, Val loss 5.298\n",
      "Ep 1 (Step 000280): Train loss 5.140, Val loss 5.286\n",
      "Ep 1 (Step 000290): Train loss 5.159, Val loss 5.262\n",
      "Ep 1 (Step 000300): Train loss 5.173, Val loss 5.219\n",
      "Ep 1 (Step 000310): Train loss 5.100, Val loss 5.224\n",
      "Ep 1 (Step 000320): Train loss 5.001, Val loss 5.214\n",
      "Ep 1 (Step 000330): Train loss 5.043, Val loss 5.207\n",
      "Ep 1 (Step 000340): Train loss 5.015, Val loss 5.199\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1991\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.931, Val loss 8.925\n",
      "Ep 1 (Step 000010): Train loss 6.945, Val loss 6.885\n",
      "Ep 1 (Step 000020): Train loss 6.443, Val loss 6.442\n",
      "Ep 1 (Step 000030): Train loss 6.430, Val loss 6.411\n",
      "Ep 1 (Step 000040): Train loss 6.284, Val loss 6.223\n",
      "Ep 1 (Step 000050): Train loss 6.065, Val loss 6.102\n",
      "Ep 1 (Step 000060): Train loss 5.930, Val loss 5.982\n",
      "Ep 1 (Step 000070): Train loss 5.840, Val loss 5.914\n",
      "Ep 1 (Step 000080): Train loss 5.707, Val loss 5.841\n",
      "Ep 1 (Step 000090): Train loss 5.793, Val loss 5.775\n",
      "Ep 1 (Step 000100): Train loss 5.560, Val loss 5.705\n",
      "Ep 1 (Step 000110): Train loss 5.588, Val loss 5.622\n",
      "Ep 1 (Step 000120): Train loss 5.567, Val loss 5.584\n",
      "Ep 1 (Step 000130): Train loss 5.583, Val loss 5.576\n",
      "Ep 1 (Step 000140): Train loss 5.495, Val loss 5.518\n",
      "Ep 1 (Step 000150): Train loss 5.460, Val loss 5.503\n",
      "Ep 1 (Step 000160): Train loss 5.382, Val loss 5.494\n",
      "Ep 1 (Step 000170): Train loss 5.314, Val loss 5.438\n",
      "Ep 1 (Step 000180): Train loss 5.349, Val loss 5.431\n",
      "Ep 1 (Step 000190): Train loss 5.262, Val loss 5.408\n",
      "Ep 1 (Step 000200): Train loss 5.226, Val loss 5.388\n",
      "Ep 1 (Step 000210): Train loss 5.187, Val loss 5.387\n",
      "Ep 1 (Step 000220): Train loss 5.151, Val loss 5.362\n",
      "Ep 1 (Step 000230): Train loss 5.224, Val loss 5.324\n",
      "Ep 1 (Step 000240): Train loss 5.210, Val loss 5.334\n",
      "Ep 1 (Step 000250): Train loss 5.167, Val loss 5.305\n",
      "Ep 1 (Step 000260): Train loss 5.194, Val loss 5.295\n",
      "Ep 1 (Step 000270): Train loss 5.148, Val loss 5.266\n",
      "Ep 1 (Step 000280): Train loss 5.173, Val loss 5.253\n",
      "Ep 1 (Step 000290): Train loss 5.101, Val loss 5.225\n",
      "Ep 1 (Step 000300): Train loss 5.102, Val loss 5.241\n",
      "Ep 1 (Step 000310): Train loss 5.095, Val loss 5.220\n",
      "Ep 1 (Step 000320): Train loss 5.036, Val loss 5.227\n",
      "Ep 1 (Step 000330): Train loss 5.057, Val loss 5.210\n",
      "Ep 1 (Step 000340): Train loss 5.065, Val loss 5.201\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2012\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.965, Val loss 8.956\n",
      "Ep 1 (Step 000010): Train loss 6.958, Val loss 6.924\n",
      "Ep 1 (Step 000020): Train loss 6.496, Val loss 6.458\n",
      "Ep 1 (Step 000030): Train loss 6.430, Val loss 6.437\n",
      "Ep 1 (Step 000040): Train loss 6.336, Val loss 6.359\n",
      "Ep 1 (Step 000050): Train loss 6.163, Val loss 6.163\n",
      "Ep 1 (Step 000060): Train loss 5.990, Val loss 6.028\n",
      "Ep 1 (Step 000070): Train loss 5.894, Val loss 5.931\n",
      "Ep 1 (Step 000080): Train loss 5.808, Val loss 5.830\n",
      "Ep 1 (Step 000090): Train loss 5.721, Val loss 5.770\n",
      "Ep 1 (Step 000100): Train loss 5.635, Val loss 5.693\n",
      "Ep 1 (Step 000110): Train loss 5.614, Val loss 5.648\n",
      "Ep 1 (Step 000120): Train loss 5.530, Val loss 5.608\n",
      "Ep 1 (Step 000130): Train loss 5.439, Val loss 5.550\n",
      "Ep 1 (Step 000140): Train loss 5.458, Val loss 5.532\n",
      "Ep 1 (Step 000150): Train loss 5.379, Val loss 5.502\n",
      "Ep 1 (Step 000160): Train loss 5.404, Val loss 5.451\n",
      "Ep 1 (Step 000170): Train loss 5.309, Val loss 5.434\n",
      "Ep 1 (Step 000180): Train loss 5.261, Val loss 5.403\n",
      "Ep 1 (Step 000190): Train loss 5.311, Val loss 5.393\n",
      "Ep 1 (Step 000200): Train loss 5.250, Val loss 5.369\n",
      "Ep 1 (Step 000210): Train loss 5.284, Val loss 5.357\n",
      "Ep 1 (Step 000220): Train loss 5.277, Val loss 5.337\n",
      "Ep 1 (Step 000230): Train loss 5.145, Val loss 5.315\n",
      "Ep 1 (Step 000240): Train loss 5.242, Val loss 5.313\n",
      "Ep 1 (Step 000250): Train loss 5.270, Val loss 5.299\n",
      "Ep 1 (Step 000260): Train loss 5.101, Val loss 5.301\n",
      "Ep 1 (Step 000270): Train loss 5.102, Val loss 5.284\n",
      "Ep 1 (Step 000280): Train loss 5.096, Val loss 5.270\n",
      "Ep 1 (Step 000290): Train loss 5.080, Val loss 5.255\n",
      "Ep 1 (Step 000300): Train loss 5.039, Val loss 5.249\n",
      "Ep 1 (Step 000310): Train loss 5.121, Val loss 5.227\n",
      "Ep 1 (Step 000320): Train loss 5.117, Val loss 5.219\n",
      "Ep 1 (Step 000330): Train loss 5.059, Val loss 5.203\n",
      "Ep 1 (Step 000340): Train loss 5.078, Val loss 5.202\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2021\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.115, Val loss 9.109\n",
      "Ep 1 (Step 000010): Train loss 7.476, Val loss 7.440\n",
      "Ep 1 (Step 000020): Train loss 6.846, Val loss 6.795\n",
      "Ep 1 (Step 000030): Train loss 6.474, Val loss 6.476\n",
      "Ep 1 (Step 000040): Train loss 6.409, Val loss 6.389\n",
      "Ep 1 (Step 000050): Train loss 6.321, Val loss 6.361\n",
      "Ep 1 (Step 000060): Train loss 6.269, Val loss 6.289\n",
      "Ep 1 (Step 000070): Train loss 6.195, Val loss 6.175\n",
      "Ep 1 (Step 000080): Train loss 6.161, Val loss 6.084\n",
      "Ep 1 (Step 000090): Train loss 5.974, Val loss 5.993\n",
      "Ep 1 (Step 000100): Train loss 5.967, Val loss 5.927\n",
      "Ep 1 (Step 000110): Train loss 5.892, Val loss 5.866\n",
      "Ep 1 (Step 000120): Train loss 5.800, Val loss 5.826\n",
      "Ep 1 (Step 000130): Train loss 5.688, Val loss 5.791\n",
      "Ep 1 (Step 000140): Train loss 5.719, Val loss 5.755\n",
      "Ep 1 (Step 000150): Train loss 5.668, Val loss 5.715\n",
      "Ep 1 (Step 000160): Train loss 5.621, Val loss 5.678\n",
      "Ep 1 (Step 000170): Train loss 5.647, Val loss 5.650\n",
      "Ep 1 (Step 000180): Train loss 5.478, Val loss 5.621\n",
      "Ep 1 (Step 000190): Train loss 5.520, Val loss 5.599\n",
      "Ep 1 (Step 000200): Train loss 5.517, Val loss 5.563\n",
      "Ep 1 (Step 000210): Train loss 5.411, Val loss 5.546\n",
      "Ep 1 (Step 000220): Train loss 5.435, Val loss 5.521\n",
      "Ep 1 (Step 000230): Train loss 5.387, Val loss 5.506\n",
      "Ep 1 (Step 000240): Train loss 5.398, Val loss 5.477\n",
      "Ep 1 (Step 000250): Train loss 5.366, Val loss 5.469\n",
      "Ep 1 (Step 000260): Train loss 5.349, Val loss 5.460\n",
      "Ep 1 (Step 000270): Train loss 5.302, Val loss 5.434\n",
      "Ep 1 (Step 000280): Train loss 5.322, Val loss 5.420\n",
      "Ep 1 (Step 000290): Train loss 5.442, Val loss 5.416\n",
      "Ep 1 (Step 000300): Train loss 5.245, Val loss 5.391\n",
      "Ep 1 (Step 000310): Train loss 5.288, Val loss 5.380\n",
      "Ep 1 (Step 000320): Train loss 5.254, Val loss 5.369\n",
      "Ep 1 (Step 000330): Train loss 5.284, Val loss 5.356\n",
      "Ep 1 (Step 000340): Train loss 5.297, Val loss 5.349\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3488\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.152, Val loss 9.152\n",
      "Ep 1 (Step 000010): Train loss 7.520, Val loss 7.537\n",
      "Ep 1 (Step 000020): Train loss 6.894, Val loss 6.852\n",
      "Ep 1 (Step 000030): Train loss 6.553, Val loss 6.507\n",
      "Ep 1 (Step 000040): Train loss 6.348, Val loss 6.402\n",
      "Ep 1 (Step 000050): Train loss 6.339, Val loss 6.364\n",
      "Ep 1 (Step 000060): Train loss 6.258, Val loss 6.284\n",
      "Ep 1 (Step 000070): Train loss 6.188, Val loss 6.165\n",
      "Ep 1 (Step 000080): Train loss 6.078, Val loss 6.066\n",
      "Ep 1 (Step 000090): Train loss 5.988, Val loss 5.986\n",
      "Ep 1 (Step 000100): Train loss 5.871, Val loss 5.944\n",
      "Ep 1 (Step 000110): Train loss 5.870, Val loss 5.893\n",
      "Ep 1 (Step 000120): Train loss 5.806, Val loss 5.852\n",
      "Ep 1 (Step 000130): Train loss 5.753, Val loss 5.801\n",
      "Ep 1 (Step 000140): Train loss 5.656, Val loss 5.753\n",
      "Ep 1 (Step 000150): Train loss 5.624, Val loss 5.726\n",
      "Ep 1 (Step 000160): Train loss 5.603, Val loss 5.699\n",
      "Ep 1 (Step 000170): Train loss 5.531, Val loss 5.674\n",
      "Ep 1 (Step 000180): Train loss 5.494, Val loss 5.644\n",
      "Ep 1 (Step 000190): Train loss 5.549, Val loss 5.621\n",
      "Ep 1 (Step 000200): Train loss 5.485, Val loss 5.600\n",
      "Ep 1 (Step 000210): Train loss 5.590, Val loss 5.579\n",
      "Ep 1 (Step 000220): Train loss 5.444, Val loss 5.540\n",
      "Ep 1 (Step 000230): Train loss 5.467, Val loss 5.524\n",
      "Ep 1 (Step 000240): Train loss 5.417, Val loss 5.508\n",
      "Ep 1 (Step 000250): Train loss 5.477, Val loss 5.489\n",
      "Ep 1 (Step 000260): Train loss 5.384, Val loss 5.469\n",
      "Ep 1 (Step 000270): Train loss 5.358, Val loss 5.453\n",
      "Ep 1 (Step 000280): Train loss 5.347, Val loss 5.436\n",
      "Ep 1 (Step 000290): Train loss 5.311, Val loss 5.419\n",
      "Ep 1 (Step 000300): Train loss 5.304, Val loss 5.413\n",
      "Ep 1 (Step 000310): Train loss 5.325, Val loss 5.392\n",
      "Ep 1 (Step 000320): Train loss 5.267, Val loss 5.385\n",
      "Ep 1 (Step 000330): Train loss 5.234, Val loss 5.381\n",
      "Ep 1 (Step 000340): Train loss 5.240, Val loss 5.363\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3626\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.145, Val loss 9.156\n",
      "Ep 1 (Step 000010): Train loss 7.554, Val loss 7.548\n",
      "Ep 1 (Step 000020): Train loss 6.876, Val loss 6.872\n",
      "Ep 1 (Step 000030): Train loss 6.504, Val loss 6.500\n",
      "Ep 1 (Step 000040): Train loss 6.391, Val loss 6.390\n",
      "Ep 1 (Step 000050): Train loss 6.320, Val loss 6.349\n",
      "Ep 1 (Step 000060): Train loss 6.265, Val loss 6.248\n",
      "Ep 1 (Step 000070): Train loss 6.091, Val loss 6.160\n",
      "Ep 1 (Step 000080): Train loss 6.060, Val loss 6.068\n",
      "Ep 1 (Step 000090): Train loss 5.917, Val loss 5.999\n",
      "Ep 1 (Step 000100): Train loss 5.822, Val loss 5.936\n",
      "Ep 1 (Step 000110): Train loss 5.821, Val loss 5.875\n",
      "Ep 1 (Step 000120): Train loss 5.727, Val loss 5.830\n",
      "Ep 1 (Step 000130): Train loss 5.743, Val loss 5.777\n",
      "Ep 1 (Step 000140): Train loss 5.656, Val loss 5.750\n",
      "Ep 1 (Step 000150): Train loss 5.637, Val loss 5.707\n",
      "Ep 1 (Step 000160): Train loss 5.583, Val loss 5.672\n",
      "Ep 1 (Step 000170): Train loss 5.523, Val loss 5.644\n",
      "Ep 1 (Step 000180): Train loss 5.657, Val loss 5.622\n",
      "Ep 1 (Step 000190): Train loss 5.440, Val loss 5.589\n",
      "Ep 1 (Step 000200): Train loss 5.541, Val loss 5.572\n",
      "Ep 1 (Step 000210): Train loss 5.447, Val loss 5.531\n",
      "Ep 1 (Step 000220): Train loss 5.497, Val loss 5.517\n",
      "Ep 1 (Step 000230): Train loss 5.386, Val loss 5.493\n",
      "Ep 1 (Step 000240): Train loss 5.372, Val loss 5.481\n",
      "Ep 1 (Step 000250): Train loss 5.435, Val loss 5.481\n",
      "Ep 1 (Step 000260): Train loss 5.321, Val loss 5.467\n",
      "Ep 1 (Step 000270): Train loss 5.294, Val loss 5.458\n",
      "Ep 1 (Step 000280): Train loss 5.243, Val loss 5.452\n",
      "Ep 1 (Step 000290): Train loss 5.336, Val loss 5.427\n",
      "Ep 1 (Step 000300): Train loss 5.220, Val loss 5.406\n",
      "Ep 1 (Step 000310): Train loss 5.281, Val loss 5.382\n",
      "Ep 1 (Step 000320): Train loss 5.257, Val loss 5.369\n",
      "Ep 1 (Step 000330): Train loss 5.186, Val loss 5.370\n",
      "Ep 1 (Step 000340): Train loss 5.134, Val loss 5.357\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3570\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.118, Val loss 9.102\n",
      "Ep 1 (Step 000010): Train loss 7.523, Val loss 7.477\n",
      "Ep 1 (Step 000020): Train loss 6.813, Val loss 6.806\n",
      "Ep 1 (Step 000030): Train loss 6.479, Val loss 6.481\n",
      "Ep 1 (Step 000040): Train loss 6.463, Val loss 6.393\n",
      "Ep 1 (Step 000050): Train loss 6.394, Val loss 6.339\n",
      "Ep 1 (Step 000060): Train loss 6.308, Val loss 6.283\n",
      "Ep 1 (Step 000070): Train loss 6.135, Val loss 6.179\n",
      "Ep 1 (Step 000080): Train loss 6.081, Val loss 6.068\n",
      "Ep 1 (Step 000090): Train loss 5.975, Val loss 5.979\n",
      "Ep 1 (Step 000100): Train loss 5.843, Val loss 5.931\n",
      "Ep 1 (Step 000110): Train loss 5.826, Val loss 5.858\n",
      "Ep 1 (Step 000120): Train loss 5.772, Val loss 5.811\n",
      "Ep 1 (Step 000130): Train loss 5.716, Val loss 5.768\n",
      "Ep 1 (Step 000140): Train loss 5.640, Val loss 5.726\n",
      "Ep 1 (Step 000150): Train loss 5.689, Val loss 5.689\n",
      "Ep 1 (Step 000160): Train loss 5.597, Val loss 5.654\n",
      "Ep 1 (Step 000170): Train loss 5.555, Val loss 5.636\n",
      "Ep 1 (Step 000180): Train loss 5.599, Val loss 5.606\n",
      "Ep 1 (Step 000190): Train loss 5.529, Val loss 5.576\n",
      "Ep 1 (Step 000200): Train loss 5.522, Val loss 5.547\n",
      "Ep 1 (Step 000210): Train loss 5.464, Val loss 5.537\n",
      "Ep 1 (Step 000220): Train loss 5.392, Val loss 5.507\n",
      "Ep 1 (Step 000230): Train loss 5.404, Val loss 5.491\n",
      "Ep 1 (Step 000240): Train loss 5.352, Val loss 5.471\n",
      "Ep 1 (Step 000250): Train loss 5.310, Val loss 5.443\n",
      "Ep 1 (Step 000260): Train loss 5.358, Val loss 5.430\n",
      "Ep 1 (Step 000270): Train loss 5.298, Val loss 5.418\n",
      "Ep 1 (Step 000280): Train loss 5.372, Val loss 5.403\n",
      "Ep 1 (Step 000290): Train loss 5.249, Val loss 5.379\n",
      "Ep 1 (Step 000300): Train loss 5.340, Val loss 5.376\n",
      "Ep 1 (Step 000310): Train loss 5.174, Val loss 5.362\n",
      "Ep 1 (Step 000320): Train loss 5.271, Val loss 5.344\n",
      "Ep 1 (Step 000330): Train loss 5.173, Val loss 5.336\n",
      "Ep 1 (Step 000340): Train loss 5.135, Val loss 5.334\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3336\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.141, Val loss 9.114\n",
      "Ep 1 (Step 000010): Train loss 7.541, Val loss 7.480\n",
      "Ep 1 (Step 000020): Train loss 6.803, Val loss 6.810\n",
      "Ep 1 (Step 000030): Train loss 6.482, Val loss 6.471\n",
      "Ep 1 (Step 000040): Train loss 6.409, Val loss 6.384\n",
      "Ep 1 (Step 000050): Train loss 6.296, Val loss 6.349\n",
      "Ep 1 (Step 000060): Train loss 6.315, Val loss 6.286\n",
      "Ep 1 (Step 000070): Train loss 6.187, Val loss 6.165\n",
      "Ep 1 (Step 000080): Train loss 6.063, Val loss 6.064\n",
      "Ep 1 (Step 000090): Train loss 5.975, Val loss 5.997\n",
      "Ep 1 (Step 000100): Train loss 5.887, Val loss 5.939\n",
      "Ep 1 (Step 000110): Train loss 5.778, Val loss 5.874\n",
      "Ep 1 (Step 000120): Train loss 5.777, Val loss 5.826\n",
      "Ep 1 (Step 000130): Train loss 5.771, Val loss 5.796\n",
      "Ep 1 (Step 000140): Train loss 5.737, Val loss 5.747\n",
      "Ep 1 (Step 000150): Train loss 5.689, Val loss 5.718\n",
      "Ep 1 (Step 000160): Train loss 5.625, Val loss 5.670\n",
      "Ep 1 (Step 000170): Train loss 5.566, Val loss 5.637\n",
      "Ep 1 (Step 000180): Train loss 5.530, Val loss 5.614\n",
      "Ep 1 (Step 000190): Train loss 5.464, Val loss 5.585\n",
      "Ep 1 (Step 000200): Train loss 5.447, Val loss 5.552\n",
      "Ep 1 (Step 000210): Train loss 5.462, Val loss 5.522\n",
      "Ep 1 (Step 000220): Train loss 5.438, Val loss 5.506\n",
      "Ep 1 (Step 000230): Train loss 5.428, Val loss 5.493\n",
      "Ep 1 (Step 000240): Train loss 5.393, Val loss 5.473\n",
      "Ep 1 (Step 000250): Train loss 5.271, Val loss 5.460\n",
      "Ep 1 (Step 000260): Train loss 5.346, Val loss 5.440\n",
      "Ep 1 (Step 000270): Train loss 5.271, Val loss 5.426\n",
      "Ep 1 (Step 000280): Train loss 5.381, Val loss 5.403\n",
      "Ep 1 (Step 000290): Train loss 5.200, Val loss 5.402\n",
      "Ep 1 (Step 000300): Train loss 5.266, Val loss 5.375\n",
      "Ep 1 (Step 000310): Train loss 5.278, Val loss 5.379\n",
      "Ep 1 (Step 000320): Train loss 5.246, Val loss 5.362\n",
      "Ep 1 (Step 000330): Train loss 5.205, Val loss 5.344\n",
      "Ep 1 (Step 000340): Train loss 5.249, Val loss 5.343\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3429\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.109, Val loss 9.117\n",
      "Ep 1 (Step 000010): Train loss 7.469, Val loss 7.428\n",
      "Ep 1 (Step 000020): Train loss 6.817, Val loss 6.778\n",
      "Ep 1 (Step 000030): Train loss 6.526, Val loss 6.482\n",
      "Ep 1 (Step 000040): Train loss 6.507, Val loss 6.398\n",
      "Ep 1 (Step 000050): Train loss 6.400, Val loss 6.364\n",
      "Ep 1 (Step 000060): Train loss 6.299, Val loss 6.319\n",
      "Ep 1 (Step 000070): Train loss 6.271, Val loss 6.242\n",
      "Ep 1 (Step 000080): Train loss 6.059, Val loss 6.116\n",
      "Ep 1 (Step 000090): Train loss 6.008, Val loss 6.020\n",
      "Ep 1 (Step 000100): Train loss 5.932, Val loss 5.963\n",
      "Ep 1 (Step 000110): Train loss 5.899, Val loss 5.906\n",
      "Ep 1 (Step 000120): Train loss 5.809, Val loss 5.842\n",
      "Ep 1 (Step 000130): Train loss 5.722, Val loss 5.798\n",
      "Ep 1 (Step 000140): Train loss 5.683, Val loss 5.756\n",
      "Ep 1 (Step 000150): Train loss 5.662, Val loss 5.706\n",
      "Ep 1 (Step 000160): Train loss 5.686, Val loss 5.678\n",
      "Ep 1 (Step 000170): Train loss 5.646, Val loss 5.667\n",
      "Ep 1 (Step 000180): Train loss 5.576, Val loss 5.623\n",
      "Ep 1 (Step 000190): Train loss 5.555, Val loss 5.607\n",
      "Ep 1 (Step 000200): Train loss 5.565, Val loss 5.569\n",
      "Ep 1 (Step 000210): Train loss 5.393, Val loss 5.545\n",
      "Ep 1 (Step 000220): Train loss 5.492, Val loss 5.526\n",
      "Ep 1 (Step 000230): Train loss 5.402, Val loss 5.509\n",
      "Ep 1 (Step 000240): Train loss 5.322, Val loss 5.481\n",
      "Ep 1 (Step 000250): Train loss 5.350, Val loss 5.448\n",
      "Ep 1 (Step 000260): Train loss 5.315, Val loss 5.449\n",
      "Ep 1 (Step 000270): Train loss 5.286, Val loss 5.431\n",
      "Ep 1 (Step 000280): Train loss 5.346, Val loss 5.415\n",
      "Ep 1 (Step 000290): Train loss 5.254, Val loss 5.412\n",
      "Ep 1 (Step 000300): Train loss 5.281, Val loss 5.385\n",
      "Ep 1 (Step 000310): Train loss 5.319, Val loss 5.378\n",
      "Ep 1 (Step 000320): Train loss 5.217, Val loss 5.350\n",
      "Ep 1 (Step 000330): Train loss 5.216, Val loss 5.337\n",
      "Ep 1 (Step 000340): Train loss 5.210, Val loss 5.329\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3293\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.963, Val loss 8.938\n",
      "Ep 1 (Step 000010): Train loss 6.865, Val loss 6.853\n",
      "Ep 1 (Step 000020): Train loss 6.463, Val loss 6.469\n",
      "Ep 1 (Step 000030): Train loss 6.416, Val loss 6.466\n",
      "Ep 1 (Step 000040): Train loss 6.368, Val loss 6.373\n",
      "Ep 1 (Step 000050): Train loss 6.224, Val loss 6.214\n",
      "Ep 1 (Step 000060): Train loss 6.094, Val loss 6.074\n",
      "Ep 1 (Step 000070): Train loss 6.020, Val loss 5.945\n",
      "Ep 1 (Step 000080): Train loss 5.721, Val loss 5.826\n",
      "Ep 1 (Step 000090): Train loss 5.756, Val loss 5.764\n",
      "Ep 1 (Step 000100): Train loss 5.654, Val loss 5.694\n",
      "Ep 1 (Step 000110): Train loss 5.590, Val loss 5.680\n",
      "Ep 1 (Step 000120): Train loss 5.564, Val loss 5.632\n",
      "Ep 1 (Step 000130): Train loss 5.549, Val loss 5.581\n",
      "Ep 1 (Step 000140): Train loss 5.417, Val loss 5.552\n",
      "Ep 1 (Step 000150): Train loss 5.422, Val loss 5.553\n",
      "Ep 1 (Step 000160): Train loss 5.427, Val loss 5.498\n",
      "Ep 1 (Step 000170): Train loss 5.476, Val loss 5.452\n",
      "Ep 1 (Step 000180): Train loss 5.387, Val loss 5.443\n",
      "Ep 1 (Step 000190): Train loss 5.356, Val loss 5.413\n",
      "Ep 1 (Step 000200): Train loss 5.348, Val loss 5.401\n",
      "Ep 1 (Step 000210): Train loss 5.197, Val loss 5.386\n",
      "Ep 1 (Step 000220): Train loss 5.238, Val loss 5.378\n",
      "Ep 1 (Step 000230): Train loss 5.290, Val loss 5.337\n",
      "Ep 1 (Step 000240): Train loss 5.113, Val loss 5.327\n",
      "Ep 1 (Step 000250): Train loss 5.212, Val loss 5.328\n",
      "Ep 1 (Step 000260): Train loss 5.215, Val loss 5.319\n",
      "Ep 1 (Step 000270): Train loss 5.162, Val loss 5.313\n",
      "Ep 1 (Step 000280): Train loss 5.125, Val loss 5.294\n",
      "Ep 1 (Step 000290): Train loss 5.094, Val loss 5.281\n",
      "Ep 1 (Step 000300): Train loss 5.031, Val loss 5.261\n",
      "Ep 1 (Step 000310): Train loss 5.107, Val loss 5.252\n",
      "Ep 1 (Step 000320): Train loss 5.060, Val loss 5.237\n",
      "Ep 1 (Step 000330): Train loss 5.008, Val loss 5.232\n",
      "Ep 1 (Step 000340): Train loss 5.168, Val loss 5.244\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2445\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.934, Val loss 8.916\n",
      "Ep 1 (Step 000010): Train loss 6.815, Val loss 6.839\n",
      "Ep 1 (Step 000020): Train loss 6.465, Val loss 6.457\n",
      "Ep 1 (Step 000030): Train loss 6.489, Val loss 6.464\n",
      "Ep 1 (Step 000040): Train loss 6.321, Val loss 6.342\n",
      "Ep 1 (Step 000050): Train loss 6.110, Val loss 6.195\n",
      "Ep 1 (Step 000060): Train loss 6.026, Val loss 6.064\n",
      "Ep 1 (Step 000070): Train loss 5.872, Val loss 5.953\n",
      "Ep 1 (Step 000080): Train loss 5.923, Val loss 5.852\n",
      "Ep 1 (Step 000090): Train loss 5.812, Val loss 5.783\n",
      "Ep 1 (Step 000100): Train loss 5.737, Val loss 5.713\n",
      "Ep 1 (Step 000110): Train loss 5.673, Val loss 5.681\n",
      "Ep 1 (Step 000120): Train loss 5.540, Val loss 5.611\n",
      "Ep 1 (Step 000130): Train loss 5.498, Val loss 5.580\n",
      "Ep 1 (Step 000140): Train loss 5.467, Val loss 5.566\n",
      "Ep 1 (Step 000150): Train loss 5.466, Val loss 5.544\n",
      "Ep 1 (Step 000160): Train loss 5.406, Val loss 5.500\n",
      "Ep 1 (Step 000170): Train loss 5.419, Val loss 5.461\n",
      "Ep 1 (Step 000180): Train loss 5.345, Val loss 5.466\n",
      "Ep 1 (Step 000190): Train loss 5.210, Val loss 5.437\n",
      "Ep 1 (Step 000200): Train loss 5.294, Val loss 5.416\n",
      "Ep 1 (Step 000210): Train loss 5.268, Val loss 5.395\n",
      "Ep 1 (Step 000220): Train loss 5.204, Val loss 5.393\n",
      "Ep 1 (Step 000230): Train loss 5.222, Val loss 5.391\n",
      "Ep 1 (Step 000240): Train loss 5.246, Val loss 5.351\n",
      "Ep 1 (Step 000250): Train loss 5.248, Val loss 5.332\n",
      "Ep 1 (Step 000260): Train loss 5.158, Val loss 5.328\n",
      "Ep 1 (Step 000270): Train loss 5.114, Val loss 5.304\n",
      "Ep 1 (Step 000280): Train loss 5.111, Val loss 5.291\n",
      "Ep 1 (Step 000290): Train loss 5.121, Val loss 5.283\n",
      "Ep 1 (Step 000300): Train loss 5.128, Val loss 5.279\n",
      "Ep 1 (Step 000310): Train loss 5.044, Val loss 5.264\n",
      "Ep 1 (Step 000320): Train loss 5.175, Val loss 5.249\n",
      "Ep 1 (Step 000330): Train loss 5.054, Val loss 5.234\n",
      "Ep 1 (Step 000340): Train loss 5.132, Val loss 5.240\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2399\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.950, Val loss 8.944\n",
      "Ep 1 (Step 000010): Train loss 6.921, Val loss 6.844\n",
      "Ep 1 (Step 000020): Train loss 6.503, Val loss 6.453\n",
      "Ep 1 (Step 000030): Train loss 6.482, Val loss 6.449\n",
      "Ep 1 (Step 000040): Train loss 6.306, Val loss 6.373\n",
      "Ep 1 (Step 000050): Train loss 6.255, Val loss 6.207\n",
      "Ep 1 (Step 000060): Train loss 6.074, Val loss 6.090\n",
      "Ep 1 (Step 000070): Train loss 5.986, Val loss 5.996\n",
      "Ep 1 (Step 000080): Train loss 5.837, Val loss 5.896\n",
      "Ep 1 (Step 000090): Train loss 5.793, Val loss 5.837\n",
      "Ep 1 (Step 000100): Train loss 5.693, Val loss 5.782\n",
      "Ep 1 (Step 000110): Train loss 5.606, Val loss 5.728\n",
      "Ep 1 (Step 000120): Train loss 5.486, Val loss 5.656\n",
      "Ep 1 (Step 000130): Train loss 5.493, Val loss 5.620\n",
      "Ep 1 (Step 000140): Train loss 5.486, Val loss 5.570\n",
      "Ep 1 (Step 000150): Train loss 5.392, Val loss 5.553\n",
      "Ep 1 (Step 000160): Train loss 5.366, Val loss 5.508\n",
      "Ep 1 (Step 000170): Train loss 5.359, Val loss 5.485\n",
      "Ep 1 (Step 000180): Train loss 5.250, Val loss 5.454\n",
      "Ep 1 (Step 000190): Train loss 5.326, Val loss 5.435\n",
      "Ep 1 (Step 000200): Train loss 5.279, Val loss 5.399\n",
      "Ep 1 (Step 000210): Train loss 5.213, Val loss 5.375\n",
      "Ep 1 (Step 000220): Train loss 5.299, Val loss 5.383\n",
      "Ep 1 (Step 000230): Train loss 5.208, Val loss 5.378\n",
      "Ep 1 (Step 000240): Train loss 5.273, Val loss 5.349\n",
      "Ep 1 (Step 000250): Train loss 5.166, Val loss 5.333\n",
      "Ep 1 (Step 000260): Train loss 5.120, Val loss 5.326\n",
      "Ep 1 (Step 000270): Train loss 5.168, Val loss 5.321\n",
      "Ep 1 (Step 000280): Train loss 5.103, Val loss 5.305\n",
      "Ep 1 (Step 000290): Train loss 4.984, Val loss 5.284\n",
      "Ep 1 (Step 000300): Train loss 5.089, Val loss 5.281\n",
      "Ep 1 (Step 000310): Train loss 5.074, Val loss 5.259\n",
      "Ep 1 (Step 000320): Train loss 5.067, Val loss 5.250\n",
      "Ep 1 (Step 000330): Train loss 5.087, Val loss 5.251\n",
      "Ep 1 (Step 000340): Train loss 5.188, Val loss 5.231\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2309\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.935, Val loss 8.917\n",
      "Ep 1 (Step 000010): Train loss 6.881, Val loss 6.812\n",
      "Ep 1 (Step 000020): Train loss 6.467, Val loss 6.437\n",
      "Ep 1 (Step 000030): Train loss 6.451, Val loss 6.445\n",
      "Ep 1 (Step 000040): Train loss 6.353, Val loss 6.329\n",
      "Ep 1 (Step 000050): Train loss 6.134, Val loss 6.161\n",
      "Ep 1 (Step 000060): Train loss 6.062, Val loss 6.043\n",
      "Ep 1 (Step 000070): Train loss 5.903, Val loss 5.921\n",
      "Ep 1 (Step 000080): Train loss 5.800, Val loss 5.822\n",
      "Ep 1 (Step 000090): Train loss 5.735, Val loss 5.769\n",
      "Ep 1 (Step 000100): Train loss 5.621, Val loss 5.704\n",
      "Ep 1 (Step 000110): Train loss 5.563, Val loss 5.659\n",
      "Ep 1 (Step 000120): Train loss 5.572, Val loss 5.620\n",
      "Ep 1 (Step 000130): Train loss 5.497, Val loss 5.581\n",
      "Ep 1 (Step 000140): Train loss 5.462, Val loss 5.536\n",
      "Ep 1 (Step 000150): Train loss 5.389, Val loss 5.515\n",
      "Ep 1 (Step 000160): Train loss 5.352, Val loss 5.481\n",
      "Ep 1 (Step 000170): Train loss 5.341, Val loss 5.450\n",
      "Ep 1 (Step 000180): Train loss 5.406, Val loss 5.439\n",
      "Ep 1 (Step 000190): Train loss 5.354, Val loss 5.403\n",
      "Ep 1 (Step 000200): Train loss 5.257, Val loss 5.399\n",
      "Ep 1 (Step 000210): Train loss 5.189, Val loss 5.377\n",
      "Ep 1 (Step 000220): Train loss 5.312, Val loss 5.340\n",
      "Ep 1 (Step 000230): Train loss 5.217, Val loss 5.334\n",
      "Ep 1 (Step 000240): Train loss 5.222, Val loss 5.339\n",
      "Ep 1 (Step 000250): Train loss 5.188, Val loss 5.323\n",
      "Ep 1 (Step 000260): Train loss 5.140, Val loss 5.305\n",
      "Ep 1 (Step 000270): Train loss 5.143, Val loss 5.275\n",
      "Ep 1 (Step 000280): Train loss 5.173, Val loss 5.276\n",
      "Ep 1 (Step 000290): Train loss 5.110, Val loss 5.248\n",
      "Ep 1 (Step 000300): Train loss 5.105, Val loss 5.241\n",
      "Ep 1 (Step 000310): Train loss 5.079, Val loss 5.229\n",
      "Ep 1 (Step 000320): Train loss 5.111, Val loss 5.223\n",
      "Ep 1 (Step 000330): Train loss 4.990, Val loss 5.216\n",
      "Ep 1 (Step 000340): Train loss 4.974, Val loss 5.201\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2011\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.926, Val loss 8.904\n",
      "Ep 1 (Step 000010): Train loss 6.866, Val loss 6.860\n",
      "Ep 1 (Step 000020): Train loss 6.565, Val loss 6.468\n",
      "Ep 1 (Step 000030): Train loss 6.406, Val loss 6.469\n",
      "Ep 1 (Step 000040): Train loss 6.372, Val loss 6.376\n",
      "Ep 1 (Step 000050): Train loss 6.216, Val loss 6.201\n",
      "Ep 1 (Step 000060): Train loss 6.101, Val loss 6.086\n",
      "Ep 1 (Step 000070): Train loss 5.901, Val loss 5.975\n",
      "Ep 1 (Step 000080): Train loss 5.817, Val loss 5.859\n",
      "Ep 1 (Step 000090): Train loss 5.731, Val loss 5.793\n",
      "Ep 1 (Step 000100): Train loss 5.780, Val loss 5.730\n",
      "Ep 1 (Step 000110): Train loss 5.623, Val loss 5.693\n",
      "Ep 1 (Step 000120): Train loss 5.557, Val loss 5.640\n",
      "Ep 1 (Step 000130): Train loss 5.501, Val loss 5.593\n",
      "Ep 1 (Step 000140): Train loss 5.369, Val loss 5.544\n",
      "Ep 1 (Step 000150): Train loss 5.402, Val loss 5.489\n",
      "Ep 1 (Step 000160): Train loss 5.377, Val loss 5.475\n",
      "Ep 1 (Step 000170): Train loss 5.418, Val loss 5.459\n",
      "Ep 1 (Step 000180): Train loss 5.373, Val loss 5.435\n",
      "Ep 1 (Step 000190): Train loss 5.361, Val loss 5.406\n",
      "Ep 1 (Step 000200): Train loss 5.287, Val loss 5.390\n",
      "Ep 1 (Step 000210): Train loss 5.319, Val loss 5.377\n",
      "Ep 1 (Step 000220): Train loss 5.253, Val loss 5.378\n",
      "Ep 1 (Step 000230): Train loss 5.273, Val loss 5.341\n",
      "Ep 1 (Step 000240): Train loss 5.203, Val loss 5.339\n",
      "Ep 1 (Step 000250): Train loss 5.222, Val loss 5.311\n",
      "Ep 1 (Step 000260): Train loss 5.085, Val loss 5.302\n",
      "Ep 1 (Step 000270): Train loss 5.186, Val loss 5.286\n",
      "Ep 1 (Step 000280): Train loss 5.149, Val loss 5.273\n",
      "Ep 1 (Step 000290): Train loss 5.097, Val loss 5.245\n",
      "Ep 1 (Step 000300): Train loss 5.114, Val loss 5.242\n",
      "Ep 1 (Step 000310): Train loss 5.044, Val loss 5.232\n",
      "Ep 1 (Step 000320): Train loss 5.089, Val loss 5.201\n",
      "Ep 1 (Step 000330): Train loss 5.125, Val loss 5.193\n",
      "Ep 1 (Step 000340): Train loss 5.042, Val loss 5.194\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1942\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.907, Val loss 8.898\n",
      "Ep 1 (Step 000010): Train loss 6.912, Val loss 6.848\n",
      "Ep 1 (Step 000020): Train loss 6.516, Val loss 6.461\n",
      "Ep 1 (Step 000030): Train loss 6.515, Val loss 6.443\n",
      "Ep 1 (Step 000040): Train loss 6.387, Val loss 6.354\n",
      "Ep 1 (Step 000050): Train loss 6.226, Val loss 6.220\n",
      "Ep 1 (Step 000060): Train loss 6.034, Val loss 6.061\n",
      "Ep 1 (Step 000070): Train loss 5.932, Val loss 5.964\n",
      "Ep 1 (Step 000080): Train loss 5.866, Val loss 5.864\n",
      "Ep 1 (Step 000090): Train loss 5.784, Val loss 5.790\n",
      "Ep 1 (Step 000100): Train loss 5.603, Val loss 5.733\n",
      "Ep 1 (Step 000110): Train loss 5.624, Val loss 5.662\n",
      "Ep 1 (Step 000120): Train loss 5.564, Val loss 5.606\n",
      "Ep 1 (Step 000130): Train loss 5.542, Val loss 5.590\n",
      "Ep 1 (Step 000140): Train loss 5.438, Val loss 5.548\n",
      "Ep 1 (Step 000150): Train loss 5.470, Val loss 5.529\n",
      "Ep 1 (Step 000160): Train loss 5.394, Val loss 5.499\n",
      "Ep 1 (Step 000170): Train loss 5.403, Val loss 5.462\n",
      "Ep 1 (Step 000180): Train loss 5.380, Val loss 5.421\n",
      "Ep 1 (Step 000190): Train loss 5.345, Val loss 5.424\n",
      "Ep 1 (Step 000200): Train loss 5.304, Val loss 5.403\n",
      "Ep 1 (Step 000210): Train loss 5.282, Val loss 5.378\n",
      "Ep 1 (Step 000220): Train loss 5.243, Val loss 5.354\n",
      "Ep 1 (Step 000230): Train loss 5.181, Val loss 5.340\n",
      "Ep 1 (Step 000240): Train loss 5.230, Val loss 5.324\n",
      "Ep 1 (Step 000250): Train loss 5.216, Val loss 5.314\n",
      "Ep 1 (Step 000260): Train loss 5.235, Val loss 5.312\n",
      "Ep 1 (Step 000270): Train loss 5.168, Val loss 5.287\n",
      "Ep 1 (Step 000280): Train loss 5.138, Val loss 5.273\n",
      "Ep 1 (Step 000290): Train loss 5.008, Val loss 5.262\n",
      "Ep 1 (Step 000300): Train loss 5.088, Val loss 5.249\n",
      "Ep 1 (Step 000310): Train loss 5.011, Val loss 5.226\n",
      "Ep 1 (Step 000320): Train loss 5.099, Val loss 5.211\n",
      "Ep 1 (Step 000330): Train loss 5.031, Val loss 5.217\n",
      "Ep 1 (Step 000340): Train loss 5.042, Val loss 5.201\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2005\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.102, Val loss 9.111\n",
      "Ep 1 (Step 000010): Train loss 7.499, Val loss 7.501\n",
      "Ep 1 (Step 000020): Train loss 6.827, Val loss 6.829\n",
      "Ep 1 (Step 000030): Train loss 6.521, Val loss 6.495\n",
      "Ep 1 (Step 000040): Train loss 6.392, Val loss 6.410\n",
      "Ep 1 (Step 000050): Train loss 6.359, Val loss 6.373\n",
      "Ep 1 (Step 000060): Train loss 6.315, Val loss 6.321\n",
      "Ep 1 (Step 000070): Train loss 6.155, Val loss 6.211\n",
      "Ep 1 (Step 000080): Train loss 6.090, Val loss 6.087\n",
      "Ep 1 (Step 000090): Train loss 5.999, Val loss 6.034\n",
      "Ep 1 (Step 000100): Train loss 5.918, Val loss 5.966\n",
      "Ep 1 (Step 000110): Train loss 5.923, Val loss 5.921\n",
      "Ep 1 (Step 000120): Train loss 5.775, Val loss 5.872\n",
      "Ep 1 (Step 000130): Train loss 5.736, Val loss 5.827\n",
      "Ep 1 (Step 000140): Train loss 5.737, Val loss 5.772\n",
      "Ep 1 (Step 000150): Train loss 5.682, Val loss 5.737\n",
      "Ep 1 (Step 000160): Train loss 5.657, Val loss 5.697\n",
      "Ep 1 (Step 000170): Train loss 5.606, Val loss 5.674\n",
      "Ep 1 (Step 000180): Train loss 5.602, Val loss 5.649\n",
      "Ep 1 (Step 000190): Train loss 5.542, Val loss 5.609\n",
      "Ep 1 (Step 000200): Train loss 5.559, Val loss 5.601\n",
      "Ep 1 (Step 000210): Train loss 5.518, Val loss 5.576\n",
      "Ep 1 (Step 000220): Train loss 5.385, Val loss 5.563\n",
      "Ep 1 (Step 000230): Train loss 5.473, Val loss 5.540\n",
      "Ep 1 (Step 000240): Train loss 5.438, Val loss 5.518\n",
      "Ep 1 (Step 000250): Train loss 5.438, Val loss 5.487\n",
      "Ep 1 (Step 000260): Train loss 5.493, Val loss 5.467\n",
      "Ep 1 (Step 000270): Train loss 5.409, Val loss 5.461\n",
      "Ep 1 (Step 000280): Train loss 5.384, Val loss 5.452\n",
      "Ep 1 (Step 000290): Train loss 5.353, Val loss 5.437\n",
      "Ep 1 (Step 000300): Train loss 5.381, Val loss 5.436\n",
      "Ep 1 (Step 000310): Train loss 5.342, Val loss 5.417\n",
      "Ep 1 (Step 000320): Train loss 5.322, Val loss 5.405\n",
      "Ep 1 (Step 000330): Train loss 5.233, Val loss 5.386\n",
      "Ep 1 (Step 000340): Train loss 5.247, Val loss 5.382\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3820\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.020, Val loss 9.020\n",
      "Ep 1 (Step 000010): Train loss 7.431, Val loss 7.406\n",
      "Ep 1 (Step 000020): Train loss 6.776, Val loss 6.762\n",
      "Ep 1 (Step 000030): Train loss 6.470, Val loss 6.468\n",
      "Ep 1 (Step 000040): Train loss 6.436, Val loss 6.399\n",
      "Ep 1 (Step 000050): Train loss 6.373, Val loss 6.382\n",
      "Ep 1 (Step 000060): Train loss 6.329, Val loss 6.336\n",
      "Ep 1 (Step 000070): Train loss 6.197, Val loss 6.239\n",
      "Ep 1 (Step 000080): Train loss 6.186, Val loss 6.160\n",
      "Ep 1 (Step 000090): Train loss 6.062, Val loss 6.054\n",
      "Ep 1 (Step 000100): Train loss 5.928, Val loss 5.994\n",
      "Ep 1 (Step 000110): Train loss 5.949, Val loss 5.951\n",
      "Ep 1 (Step 000120): Train loss 5.901, Val loss 5.889\n",
      "Ep 1 (Step 000130): Train loss 5.762, Val loss 5.841\n",
      "Ep 1 (Step 000140): Train loss 5.731, Val loss 5.781\n",
      "Ep 1 (Step 000150): Train loss 5.715, Val loss 5.750\n",
      "Ep 1 (Step 000160): Train loss 5.704, Val loss 5.709\n",
      "Ep 1 (Step 000170): Train loss 5.613, Val loss 5.697\n",
      "Ep 1 (Step 000180): Train loss 5.658, Val loss 5.682\n",
      "Ep 1 (Step 000190): Train loss 5.510, Val loss 5.635\n",
      "Ep 1 (Step 000200): Train loss 5.571, Val loss 5.608\n",
      "Ep 1 (Step 000210): Train loss 5.467, Val loss 5.575\n",
      "Ep 1 (Step 000220): Train loss 5.460, Val loss 5.562\n",
      "Ep 1 (Step 000230): Train loss 5.372, Val loss 5.534\n",
      "Ep 1 (Step 000240): Train loss 5.349, Val loss 5.531\n",
      "Ep 1 (Step 000250): Train loss 5.429, Val loss 5.504\n",
      "Ep 1 (Step 000260): Train loss 5.333, Val loss 5.470\n",
      "Ep 1 (Step 000270): Train loss 5.367, Val loss 5.458\n",
      "Ep 1 (Step 000280): Train loss 5.319, Val loss 5.445\n",
      "Ep 1 (Step 000290): Train loss 5.308, Val loss 5.450\n",
      "Ep 1 (Step 000300): Train loss 5.288, Val loss 5.426\n",
      "Ep 1 (Step 000310): Train loss 5.313, Val loss 5.415\n",
      "Ep 1 (Step 000320): Train loss 5.244, Val loss 5.399\n",
      "Ep 1 (Step 000330): Train loss 5.282, Val loss 5.387\n",
      "Ep 1 (Step 000340): Train loss 5.335, Val loss 5.384\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3843\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.103, Val loss 9.077\n",
      "Ep 1 (Step 000010): Train loss 7.531, Val loss 7.503\n",
      "Ep 1 (Step 000020): Train loss 6.917, Val loss 6.839\n",
      "Ep 1 (Step 000030): Train loss 6.562, Val loss 6.498\n",
      "Ep 1 (Step 000040): Train loss 6.408, Val loss 6.382\n",
      "Ep 1 (Step 000050): Train loss 6.320, Val loss 6.357\n",
      "Ep 1 (Step 000060): Train loss 6.251, Val loss 6.299\n",
      "Ep 1 (Step 000070): Train loss 6.168, Val loss 6.198\n",
      "Ep 1 (Step 000080): Train loss 6.142, Val loss 6.148\n",
      "Ep 1 (Step 000090): Train loss 6.007, Val loss 6.045\n",
      "Ep 1 (Step 000100): Train loss 5.913, Val loss 5.984\n",
      "Ep 1 (Step 000110): Train loss 5.918, Val loss 5.937\n",
      "Ep 1 (Step 000120): Train loss 5.866, Val loss 5.887\n",
      "Ep 1 (Step 000130): Train loss 5.792, Val loss 5.838\n",
      "Ep 1 (Step 000140): Train loss 5.725, Val loss 5.810\n",
      "Ep 1 (Step 000150): Train loss 5.722, Val loss 5.764\n",
      "Ep 1 (Step 000160): Train loss 5.641, Val loss 5.708\n",
      "Ep 1 (Step 000170): Train loss 5.695, Val loss 5.687\n",
      "Ep 1 (Step 000180): Train loss 5.643, Val loss 5.655\n",
      "Ep 1 (Step 000190): Train loss 5.606, Val loss 5.613\n",
      "Ep 1 (Step 000200): Train loss 5.589, Val loss 5.609\n",
      "Ep 1 (Step 000210): Train loss 5.520, Val loss 5.580\n",
      "Ep 1 (Step 000220): Train loss 5.517, Val loss 5.564\n",
      "Ep 1 (Step 000230): Train loss 5.358, Val loss 5.529\n",
      "Ep 1 (Step 000240): Train loss 5.451, Val loss 5.534\n",
      "Ep 1 (Step 000250): Train loss 5.405, Val loss 5.499\n",
      "Ep 1 (Step 000260): Train loss 5.319, Val loss 5.492\n",
      "Ep 1 (Step 000270): Train loss 5.449, Val loss 5.479\n",
      "Ep 1 (Step 000280): Train loss 5.308, Val loss 5.454\n",
      "Ep 1 (Step 000290): Train loss 5.380, Val loss 5.459\n",
      "Ep 1 (Step 000300): Train loss 5.355, Val loss 5.453\n",
      "Ep 1 (Step 000310): Train loss 5.327, Val loss 5.431\n",
      "Ep 1 (Step 000320): Train loss 5.237, Val loss 5.414\n",
      "Ep 1 (Step 000330): Train loss 5.336, Val loss 5.386\n",
      "Ep 1 (Step 000340): Train loss 5.209, Val loss 5.377\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3766\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.079, Val loss 9.074\n",
      "Ep 1 (Step 000010): Train loss 7.433, Val loss 7.440\n",
      "Ep 1 (Step 000020): Train loss 6.790, Val loss 6.774\n",
      "Ep 1 (Step 000030): Train loss 6.450, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.378, Val loss 6.405\n",
      "Ep 1 (Step 000050): Train loss 6.385, Val loss 6.385\n",
      "Ep 1 (Step 000060): Train loss 6.346, Val loss 6.352\n",
      "Ep 1 (Step 000070): Train loss 6.215, Val loss 6.243\n",
      "Ep 1 (Step 000080): Train loss 6.073, Val loss 6.138\n",
      "Ep 1 (Step 000090): Train loss 5.996, Val loss 6.053\n",
      "Ep 1 (Step 000100): Train loss 5.931, Val loss 5.977\n",
      "Ep 1 (Step 000110): Train loss 5.904, Val loss 5.905\n",
      "Ep 1 (Step 000120): Train loss 5.848, Val loss 5.861\n",
      "Ep 1 (Step 000130): Train loss 5.730, Val loss 5.818\n",
      "Ep 1 (Step 000140): Train loss 5.713, Val loss 5.775\n",
      "Ep 1 (Step 000150): Train loss 5.762, Val loss 5.753\n",
      "Ep 1 (Step 000160): Train loss 5.600, Val loss 5.717\n",
      "Ep 1 (Step 000170): Train loss 5.635, Val loss 5.678\n",
      "Ep 1 (Step 000180): Train loss 5.637, Val loss 5.653\n",
      "Ep 1 (Step 000190): Train loss 5.576, Val loss 5.623\n",
      "Ep 1 (Step 000200): Train loss 5.484, Val loss 5.589\n",
      "Ep 1 (Step 000210): Train loss 5.552, Val loss 5.585\n",
      "Ep 1 (Step 000220): Train loss 5.498, Val loss 5.550\n",
      "Ep 1 (Step 000230): Train loss 5.473, Val loss 5.537\n",
      "Ep 1 (Step 000240): Train loss 5.511, Val loss 5.517\n",
      "Ep 1 (Step 000250): Train loss 5.407, Val loss 5.498\n",
      "Ep 1 (Step 000260): Train loss 5.340, Val loss 5.478\n",
      "Ep 1 (Step 000270): Train loss 5.339, Val loss 5.459\n",
      "Ep 1 (Step 000280): Train loss 5.348, Val loss 5.438\n",
      "Ep 1 (Step 000290): Train loss 5.277, Val loss 5.431\n",
      "Ep 1 (Step 000300): Train loss 5.303, Val loss 5.424\n",
      "Ep 1 (Step 000310): Train loss 5.358, Val loss 5.387\n",
      "Ep 1 (Step 000320): Train loss 5.280, Val loss 5.371\n",
      "Ep 1 (Step 000330): Train loss 5.299, Val loss 5.357\n",
      "Ep 1 (Step 000340): Train loss 5.243, Val loss 5.357\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3566\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.110, Val loss 9.094\n",
      "Ep 1 (Step 000010): Train loss 7.498, Val loss 7.472\n",
      "Ep 1 (Step 000020): Train loss 6.824, Val loss 6.800\n",
      "Ep 1 (Step 000030): Train loss 6.496, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.438, Val loss 6.400\n",
      "Ep 1 (Step 000050): Train loss 6.393, Val loss 6.362\n",
      "Ep 1 (Step 000060): Train loss 6.333, Val loss 6.328\n",
      "Ep 1 (Step 000070): Train loss 6.200, Val loss 6.245\n",
      "Ep 1 (Step 000080): Train loss 6.110, Val loss 6.140\n",
      "Ep 1 (Step 000090): Train loss 6.062, Val loss 6.039\n",
      "Ep 1 (Step 000100): Train loss 5.960, Val loss 5.994\n",
      "Ep 1 (Step 000110): Train loss 5.846, Val loss 5.924\n",
      "Ep 1 (Step 000120): Train loss 5.824, Val loss 5.874\n",
      "Ep 1 (Step 000130): Train loss 5.802, Val loss 5.814\n",
      "Ep 1 (Step 000140): Train loss 5.731, Val loss 5.775\n",
      "Ep 1 (Step 000150): Train loss 5.731, Val loss 5.743\n",
      "Ep 1 (Step 000160): Train loss 5.682, Val loss 5.715\n",
      "Ep 1 (Step 000170): Train loss 5.596, Val loss 5.688\n",
      "Ep 1 (Step 000180): Train loss 5.577, Val loss 5.653\n",
      "Ep 1 (Step 000190): Train loss 5.568, Val loss 5.626\n",
      "Ep 1 (Step 000200): Train loss 5.543, Val loss 5.588\n",
      "Ep 1 (Step 000210): Train loss 5.486, Val loss 5.571\n",
      "Ep 1 (Step 000220): Train loss 5.507, Val loss 5.539\n",
      "Ep 1 (Step 000230): Train loss 5.414, Val loss 5.526\n",
      "Ep 1 (Step 000240): Train loss 5.386, Val loss 5.509\n",
      "Ep 1 (Step 000250): Train loss 5.358, Val loss 5.493\n",
      "Ep 1 (Step 000260): Train loss 5.295, Val loss 5.465\n",
      "Ep 1 (Step 000270): Train loss 5.376, Val loss 5.444\n",
      "Ep 1 (Step 000280): Train loss 5.419, Val loss 5.431\n",
      "Ep 1 (Step 000290): Train loss 5.364, Val loss 5.424\n",
      "Ep 1 (Step 000300): Train loss 5.299, Val loss 5.415\n",
      "Ep 1 (Step 000310): Train loss 5.334, Val loss 5.403\n",
      "Ep 1 (Step 000320): Train loss 5.232, Val loss 5.387\n",
      "Ep 1 (Step 000330): Train loss 5.264, Val loss 5.383\n",
      "Ep 1 (Step 000340): Train loss 5.285, Val loss 5.361\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3615\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.074, Val loss 9.050\n",
      "Ep 1 (Step 000010): Train loss 7.454, Val loss 7.445\n",
      "Ep 1 (Step 000020): Train loss 6.757, Val loss 6.795\n",
      "Ep 1 (Step 000030): Train loss 6.530, Val loss 6.489\n",
      "Ep 1 (Step 000040): Train loss 6.430, Val loss 6.395\n",
      "Ep 1 (Step 000050): Train loss 6.393, Val loss 6.375\n",
      "Ep 1 (Step 000060): Train loss 6.361, Val loss 6.333\n",
      "Ep 1 (Step 000070): Train loss 6.189, Val loss 6.215\n",
      "Ep 1 (Step 000080): Train loss 6.041, Val loss 6.124\n",
      "Ep 1 (Step 000090): Train loss 6.005, Val loss 6.030\n",
      "Ep 1 (Step 000100): Train loss 5.979, Val loss 5.973\n",
      "Ep 1 (Step 000110): Train loss 5.882, Val loss 5.914\n",
      "Ep 1 (Step 000120): Train loss 5.825, Val loss 5.857\n",
      "Ep 1 (Step 000130): Train loss 5.867, Val loss 5.805\n",
      "Ep 1 (Step 000140): Train loss 5.715, Val loss 5.773\n",
      "Ep 1 (Step 000150): Train loss 5.702, Val loss 5.717\n",
      "Ep 1 (Step 000160): Train loss 5.639, Val loss 5.693\n",
      "Ep 1 (Step 000170): Train loss 5.603, Val loss 5.657\n",
      "Ep 1 (Step 000180): Train loss 5.560, Val loss 5.621\n",
      "Ep 1 (Step 000190): Train loss 5.559, Val loss 5.609\n",
      "Ep 1 (Step 000200): Train loss 5.433, Val loss 5.570\n",
      "Ep 1 (Step 000210): Train loss 5.546, Val loss 5.561\n",
      "Ep 1 (Step 000220): Train loss 5.379, Val loss 5.542\n",
      "Ep 1 (Step 000230): Train loss 5.448, Val loss 5.535\n",
      "Ep 1 (Step 000240): Train loss 5.487, Val loss 5.508\n",
      "Ep 1 (Step 000250): Train loss 5.428, Val loss 5.495\n",
      "Ep 1 (Step 000260): Train loss 5.344, Val loss 5.470\n",
      "Ep 1 (Step 000270): Train loss 5.329, Val loss 5.453\n",
      "Ep 1 (Step 000280): Train loss 5.353, Val loss 5.433\n",
      "Ep 1 (Step 000290): Train loss 5.369, Val loss 5.429\n",
      "Ep 1 (Step 000300): Train loss 5.332, Val loss 5.416\n",
      "Ep 1 (Step 000310): Train loss 5.353, Val loss 5.398\n",
      "Ep 1 (Step 000320): Train loss 5.224, Val loss 5.383\n",
      "Ep 1 (Step 000330): Train loss 5.189, Val loss 5.372\n",
      "Ep 1 (Step 000340): Train loss 5.329, Val loss 5.363\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3633\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.777, Val loss 8.759\n",
      "Ep 1 (Step 000010): Train loss 6.869, Val loss 6.853\n",
      "Ep 1 (Step 000020): Train loss 6.432, Val loss 6.473\n",
      "Ep 1 (Step 000030): Train loss 6.447, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.313, Val loss 6.370\n",
      "Ep 1 (Step 000050): Train loss 6.299, Val loss 6.250\n",
      "Ep 1 (Step 000060): Train loss 6.109, Val loss 6.109\n",
      "Ep 1 (Step 000070): Train loss 6.017, Val loss 6.017\n",
      "Ep 1 (Step 000080): Train loss 5.842, Val loss 5.899\n",
      "Ep 1 (Step 000090): Train loss 5.767, Val loss 5.853\n",
      "Ep 1 (Step 000100): Train loss 5.832, Val loss 5.794\n",
      "Ep 1 (Step 000110): Train loss 5.640, Val loss 5.731\n",
      "Ep 1 (Step 000120): Train loss 5.588, Val loss 5.664\n",
      "Ep 1 (Step 000130): Train loss 5.561, Val loss 5.636\n",
      "Ep 1 (Step 000140): Train loss 5.499, Val loss 5.620\n",
      "Ep 1 (Step 000150): Train loss 5.561, Val loss 5.567\n",
      "Ep 1 (Step 000160): Train loss 5.455, Val loss 5.544\n",
      "Ep 1 (Step 000170): Train loss 5.430, Val loss 5.523\n",
      "Ep 1 (Step 000180): Train loss 5.385, Val loss 5.520\n",
      "Ep 1 (Step 000190): Train loss 5.370, Val loss 5.482\n",
      "Ep 1 (Step 000200): Train loss 5.331, Val loss 5.468\n",
      "Ep 1 (Step 000210): Train loss 5.263, Val loss 5.455\n",
      "Ep 1 (Step 000220): Train loss 5.346, Val loss 5.427\n",
      "Ep 1 (Step 000230): Train loss 5.305, Val loss 5.409\n",
      "Ep 1 (Step 000240): Train loss 5.303, Val loss 5.390\n",
      "Ep 1 (Step 000250): Train loss 5.318, Val loss 5.378\n",
      "Ep 1 (Step 000260): Train loss 5.261, Val loss 5.365\n",
      "Ep 1 (Step 000270): Train loss 5.183, Val loss 5.349\n",
      "Ep 1 (Step 000280): Train loss 5.196, Val loss 5.346\n",
      "Ep 1 (Step 000290): Train loss 5.189, Val loss 5.327\n",
      "Ep 1 (Step 000300): Train loss 5.135, Val loss 5.325\n",
      "Ep 1 (Step 000310): Train loss 5.226, Val loss 5.298\n",
      "Ep 1 (Step 000320): Train loss 5.221, Val loss 5.290\n",
      "Ep 1 (Step 000330): Train loss 5.031, Val loss 5.292\n",
      "Ep 1 (Step 000340): Train loss 5.115, Val loss 5.267\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2666\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.815, Val loss 8.784\n",
      "Ep 1 (Step 000010): Train loss 6.895, Val loss 6.875\n",
      "Ep 1 (Step 000020): Train loss 6.458, Val loss 6.493\n",
      "Ep 1 (Step 000030): Train loss 6.433, Val loss 6.464\n",
      "Ep 1 (Step 000040): Train loss 6.255, Val loss 6.331\n",
      "Ep 1 (Step 000050): Train loss 6.112, Val loss 6.184\n",
      "Ep 1 (Step 000060): Train loss 5.996, Val loss 6.089\n",
      "Ep 1 (Step 000070): Train loss 5.836, Val loss 5.954\n",
      "Ep 1 (Step 000080): Train loss 5.791, Val loss 5.886\n",
      "Ep 1 (Step 000090): Train loss 5.790, Val loss 5.811\n",
      "Ep 1 (Step 000100): Train loss 5.680, Val loss 5.773\n",
      "Ep 1 (Step 000110): Train loss 5.675, Val loss 5.727\n",
      "Ep 1 (Step 000120): Train loss 5.645, Val loss 5.672\n",
      "Ep 1 (Step 000130): Train loss 5.490, Val loss 5.622\n",
      "Ep 1 (Step 000140): Train loss 5.533, Val loss 5.577\n",
      "Ep 1 (Step 000150): Train loss 5.520, Val loss 5.549\n",
      "Ep 1 (Step 000160): Train loss 5.446, Val loss 5.543\n",
      "Ep 1 (Step 000170): Train loss 5.420, Val loss 5.497\n",
      "Ep 1 (Step 000180): Train loss 5.389, Val loss 5.463\n",
      "Ep 1 (Step 000190): Train loss 5.342, Val loss 5.432\n",
      "Ep 1 (Step 000200): Train loss 5.379, Val loss 5.423\n",
      "Ep 1 (Step 000210): Train loss 5.273, Val loss 5.420\n",
      "Ep 1 (Step 000220): Train loss 5.368, Val loss 5.409\n",
      "Ep 1 (Step 000230): Train loss 5.257, Val loss 5.392\n",
      "Ep 1 (Step 000240): Train loss 5.288, Val loss 5.364\n",
      "Ep 1 (Step 000250): Train loss 5.226, Val loss 5.376\n",
      "Ep 1 (Step 000260): Train loss 5.100, Val loss 5.350\n",
      "Ep 1 (Step 000270): Train loss 5.280, Val loss 5.312\n",
      "Ep 1 (Step 000280): Train loss 5.206, Val loss 5.326\n",
      "Ep 1 (Step 000290): Train loss 5.280, Val loss 5.304\n",
      "Ep 1 (Step 000300): Train loss 5.209, Val loss 5.285\n",
      "Ep 1 (Step 000310): Train loss 5.210, Val loss 5.294\n",
      "Ep 1 (Step 000320): Train loss 5.146, Val loss 5.281\n",
      "Ep 1 (Step 000330): Train loss 5.103, Val loss 5.270\n",
      "Ep 1 (Step 000340): Train loss 5.123, Val loss 5.285\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2847\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.846, Val loss 8.797\n",
      "Ep 1 (Step 000010): Train loss 6.878, Val loss 6.831\n",
      "Ep 1 (Step 000020): Train loss 6.509, Val loss 6.459\n",
      "Ep 1 (Step 000030): Train loss 6.437, Val loss 6.461\n",
      "Ep 1 (Step 000040): Train loss 6.360, Val loss 6.328\n",
      "Ep 1 (Step 000050): Train loss 6.278, Val loss 6.252\n",
      "Ep 1 (Step 000060): Train loss 6.029, Val loss 6.084\n",
      "Ep 1 (Step 000070): Train loss 5.911, Val loss 5.971\n",
      "Ep 1 (Step 000080): Train loss 5.798, Val loss 5.889\n",
      "Ep 1 (Step 000090): Train loss 5.839, Val loss 5.868\n",
      "Ep 1 (Step 000100): Train loss 5.688, Val loss 5.764\n",
      "Ep 1 (Step 000110): Train loss 5.593, Val loss 5.710\n",
      "Ep 1 (Step 000120): Train loss 5.643, Val loss 5.649\n",
      "Ep 1 (Step 000130): Train loss 5.553, Val loss 5.630\n",
      "Ep 1 (Step 000140): Train loss 5.494, Val loss 5.597\n",
      "Ep 1 (Step 000150): Train loss 5.431, Val loss 5.551\n",
      "Ep 1 (Step 000160): Train loss 5.466, Val loss 5.543\n",
      "Ep 1 (Step 000170): Train loss 5.413, Val loss 5.503\n",
      "Ep 1 (Step 000180): Train loss 5.385, Val loss 5.456\n",
      "Ep 1 (Step 000190): Train loss 5.387, Val loss 5.467\n",
      "Ep 1 (Step 000200): Train loss 5.330, Val loss 5.434\n",
      "Ep 1 (Step 000210): Train loss 5.285, Val loss 5.418\n",
      "Ep 1 (Step 000220): Train loss 5.304, Val loss 5.411\n",
      "Ep 1 (Step 000230): Train loss 5.205, Val loss 5.386\n",
      "Ep 1 (Step 000240): Train loss 5.162, Val loss 5.364\n",
      "Ep 1 (Step 000250): Train loss 5.171, Val loss 5.360\n",
      "Ep 1 (Step 000260): Train loss 5.207, Val loss 5.344\n",
      "Ep 1 (Step 000270): Train loss 5.292, Val loss 5.331\n",
      "Ep 1 (Step 000280): Train loss 5.185, Val loss 5.343\n",
      "Ep 1 (Step 000290): Train loss 5.199, Val loss 5.315\n",
      "Ep 1 (Step 000300): Train loss 5.189, Val loss 5.305\n",
      "Ep 1 (Step 000310): Train loss 5.119, Val loss 5.303\n",
      "Ep 1 (Step 000320): Train loss 5.097, Val loss 5.291\n",
      "Ep 1 (Step 000330): Train loss 5.095, Val loss 5.281\n",
      "Ep 1 (Step 000340): Train loss 5.131, Val loss 5.302\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3024\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.860, Val loss 8.833\n",
      "Ep 1 (Step 000010): Train loss 6.879, Val loss 6.864\n",
      "Ep 1 (Step 000020): Train loss 6.518, Val loss 6.497\n",
      "Ep 1 (Step 000030): Train loss 6.513, Val loss 6.468\n",
      "Ep 1 (Step 000040): Train loss 6.409, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.222, Val loss 6.208\n",
      "Ep 1 (Step 000060): Train loss 6.023, Val loss 6.074\n",
      "Ep 1 (Step 000070): Train loss 5.892, Val loss 5.982\n",
      "Ep 1 (Step 000080): Train loss 5.825, Val loss 5.880\n",
      "Ep 1 (Step 000090): Train loss 5.795, Val loss 5.825\n",
      "Ep 1 (Step 000100): Train loss 5.683, Val loss 5.768\n",
      "Ep 1 (Step 000110): Train loss 5.681, Val loss 5.728\n",
      "Ep 1 (Step 000120): Train loss 5.538, Val loss 5.644\n",
      "Ep 1 (Step 000130): Train loss 5.652, Val loss 5.598\n",
      "Ep 1 (Step 000140): Train loss 5.531, Val loss 5.570\n",
      "Ep 1 (Step 000150): Train loss 5.407, Val loss 5.554\n",
      "Ep 1 (Step 000160): Train loss 5.365, Val loss 5.518\n",
      "Ep 1 (Step 000170): Train loss 5.363, Val loss 5.509\n",
      "Ep 1 (Step 000180): Train loss 5.368, Val loss 5.490\n",
      "Ep 1 (Step 000190): Train loss 5.328, Val loss 5.443\n",
      "Ep 1 (Step 000200): Train loss 5.251, Val loss 5.429\n",
      "Ep 1 (Step 000210): Train loss 5.213, Val loss 5.395\n",
      "Ep 1 (Step 000220): Train loss 5.272, Val loss 5.386\n",
      "Ep 1 (Step 000230): Train loss 5.244, Val loss 5.379\n",
      "Ep 1 (Step 000240): Train loss 5.249, Val loss 5.356\n",
      "Ep 1 (Step 000250): Train loss 5.219, Val loss 5.341\n",
      "Ep 1 (Step 000260): Train loss 5.234, Val loss 5.327\n",
      "Ep 1 (Step 000270): Train loss 5.216, Val loss 5.328\n",
      "Ep 1 (Step 000280): Train loss 5.164, Val loss 5.317\n",
      "Ep 1 (Step 000290): Train loss 5.177, Val loss 5.305\n",
      "Ep 1 (Step 000300): Train loss 5.133, Val loss 5.295\n",
      "Ep 1 (Step 000310): Train loss 5.123, Val loss 5.268\n",
      "Ep 1 (Step 000320): Train loss 5.159, Val loss 5.260\n",
      "Ep 1 (Step 000330): Train loss 5.199, Val loss 5.250\n",
      "Ep 1 (Step 000340): Train loss 5.113, Val loss 5.243\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2431\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.799, Val loss 8.767\n",
      "Ep 1 (Step 000010): Train loss 6.846, Val loss 6.830\n",
      "Ep 1 (Step 000020): Train loss 6.498, Val loss 6.459\n",
      "Ep 1 (Step 000030): Train loss 6.397, Val loss 6.434\n",
      "Ep 1 (Step 000040): Train loss 6.400, Val loss 6.358\n",
      "Ep 1 (Step 000050): Train loss 6.280, Val loss 6.224\n",
      "Ep 1 (Step 000060): Train loss 6.042, Val loss 6.076\n",
      "Ep 1 (Step 000070): Train loss 5.918, Val loss 5.971\n",
      "Ep 1 (Step 000080): Train loss 5.913, Val loss 5.884\n",
      "Ep 1 (Step 000090): Train loss 5.779, Val loss 5.815\n",
      "Ep 1 (Step 000100): Train loss 5.658, Val loss 5.747\n",
      "Ep 1 (Step 000110): Train loss 5.654, Val loss 5.701\n",
      "Ep 1 (Step 000120): Train loss 5.536, Val loss 5.642\n",
      "Ep 1 (Step 000130): Train loss 5.542, Val loss 5.597\n",
      "Ep 1 (Step 000140): Train loss 5.626, Val loss 5.547\n",
      "Ep 1 (Step 000150): Train loss 5.406, Val loss 5.516\n",
      "Ep 1 (Step 000160): Train loss 5.397, Val loss 5.493\n",
      "Ep 1 (Step 000170): Train loss 5.377, Val loss 5.467\n",
      "Ep 1 (Step 000180): Train loss 5.429, Val loss 5.459\n",
      "Ep 1 (Step 000190): Train loss 5.369, Val loss 5.438\n",
      "Ep 1 (Step 000200): Train loss 5.347, Val loss 5.416\n",
      "Ep 1 (Step 000210): Train loss 5.293, Val loss 5.395\n",
      "Ep 1 (Step 000220): Train loss 5.220, Val loss 5.389\n",
      "Ep 1 (Step 000230): Train loss 5.293, Val loss 5.367\n",
      "Ep 1 (Step 000240): Train loss 5.174, Val loss 5.355\n",
      "Ep 1 (Step 000250): Train loss 5.186, Val loss 5.315\n",
      "Ep 1 (Step 000260): Train loss 5.224, Val loss 5.313\n",
      "Ep 1 (Step 000270): Train loss 5.227, Val loss 5.315\n",
      "Ep 1 (Step 000280): Train loss 5.154, Val loss 5.303\n",
      "Ep 1 (Step 000290): Train loss 5.175, Val loss 5.294\n",
      "Ep 1 (Step 000300): Train loss 5.192, Val loss 5.290\n",
      "Ep 1 (Step 000310): Train loss 5.167, Val loss 5.274\n",
      "Ep 1 (Step 000320): Train loss 5.117, Val loss 5.266\n",
      "Ep 1 (Step 000330): Train loss 5.204, Val loss 5.254\n",
      "Ep 1 (Step 000340): Train loss 5.141, Val loss 5.241\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2406\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.816, Val loss 8.784\n",
      "Ep 1 (Step 000010): Train loss 6.881, Val loss 6.857\n",
      "Ep 1 (Step 000020): Train loss 6.456, Val loss 6.473\n",
      "Ep 1 (Step 000030): Train loss 6.484, Val loss 6.454\n",
      "Ep 1 (Step 000040): Train loss 6.409, Val loss 6.349\n",
      "Ep 1 (Step 000050): Train loss 6.191, Val loss 6.200\n",
      "Ep 1 (Step 000060): Train loss 6.085, Val loss 6.041\n",
      "Ep 1 (Step 000070): Train loss 5.913, Val loss 5.958\n",
      "Ep 1 (Step 000080): Train loss 5.817, Val loss 5.859\n",
      "Ep 1 (Step 000090): Train loss 5.746, Val loss 5.791\n",
      "Ep 1 (Step 000100): Train loss 5.730, Val loss 5.743\n",
      "Ep 1 (Step 000110): Train loss 5.635, Val loss 5.684\n",
      "Ep 1 (Step 000120): Train loss 5.571, Val loss 5.636\n",
      "Ep 1 (Step 000130): Train loss 5.562, Val loss 5.599\n",
      "Ep 1 (Step 000140): Train loss 5.535, Val loss 5.582\n",
      "Ep 1 (Step 000150): Train loss 5.404, Val loss 5.545\n",
      "Ep 1 (Step 000160): Train loss 5.362, Val loss 5.511\n",
      "Ep 1 (Step 000170): Train loss 5.458, Val loss 5.476\n",
      "Ep 1 (Step 000180): Train loss 5.421, Val loss 5.448\n",
      "Ep 1 (Step 000190): Train loss 5.353, Val loss 5.424\n",
      "Ep 1 (Step 000200): Train loss 5.349, Val loss 5.408\n",
      "Ep 1 (Step 000210): Train loss 5.371, Val loss 5.410\n",
      "Ep 1 (Step 000220): Train loss 5.293, Val loss 5.404\n",
      "Ep 1 (Step 000230): Train loss 5.225, Val loss 5.378\n",
      "Ep 1 (Step 000240): Train loss 5.234, Val loss 5.359\n",
      "Ep 1 (Step 000250): Train loss 5.210, Val loss 5.338\n",
      "Ep 1 (Step 000260): Train loss 5.129, Val loss 5.308\n",
      "Ep 1 (Step 000270): Train loss 5.217, Val loss 5.320\n",
      "Ep 1 (Step 000280): Train loss 5.172, Val loss 5.301\n",
      "Ep 1 (Step 000290): Train loss 5.207, Val loss 5.311\n",
      "Ep 1 (Step 000300): Train loss 5.131, Val loss 5.275\n",
      "Ep 1 (Step 000310): Train loss 5.109, Val loss 5.262\n",
      "Ep 1 (Step 000320): Train loss 5.059, Val loss 5.258\n",
      "Ep 1 (Step 000330): Train loss 5.130, Val loss 5.250\n",
      "Ep 1 (Step 000340): Train loss 5.051, Val loss 5.242\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2419\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.089, Val loss 9.080\n",
      "Ep 1 (Step 000010): Train loss 7.494, Val loss 7.491\n",
      "Ep 1 (Step 000020): Train loss 6.803, Val loss 6.821\n",
      "Ep 1 (Step 000030): Train loss 6.480, Val loss 6.502\n",
      "Ep 1 (Step 000040): Train loss 6.447, Val loss 6.421\n",
      "Ep 1 (Step 000050): Train loss 6.320, Val loss 6.389\n",
      "Ep 1 (Step 000060): Train loss 6.344, Val loss 6.344\n",
      "Ep 1 (Step 000070): Train loss 6.239, Val loss 6.241\n",
      "Ep 1 (Step 000080): Train loss 6.059, Val loss 6.136\n",
      "Ep 1 (Step 000090): Train loss 5.952, Val loss 6.040\n",
      "Ep 1 (Step 000100): Train loss 5.991, Val loss 5.962\n",
      "Ep 1 (Step 000110): Train loss 5.848, Val loss 5.907\n",
      "Ep 1 (Step 000120): Train loss 5.869, Val loss 5.865\n",
      "Ep 1 (Step 000130): Train loss 5.719, Val loss 5.827\n",
      "Ep 1 (Step 000140): Train loss 5.736, Val loss 5.768\n",
      "Ep 1 (Step 000150): Train loss 5.766, Val loss 5.741\n",
      "Ep 1 (Step 000160): Train loss 5.599, Val loss 5.730\n",
      "Ep 1 (Step 000170): Train loss 5.625, Val loss 5.686\n",
      "Ep 1 (Step 000180): Train loss 5.550, Val loss 5.660\n",
      "Ep 1 (Step 000190): Train loss 5.558, Val loss 5.621\n",
      "Ep 1 (Step 000200): Train loss 5.467, Val loss 5.613\n",
      "Ep 1 (Step 000210): Train loss 5.550, Val loss 5.580\n",
      "Ep 1 (Step 000220): Train loss 5.489, Val loss 5.563\n",
      "Ep 1 (Step 000230): Train loss 5.421, Val loss 5.535\n",
      "Ep 1 (Step 000240): Train loss 5.477, Val loss 5.522\n",
      "Ep 1 (Step 000250): Train loss 5.402, Val loss 5.501\n",
      "Ep 1 (Step 000260): Train loss 5.386, Val loss 5.486\n",
      "Ep 1 (Step 000270): Train loss 5.397, Val loss 5.461\n",
      "Ep 1 (Step 000280): Train loss 5.284, Val loss 5.460\n",
      "Ep 1 (Step 000290): Train loss 5.298, Val loss 5.436\n",
      "Ep 1 (Step 000300): Train loss 5.355, Val loss 5.431\n",
      "Ep 1 (Step 000310): Train loss 5.259, Val loss 5.419\n",
      "Ep 1 (Step 000320): Train loss 5.297, Val loss 5.398\n",
      "Ep 1 (Step 000330): Train loss 5.290, Val loss 5.389\n",
      "Ep 1 (Step 000340): Train loss 5.219, Val loss 5.385\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3847\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.068, Val loss 9.061\n",
      "Ep 1 (Step 000010): Train loss 7.479, Val loss 7.451\n",
      "Ep 1 (Step 000020): Train loss 6.820, Val loss 6.818\n",
      "Ep 1 (Step 000030): Train loss 6.555, Val loss 6.510\n",
      "Ep 1 (Step 000040): Train loss 6.365, Val loss 6.415\n",
      "Ep 1 (Step 000050): Train loss 6.324, Val loss 6.380\n",
      "Ep 1 (Step 000060): Train loss 6.354, Val loss 6.340\n",
      "Ep 1 (Step 000070): Train loss 6.272, Val loss 6.216\n",
      "Ep 1 (Step 000080): Train loss 6.022, Val loss 6.133\n",
      "Ep 1 (Step 000090): Train loss 6.040, Val loss 6.036\n",
      "Ep 1 (Step 000100): Train loss 6.012, Val loss 5.975\n",
      "Ep 1 (Step 000110): Train loss 5.805, Val loss 5.924\n",
      "Ep 1 (Step 000120): Train loss 5.881, Val loss 5.871\n",
      "Ep 1 (Step 000130): Train loss 5.824, Val loss 5.819\n",
      "Ep 1 (Step 000140): Train loss 5.747, Val loss 5.783\n",
      "Ep 1 (Step 000150): Train loss 5.754, Val loss 5.755\n",
      "Ep 1 (Step 000160): Train loss 5.643, Val loss 5.717\n",
      "Ep 1 (Step 000170): Train loss 5.675, Val loss 5.680\n",
      "Ep 1 (Step 000180): Train loss 5.656, Val loss 5.674\n",
      "Ep 1 (Step 000190): Train loss 5.573, Val loss 5.640\n",
      "Ep 1 (Step 000200): Train loss 5.542, Val loss 5.619\n",
      "Ep 1 (Step 000210): Train loss 5.514, Val loss 5.598\n",
      "Ep 1 (Step 000220): Train loss 5.433, Val loss 5.561\n",
      "Ep 1 (Step 000230): Train loss 5.423, Val loss 5.537\n",
      "Ep 1 (Step 000240): Train loss 5.460, Val loss 5.521\n",
      "Ep 1 (Step 000250): Train loss 5.434, Val loss 5.505\n",
      "Ep 1 (Step 000260): Train loss 5.340, Val loss 5.499\n",
      "Ep 1 (Step 000270): Train loss 5.403, Val loss 5.481\n",
      "Ep 1 (Step 000280): Train loss 5.319, Val loss 5.464\n",
      "Ep 1 (Step 000290): Train loss 5.250, Val loss 5.449\n",
      "Ep 1 (Step 000300): Train loss 5.290, Val loss 5.438\n",
      "Ep 1 (Step 000310): Train loss 5.250, Val loss 5.430\n",
      "Ep 1 (Step 000320): Train loss 5.312, Val loss 5.416\n",
      "Ep 1 (Step 000330): Train loss 5.239, Val loss 5.409\n",
      "Ep 1 (Step 000340): Train loss 5.257, Val loss 5.397\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3972\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.058, Val loss 9.037\n",
      "Ep 1 (Step 000010): Train loss 7.480, Val loss 7.454\n",
      "Ep 1 (Step 000020): Train loss 6.867, Val loss 6.812\n",
      "Ep 1 (Step 000030): Train loss 6.540, Val loss 6.501\n",
      "Ep 1 (Step 000040): Train loss 6.437, Val loss 6.432\n",
      "Ep 1 (Step 000050): Train loss 6.394, Val loss 6.381\n",
      "Ep 1 (Step 000060): Train loss 6.358, Val loss 6.325\n",
      "Ep 1 (Step 000070): Train loss 6.173, Val loss 6.211\n",
      "Ep 1 (Step 000080): Train loss 6.032, Val loss 6.097\n",
      "Ep 1 (Step 000090): Train loss 6.030, Val loss 6.029\n",
      "Ep 1 (Step 000100): Train loss 5.922, Val loss 5.970\n",
      "Ep 1 (Step 000110): Train loss 5.870, Val loss 5.915\n",
      "Ep 1 (Step 000120): Train loss 5.835, Val loss 5.867\n",
      "Ep 1 (Step 000130): Train loss 5.750, Val loss 5.807\n",
      "Ep 1 (Step 000140): Train loss 5.766, Val loss 5.793\n",
      "Ep 1 (Step 000150): Train loss 5.674, Val loss 5.741\n",
      "Ep 1 (Step 000160): Train loss 5.619, Val loss 5.701\n",
      "Ep 1 (Step 000170): Train loss 5.609, Val loss 5.661\n",
      "Ep 1 (Step 000180): Train loss 5.570, Val loss 5.652\n",
      "Ep 1 (Step 000190): Train loss 5.543, Val loss 5.607\n",
      "Ep 1 (Step 000200): Train loss 5.507, Val loss 5.586\n",
      "Ep 1 (Step 000210): Train loss 5.528, Val loss 5.567\n",
      "Ep 1 (Step 000220): Train loss 5.396, Val loss 5.555\n",
      "Ep 1 (Step 000230): Train loss 5.516, Val loss 5.524\n",
      "Ep 1 (Step 000240): Train loss 5.381, Val loss 5.514\n",
      "Ep 1 (Step 000250): Train loss 5.435, Val loss 5.496\n",
      "Ep 1 (Step 000260): Train loss 5.337, Val loss 5.465\n",
      "Ep 1 (Step 000270): Train loss 5.279, Val loss 5.468\n",
      "Ep 1 (Step 000280): Train loss 5.297, Val loss 5.450\n",
      "Ep 1 (Step 000290): Train loss 5.307, Val loss 5.440\n",
      "Ep 1 (Step 000300): Train loss 5.316, Val loss 5.417\n",
      "Ep 1 (Step 000310): Train loss 5.251, Val loss 5.411\n",
      "Ep 1 (Step 000320): Train loss 5.298, Val loss 5.395\n",
      "Ep 1 (Step 000330): Train loss 5.226, Val loss 5.385\n",
      "Ep 1 (Step 000340): Train loss 5.339, Val loss 5.373\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3731\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.052, Val loss 9.054\n",
      "Ep 1 (Step 000010): Train loss 7.466, Val loss 7.450\n",
      "Ep 1 (Step 000020): Train loss 6.757, Val loss 6.792\n",
      "Ep 1 (Step 000030): Train loss 6.504, Val loss 6.500\n",
      "Ep 1 (Step 000040): Train loss 6.417, Val loss 6.425\n",
      "Ep 1 (Step 000050): Train loss 6.379, Val loss 6.386\n",
      "Ep 1 (Step 000060): Train loss 6.284, Val loss 6.329\n",
      "Ep 1 (Step 000070): Train loss 6.240, Val loss 6.256\n",
      "Ep 1 (Step 000080): Train loss 6.122, Val loss 6.144\n",
      "Ep 1 (Step 000090): Train loss 6.084, Val loss 6.063\n",
      "Ep 1 (Step 000100): Train loss 6.031, Val loss 5.991\n",
      "Ep 1 (Step 000110): Train loss 5.908, Val loss 5.952\n",
      "Ep 1 (Step 000120): Train loss 5.774, Val loss 5.889\n",
      "Ep 1 (Step 000130): Train loss 5.763, Val loss 5.841\n",
      "Ep 1 (Step 000140): Train loss 5.686, Val loss 5.795\n",
      "Ep 1 (Step 000150): Train loss 5.676, Val loss 5.769\n",
      "Ep 1 (Step 000160): Train loss 5.634, Val loss 5.728\n",
      "Ep 1 (Step 000170): Train loss 5.654, Val loss 5.691\n",
      "Ep 1 (Step 000180): Train loss 5.574, Val loss 5.653\n",
      "Ep 1 (Step 000190): Train loss 5.674, Val loss 5.634\n",
      "Ep 1 (Step 000200): Train loss 5.537, Val loss 5.590\n",
      "Ep 1 (Step 000210): Train loss 5.547, Val loss 5.579\n",
      "Ep 1 (Step 000220): Train loss 5.462, Val loss 5.560\n",
      "Ep 1 (Step 000230): Train loss 5.534, Val loss 5.552\n",
      "Ep 1 (Step 000240): Train loss 5.433, Val loss 5.523\n",
      "Ep 1 (Step 000250): Train loss 5.433, Val loss 5.508\n",
      "Ep 1 (Step 000260): Train loss 5.431, Val loss 5.487\n",
      "Ep 1 (Step 000270): Train loss 5.320, Val loss 5.476\n",
      "Ep 1 (Step 000280): Train loss 5.376, Val loss 5.467\n",
      "Ep 1 (Step 000290): Train loss 5.313, Val loss 5.452\n",
      "Ep 1 (Step 000300): Train loss 5.370, Val loss 5.423\n",
      "Ep 1 (Step 000310): Train loss 5.216, Val loss 5.400\n",
      "Ep 1 (Step 000320): Train loss 5.279, Val loss 5.390\n",
      "Ep 1 (Step 000330): Train loss 5.276, Val loss 5.385\n",
      "Ep 1 (Step 000340): Train loss 5.311, Val loss 5.361\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3614\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.047, Val loss 9.038\n",
      "Ep 1 (Step 000010): Train loss 7.487, Val loss 7.417\n",
      "Ep 1 (Step 000020): Train loss 6.862, Val loss 6.778\n",
      "Ep 1 (Step 000030): Train loss 6.519, Val loss 6.489\n",
      "Ep 1 (Step 000040): Train loss 6.387, Val loss 6.404\n",
      "Ep 1 (Step 000050): Train loss 6.382, Val loss 6.381\n",
      "Ep 1 (Step 000060): Train loss 6.343, Val loss 6.332\n",
      "Ep 1 (Step 000070): Train loss 6.293, Val loss 6.213\n",
      "Ep 1 (Step 000080): Train loss 6.055, Val loss 6.114\n",
      "Ep 1 (Step 000090): Train loss 6.017, Val loss 6.060\n",
      "Ep 1 (Step 000100): Train loss 5.961, Val loss 5.980\n",
      "Ep 1 (Step 000110): Train loss 6.022, Val loss 5.917\n",
      "Ep 1 (Step 000120): Train loss 5.843, Val loss 5.871\n",
      "Ep 1 (Step 000130): Train loss 5.801, Val loss 5.836\n",
      "Ep 1 (Step 000140): Train loss 5.823, Val loss 5.788\n",
      "Ep 1 (Step 000150): Train loss 5.739, Val loss 5.742\n",
      "Ep 1 (Step 000160): Train loss 5.658, Val loss 5.705\n",
      "Ep 1 (Step 000170): Train loss 5.636, Val loss 5.687\n",
      "Ep 1 (Step 000180): Train loss 5.577, Val loss 5.648\n",
      "Ep 1 (Step 000190): Train loss 5.496, Val loss 5.621\n",
      "Ep 1 (Step 000200): Train loss 5.463, Val loss 5.593\n",
      "Ep 1 (Step 000210): Train loss 5.453, Val loss 5.575\n",
      "Ep 1 (Step 000220): Train loss 5.500, Val loss 5.562\n",
      "Ep 1 (Step 000230): Train loss 5.432, Val loss 5.531\n",
      "Ep 1 (Step 000240): Train loss 5.509, Val loss 5.551\n",
      "Ep 1 (Step 000250): Train loss 5.376, Val loss 5.500\n",
      "Ep 1 (Step 000260): Train loss 5.400, Val loss 5.489\n",
      "Ep 1 (Step 000270): Train loss 5.374, Val loss 5.457\n",
      "Ep 1 (Step 000280): Train loss 5.361, Val loss 5.438\n",
      "Ep 1 (Step 000290): Train loss 5.411, Val loss 5.428\n",
      "Ep 1 (Step 000300): Train loss 5.300, Val loss 5.421\n",
      "Ep 1 (Step 000310): Train loss 5.348, Val loss 5.407\n",
      "Ep 1 (Step 000320): Train loss 5.430, Val loss 5.406\n",
      "Ep 1 (Step 000330): Train loss 5.287, Val loss 5.389\n",
      "Ep 1 (Step 000340): Train loss 5.254, Val loss 5.366\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3658\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.086, Val loss 9.089\n",
      "Ep 1 (Step 000010): Train loss 7.539, Val loss 7.500\n",
      "Ep 1 (Step 000020): Train loss 6.843, Val loss 6.839\n",
      "Ep 1 (Step 000030): Train loss 6.497, Val loss 6.502\n",
      "Ep 1 (Step 000040): Train loss 6.368, Val loss 6.395\n",
      "Ep 1 (Step 000050): Train loss 6.323, Val loss 6.360\n",
      "Ep 1 (Step 000060): Train loss 6.257, Val loss 6.306\n",
      "Ep 1 (Step 000070): Train loss 6.153, Val loss 6.224\n",
      "Ep 1 (Step 000080): Train loss 6.063, Val loss 6.117\n",
      "Ep 1 (Step 000090): Train loss 6.090, Val loss 6.028\n",
      "Ep 1 (Step 000100): Train loss 6.005, Val loss 5.974\n",
      "Ep 1 (Step 000110): Train loss 5.847, Val loss 5.912\n",
      "Ep 1 (Step 000120): Train loss 5.884, Val loss 5.858\n",
      "Ep 1 (Step 000130): Train loss 5.814, Val loss 5.806\n",
      "Ep 1 (Step 000140): Train loss 5.766, Val loss 5.776\n",
      "Ep 1 (Step 000150): Train loss 5.683, Val loss 5.733\n",
      "Ep 1 (Step 000160): Train loss 5.619, Val loss 5.703\n",
      "Ep 1 (Step 000170): Train loss 5.688, Val loss 5.669\n",
      "Ep 1 (Step 000180): Train loss 5.591, Val loss 5.649\n",
      "Ep 1 (Step 000190): Train loss 5.428, Val loss 5.600\n",
      "Ep 1 (Step 000200): Train loss 5.509, Val loss 5.584\n",
      "Ep 1 (Step 000210): Train loss 5.485, Val loss 5.573\n",
      "Ep 1 (Step 000220): Train loss 5.409, Val loss 5.538\n",
      "Ep 1 (Step 000230): Train loss 5.490, Val loss 5.519\n",
      "Ep 1 (Step 000240): Train loss 5.377, Val loss 5.503\n",
      "Ep 1 (Step 000250): Train loss 5.427, Val loss 5.491\n",
      "Ep 1 (Step 000260): Train loss 5.383, Val loss 5.474\n",
      "Ep 1 (Step 000270): Train loss 5.378, Val loss 5.454\n",
      "Ep 1 (Step 000280): Train loss 5.337, Val loss 5.435\n",
      "Ep 1 (Step 000290): Train loss 5.329, Val loss 5.402\n",
      "Ep 1 (Step 000300): Train loss 5.275, Val loss 5.400\n",
      "Ep 1 (Step 000310): Train loss 5.290, Val loss 5.396\n",
      "Ep 1 (Step 000320): Train loss 5.238, Val loss 5.396\n",
      "Ep 1 (Step 000330): Train loss 5.212, Val loss 5.376\n",
      "Ep 1 (Step 000340): Train loss 5.201, Val loss 5.363\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3628\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.849, Val loss 8.829\n",
      "Ep 1 (Step 000010): Train loss 6.901, Val loss 6.912\n",
      "Ep 1 (Step 000020): Train loss 6.494, Val loss 6.516\n",
      "Ep 1 (Step 000030): Train loss 6.429, Val loss 6.466\n",
      "Ep 1 (Step 000040): Train loss 6.333, Val loss 6.381\n",
      "Ep 1 (Step 000050): Train loss 6.247, Val loss 6.267\n",
      "Ep 1 (Step 000060): Train loss 6.165, Val loss 6.135\n",
      "Ep 1 (Step 000070): Train loss 5.987, Val loss 6.026\n",
      "Ep 1 (Step 000080): Train loss 5.968, Val loss 5.950\n",
      "Ep 1 (Step 000090): Train loss 5.826, Val loss 5.881\n",
      "Ep 1 (Step 000100): Train loss 5.754, Val loss 5.801\n",
      "Ep 1 (Step 000110): Train loss 5.700, Val loss 5.753\n",
      "Ep 1 (Step 000120): Train loss 5.610, Val loss 5.698\n",
      "Ep 1 (Step 000130): Train loss 5.566, Val loss 5.653\n",
      "Ep 1 (Step 000140): Train loss 5.602, Val loss 5.620\n",
      "Ep 1 (Step 000150): Train loss 5.505, Val loss 5.577\n",
      "Ep 1 (Step 000160): Train loss 5.481, Val loss 5.530\n",
      "Ep 1 (Step 000170): Train loss 5.463, Val loss 5.534\n",
      "Ep 1 (Step 000180): Train loss 5.400, Val loss 5.518\n",
      "Ep 1 (Step 000190): Train loss 5.335, Val loss 5.492\n",
      "Ep 1 (Step 000200): Train loss 5.332, Val loss 5.485\n",
      "Ep 1 (Step 000210): Train loss 5.385, Val loss 5.458\n",
      "Ep 1 (Step 000220): Train loss 5.322, Val loss 5.420\n",
      "Ep 1 (Step 000230): Train loss 5.315, Val loss 5.424\n",
      "Ep 1 (Step 000240): Train loss 5.326, Val loss 5.401\n",
      "Ep 1 (Step 000250): Train loss 5.312, Val loss 5.386\n",
      "Ep 1 (Step 000260): Train loss 5.329, Val loss 5.376\n",
      "Ep 1 (Step 000270): Train loss 5.228, Val loss 5.356\n",
      "Ep 1 (Step 000280): Train loss 5.195, Val loss 5.349\n",
      "Ep 1 (Step 000290): Train loss 5.104, Val loss 5.320\n",
      "Ep 1 (Step 000300): Train loss 5.149, Val loss 5.310\n",
      "Ep 1 (Step 000310): Train loss 5.156, Val loss 5.303\n",
      "Ep 1 (Step 000320): Train loss 5.128, Val loss 5.286\n",
      "Ep 1 (Step 000330): Train loss 5.085, Val loss 5.271\n",
      "Ep 1 (Step 000340): Train loss 5.159, Val loss 5.272\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2718\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.805, Val loss 8.800\n",
      "Ep 1 (Step 000010): Train loss 6.879, Val loss 6.854\n",
      "Ep 1 (Step 000020): Train loss 6.499, Val loss 6.489\n",
      "Ep 1 (Step 000030): Train loss 6.517, Val loss 6.481\n",
      "Ep 1 (Step 000040): Train loss 6.352, Val loss 6.385\n",
      "Ep 1 (Step 000050): Train loss 6.180, Val loss 6.226\n",
      "Ep 1 (Step 000060): Train loss 6.062, Val loss 6.089\n",
      "Ep 1 (Step 000070): Train loss 5.970, Val loss 5.992\n",
      "Ep 1 (Step 000080): Train loss 5.907, Val loss 5.932\n",
      "Ep 1 (Step 000090): Train loss 5.782, Val loss 5.862\n",
      "Ep 1 (Step 000100): Train loss 5.625, Val loss 5.755\n",
      "Ep 1 (Step 000110): Train loss 5.626, Val loss 5.719\n",
      "Ep 1 (Step 000120): Train loss 5.616, Val loss 5.670\n",
      "Ep 1 (Step 000130): Train loss 5.568, Val loss 5.653\n",
      "Ep 1 (Step 000140): Train loss 5.529, Val loss 5.615\n",
      "Ep 1 (Step 000150): Train loss 5.514, Val loss 5.565\n",
      "Ep 1 (Step 000160): Train loss 5.476, Val loss 5.551\n",
      "Ep 1 (Step 000170): Train loss 5.467, Val loss 5.497\n",
      "Ep 1 (Step 000180): Train loss 5.438, Val loss 5.496\n",
      "Ep 1 (Step 000190): Train loss 5.381, Val loss 5.464\n",
      "Ep 1 (Step 000200): Train loss 5.290, Val loss 5.460\n",
      "Ep 1 (Step 000210): Train loss 5.302, Val loss 5.440\n",
      "Ep 1 (Step 000220): Train loss 5.269, Val loss 5.403\n",
      "Ep 1 (Step 000230): Train loss 5.350, Val loss 5.393\n",
      "Ep 1 (Step 000240): Train loss 5.367, Val loss 5.379\n",
      "Ep 1 (Step 000250): Train loss 5.262, Val loss 5.369\n",
      "Ep 1 (Step 000260): Train loss 5.251, Val loss 5.336\n",
      "Ep 1 (Step 000270): Train loss 5.245, Val loss 5.336\n",
      "Ep 1 (Step 000280): Train loss 5.161, Val loss 5.321\n",
      "Ep 1 (Step 000290): Train loss 5.183, Val loss 5.309\n",
      "Ep 1 (Step 000300): Train loss 5.212, Val loss 5.292\n",
      "Ep 1 (Step 000310): Train loss 5.128, Val loss 5.275\n",
      "Ep 1 (Step 000320): Train loss 5.168, Val loss 5.288\n",
      "Ep 1 (Step 000330): Train loss 5.248, Val loss 5.281\n",
      "Ep 1 (Step 000340): Train loss 5.201, Val loss 5.293\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2929\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.816, Val loss 8.802\n",
      "Ep 1 (Step 000010): Train loss 6.836, Val loss 6.852\n",
      "Ep 1 (Step 000020): Train loss 6.465, Val loss 6.470\n",
      "Ep 1 (Step 000030): Train loss 6.439, Val loss 6.457\n",
      "Ep 1 (Step 000040): Train loss 6.346, Val loss 6.396\n",
      "Ep 1 (Step 000050): Train loss 6.299, Val loss 6.259\n",
      "Ep 1 (Step 000060): Train loss 6.182, Val loss 6.108\n",
      "Ep 1 (Step 000070): Train loss 6.004, Val loss 6.023\n",
      "Ep 1 (Step 000080): Train loss 5.886, Val loss 5.917\n",
      "Ep 1 (Step 000090): Train loss 5.826, Val loss 5.808\n",
      "Ep 1 (Step 000100): Train loss 5.763, Val loss 5.760\n",
      "Ep 1 (Step 000110): Train loss 5.703, Val loss 5.720\n",
      "Ep 1 (Step 000120): Train loss 5.577, Val loss 5.670\n",
      "Ep 1 (Step 000130): Train loss 5.521, Val loss 5.607\n",
      "Ep 1 (Step 000140): Train loss 5.496, Val loss 5.580\n",
      "Ep 1 (Step 000150): Train loss 5.518, Val loss 5.558\n",
      "Ep 1 (Step 000160): Train loss 5.504, Val loss 5.512\n",
      "Ep 1 (Step 000170): Train loss 5.394, Val loss 5.515\n",
      "Ep 1 (Step 000180): Train loss 5.444, Val loss 5.488\n",
      "Ep 1 (Step 000190): Train loss 5.322, Val loss 5.472\n",
      "Ep 1 (Step 000200): Train loss 5.396, Val loss 5.453\n",
      "Ep 1 (Step 000210): Train loss 5.350, Val loss 5.419\n",
      "Ep 1 (Step 000220): Train loss 5.340, Val loss 5.396\n",
      "Ep 1 (Step 000230): Train loss 5.244, Val loss 5.388\n",
      "Ep 1 (Step 000240): Train loss 5.277, Val loss 5.391\n",
      "Ep 1 (Step 000250): Train loss 5.184, Val loss 5.365\n",
      "Ep 1 (Step 000260): Train loss 5.372, Val loss 5.370\n",
      "Ep 1 (Step 000270): Train loss 5.158, Val loss 5.356\n",
      "Ep 1 (Step 000280): Train loss 5.207, Val loss 5.325\n",
      "Ep 1 (Step 000290): Train loss 5.185, Val loss 5.319\n",
      "Ep 1 (Step 000300): Train loss 5.240, Val loss 5.318\n",
      "Ep 1 (Step 000310): Train loss 5.323, Val loss 5.303\n",
      "Ep 1 (Step 000320): Train loss 5.182, Val loss 5.282\n",
      "Ep 1 (Step 000330): Train loss 5.180, Val loss 5.291\n",
      "Ep 1 (Step 000340): Train loss 5.228, Val loss 5.276\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2756\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.815, Val loss 8.772\n",
      "Ep 1 (Step 000010): Train loss 6.874, Val loss 6.834\n",
      "Ep 1 (Step 000020): Train loss 6.497, Val loss 6.476\n",
      "Ep 1 (Step 000030): Train loss 6.404, Val loss 6.476\n",
      "Ep 1 (Step 000040): Train loss 6.381, Val loss 6.343\n",
      "Ep 1 (Step 000050): Train loss 6.174, Val loss 6.197\n",
      "Ep 1 (Step 000060): Train loss 5.995, Val loss 6.073\n",
      "Ep 1 (Step 000070): Train loss 5.923, Val loss 5.965\n",
      "Ep 1 (Step 000080): Train loss 5.839, Val loss 5.863\n",
      "Ep 1 (Step 000090): Train loss 5.784, Val loss 5.799\n",
      "Ep 1 (Step 000100): Train loss 5.710, Val loss 5.747\n",
      "Ep 1 (Step 000110): Train loss 5.573, Val loss 5.687\n",
      "Ep 1 (Step 000120): Train loss 5.549, Val loss 5.645\n",
      "Ep 1 (Step 000130): Train loss 5.534, Val loss 5.600\n",
      "Ep 1 (Step 000140): Train loss 5.487, Val loss 5.574\n",
      "Ep 1 (Step 000150): Train loss 5.534, Val loss 5.564\n",
      "Ep 1 (Step 000160): Train loss 5.421, Val loss 5.533\n",
      "Ep 1 (Step 000170): Train loss 5.405, Val loss 5.487\n",
      "Ep 1 (Step 000180): Train loss 5.387, Val loss 5.454\n",
      "Ep 1 (Step 000190): Train loss 5.369, Val loss 5.432\n",
      "Ep 1 (Step 000200): Train loss 5.323, Val loss 5.422\n",
      "Ep 1 (Step 000210): Train loss 5.344, Val loss 5.404\n",
      "Ep 1 (Step 000220): Train loss 5.331, Val loss 5.390\n",
      "Ep 1 (Step 000230): Train loss 5.201, Val loss 5.362\n",
      "Ep 1 (Step 000240): Train loss 5.213, Val loss 5.328\n",
      "Ep 1 (Step 000250): Train loss 5.311, Val loss 5.315\n",
      "Ep 1 (Step 000260): Train loss 5.185, Val loss 5.320\n",
      "Ep 1 (Step 000270): Train loss 5.105, Val loss 5.306\n",
      "Ep 1 (Step 000280): Train loss 5.231, Val loss 5.307\n",
      "Ep 1 (Step 000290): Train loss 5.227, Val loss 5.299\n",
      "Ep 1 (Step 000300): Train loss 5.182, Val loss 5.273\n",
      "Ep 1 (Step 000310): Train loss 5.124, Val loss 5.261\n",
      "Ep 1 (Step 000320): Train loss 5.083, Val loss 5.253\n",
      "Ep 1 (Step 000330): Train loss 5.055, Val loss 5.249\n",
      "Ep 1 (Step 000340): Train loss 5.098, Val loss 5.233\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2332\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.843, Val loss 8.811\n",
      "Ep 1 (Step 000010): Train loss 6.855, Val loss 6.871\n",
      "Ep 1 (Step 000020): Train loss 6.481, Val loss 6.490\n",
      "Ep 1 (Step 000030): Train loss 6.446, Val loss 6.465\n",
      "Ep 1 (Step 000040): Train loss 6.404, Val loss 6.387\n",
      "Ep 1 (Step 000050): Train loss 6.222, Val loss 6.232\n",
      "Ep 1 (Step 000060): Train loss 6.047, Val loss 6.080\n",
      "Ep 1 (Step 000070): Train loss 5.976, Val loss 5.985\n",
      "Ep 1 (Step 000080): Train loss 5.881, Val loss 5.921\n",
      "Ep 1 (Step 000090): Train loss 5.781, Val loss 5.849\n",
      "Ep 1 (Step 000100): Train loss 5.690, Val loss 5.769\n",
      "Ep 1 (Step 000110): Train loss 5.579, Val loss 5.707\n",
      "Ep 1 (Step 000120): Train loss 5.605, Val loss 5.670\n",
      "Ep 1 (Step 000130): Train loss 5.562, Val loss 5.635\n",
      "Ep 1 (Step 000140): Train loss 5.556, Val loss 5.600\n",
      "Ep 1 (Step 000150): Train loss 5.494, Val loss 5.551\n",
      "Ep 1 (Step 000160): Train loss 5.402, Val loss 5.510\n",
      "Ep 1 (Step 000170): Train loss 5.375, Val loss 5.482\n",
      "Ep 1 (Step 000180): Train loss 5.458, Val loss 5.467\n",
      "Ep 1 (Step 000190): Train loss 5.330, Val loss 5.445\n",
      "Ep 1 (Step 000200): Train loss 5.341, Val loss 5.423\n",
      "Ep 1 (Step 000210): Train loss 5.311, Val loss 5.404\n",
      "Ep 1 (Step 000220): Train loss 5.292, Val loss 5.394\n",
      "Ep 1 (Step 000230): Train loss 5.256, Val loss 5.367\n",
      "Ep 1 (Step 000240): Train loss 5.302, Val loss 5.377\n",
      "Ep 1 (Step 000250): Train loss 5.201, Val loss 5.357\n",
      "Ep 1 (Step 000260): Train loss 5.156, Val loss 5.347\n",
      "Ep 1 (Step 000270): Train loss 5.232, Val loss 5.315\n",
      "Ep 1 (Step 000280): Train loss 5.209, Val loss 5.320\n",
      "Ep 1 (Step 000290): Train loss 5.155, Val loss 5.302\n",
      "Ep 1 (Step 000300): Train loss 5.198, Val loss 5.286\n",
      "Ep 1 (Step 000310): Train loss 5.046, Val loss 5.290\n",
      "Ep 1 (Step 000320): Train loss 5.134, Val loss 5.265\n",
      "Ep 1 (Step 000330): Train loss 5.048, Val loss 5.270\n",
      "Ep 1 (Step 000340): Train loss 5.050, Val loss 5.252\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2518\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.825, Val loss 8.817\n",
      "Ep 1 (Step 000010): Train loss 6.925, Val loss 6.881\n",
      "Ep 1 (Step 000020): Train loss 6.512, Val loss 6.483\n",
      "Ep 1 (Step 000030): Train loss 6.516, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.393, Val loss 6.377\n",
      "Ep 1 (Step 000050): Train loss 6.232, Val loss 6.269\n",
      "Ep 1 (Step 000060): Train loss 6.150, Val loss 6.106\n",
      "Ep 1 (Step 000070): Train loss 6.037, Val loss 5.997\n",
      "Ep 1 (Step 000080): Train loss 5.867, Val loss 5.905\n",
      "Ep 1 (Step 000090): Train loss 5.843, Val loss 5.838\n",
      "Ep 1 (Step 000100): Train loss 5.649, Val loss 5.763\n",
      "Ep 1 (Step 000110): Train loss 5.697, Val loss 5.736\n",
      "Ep 1 (Step 000120): Train loss 5.554, Val loss 5.666\n",
      "Ep 1 (Step 000130): Train loss 5.599, Val loss 5.626\n",
      "Ep 1 (Step 000140): Train loss 5.508, Val loss 5.598\n",
      "Ep 1 (Step 000150): Train loss 5.516, Val loss 5.543\n",
      "Ep 1 (Step 000160): Train loss 5.462, Val loss 5.531\n",
      "Ep 1 (Step 000170): Train loss 5.378, Val loss 5.510\n",
      "Ep 1 (Step 000180): Train loss 5.351, Val loss 5.484\n",
      "Ep 1 (Step 000190): Train loss 5.363, Val loss 5.476\n",
      "Ep 1 (Step 000200): Train loss 5.323, Val loss 5.440\n",
      "Ep 1 (Step 000210): Train loss 5.263, Val loss 5.421\n",
      "Ep 1 (Step 000220): Train loss 5.270, Val loss 5.397\n",
      "Ep 1 (Step 000230): Train loss 5.238, Val loss 5.379\n",
      "Ep 1 (Step 000240): Train loss 5.271, Val loss 5.362\n",
      "Ep 1 (Step 000250): Train loss 5.288, Val loss 5.355\n",
      "Ep 1 (Step 000260): Train loss 5.265, Val loss 5.341\n",
      "Ep 1 (Step 000270): Train loss 5.168, Val loss 5.346\n",
      "Ep 1 (Step 000280): Train loss 5.155, Val loss 5.305\n",
      "Ep 1 (Step 000290): Train loss 5.196, Val loss 5.319\n",
      "Ep 1 (Step 000300): Train loss 5.210, Val loss 5.307\n",
      "Ep 1 (Step 000310): Train loss 5.116, Val loss 5.298\n",
      "Ep 1 (Step 000320): Train loss 5.075, Val loss 5.280\n",
      "Ep 1 (Step 000330): Train loss 5.130, Val loss 5.257\n",
      "Ep 1 (Step 000340): Train loss 5.002, Val loss 5.248\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2479\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.058, Val loss 9.065\n",
      "Ep 1 (Step 000010): Train loss 7.494, Val loss 7.464\n",
      "Ep 1 (Step 000020): Train loss 6.827, Val loss 6.799\n",
      "Ep 1 (Step 000030): Train loss 6.450, Val loss 6.468\n",
      "Ep 1 (Step 000040): Train loss 6.427, Val loss 6.368\n",
      "Ep 1 (Step 000050): Train loss 6.282, Val loss 6.325\n",
      "Ep 1 (Step 000060): Train loss 6.317, Val loss 6.238\n",
      "Ep 1 (Step 000070): Train loss 6.157, Val loss 6.109\n",
      "Ep 1 (Step 000080): Train loss 6.015, Val loss 6.026\n",
      "Ep 1 (Step 000090): Train loss 5.914, Val loss 5.970\n",
      "Ep 1 (Step 000100): Train loss 5.921, Val loss 5.910\n",
      "Ep 1 (Step 000110): Train loss 5.819, Val loss 5.843\n",
      "Ep 1 (Step 000120): Train loss 5.749, Val loss 5.800\n",
      "Ep 1 (Step 000130): Train loss 5.766, Val loss 5.761\n",
      "Ep 1 (Step 000140): Train loss 5.644, Val loss 5.718\n",
      "Ep 1 (Step 000150): Train loss 5.601, Val loss 5.682\n",
      "Ep 1 (Step 000160): Train loss 5.628, Val loss 5.657\n",
      "Ep 1 (Step 000170): Train loss 5.529, Val loss 5.635\n",
      "Ep 1 (Step 000180): Train loss 5.538, Val loss 5.616\n",
      "Ep 1 (Step 000190): Train loss 5.541, Val loss 5.581\n",
      "Ep 1 (Step 000200): Train loss 5.461, Val loss 5.544\n",
      "Ep 1 (Step 000210): Train loss 5.422, Val loss 5.514\n",
      "Ep 1 (Step 000220): Train loss 5.426, Val loss 5.503\n",
      "Ep 1 (Step 000230): Train loss 5.397, Val loss 5.477\n",
      "Ep 1 (Step 000240): Train loss 5.442, Val loss 5.468\n",
      "Ep 1 (Step 000250): Train loss 5.375, Val loss 5.444\n",
      "Ep 1 (Step 000260): Train loss 5.340, Val loss 5.424\n",
      "Ep 1 (Step 000270): Train loss 5.333, Val loss 5.409\n",
      "Ep 1 (Step 000280): Train loss 5.361, Val loss 5.415\n",
      "Ep 1 (Step 000290): Train loss 5.228, Val loss 5.389\n",
      "Ep 1 (Step 000300): Train loss 5.198, Val loss 5.378\n",
      "Ep 1 (Step 000310): Train loss 5.278, Val loss 5.362\n",
      "Ep 1 (Step 000320): Train loss 5.197, Val loss 5.346\n",
      "Ep 1 (Step 000330): Train loss 5.232, Val loss 5.326\n",
      "Ep 1 (Step 000340): Train loss 5.236, Val loss 5.312\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3120\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.043, Val loss 9.012\n",
      "Ep 1 (Step 000010): Train loss 7.469, Val loss 7.422\n",
      "Ep 1 (Step 000020): Train loss 6.767, Val loss 6.783\n",
      "Ep 1 (Step 000030): Train loss 6.496, Val loss 6.467\n",
      "Ep 1 (Step 000040): Train loss 6.414, Val loss 6.378\n",
      "Ep 1 (Step 000050): Train loss 6.261, Val loss 6.330\n",
      "Ep 1 (Step 000060): Train loss 6.215, Val loss 6.226\n",
      "Ep 1 (Step 000070): Train loss 6.084, Val loss 6.110\n",
      "Ep 1 (Step 000080): Train loss 6.011, Val loss 6.035\n",
      "Ep 1 (Step 000090): Train loss 5.907, Val loss 5.956\n",
      "Ep 1 (Step 000100): Train loss 5.849, Val loss 5.897\n",
      "Ep 1 (Step 000110): Train loss 5.812, Val loss 5.848\n",
      "Ep 1 (Step 000120): Train loss 5.754, Val loss 5.816\n",
      "Ep 1 (Step 000130): Train loss 5.708, Val loss 5.755\n",
      "Ep 1 (Step 000140): Train loss 5.608, Val loss 5.714\n",
      "Ep 1 (Step 000150): Train loss 5.617, Val loss 5.678\n",
      "Ep 1 (Step 000160): Train loss 5.572, Val loss 5.671\n",
      "Ep 1 (Step 000170): Train loss 5.590, Val loss 5.640\n",
      "Ep 1 (Step 000180): Train loss 5.493, Val loss 5.608\n",
      "Ep 1 (Step 000190): Train loss 5.432, Val loss 5.564\n",
      "Ep 1 (Step 000200): Train loss 5.460, Val loss 5.541\n",
      "Ep 1 (Step 000210): Train loss 5.383, Val loss 5.524\n",
      "Ep 1 (Step 000220): Train loss 5.470, Val loss 5.497\n",
      "Ep 1 (Step 000230): Train loss 5.415, Val loss 5.494\n",
      "Ep 1 (Step 000240): Train loss 5.357, Val loss 5.469\n",
      "Ep 1 (Step 000250): Train loss 5.318, Val loss 5.443\n",
      "Ep 1 (Step 000260): Train loss 5.333, Val loss 5.433\n",
      "Ep 1 (Step 000270): Train loss 5.271, Val loss 5.422\n",
      "Ep 1 (Step 000280): Train loss 5.329, Val loss 5.407\n",
      "Ep 1 (Step 000290): Train loss 5.302, Val loss 5.389\n",
      "Ep 1 (Step 000300): Train loss 5.206, Val loss 5.377\n",
      "Ep 1 (Step 000310): Train loss 5.170, Val loss 5.363\n",
      "Ep 1 (Step 000320): Train loss 5.257, Val loss 5.353\n",
      "Ep 1 (Step 000330): Train loss 5.226, Val loss 5.352\n",
      "Ep 1 (Step 000340): Train loss 5.218, Val loss 5.321\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3209\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.065, Val loss 9.044\n",
      "Ep 1 (Step 000010): Train loss 7.436, Val loss 7.415\n",
      "Ep 1 (Step 000020): Train loss 6.803, Val loss 6.782\n",
      "Ep 1 (Step 000030): Train loss 6.497, Val loss 6.473\n",
      "Ep 1 (Step 000040): Train loss 6.389, Val loss 6.381\n",
      "Ep 1 (Step 000050): Train loss 6.285, Val loss 6.327\n",
      "Ep 1 (Step 000060): Train loss 6.231, Val loss 6.242\n",
      "Ep 1 (Step 000070): Train loss 6.116, Val loss 6.132\n",
      "Ep 1 (Step 000080): Train loss 6.034, Val loss 6.046\n",
      "Ep 1 (Step 000090): Train loss 5.887, Val loss 5.961\n",
      "Ep 1 (Step 000100): Train loss 5.917, Val loss 5.917\n",
      "Ep 1 (Step 000110): Train loss 5.813, Val loss 5.852\n",
      "Ep 1 (Step 000120): Train loss 5.785, Val loss 5.794\n",
      "Ep 1 (Step 000130): Train loss 5.670, Val loss 5.749\n",
      "Ep 1 (Step 000140): Train loss 5.690, Val loss 5.708\n",
      "Ep 1 (Step 000150): Train loss 5.660, Val loss 5.693\n",
      "Ep 1 (Step 000160): Train loss 5.572, Val loss 5.650\n",
      "Ep 1 (Step 000170): Train loss 5.513, Val loss 5.622\n",
      "Ep 1 (Step 000180): Train loss 5.461, Val loss 5.587\n",
      "Ep 1 (Step 000190): Train loss 5.478, Val loss 5.564\n",
      "Ep 1 (Step 000200): Train loss 5.492, Val loss 5.542\n",
      "Ep 1 (Step 000210): Train loss 5.420, Val loss 5.511\n",
      "Ep 1 (Step 000220): Train loss 5.375, Val loss 5.509\n",
      "Ep 1 (Step 000230): Train loss 5.347, Val loss 5.487\n",
      "Ep 1 (Step 000240): Train loss 5.321, Val loss 5.464\n",
      "Ep 1 (Step 000250): Train loss 5.285, Val loss 5.442\n",
      "Ep 1 (Step 000260): Train loss 5.364, Val loss 5.417\n",
      "Ep 1 (Step 000270): Train loss 5.364, Val loss 5.420\n",
      "Ep 1 (Step 000280): Train loss 5.346, Val loss 5.399\n",
      "Ep 1 (Step 000290): Train loss 5.306, Val loss 5.387\n",
      "Ep 1 (Step 000300): Train loss 5.198, Val loss 5.373\n",
      "Ep 1 (Step 000310): Train loss 5.282, Val loss 5.367\n",
      "Ep 1 (Step 000320): Train loss 5.178, Val loss 5.351\n",
      "Ep 1 (Step 000330): Train loss 5.208, Val loss 5.346\n",
      "Ep 1 (Step 000340): Train loss 5.194, Val loss 5.323\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3227\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.051, Val loss 9.038\n",
      "Ep 1 (Step 000010): Train loss 7.435, Val loss 7.438\n",
      "Ep 1 (Step 000020): Train loss 6.843, Val loss 6.796\n",
      "Ep 1 (Step 000030): Train loss 6.502, Val loss 6.479\n",
      "Ep 1 (Step 000040): Train loss 6.350, Val loss 6.388\n",
      "Ep 1 (Step 000050): Train loss 6.374, Val loss 6.350\n",
      "Ep 1 (Step 000060): Train loss 6.242, Val loss 6.228\n",
      "Ep 1 (Step 000070): Train loss 6.148, Val loss 6.116\n",
      "Ep 1 (Step 000080): Train loss 6.034, Val loss 6.029\n",
      "Ep 1 (Step 000090): Train loss 5.982, Val loss 5.953\n",
      "Ep 1 (Step 000100): Train loss 5.880, Val loss 5.880\n",
      "Ep 1 (Step 000110): Train loss 5.835, Val loss 5.820\n",
      "Ep 1 (Step 000120): Train loss 5.770, Val loss 5.773\n",
      "Ep 1 (Step 000130): Train loss 5.713, Val loss 5.747\n",
      "Ep 1 (Step 000140): Train loss 5.705, Val loss 5.714\n",
      "Ep 1 (Step 000150): Train loss 5.690, Val loss 5.678\n",
      "Ep 1 (Step 000160): Train loss 5.568, Val loss 5.640\n",
      "Ep 1 (Step 000170): Train loss 5.487, Val loss 5.603\n",
      "Ep 1 (Step 000180): Train loss 5.465, Val loss 5.571\n",
      "Ep 1 (Step 000190): Train loss 5.452, Val loss 5.547\n",
      "Ep 1 (Step 000200): Train loss 5.438, Val loss 5.521\n",
      "Ep 1 (Step 000210): Train loss 5.407, Val loss 5.496\n",
      "Ep 1 (Step 000220): Train loss 5.433, Val loss 5.475\n",
      "Ep 1 (Step 000230): Train loss 5.343, Val loss 5.451\n",
      "Ep 1 (Step 000240): Train loss 5.305, Val loss 5.428\n",
      "Ep 1 (Step 000250): Train loss 5.255, Val loss 5.405\n",
      "Ep 1 (Step 000260): Train loss 5.323, Val loss 5.405\n",
      "Ep 1 (Step 000270): Train loss 5.226, Val loss 5.399\n",
      "Ep 1 (Step 000280): Train loss 5.325, Val loss 5.382\n",
      "Ep 1 (Step 000290): Train loss 5.321, Val loss 5.374\n",
      "Ep 1 (Step 000300): Train loss 5.222, Val loss 5.363\n",
      "Ep 1 (Step 000310): Train loss 5.207, Val loss 5.347\n",
      "Ep 1 (Step 000320): Train loss 5.261, Val loss 5.330\n",
      "Ep 1 (Step 000330): Train loss 5.185, Val loss 5.313\n",
      "Ep 1 (Step 000340): Train loss 5.197, Val loss 5.312\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3115\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.061, Val loss 9.047\n",
      "Ep 1 (Step 000010): Train loss 7.440, Val loss 7.417\n",
      "Ep 1 (Step 000020): Train loss 6.752, Val loss 6.772\n",
      "Ep 1 (Step 000030): Train loss 6.513, Val loss 6.458\n",
      "Ep 1 (Step 000040): Train loss 6.415, Val loss 6.372\n",
      "Ep 1 (Step 000050): Train loss 6.319, Val loss 6.328\n",
      "Ep 1 (Step 000060): Train loss 6.243, Val loss 6.257\n",
      "Ep 1 (Step 000070): Train loss 6.062, Val loss 6.121\n",
      "Ep 1 (Step 000080): Train loss 6.058, Val loss 6.028\n",
      "Ep 1 (Step 000090): Train loss 5.909, Val loss 5.955\n",
      "Ep 1 (Step 000100): Train loss 5.867, Val loss 5.921\n",
      "Ep 1 (Step 000110): Train loss 5.767, Val loss 5.844\n",
      "Ep 1 (Step 000120): Train loss 5.728, Val loss 5.799\n",
      "Ep 1 (Step 000130): Train loss 5.646, Val loss 5.759\n",
      "Ep 1 (Step 000140): Train loss 5.650, Val loss 5.701\n",
      "Ep 1 (Step 000150): Train loss 5.603, Val loss 5.671\n",
      "Ep 1 (Step 000160): Train loss 5.607, Val loss 5.642\n",
      "Ep 1 (Step 000170): Train loss 5.555, Val loss 5.613\n",
      "Ep 1 (Step 000180): Train loss 5.544, Val loss 5.575\n",
      "Ep 1 (Step 000190): Train loss 5.433, Val loss 5.550\n",
      "Ep 1 (Step 000200): Train loss 5.559, Val loss 5.523\n",
      "Ep 1 (Step 000210): Train loss 5.420, Val loss 5.513\n",
      "Ep 1 (Step 000220): Train loss 5.478, Val loss 5.493\n",
      "Ep 1 (Step 000230): Train loss 5.377, Val loss 5.458\n",
      "Ep 1 (Step 000240): Train loss 5.352, Val loss 5.443\n",
      "Ep 1 (Step 000250): Train loss 5.351, Val loss 5.432\n",
      "Ep 1 (Step 000260): Train loss 5.321, Val loss 5.413\n",
      "Ep 1 (Step 000270): Train loss 5.296, Val loss 5.416\n",
      "Ep 1 (Step 000280): Train loss 5.307, Val loss 5.388\n",
      "Ep 1 (Step 000290): Train loss 5.308, Val loss 5.368\n",
      "Ep 1 (Step 000300): Train loss 5.204, Val loss 5.359\n",
      "Ep 1 (Step 000310): Train loss 5.257, Val loss 5.347\n",
      "Ep 1 (Step 000320): Train loss 5.199, Val loss 5.339\n",
      "Ep 1 (Step 000330): Train loss 5.264, Val loss 5.333\n",
      "Ep 1 (Step 000340): Train loss 5.162, Val loss 5.308\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3076\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.081, Val loss 9.067\n",
      "Ep 1 (Step 000010): Train loss 7.518, Val loss 7.447\n",
      "Ep 1 (Step 000020): Train loss 6.784, Val loss 6.774\n",
      "Ep 1 (Step 000030): Train loss 6.459, Val loss 6.461\n",
      "Ep 1 (Step 000040): Train loss 6.395, Val loss 6.356\n",
      "Ep 1 (Step 000050): Train loss 6.336, Val loss 6.341\n",
      "Ep 1 (Step 000060): Train loss 6.226, Val loss 6.244\n",
      "Ep 1 (Step 000070): Train loss 6.119, Val loss 6.112\n",
      "Ep 1 (Step 000080): Train loss 6.032, Val loss 6.013\n",
      "Ep 1 (Step 000090): Train loss 5.918, Val loss 5.941\n",
      "Ep 1 (Step 000100): Train loss 5.823, Val loss 5.882\n",
      "Ep 1 (Step 000110): Train loss 5.774, Val loss 5.827\n",
      "Ep 1 (Step 000120): Train loss 5.813, Val loss 5.774\n",
      "Ep 1 (Step 000130): Train loss 5.717, Val loss 5.726\n",
      "Ep 1 (Step 000140): Train loss 5.605, Val loss 5.694\n",
      "Ep 1 (Step 000150): Train loss 5.600, Val loss 5.665\n",
      "Ep 1 (Step 000160): Train loss 5.513, Val loss 5.616\n",
      "Ep 1 (Step 000170): Train loss 5.563, Val loss 5.584\n",
      "Ep 1 (Step 000180): Train loss 5.498, Val loss 5.568\n",
      "Ep 1 (Step 000190): Train loss 5.478, Val loss 5.536\n",
      "Ep 1 (Step 000200): Train loss 5.465, Val loss 5.509\n",
      "Ep 1 (Step 000210): Train loss 5.365, Val loss 5.490\n",
      "Ep 1 (Step 000220): Train loss 5.393, Val loss 5.476\n",
      "Ep 1 (Step 000230): Train loss 5.325, Val loss 5.459\n",
      "Ep 1 (Step 000240): Train loss 5.364, Val loss 5.433\n",
      "Ep 1 (Step 000250): Train loss 5.325, Val loss 5.429\n",
      "Ep 1 (Step 000260): Train loss 5.339, Val loss 5.404\n",
      "Ep 1 (Step 000270): Train loss 5.306, Val loss 5.400\n",
      "Ep 1 (Step 000280): Train loss 5.248, Val loss 5.377\n",
      "Ep 1 (Step 000290): Train loss 5.298, Val loss 5.370\n",
      "Ep 1 (Step 000300): Train loss 5.197, Val loss 5.358\n",
      "Ep 1 (Step 000310): Train loss 5.239, Val loss 5.347\n",
      "Ep 1 (Step 000320): Train loss 5.199, Val loss 5.330\n",
      "Ep 1 (Step 000330): Train loss 5.243, Val loss 5.323\n",
      "Ep 1 (Step 000340): Train loss 5.145, Val loss 5.309\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3091\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.732, Val loss 8.739\n",
      "Ep 1 (Step 000010): Train loss 6.888, Val loss 6.831\n",
      "Ep 1 (Step 000020): Train loss 6.446, Val loss 6.435\n",
      "Ep 1 (Step 000030): Train loss 6.458, Val loss 6.397\n",
      "Ep 1 (Step 000040): Train loss 6.251, Val loss 6.228\n",
      "Ep 1 (Step 000050): Train loss 6.170, Val loss 6.080\n",
      "Ep 1 (Step 000060): Train loss 5.951, Val loss 5.966\n",
      "Ep 1 (Step 000070): Train loss 5.928, Val loss 5.883\n",
      "Ep 1 (Step 000080): Train loss 5.781, Val loss 5.808\n",
      "Ep 1 (Step 000090): Train loss 5.668, Val loss 5.742\n",
      "Ep 1 (Step 000100): Train loss 5.599, Val loss 5.683\n",
      "Ep 1 (Step 000110): Train loss 5.614, Val loss 5.643\n",
      "Ep 1 (Step 000120): Train loss 5.430, Val loss 5.608\n",
      "Ep 1 (Step 000130): Train loss 5.552, Val loss 5.578\n",
      "Ep 1 (Step 000140): Train loss 5.398, Val loss 5.538\n",
      "Ep 1 (Step 000150): Train loss 5.463, Val loss 5.494\n",
      "Ep 1 (Step 000160): Train loss 5.385, Val loss 5.476\n",
      "Ep 1 (Step 000170): Train loss 5.307, Val loss 5.458\n",
      "Ep 1 (Step 000180): Train loss 5.265, Val loss 5.432\n",
      "Ep 1 (Step 000190): Train loss 5.302, Val loss 5.415\n",
      "Ep 1 (Step 000200): Train loss 5.247, Val loss 5.396\n",
      "Ep 1 (Step 000210): Train loss 5.326, Val loss 5.394\n",
      "Ep 1 (Step 000220): Train loss 5.225, Val loss 5.357\n",
      "Ep 1 (Step 000230): Train loss 5.194, Val loss 5.355\n",
      "Ep 1 (Step 000240): Train loss 5.201, Val loss 5.341\n",
      "Ep 1 (Step 000250): Train loss 5.165, Val loss 5.309\n",
      "Ep 1 (Step 000260): Train loss 5.206, Val loss 5.307\n",
      "Ep 1 (Step 000270): Train loss 5.220, Val loss 5.295\n",
      "Ep 1 (Step 000280): Train loss 5.143, Val loss 5.281\n",
      "Ep 1 (Step 000290): Train loss 5.078, Val loss 5.261\n",
      "Ep 1 (Step 000300): Train loss 5.074, Val loss 5.248\n",
      "Ep 1 (Step 000310): Train loss 5.116, Val loss 5.233\n",
      "Ep 1 (Step 000320): Train loss 5.082, Val loss 5.210\n",
      "Ep 1 (Step 000330): Train loss 5.071, Val loss 5.228\n",
      "Ep 1 (Step 000340): Train loss 5.060, Val loss 5.242\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2421\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.887, Val loss 8.902\n",
      "Ep 1 (Step 000010): Train loss 6.868, Val loss 6.856\n",
      "Ep 1 (Step 000020): Train loss 6.530, Val loss 6.495\n",
      "Ep 1 (Step 000030): Train loss 6.476, Val loss 6.456\n",
      "Ep 1 (Step 000040): Train loss 6.302, Val loss 6.331\n",
      "Ep 1 (Step 000050): Train loss 6.086, Val loss 6.141\n",
      "Ep 1 (Step 000060): Train loss 5.948, Val loss 6.018\n",
      "Ep 1 (Step 000070): Train loss 5.866, Val loss 5.905\n",
      "Ep 1 (Step 000080): Train loss 5.823, Val loss 5.824\n",
      "Ep 1 (Step 000090): Train loss 5.721, Val loss 5.768\n",
      "Ep 1 (Step 000100): Train loss 5.628, Val loss 5.711\n",
      "Ep 1 (Step 000110): Train loss 5.556, Val loss 5.647\n",
      "Ep 1 (Step 000120): Train loss 5.569, Val loss 5.611\n",
      "Ep 1 (Step 000130): Train loss 5.396, Val loss 5.565\n",
      "Ep 1 (Step 000140): Train loss 5.371, Val loss 5.528\n",
      "Ep 1 (Step 000150): Train loss 5.426, Val loss 5.511\n",
      "Ep 1 (Step 000160): Train loss 5.337, Val loss 5.489\n",
      "Ep 1 (Step 000170): Train loss 5.468, Val loss 5.463\n",
      "Ep 1 (Step 000180): Train loss 5.371, Val loss 5.437\n",
      "Ep 1 (Step 000190): Train loss 5.323, Val loss 5.410\n",
      "Ep 1 (Step 000200): Train loss 5.334, Val loss 5.397\n",
      "Ep 1 (Step 000210): Train loss 5.220, Val loss 5.376\n",
      "Ep 1 (Step 000220): Train loss 5.275, Val loss 5.350\n",
      "Ep 1 (Step 000230): Train loss 5.202, Val loss 5.365\n",
      "Ep 1 (Step 000240): Train loss 5.162, Val loss 5.327\n",
      "Ep 1 (Step 000250): Train loss 5.144, Val loss 5.337\n",
      "Ep 1 (Step 000260): Train loss 5.168, Val loss 5.316\n",
      "Ep 1 (Step 000270): Train loss 5.107, Val loss 5.312\n",
      "Ep 1 (Step 000280): Train loss 5.150, Val loss 5.310\n",
      "Ep 1 (Step 000290): Train loss 5.208, Val loss 5.317\n",
      "Ep 1 (Step 000300): Train loss 5.110, Val loss 5.302\n",
      "Ep 1 (Step 000310): Train loss 5.179, Val loss 5.278\n",
      "Ep 1 (Step 000320): Train loss 5.030, Val loss 5.257\n",
      "Ep 1 (Step 000330): Train loss 5.006, Val loss 5.251\n",
      "Ep 1 (Step 000340): Train loss 5.133, Val loss 5.236\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2358\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.773, Val loss 8.778\n",
      "Ep 1 (Step 000010): Train loss 6.892, Val loss 6.798\n",
      "Ep 1 (Step 000020): Train loss 6.503, Val loss 6.446\n",
      "Ep 1 (Step 000030): Train loss 6.433, Val loss 6.464\n",
      "Ep 1 (Step 000040): Train loss 6.375, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.192, Val loss 6.207\n",
      "Ep 1 (Step 000060): Train loss 6.060, Val loss 6.080\n",
      "Ep 1 (Step 000070): Train loss 6.010, Val loss 5.964\n",
      "Ep 1 (Step 000080): Train loss 5.830, Val loss 5.886\n",
      "Ep 1 (Step 000090): Train loss 5.763, Val loss 5.812\n",
      "Ep 1 (Step 000100): Train loss 5.629, Val loss 5.748\n",
      "Ep 1 (Step 000110): Train loss 5.672, Val loss 5.693\n",
      "Ep 1 (Step 000120): Train loss 5.595, Val loss 5.661\n",
      "Ep 1 (Step 000130): Train loss 5.518, Val loss 5.597\n",
      "Ep 1 (Step 000140): Train loss 5.481, Val loss 5.561\n",
      "Ep 1 (Step 000150): Train loss 5.425, Val loss 5.541\n",
      "Ep 1 (Step 000160): Train loss 5.441, Val loss 5.521\n",
      "Ep 1 (Step 000170): Train loss 5.351, Val loss 5.480\n",
      "Ep 1 (Step 000180): Train loss 5.345, Val loss 5.473\n",
      "Ep 1 (Step 000190): Train loss 5.329, Val loss 5.430\n",
      "Ep 1 (Step 000200): Train loss 5.366, Val loss 5.416\n",
      "Ep 1 (Step 000210): Train loss 5.333, Val loss 5.392\n",
      "Ep 1 (Step 000220): Train loss 5.294, Val loss 5.385\n",
      "Ep 1 (Step 000230): Train loss 5.268, Val loss 5.363\n",
      "Ep 1 (Step 000240): Train loss 5.256, Val loss 5.343\n",
      "Ep 1 (Step 000250): Train loss 5.187, Val loss 5.334\n",
      "Ep 1 (Step 000260): Train loss 5.168, Val loss 5.319\n",
      "Ep 1 (Step 000270): Train loss 5.184, Val loss 5.299\n",
      "Ep 1 (Step 000280): Train loss 5.286, Val loss 5.301\n",
      "Ep 1 (Step 000290): Train loss 5.148, Val loss 5.294\n",
      "Ep 1 (Step 000300): Train loss 5.140, Val loss 5.271\n",
      "Ep 1 (Step 000310): Train loss 5.153, Val loss 5.254\n",
      "Ep 1 (Step 000320): Train loss 5.127, Val loss 5.228\n",
      "Ep 1 (Step 000330): Train loss 5.049, Val loss 5.239\n",
      "Ep 1 (Step 000340): Train loss 5.134, Val loss 5.216\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2161\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.843, Val loss 8.804\n",
      "Ep 1 (Step 000010): Train loss 6.908, Val loss 6.892\n",
      "Ep 1 (Step 000020): Train loss 6.503, Val loss 6.483\n",
      "Ep 1 (Step 000030): Train loss 6.410, Val loss 6.433\n",
      "Ep 1 (Step 000040): Train loss 6.298, Val loss 6.295\n",
      "Ep 1 (Step 000050): Train loss 6.075, Val loss 6.134\n",
      "Ep 1 (Step 000060): Train loss 5.942, Val loss 6.006\n",
      "Ep 1 (Step 000070): Train loss 5.864, Val loss 5.897\n",
      "Ep 1 (Step 000080): Train loss 5.797, Val loss 5.821\n",
      "Ep 1 (Step 000090): Train loss 5.698, Val loss 5.744\n",
      "Ep 1 (Step 000100): Train loss 5.588, Val loss 5.683\n",
      "Ep 1 (Step 000110): Train loss 5.593, Val loss 5.637\n",
      "Ep 1 (Step 000120): Train loss 5.568, Val loss 5.606\n",
      "Ep 1 (Step 000130): Train loss 5.430, Val loss 5.575\n",
      "Ep 1 (Step 000140): Train loss 5.469, Val loss 5.523\n",
      "Ep 1 (Step 000150): Train loss 5.344, Val loss 5.470\n",
      "Ep 1 (Step 000160): Train loss 5.425, Val loss 5.444\n",
      "Ep 1 (Step 000170): Train loss 5.408, Val loss 5.417\n",
      "Ep 1 (Step 000180): Train loss 5.286, Val loss 5.404\n",
      "Ep 1 (Step 000190): Train loss 5.346, Val loss 5.386\n",
      "Ep 1 (Step 000200): Train loss 5.392, Val loss 5.375\n",
      "Ep 1 (Step 000210): Train loss 5.286, Val loss 5.350\n",
      "Ep 1 (Step 000220): Train loss 5.254, Val loss 5.352\n",
      "Ep 1 (Step 000230): Train loss 5.190, Val loss 5.332\n",
      "Ep 1 (Step 000240): Train loss 5.187, Val loss 5.313\n",
      "Ep 1 (Step 000250): Train loss 5.205, Val loss 5.278\n",
      "Ep 1 (Step 000260): Train loss 5.235, Val loss 5.277\n",
      "Ep 1 (Step 000270): Train loss 5.131, Val loss 5.268\n",
      "Ep 1 (Step 000280): Train loss 5.054, Val loss 5.238\n",
      "Ep 1 (Step 000290): Train loss 5.104, Val loss 5.231\n",
      "Ep 1 (Step 000300): Train loss 5.120, Val loss 5.218\n",
      "Ep 1 (Step 000310): Train loss 5.042, Val loss 5.211\n",
      "Ep 1 (Step 000320): Train loss 5.104, Val loss 5.214\n",
      "Ep 1 (Step 000330): Train loss 5.094, Val loss 5.214\n",
      "Ep 1 (Step 000340): Train loss 5.069, Val loss 5.204\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2037\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.875, Val loss 8.843\n",
      "Ep 1 (Step 000010): Train loss 6.911, Val loss 6.885\n",
      "Ep 1 (Step 000020): Train loss 6.465, Val loss 6.471\n",
      "Ep 1 (Step 000030): Train loss 6.512, Val loss 6.425\n",
      "Ep 1 (Step 000040): Train loss 6.305, Val loss 6.260\n",
      "Ep 1 (Step 000050): Train loss 6.202, Val loss 6.136\n",
      "Ep 1 (Step 000060): Train loss 5.946, Val loss 6.000\n",
      "Ep 1 (Step 000070): Train loss 5.807, Val loss 5.889\n",
      "Ep 1 (Step 000080): Train loss 5.804, Val loss 5.824\n",
      "Ep 1 (Step 000090): Train loss 5.720, Val loss 5.757\n",
      "Ep 1 (Step 000100): Train loss 5.580, Val loss 5.690\n",
      "Ep 1 (Step 000110): Train loss 5.602, Val loss 5.646\n",
      "Ep 1 (Step 000120): Train loss 5.539, Val loss 5.598\n",
      "Ep 1 (Step 000130): Train loss 5.510, Val loss 5.554\n",
      "Ep 1 (Step 000140): Train loss 5.454, Val loss 5.518\n",
      "Ep 1 (Step 000150): Train loss 5.344, Val loss 5.481\n",
      "Ep 1 (Step 000160): Train loss 5.424, Val loss 5.464\n",
      "Ep 1 (Step 000170): Train loss 5.313, Val loss 5.417\n",
      "Ep 1 (Step 000180): Train loss 5.245, Val loss 5.398\n",
      "Ep 1 (Step 000190): Train loss 5.311, Val loss 5.394\n",
      "Ep 1 (Step 000200): Train loss 5.307, Val loss 5.377\n",
      "Ep 1 (Step 000210): Train loss 5.218, Val loss 5.351\n",
      "Ep 1 (Step 000220): Train loss 5.271, Val loss 5.330\n",
      "Ep 1 (Step 000230): Train loss 5.203, Val loss 5.327\n",
      "Ep 1 (Step 000240): Train loss 5.193, Val loss 5.302\n",
      "Ep 1 (Step 000250): Train loss 5.181, Val loss 5.295\n",
      "Ep 1 (Step 000260): Train loss 5.208, Val loss 5.308\n",
      "Ep 1 (Step 000270): Train loss 5.039, Val loss 5.272\n",
      "Ep 1 (Step 000280): Train loss 5.064, Val loss 5.269\n",
      "Ep 1 (Step 000290): Train loss 5.048, Val loss 5.250\n",
      "Ep 1 (Step 000300): Train loss 5.058, Val loss 5.247\n",
      "Ep 1 (Step 000310): Train loss 5.106, Val loss 5.246\n",
      "Ep 1 (Step 000320): Train loss 5.200, Val loss 5.228\n",
      "Ep 1 (Step 000330): Train loss 5.025, Val loss 5.221\n",
      "Ep 1 (Step 000340): Train loss 5.055, Val loss 5.215\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2154\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.861, Val loss 8.844\n",
      "Ep 1 (Step 000010): Train loss 6.832, Val loss 6.815\n",
      "Ep 1 (Step 000020): Train loss 6.458, Val loss 6.467\n",
      "Ep 1 (Step 000030): Train loss 6.411, Val loss 6.441\n",
      "Ep 1 (Step 000040): Train loss 6.317, Val loss 6.293\n",
      "Ep 1 (Step 000050): Train loss 6.216, Val loss 6.181\n",
      "Ep 1 (Step 000060): Train loss 5.932, Val loss 6.014\n",
      "Ep 1 (Step 000070): Train loss 5.876, Val loss 5.943\n",
      "Ep 1 (Step 000080): Train loss 5.805, Val loss 5.874\n",
      "Ep 1 (Step 000090): Train loss 5.641, Val loss 5.782\n",
      "Ep 1 (Step 000100): Train loss 5.672, Val loss 5.744\n",
      "Ep 1 (Step 000110): Train loss 5.630, Val loss 5.649\n",
      "Ep 1 (Step 000120): Train loss 5.629, Val loss 5.600\n",
      "Ep 1 (Step 000130): Train loss 5.448, Val loss 5.571\n",
      "Ep 1 (Step 000140): Train loss 5.438, Val loss 5.541\n",
      "Ep 1 (Step 000150): Train loss 5.481, Val loss 5.497\n",
      "Ep 1 (Step 000160): Train loss 5.299, Val loss 5.461\n",
      "Ep 1 (Step 000170): Train loss 5.332, Val loss 5.431\n",
      "Ep 1 (Step 000180): Train loss 5.283, Val loss 5.410\n",
      "Ep 1 (Step 000190): Train loss 5.231, Val loss 5.405\n",
      "Ep 1 (Step 000200): Train loss 5.251, Val loss 5.367\n",
      "Ep 1 (Step 000210): Train loss 5.232, Val loss 5.351\n",
      "Ep 1 (Step 000220): Train loss 5.312, Val loss 5.329\n",
      "Ep 1 (Step 000230): Train loss 5.218, Val loss 5.331\n",
      "Ep 1 (Step 000240): Train loss 5.218, Val loss 5.316\n",
      "Ep 1 (Step 000250): Train loss 5.129, Val loss 5.292\n",
      "Ep 1 (Step 000260): Train loss 5.234, Val loss 5.275\n",
      "Ep 1 (Step 000270): Train loss 5.173, Val loss 5.260\n",
      "Ep 1 (Step 000280): Train loss 5.066, Val loss 5.245\n",
      "Ep 1 (Step 000290): Train loss 5.080, Val loss 5.238\n",
      "Ep 1 (Step 000300): Train loss 5.158, Val loss 5.217\n",
      "Ep 1 (Step 000310): Train loss 5.085, Val loss 5.231\n",
      "Ep 1 (Step 000320): Train loss 5.117, Val loss 5.219\n",
      "Ep 1 (Step 000330): Train loss 5.001, Val loss 5.211\n",
      "Ep 1 (Step 000340): Train loss 4.979, Val loss 5.188\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1879\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.052, Val loss 9.059\n",
      "Ep 1 (Step 000010): Train loss 7.465, Val loss 7.472\n",
      "Ep 1 (Step 000020): Train loss 6.807, Val loss 6.800\n",
      "Ep 1 (Step 000030): Train loss 6.442, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.371, Val loss 6.374\n",
      "Ep 1 (Step 000050): Train loss 6.280, Val loss 6.313\n",
      "Ep 1 (Step 000060): Train loss 6.204, Val loss 6.232\n",
      "Ep 1 (Step 000070): Train loss 6.100, Val loss 6.114\n",
      "Ep 1 (Step 000080): Train loss 6.032, Val loss 6.018\n",
      "Ep 1 (Step 000090): Train loss 5.936, Val loss 5.957\n",
      "Ep 1 (Step 000100): Train loss 5.923, Val loss 5.897\n",
      "Ep 1 (Step 000110): Train loss 5.848, Val loss 5.865\n",
      "Ep 1 (Step 000120): Train loss 5.714, Val loss 5.796\n",
      "Ep 1 (Step 000130): Train loss 5.702, Val loss 5.774\n",
      "Ep 1 (Step 000140): Train loss 5.681, Val loss 5.735\n",
      "Ep 1 (Step 000150): Train loss 5.611, Val loss 5.691\n",
      "Ep 1 (Step 000160): Train loss 5.708, Val loss 5.655\n",
      "Ep 1 (Step 000170): Train loss 5.501, Val loss 5.625\n",
      "Ep 1 (Step 000180): Train loss 5.493, Val loss 5.606\n",
      "Ep 1 (Step 000190): Train loss 5.425, Val loss 5.574\n",
      "Ep 1 (Step 000200): Train loss 5.399, Val loss 5.554\n",
      "Ep 1 (Step 000210): Train loss 5.403, Val loss 5.528\n",
      "Ep 1 (Step 000220): Train loss 5.414, Val loss 5.501\n",
      "Ep 1 (Step 000230): Train loss 5.354, Val loss 5.475\n",
      "Ep 1 (Step 000240): Train loss 5.364, Val loss 5.463\n",
      "Ep 1 (Step 000250): Train loss 5.391, Val loss 5.441\n",
      "Ep 1 (Step 000260): Train loss 5.254, Val loss 5.430\n",
      "Ep 1 (Step 000270): Train loss 5.316, Val loss 5.423\n",
      "Ep 1 (Step 000280): Train loss 5.241, Val loss 5.403\n",
      "Ep 1 (Step 000290): Train loss 5.206, Val loss 5.386\n",
      "Ep 1 (Step 000300): Train loss 5.323, Val loss 5.380\n",
      "Ep 1 (Step 000310): Train loss 5.212, Val loss 5.362\n",
      "Ep 1 (Step 000320): Train loss 5.180, Val loss 5.348\n",
      "Ep 1 (Step 000330): Train loss 5.233, Val loss 5.326\n",
      "Ep 1 (Step 000340): Train loss 5.281, Val loss 5.317\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3166\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.070, Val loss 9.058\n",
      "Ep 1 (Step 000010): Train loss 7.499, Val loss 7.461\n",
      "Ep 1 (Step 000020): Train loss 6.797, Val loss 6.794\n",
      "Ep 1 (Step 000030): Train loss 6.501, Val loss 6.469\n",
      "Ep 1 (Step 000040): Train loss 6.373, Val loss 6.377\n",
      "Ep 1 (Step 000050): Train loss 6.383, Val loss 6.333\n",
      "Ep 1 (Step 000060): Train loss 6.305, Val loss 6.243\n",
      "Ep 1 (Step 000070): Train loss 6.126, Val loss 6.123\n",
      "Ep 1 (Step 000080): Train loss 6.061, Val loss 6.040\n",
      "Ep 1 (Step 000090): Train loss 5.937, Val loss 5.963\n",
      "Ep 1 (Step 000100): Train loss 5.830, Val loss 5.903\n",
      "Ep 1 (Step 000110): Train loss 5.802, Val loss 5.850\n",
      "Ep 1 (Step 000120): Train loss 5.783, Val loss 5.798\n",
      "Ep 1 (Step 000130): Train loss 5.642, Val loss 5.750\n",
      "Ep 1 (Step 000140): Train loss 5.665, Val loss 5.720\n",
      "Ep 1 (Step 000150): Train loss 5.623, Val loss 5.679\n",
      "Ep 1 (Step 000160): Train loss 5.577, Val loss 5.654\n",
      "Ep 1 (Step 000170): Train loss 5.534, Val loss 5.624\n",
      "Ep 1 (Step 000180): Train loss 5.472, Val loss 5.599\n",
      "Ep 1 (Step 000190): Train loss 5.425, Val loss 5.570\n",
      "Ep 1 (Step 000200): Train loss 5.483, Val loss 5.558\n",
      "Ep 1 (Step 000210): Train loss 5.438, Val loss 5.518\n",
      "Ep 1 (Step 000220): Train loss 5.480, Val loss 5.507\n",
      "Ep 1 (Step 000230): Train loss 5.379, Val loss 5.505\n",
      "Ep 1 (Step 000240): Train loss 5.293, Val loss 5.480\n",
      "Ep 1 (Step 000250): Train loss 5.298, Val loss 5.468\n",
      "Ep 1 (Step 000260): Train loss 5.362, Val loss 5.442\n",
      "Ep 1 (Step 000270): Train loss 5.384, Val loss 5.440\n",
      "Ep 1 (Step 000280): Train loss 5.331, Val loss 5.409\n",
      "Ep 1 (Step 000290): Train loss 5.264, Val loss 5.400\n",
      "Ep 1 (Step 000300): Train loss 5.247, Val loss 5.384\n",
      "Ep 1 (Step 000310): Train loss 5.200, Val loss 5.369\n",
      "Ep 1 (Step 000320): Train loss 5.259, Val loss 5.357\n",
      "Ep 1 (Step 000330): Train loss 5.182, Val loss 5.350\n",
      "Ep 1 (Step 000340): Train loss 5.136, Val loss 5.333\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3327\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.061, Val loss 9.031\n",
      "Ep 1 (Step 000010): Train loss 7.486, Val loss 7.450\n",
      "Ep 1 (Step 000020): Train loss 6.819, Val loss 6.776\n",
      "Ep 1 (Step 000030): Train loss 6.532, Val loss 6.464\n",
      "Ep 1 (Step 000040): Train loss 6.362, Val loss 6.380\n",
      "Ep 1 (Step 000050): Train loss 6.352, Val loss 6.329\n",
      "Ep 1 (Step 000060): Train loss 6.232, Val loss 6.240\n",
      "Ep 1 (Step 000070): Train loss 6.125, Val loss 6.119\n",
      "Ep 1 (Step 000080): Train loss 6.038, Val loss 6.019\n",
      "Ep 1 (Step 000090): Train loss 5.880, Val loss 5.948\n",
      "Ep 1 (Step 000100): Train loss 5.844, Val loss 5.887\n",
      "Ep 1 (Step 000110): Train loss 5.756, Val loss 5.831\n",
      "Ep 1 (Step 000120): Train loss 5.734, Val loss 5.795\n",
      "Ep 1 (Step 000130): Train loss 5.672, Val loss 5.752\n",
      "Ep 1 (Step 000140): Train loss 5.640, Val loss 5.717\n",
      "Ep 1 (Step 000150): Train loss 5.582, Val loss 5.681\n",
      "Ep 1 (Step 000160): Train loss 5.635, Val loss 5.656\n",
      "Ep 1 (Step 000170): Train loss 5.614, Val loss 5.618\n",
      "Ep 1 (Step 000180): Train loss 5.603, Val loss 5.579\n",
      "Ep 1 (Step 000190): Train loss 5.597, Val loss 5.561\n",
      "Ep 1 (Step 000200): Train loss 5.443, Val loss 5.534\n",
      "Ep 1 (Step 000210): Train loss 5.440, Val loss 5.517\n",
      "Ep 1 (Step 000220): Train loss 5.494, Val loss 5.485\n",
      "Ep 1 (Step 000230): Train loss 5.402, Val loss 5.485\n",
      "Ep 1 (Step 000240): Train loss 5.346, Val loss 5.467\n",
      "Ep 1 (Step 000250): Train loss 5.380, Val loss 5.443\n",
      "Ep 1 (Step 000260): Train loss 5.310, Val loss 5.435\n",
      "Ep 1 (Step 000270): Train loss 5.253, Val loss 5.427\n",
      "Ep 1 (Step 000280): Train loss 5.268, Val loss 5.405\n",
      "Ep 1 (Step 000290): Train loss 5.238, Val loss 5.386\n",
      "Ep 1 (Step 000300): Train loss 5.308, Val loss 5.371\n",
      "Ep 1 (Step 000310): Train loss 5.308, Val loss 5.374\n",
      "Ep 1 (Step 000320): Train loss 5.188, Val loss 5.359\n",
      "Ep 1 (Step 000330): Train loss 5.249, Val loss 5.346\n",
      "Ep 1 (Step 000340): Train loss 5.183, Val loss 5.320\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3203\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.089, Val loss 9.081\n",
      "Ep 1 (Step 000010): Train loss 7.516, Val loss 7.455\n",
      "Ep 1 (Step 000020): Train loss 6.810, Val loss 6.788\n",
      "Ep 1 (Step 000030): Train loss 6.502, Val loss 6.456\n",
      "Ep 1 (Step 000040): Train loss 6.405, Val loss 6.358\n",
      "Ep 1 (Step 000050): Train loss 6.333, Val loss 6.316\n",
      "Ep 1 (Step 000060): Train loss 6.250, Val loss 6.226\n",
      "Ep 1 (Step 000070): Train loss 6.122, Val loss 6.105\n",
      "Ep 1 (Step 000080): Train loss 5.984, Val loss 6.023\n",
      "Ep 1 (Step 000090): Train loss 5.892, Val loss 5.966\n",
      "Ep 1 (Step 000100): Train loss 5.851, Val loss 5.891\n",
      "Ep 1 (Step 000110): Train loss 5.813, Val loss 5.834\n",
      "Ep 1 (Step 000120): Train loss 5.848, Val loss 5.788\n",
      "Ep 1 (Step 000130): Train loss 5.688, Val loss 5.742\n",
      "Ep 1 (Step 000140): Train loss 5.626, Val loss 5.709\n",
      "Ep 1 (Step 000150): Train loss 5.569, Val loss 5.671\n",
      "Ep 1 (Step 000160): Train loss 5.631, Val loss 5.636\n",
      "Ep 1 (Step 000170): Train loss 5.515, Val loss 5.608\n",
      "Ep 1 (Step 000180): Train loss 5.441, Val loss 5.570\n",
      "Ep 1 (Step 000190): Train loss 5.493, Val loss 5.545\n",
      "Ep 1 (Step 000200): Train loss 5.476, Val loss 5.532\n",
      "Ep 1 (Step 000210): Train loss 5.434, Val loss 5.518\n",
      "Ep 1 (Step 000220): Train loss 5.396, Val loss 5.486\n",
      "Ep 1 (Step 000230): Train loss 5.380, Val loss 5.471\n",
      "Ep 1 (Step 000240): Train loss 5.463, Val loss 5.453\n",
      "Ep 1 (Step 000250): Train loss 5.352, Val loss 5.429\n",
      "Ep 1 (Step 000260): Train loss 5.335, Val loss 5.411\n",
      "Ep 1 (Step 000270): Train loss 5.324, Val loss 5.400\n",
      "Ep 1 (Step 000280): Train loss 5.208, Val loss 5.409\n",
      "Ep 1 (Step 000290): Train loss 5.230, Val loss 5.395\n",
      "Ep 1 (Step 000300): Train loss 5.271, Val loss 5.361\n",
      "Ep 1 (Step 000310): Train loss 5.146, Val loss 5.355\n",
      "Ep 1 (Step 000320): Train loss 5.191, Val loss 5.338\n",
      "Ep 1 (Step 000330): Train loss 5.220, Val loss 5.325\n",
      "Ep 1 (Step 000340): Train loss 5.112, Val loss 5.314\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3137\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.068, Val loss 9.064\n",
      "Ep 1 (Step 000010): Train loss 7.476, Val loss 7.421\n",
      "Ep 1 (Step 000020): Train loss 6.800, Val loss 6.784\n",
      "Ep 1 (Step 000030): Train loss 6.506, Val loss 6.479\n",
      "Ep 1 (Step 000040): Train loss 6.397, Val loss 6.402\n",
      "Ep 1 (Step 000050): Train loss 6.389, Val loss 6.351\n",
      "Ep 1 (Step 000060): Train loss 6.162, Val loss 6.247\n",
      "Ep 1 (Step 000070): Train loss 6.150, Val loss 6.131\n",
      "Ep 1 (Step 000080): Train loss 5.997, Val loss 6.049\n",
      "Ep 1 (Step 000090): Train loss 5.946, Val loss 5.991\n",
      "Ep 1 (Step 000100): Train loss 5.903, Val loss 5.933\n",
      "Ep 1 (Step 000110): Train loss 5.793, Val loss 5.872\n",
      "Ep 1 (Step 000120): Train loss 5.823, Val loss 5.808\n",
      "Ep 1 (Step 000130): Train loss 5.690, Val loss 5.777\n",
      "Ep 1 (Step 000140): Train loss 5.663, Val loss 5.748\n",
      "Ep 1 (Step 000150): Train loss 5.567, Val loss 5.705\n",
      "Ep 1 (Step 000160): Train loss 5.650, Val loss 5.660\n",
      "Ep 1 (Step 000170): Train loss 5.612, Val loss 5.632\n",
      "Ep 1 (Step 000180): Train loss 5.492, Val loss 5.597\n",
      "Ep 1 (Step 000190): Train loss 5.431, Val loss 5.561\n",
      "Ep 1 (Step 000200): Train loss 5.494, Val loss 5.530\n",
      "Ep 1 (Step 000210): Train loss 5.356, Val loss 5.510\n",
      "Ep 1 (Step 000220): Train loss 5.447, Val loss 5.493\n",
      "Ep 1 (Step 000230): Train loss 5.331, Val loss 5.482\n",
      "Ep 1 (Step 000240): Train loss 5.311, Val loss 5.469\n",
      "Ep 1 (Step 000250): Train loss 5.349, Val loss 5.437\n",
      "Ep 1 (Step 000260): Train loss 5.353, Val loss 5.426\n",
      "Ep 1 (Step 000270): Train loss 5.257, Val loss 5.405\n",
      "Ep 1 (Step 000280): Train loss 5.271, Val loss 5.389\n",
      "Ep 1 (Step 000290): Train loss 5.344, Val loss 5.381\n",
      "Ep 1 (Step 000300): Train loss 5.185, Val loss 5.370\n",
      "Ep 1 (Step 000310): Train loss 5.244, Val loss 5.359\n",
      "Ep 1 (Step 000320): Train loss 5.241, Val loss 5.340\n",
      "Ep 1 (Step 000330): Train loss 5.076, Val loss 5.338\n",
      "Ep 1 (Step 000340): Train loss 5.157, Val loss 5.318\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3180\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.082, Val loss 9.066\n",
      "Ep 1 (Step 000010): Train loss 7.513, Val loss 7.472\n",
      "Ep 1 (Step 000020): Train loss 6.831, Val loss 6.799\n",
      "Ep 1 (Step 000030): Train loss 6.533, Val loss 6.484\n",
      "Ep 1 (Step 000040): Train loss 6.423, Val loss 6.393\n",
      "Ep 1 (Step 000050): Train loss 6.262, Val loss 6.337\n",
      "Ep 1 (Step 000060): Train loss 6.262, Val loss 6.228\n",
      "Ep 1 (Step 000070): Train loss 6.122, Val loss 6.092\n",
      "Ep 1 (Step 000080): Train loss 6.060, Val loss 6.044\n",
      "Ep 1 (Step 000090): Train loss 5.972, Val loss 5.972\n",
      "Ep 1 (Step 000100): Train loss 5.888, Val loss 5.918\n",
      "Ep 1 (Step 000110): Train loss 5.747, Val loss 5.859\n",
      "Ep 1 (Step 000120): Train loss 5.722, Val loss 5.798\n",
      "Ep 1 (Step 000130): Train loss 5.735, Val loss 5.754\n",
      "Ep 1 (Step 000140): Train loss 5.690, Val loss 5.718\n",
      "Ep 1 (Step 000150): Train loss 5.663, Val loss 5.670\n",
      "Ep 1 (Step 000160): Train loss 5.654, Val loss 5.642\n",
      "Ep 1 (Step 000170): Train loss 5.491, Val loss 5.613\n",
      "Ep 1 (Step 000180): Train loss 5.501, Val loss 5.579\n",
      "Ep 1 (Step 000190): Train loss 5.482, Val loss 5.561\n",
      "Ep 1 (Step 000200): Train loss 5.439, Val loss 5.531\n",
      "Ep 1 (Step 000210): Train loss 5.444, Val loss 5.509\n",
      "Ep 1 (Step 000220): Train loss 5.350, Val loss 5.486\n",
      "Ep 1 (Step 000230): Train loss 5.387, Val loss 5.475\n",
      "Ep 1 (Step 000240): Train loss 5.332, Val loss 5.465\n",
      "Ep 1 (Step 000250): Train loss 5.354, Val loss 5.441\n",
      "Ep 1 (Step 000260): Train loss 5.293, Val loss 5.415\n",
      "Ep 1 (Step 000270): Train loss 5.237, Val loss 5.402\n",
      "Ep 1 (Step 000280): Train loss 5.181, Val loss 5.383\n",
      "Ep 1 (Step 000290): Train loss 5.178, Val loss 5.374\n",
      "Ep 1 (Step 000300): Train loss 5.283, Val loss 5.352\n",
      "Ep 1 (Step 000310): Train loss 5.170, Val loss 5.345\n",
      "Ep 1 (Step 000320): Train loss 5.282, Val loss 5.331\n",
      "Ep 1 (Step 000330): Train loss 5.209, Val loss 5.329\n",
      "Ep 1 (Step 000340): Train loss 5.239, Val loss 5.295\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2951\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.815, Val loss 8.807\n",
      "Ep 1 (Step 000010): Train loss 6.891, Val loss 6.859\n",
      "Ep 1 (Step 000020): Train loss 6.460, Val loss 6.448\n",
      "Ep 1 (Step 000030): Train loss 6.452, Val loss 6.424\n",
      "Ep 1 (Step 000040): Train loss 6.333, Val loss 6.313\n",
      "Ep 1 (Step 000050): Train loss 6.186, Val loss 6.168\n",
      "Ep 1 (Step 000060): Train loss 5.956, Val loss 6.036\n",
      "Ep 1 (Step 000070): Train loss 5.935, Val loss 5.936\n",
      "Ep 1 (Step 000080): Train loss 5.809, Val loss 5.861\n",
      "Ep 1 (Step 000090): Train loss 5.672, Val loss 5.795\n",
      "Ep 1 (Step 000100): Train loss 5.601, Val loss 5.723\n",
      "Ep 1 (Step 000110): Train loss 5.675, Val loss 5.656\n",
      "Ep 1 (Step 000120): Train loss 5.517, Val loss 5.613\n",
      "Ep 1 (Step 000130): Train loss 5.547, Val loss 5.576\n",
      "Ep 1 (Step 000140): Train loss 5.477, Val loss 5.561\n",
      "Ep 1 (Step 000150): Train loss 5.463, Val loss 5.538\n",
      "Ep 1 (Step 000160): Train loss 5.372, Val loss 5.500\n",
      "Ep 1 (Step 000170): Train loss 5.350, Val loss 5.471\n",
      "Ep 1 (Step 000180): Train loss 5.449, Val loss 5.455\n",
      "Ep 1 (Step 000190): Train loss 5.348, Val loss 5.434\n",
      "Ep 1 (Step 000200): Train loss 5.298, Val loss 5.431\n",
      "Ep 1 (Step 000210): Train loss 5.277, Val loss 5.405\n",
      "Ep 1 (Step 000220): Train loss 5.266, Val loss 5.394\n",
      "Ep 1 (Step 000230): Train loss 5.268, Val loss 5.360\n",
      "Ep 1 (Step 000240): Train loss 5.242, Val loss 5.359\n",
      "Ep 1 (Step 000250): Train loss 5.186, Val loss 5.343\n",
      "Ep 1 (Step 000260): Train loss 5.231, Val loss 5.319\n",
      "Ep 1 (Step 000270): Train loss 5.270, Val loss 5.308\n",
      "Ep 1 (Step 000280): Train loss 5.122, Val loss 5.308\n",
      "Ep 1 (Step 000290): Train loss 5.126, Val loss 5.291\n",
      "Ep 1 (Step 000300): Train loss 5.167, Val loss 5.286\n",
      "Ep 1 (Step 000310): Train loss 5.153, Val loss 5.262\n",
      "Ep 1 (Step 000320): Train loss 5.065, Val loss 5.258\n",
      "Ep 1 (Step 000330): Train loss 5.152, Val loss 5.248\n",
      "Ep 1 (Step 000340): Train loss 5.129, Val loss 5.230\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2295\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.791, Val loss 8.790\n",
      "Ep 1 (Step 000010): Train loss 6.907, Val loss 6.855\n",
      "Ep 1 (Step 000020): Train loss 6.427, Val loss 6.446\n",
      "Ep 1 (Step 000030): Train loss 6.412, Val loss 6.424\n",
      "Ep 1 (Step 000040): Train loss 6.308, Val loss 6.291\n",
      "Ep 1 (Step 000050): Train loss 6.122, Val loss 6.174\n",
      "Ep 1 (Step 000060): Train loss 6.048, Val loss 6.027\n",
      "Ep 1 (Step 000070): Train loss 5.930, Val loss 5.931\n",
      "Ep 1 (Step 000080): Train loss 5.808, Val loss 5.860\n",
      "Ep 1 (Step 000090): Train loss 5.736, Val loss 5.799\n",
      "Ep 1 (Step 000100): Train loss 5.697, Val loss 5.719\n",
      "Ep 1 (Step 000110): Train loss 5.581, Val loss 5.661\n",
      "Ep 1 (Step 000120): Train loss 5.558, Val loss 5.630\n",
      "Ep 1 (Step 000130): Train loss 5.490, Val loss 5.585\n",
      "Ep 1 (Step 000140): Train loss 5.396, Val loss 5.540\n",
      "Ep 1 (Step 000150): Train loss 5.470, Val loss 5.527\n",
      "Ep 1 (Step 000160): Train loss 5.376, Val loss 5.487\n",
      "Ep 1 (Step 000170): Train loss 5.312, Val loss 5.474\n",
      "Ep 1 (Step 000180): Train loss 5.241, Val loss 5.453\n",
      "Ep 1 (Step 000190): Train loss 5.352, Val loss 5.443\n",
      "Ep 1 (Step 000200): Train loss 5.186, Val loss 5.419\n",
      "Ep 1 (Step 000210): Train loss 5.303, Val loss 5.393\n",
      "Ep 1 (Step 000220): Train loss 5.191, Val loss 5.368\n",
      "Ep 1 (Step 000230): Train loss 5.202, Val loss 5.370\n",
      "Ep 1 (Step 000240): Train loss 5.151, Val loss 5.315\n",
      "Ep 1 (Step 000250): Train loss 5.178, Val loss 5.317\n",
      "Ep 1 (Step 000260): Train loss 5.227, Val loss 5.298\n",
      "Ep 1 (Step 000270): Train loss 5.253, Val loss 5.268\n",
      "Ep 1 (Step 000280): Train loss 5.191, Val loss 5.247\n",
      "Ep 1 (Step 000290): Train loss 5.165, Val loss 5.252\n",
      "Ep 1 (Step 000300): Train loss 5.043, Val loss 5.260\n",
      "Ep 1 (Step 000310): Train loss 5.074, Val loss 5.238\n",
      "Ep 1 (Step 000320): Train loss 5.037, Val loss 5.235\n",
      "Ep 1 (Step 000330): Train loss 5.160, Val loss 5.244\n",
      "Ep 1 (Step 000340): Train loss 5.038, Val loss 5.221\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2213\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.789, Val loss 8.785\n",
      "Ep 1 (Step 000010): Train loss 6.892, Val loss 6.875\n",
      "Ep 1 (Step 000020): Train loss 6.464, Val loss 6.479\n",
      "Ep 1 (Step 000030): Train loss 6.439, Val loss 6.441\n",
      "Ep 1 (Step 000040): Train loss 6.261, Val loss 6.301\n",
      "Ep 1 (Step 000050): Train loss 6.085, Val loss 6.129\n",
      "Ep 1 (Step 000060): Train loss 5.936, Val loss 6.008\n",
      "Ep 1 (Step 000070): Train loss 5.832, Val loss 5.917\n",
      "Ep 1 (Step 000080): Train loss 5.763, Val loss 5.848\n",
      "Ep 1 (Step 000090): Train loss 5.685, Val loss 5.763\n",
      "Ep 1 (Step 000100): Train loss 5.665, Val loss 5.702\n",
      "Ep 1 (Step 000110): Train loss 5.517, Val loss 5.652\n",
      "Ep 1 (Step 000120): Train loss 5.553, Val loss 5.603\n",
      "Ep 1 (Step 000130): Train loss 5.526, Val loss 5.567\n",
      "Ep 1 (Step 000140): Train loss 5.460, Val loss 5.516\n",
      "Ep 1 (Step 000150): Train loss 5.344, Val loss 5.485\n",
      "Ep 1 (Step 000160): Train loss 5.292, Val loss 5.484\n",
      "Ep 1 (Step 000170): Train loss 5.330, Val loss 5.443\n",
      "Ep 1 (Step 000180): Train loss 5.355, Val loss 5.417\n",
      "Ep 1 (Step 000190): Train loss 5.296, Val loss 5.404\n",
      "Ep 1 (Step 000200): Train loss 5.301, Val loss 5.392\n",
      "Ep 1 (Step 000210): Train loss 5.247, Val loss 5.374\n",
      "Ep 1 (Step 000220): Train loss 5.213, Val loss 5.348\n",
      "Ep 1 (Step 000230): Train loss 5.303, Val loss 5.336\n",
      "Ep 1 (Step 000240): Train loss 5.188, Val loss 5.332\n",
      "Ep 1 (Step 000250): Train loss 5.115, Val loss 5.328\n",
      "Ep 1 (Step 000260): Train loss 5.142, Val loss 5.305\n",
      "Ep 1 (Step 000270): Train loss 5.153, Val loss 5.287\n",
      "Ep 1 (Step 000280): Train loss 5.183, Val loss 5.270\n",
      "Ep 1 (Step 000290): Train loss 5.160, Val loss 5.279\n",
      "Ep 1 (Step 000300): Train loss 5.121, Val loss 5.261\n",
      "Ep 1 (Step 000310): Train loss 5.027, Val loss 5.240\n",
      "Ep 1 (Step 000320): Train loss 5.014, Val loss 5.232\n",
      "Ep 1 (Step 000330): Train loss 5.056, Val loss 5.228\n",
      "Ep 1 (Step 000340): Train loss 5.073, Val loss 5.226\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2259\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.844, Val loss 8.817\n",
      "Ep 1 (Step 000010): Train loss 6.875, Val loss 6.825\n",
      "Ep 1 (Step 000020): Train loss 6.494, Val loss 6.477\n",
      "Ep 1 (Step 000030): Train loss 6.442, Val loss 6.473\n",
      "Ep 1 (Step 000040): Train loss 6.368, Val loss 6.370\n",
      "Ep 1 (Step 000050): Train loss 6.275, Val loss 6.206\n",
      "Ep 1 (Step 000060): Train loss 6.036, Val loss 6.066\n",
      "Ep 1 (Step 000070): Train loss 5.940, Val loss 5.976\n",
      "Ep 1 (Step 000080): Train loss 5.828, Val loss 5.866\n",
      "Ep 1 (Step 000090): Train loss 5.750, Val loss 5.792\n",
      "Ep 1 (Step 000100): Train loss 5.644, Val loss 5.717\n",
      "Ep 1 (Step 000110): Train loss 5.556, Val loss 5.662\n",
      "Ep 1 (Step 000120): Train loss 5.589, Val loss 5.617\n",
      "Ep 1 (Step 000130): Train loss 5.460, Val loss 5.576\n",
      "Ep 1 (Step 000140): Train loss 5.444, Val loss 5.545\n",
      "Ep 1 (Step 000150): Train loss 5.377, Val loss 5.506\n",
      "Ep 1 (Step 000160): Train loss 5.316, Val loss 5.476\n",
      "Ep 1 (Step 000170): Train loss 5.301, Val loss 5.462\n",
      "Ep 1 (Step 000180): Train loss 5.366, Val loss 5.446\n",
      "Ep 1 (Step 000190): Train loss 5.271, Val loss 5.428\n",
      "Ep 1 (Step 000200): Train loss 5.362, Val loss 5.395\n",
      "Ep 1 (Step 000210): Train loss 5.275, Val loss 5.376\n",
      "Ep 1 (Step 000220): Train loss 5.234, Val loss 5.335\n",
      "Ep 1 (Step 000230): Train loss 5.191, Val loss 5.337\n",
      "Ep 1 (Step 000240): Train loss 5.153, Val loss 5.309\n",
      "Ep 1 (Step 000250): Train loss 5.145, Val loss 5.285\n",
      "Ep 1 (Step 000260): Train loss 5.155, Val loss 5.274\n",
      "Ep 1 (Step 000270): Train loss 5.176, Val loss 5.255\n",
      "Ep 1 (Step 000280): Train loss 5.193, Val loss 5.254\n",
      "Ep 1 (Step 000290): Train loss 5.116, Val loss 5.239\n",
      "Ep 1 (Step 000300): Train loss 5.115, Val loss 5.246\n",
      "Ep 1 (Step 000310): Train loss 5.108, Val loss 5.228\n",
      "Ep 1 (Step 000320): Train loss 5.223, Val loss 5.206\n",
      "Ep 1 (Step 000330): Train loss 5.052, Val loss 5.203\n",
      "Ep 1 (Step 000340): Train loss 5.122, Val loss 5.191\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1907\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.800, Val loss 8.806\n",
      "Ep 1 (Step 000010): Train loss 6.819, Val loss 6.815\n",
      "Ep 1 (Step 000020): Train loss 6.483, Val loss 6.488\n",
      "Ep 1 (Step 000030): Train loss 6.436, Val loss 6.473\n",
      "Ep 1 (Step 000040): Train loss 6.315, Val loss 6.317\n",
      "Ep 1 (Step 000050): Train loss 6.193, Val loss 6.128\n",
      "Ep 1 (Step 000060): Train loss 5.987, Val loss 5.990\n",
      "Ep 1 (Step 000070): Train loss 5.902, Val loss 5.890\n",
      "Ep 1 (Step 000080): Train loss 5.745, Val loss 5.811\n",
      "Ep 1 (Step 000090): Train loss 5.773, Val loss 5.741\n",
      "Ep 1 (Step 000100): Train loss 5.573, Val loss 5.697\n",
      "Ep 1 (Step 000110): Train loss 5.536, Val loss 5.634\n",
      "Ep 1 (Step 000120): Train loss 5.451, Val loss 5.588\n",
      "Ep 1 (Step 000130): Train loss 5.485, Val loss 5.542\n",
      "Ep 1 (Step 000140): Train loss 5.358, Val loss 5.528\n",
      "Ep 1 (Step 000150): Train loss 5.460, Val loss 5.504\n",
      "Ep 1 (Step 000160): Train loss 5.356, Val loss 5.483\n",
      "Ep 1 (Step 000170): Train loss 5.296, Val loss 5.452\n",
      "Ep 1 (Step 000180): Train loss 5.261, Val loss 5.394\n",
      "Ep 1 (Step 000190): Train loss 5.297, Val loss 5.396\n",
      "Ep 1 (Step 000200): Train loss 5.280, Val loss 5.383\n",
      "Ep 1 (Step 000210): Train loss 5.296, Val loss 5.357\n",
      "Ep 1 (Step 000220): Train loss 5.225, Val loss 5.348\n",
      "Ep 1 (Step 000230): Train loss 5.280, Val loss 5.322\n",
      "Ep 1 (Step 000240): Train loss 5.175, Val loss 5.304\n",
      "Ep 1 (Step 000250): Train loss 5.162, Val loss 5.293\n",
      "Ep 1 (Step 000260): Train loss 5.087, Val loss 5.274\n",
      "Ep 1 (Step 000270): Train loss 5.164, Val loss 5.255\n",
      "Ep 1 (Step 000280): Train loss 5.009, Val loss 5.245\n",
      "Ep 1 (Step 000290): Train loss 5.092, Val loss 5.229\n",
      "Ep 1 (Step 000300): Train loss 5.096, Val loss 5.239\n",
      "Ep 1 (Step 000310): Train loss 5.029, Val loss 5.230\n",
      "Ep 1 (Step 000320): Train loss 5.153, Val loss 5.204\n",
      "Ep 1 (Step 000330): Train loss 5.089, Val loss 5.203\n",
      "Ep 1 (Step 000340): Train loss 4.967, Val loss 5.201\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2011\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.796, Val loss 8.767\n",
      "Ep 1 (Step 000010): Train loss 6.860, Val loss 6.806\n",
      "Ep 1 (Step 000020): Train loss 6.479, Val loss 6.463\n",
      "Ep 1 (Step 000030): Train loss 6.429, Val loss 6.410\n",
      "Ep 1 (Step 000040): Train loss 6.382, Val loss 6.304\n",
      "Ep 1 (Step 000050): Train loss 6.158, Val loss 6.168\n",
      "Ep 1 (Step 000060): Train loss 5.994, Val loss 5.993\n",
      "Ep 1 (Step 000070): Train loss 5.912, Val loss 5.896\n",
      "Ep 1 (Step 000080): Train loss 5.781, Val loss 5.815\n",
      "Ep 1 (Step 000090): Train loss 5.669, Val loss 5.739\n",
      "Ep 1 (Step 000100): Train loss 5.749, Val loss 5.691\n",
      "Ep 1 (Step 000110): Train loss 5.644, Val loss 5.632\n",
      "Ep 1 (Step 000120): Train loss 5.472, Val loss 5.575\n",
      "Ep 1 (Step 000130): Train loss 5.496, Val loss 5.545\n",
      "Ep 1 (Step 000140): Train loss 5.530, Val loss 5.528\n",
      "Ep 1 (Step 000150): Train loss 5.398, Val loss 5.520\n",
      "Ep 1 (Step 000160): Train loss 5.423, Val loss 5.473\n",
      "Ep 1 (Step 000170): Train loss 5.351, Val loss 5.449\n",
      "Ep 1 (Step 000180): Train loss 5.296, Val loss 5.429\n",
      "Ep 1 (Step 000190): Train loss 5.309, Val loss 5.380\n",
      "Ep 1 (Step 000200): Train loss 5.332, Val loss 5.370\n",
      "Ep 1 (Step 000210): Train loss 5.266, Val loss 5.350\n",
      "Ep 1 (Step 000220): Train loss 5.240, Val loss 5.340\n",
      "Ep 1 (Step 000230): Train loss 5.220, Val loss 5.341\n",
      "Ep 1 (Step 000240): Train loss 5.165, Val loss 5.320\n",
      "Ep 1 (Step 000250): Train loss 5.129, Val loss 5.287\n",
      "Ep 1 (Step 000260): Train loss 5.067, Val loss 5.304\n",
      "Ep 1 (Step 000270): Train loss 5.073, Val loss 5.273\n",
      "Ep 1 (Step 000280): Train loss 5.104, Val loss 5.246\n",
      "Ep 1 (Step 000290): Train loss 5.115, Val loss 5.254\n",
      "Ep 1 (Step 000300): Train loss 5.022, Val loss 5.246\n",
      "Ep 1 (Step 000310): Train loss 5.059, Val loss 5.236\n",
      "Ep 1 (Step 000320): Train loss 5.025, Val loss 5.218\n",
      "Ep 1 (Step 000330): Train loss 5.077, Val loss 5.214\n",
      "Ep 1 (Step 000340): Train loss 5.020, Val loss 5.217\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2174\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.908, Val loss 8.899\n",
      "Ep 1 (Step 000010): Train loss 7.468, Val loss 7.408\n",
      "Ep 1 (Step 000020): Train loss 6.770, Val loss 6.779\n",
      "Ep 1 (Step 000030): Train loss 6.478, Val loss 6.468\n",
      "Ep 1 (Step 000040): Train loss 6.390, Val loss 6.386\n",
      "Ep 1 (Step 000050): Train loss 6.328, Val loss 6.337\n",
      "Ep 1 (Step 000060): Train loss 6.224, Val loss 6.249\n",
      "Ep 1 (Step 000070): Train loss 6.163, Val loss 6.146\n",
      "Ep 1 (Step 000080): Train loss 6.060, Val loss 6.104\n",
      "Ep 1 (Step 000090): Train loss 6.031, Val loss 5.986\n",
      "Ep 1 (Step 000100): Train loss 5.840, Val loss 5.926\n",
      "Ep 1 (Step 000110): Train loss 5.789, Val loss 5.883\n",
      "Ep 1 (Step 000120): Train loss 5.782, Val loss 5.829\n",
      "Ep 1 (Step 000130): Train loss 5.722, Val loss 5.789\n",
      "Ep 1 (Step 000140): Train loss 5.695, Val loss 5.746\n",
      "Ep 1 (Step 000150): Train loss 5.740, Val loss 5.727\n",
      "Ep 1 (Step 000160): Train loss 5.626, Val loss 5.665\n",
      "Ep 1 (Step 000170): Train loss 5.516, Val loss 5.639\n",
      "Ep 1 (Step 000180): Train loss 5.553, Val loss 5.606\n",
      "Ep 1 (Step 000190): Train loss 5.474, Val loss 5.582\n",
      "Ep 1 (Step 000200): Train loss 5.519, Val loss 5.556\n",
      "Ep 1 (Step 000210): Train loss 5.458, Val loss 5.540\n",
      "Ep 1 (Step 000220): Train loss 5.448, Val loss 5.512\n",
      "Ep 1 (Step 000230): Train loss 5.478, Val loss 5.502\n",
      "Ep 1 (Step 000240): Train loss 5.400, Val loss 5.497\n",
      "Ep 1 (Step 000250): Train loss 5.360, Val loss 5.478\n",
      "Ep 1 (Step 000260): Train loss 5.400, Val loss 5.471\n",
      "Ep 1 (Step 000270): Train loss 5.352, Val loss 5.448\n",
      "Ep 1 (Step 000280): Train loss 5.360, Val loss 5.422\n",
      "Ep 1 (Step 000290): Train loss 5.361, Val loss 5.422\n",
      "Ep 1 (Step 000300): Train loss 5.311, Val loss 5.403\n",
      "Ep 1 (Step 000310): Train loss 5.278, Val loss 5.394\n",
      "Ep 1 (Step 000320): Train loss 5.313, Val loss 5.394\n",
      "Ep 1 (Step 000330): Train loss 5.330, Val loss 5.378\n",
      "Ep 1 (Step 000340): Train loss 5.192, Val loss 5.368\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3683\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.920, Val loss 8.915\n",
      "Ep 1 (Step 000010): Train loss 7.441, Val loss 7.416\n",
      "Ep 1 (Step 000020): Train loss 6.782, Val loss 6.770\n",
      "Ep 1 (Step 000030): Train loss 6.505, Val loss 6.468\n",
      "Ep 1 (Step 000040): Train loss 6.357, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.293, Val loss 6.340\n",
      "Ep 1 (Step 000060): Train loss 6.256, Val loss 6.264\n",
      "Ep 1 (Step 000070): Train loss 6.092, Val loss 6.145\n",
      "Ep 1 (Step 000080): Train loss 6.096, Val loss 6.071\n",
      "Ep 1 (Step 000090): Train loss 5.968, Val loss 5.989\n",
      "Ep 1 (Step 000100): Train loss 5.910, Val loss 5.924\n",
      "Ep 1 (Step 000110): Train loss 5.821, Val loss 5.860\n",
      "Ep 1 (Step 000120): Train loss 5.768, Val loss 5.818\n",
      "Ep 1 (Step 000130): Train loss 5.731, Val loss 5.776\n",
      "Ep 1 (Step 000140): Train loss 5.724, Val loss 5.738\n",
      "Ep 1 (Step 000150): Train loss 5.714, Val loss 5.709\n",
      "Ep 1 (Step 000160): Train loss 5.550, Val loss 5.662\n",
      "Ep 1 (Step 000170): Train loss 5.564, Val loss 5.655\n",
      "Ep 1 (Step 000180): Train loss 5.646, Val loss 5.609\n",
      "Ep 1 (Step 000190): Train loss 5.525, Val loss 5.588\n",
      "Ep 1 (Step 000200): Train loss 5.537, Val loss 5.574\n",
      "Ep 1 (Step 000210): Train loss 5.481, Val loss 5.543\n",
      "Ep 1 (Step 000220): Train loss 5.486, Val loss 5.535\n",
      "Ep 1 (Step 000230): Train loss 5.388, Val loss 5.506\n",
      "Ep 1 (Step 000240): Train loss 5.409, Val loss 5.492\n",
      "Ep 1 (Step 000250): Train loss 5.372, Val loss 5.479\n",
      "Ep 1 (Step 000260): Train loss 5.268, Val loss 5.464\n",
      "Ep 1 (Step 000270): Train loss 5.310, Val loss 5.438\n",
      "Ep 1 (Step 000280): Train loss 5.341, Val loss 5.432\n",
      "Ep 1 (Step 000290): Train loss 5.287, Val loss 5.417\n",
      "Ep 1 (Step 000300): Train loss 5.304, Val loss 5.388\n",
      "Ep 1 (Step 000310): Train loss 5.277, Val loss 5.373\n",
      "Ep 1 (Step 000320): Train loss 5.236, Val loss 5.348\n",
      "Ep 1 (Step 000330): Train loss 5.256, Val loss 5.356\n",
      "Ep 1 (Step 000340): Train loss 5.303, Val loss 5.349\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3492\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.908, Val loss 8.877\n",
      "Ep 1 (Step 000010): Train loss 7.464, Val loss 7.418\n",
      "Ep 1 (Step 000020): Train loss 6.742, Val loss 6.772\n",
      "Ep 1 (Step 000030): Train loss 6.495, Val loss 6.476\n",
      "Ep 1 (Step 000040): Train loss 6.369, Val loss 6.379\n",
      "Ep 1 (Step 000050): Train loss 6.318, Val loss 6.336\n",
      "Ep 1 (Step 000060): Train loss 6.205, Val loss 6.235\n",
      "Ep 1 (Step 000070): Train loss 6.170, Val loss 6.170\n",
      "Ep 1 (Step 000080): Train loss 6.021, Val loss 6.042\n",
      "Ep 1 (Step 000090): Train loss 5.940, Val loss 5.965\n",
      "Ep 1 (Step 000100): Train loss 5.840, Val loss 5.914\n",
      "Ep 1 (Step 000110): Train loss 5.804, Val loss 5.834\n",
      "Ep 1 (Step 000120): Train loss 5.735, Val loss 5.781\n",
      "Ep 1 (Step 000130): Train loss 5.715, Val loss 5.756\n",
      "Ep 1 (Step 000140): Train loss 5.701, Val loss 5.727\n",
      "Ep 1 (Step 000150): Train loss 5.721, Val loss 5.685\n",
      "Ep 1 (Step 000160): Train loss 5.627, Val loss 5.655\n",
      "Ep 1 (Step 000170): Train loss 5.562, Val loss 5.626\n",
      "Ep 1 (Step 000180): Train loss 5.508, Val loss 5.617\n",
      "Ep 1 (Step 000190): Train loss 5.535, Val loss 5.583\n",
      "Ep 1 (Step 000200): Train loss 5.507, Val loss 5.572\n",
      "Ep 1 (Step 000210): Train loss 5.507, Val loss 5.548\n",
      "Ep 1 (Step 000220): Train loss 5.554, Val loss 5.528\n",
      "Ep 1 (Step 000230): Train loss 5.420, Val loss 5.487\n",
      "Ep 1 (Step 000240): Train loss 5.388, Val loss 5.484\n",
      "Ep 1 (Step 000250): Train loss 5.386, Val loss 5.473\n",
      "Ep 1 (Step 000260): Train loss 5.418, Val loss 5.464\n",
      "Ep 1 (Step 000270): Train loss 5.337, Val loss 5.447\n",
      "Ep 1 (Step 000280): Train loss 5.289, Val loss 5.430\n",
      "Ep 1 (Step 000290): Train loss 5.284, Val loss 5.419\n",
      "Ep 1 (Step 000300): Train loss 5.311, Val loss 5.407\n",
      "Ep 1 (Step 000310): Train loss 5.215, Val loss 5.404\n",
      "Ep 1 (Step 000320): Train loss 5.173, Val loss 5.388\n",
      "Ep 1 (Step 000330): Train loss 5.314, Val loss 5.375\n",
      "Ep 1 (Step 000340): Train loss 5.284, Val loss 5.350\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3502\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.920, Val loss 8.903\n",
      "Ep 1 (Step 000010): Train loss 7.460, Val loss 7.424\n",
      "Ep 1 (Step 000020): Train loss 6.828, Val loss 6.789\n",
      "Ep 1 (Step 000030): Train loss 6.467, Val loss 6.477\n",
      "Ep 1 (Step 000040): Train loss 6.406, Val loss 6.392\n",
      "Ep 1 (Step 000050): Train loss 6.370, Val loss 6.363\n",
      "Ep 1 (Step 000060): Train loss 6.251, Val loss 6.287\n",
      "Ep 1 (Step 000070): Train loss 6.160, Val loss 6.159\n",
      "Ep 1 (Step 000080): Train loss 6.125, Val loss 6.113\n",
      "Ep 1 (Step 000090): Train loss 6.023, Val loss 6.025\n",
      "Ep 1 (Step 000100): Train loss 5.932, Val loss 5.960\n",
      "Ep 1 (Step 000110): Train loss 5.878, Val loss 5.887\n",
      "Ep 1 (Step 000120): Train loss 5.814, Val loss 5.846\n",
      "Ep 1 (Step 000130): Train loss 5.799, Val loss 5.792\n",
      "Ep 1 (Step 000140): Train loss 5.736, Val loss 5.753\n",
      "Ep 1 (Step 000150): Train loss 5.649, Val loss 5.712\n",
      "Ep 1 (Step 000160): Train loss 5.637, Val loss 5.665\n",
      "Ep 1 (Step 000170): Train loss 5.515, Val loss 5.643\n",
      "Ep 1 (Step 000180): Train loss 5.581, Val loss 5.613\n",
      "Ep 1 (Step 000190): Train loss 5.480, Val loss 5.574\n",
      "Ep 1 (Step 000200): Train loss 5.466, Val loss 5.554\n",
      "Ep 1 (Step 000210): Train loss 5.439, Val loss 5.539\n",
      "Ep 1 (Step 000220): Train loss 5.472, Val loss 5.514\n",
      "Ep 1 (Step 000230): Train loss 5.416, Val loss 5.500\n",
      "Ep 1 (Step 000240): Train loss 5.387, Val loss 5.490\n",
      "Ep 1 (Step 000250): Train loss 5.321, Val loss 5.469\n",
      "Ep 1 (Step 000260): Train loss 5.312, Val loss 5.447\n",
      "Ep 1 (Step 000270): Train loss 5.298, Val loss 5.434\n",
      "Ep 1 (Step 000280): Train loss 5.276, Val loss 5.414\n",
      "Ep 1 (Step 000290): Train loss 5.307, Val loss 5.407\n",
      "Ep 1 (Step 000300): Train loss 5.288, Val loss 5.396\n",
      "Ep 1 (Step 000310): Train loss 5.147, Val loss 5.395\n",
      "Ep 1 (Step 000320): Train loss 5.256, Val loss 5.361\n",
      "Ep 1 (Step 000330): Train loss 5.287, Val loss 5.349\n",
      "Ep 1 (Step 000340): Train loss 5.213, Val loss 5.339\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3393\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.921, Val loss 8.912\n",
      "Ep 1 (Step 000010): Train loss 7.450, Val loss 7.408\n",
      "Ep 1 (Step 000020): Train loss 6.725, Val loss 6.762\n",
      "Ep 1 (Step 000030): Train loss 6.480, Val loss 6.470\n",
      "Ep 1 (Step 000040): Train loss 6.405, Val loss 6.389\n",
      "Ep 1 (Step 000050): Train loss 6.422, Val loss 6.377\n",
      "Ep 1 (Step 000060): Train loss 6.212, Val loss 6.282\n",
      "Ep 1 (Step 000070): Train loss 6.109, Val loss 6.123\n",
      "Ep 1 (Step 000080): Train loss 5.941, Val loss 6.053\n",
      "Ep 1 (Step 000090): Train loss 5.944, Val loss 5.991\n",
      "Ep 1 (Step 000100): Train loss 5.897, Val loss 5.915\n",
      "Ep 1 (Step 000110): Train loss 5.912, Val loss 5.871\n",
      "Ep 1 (Step 000120): Train loss 5.744, Val loss 5.800\n",
      "Ep 1 (Step 000130): Train loss 5.732, Val loss 5.774\n",
      "Ep 1 (Step 000140): Train loss 5.641, Val loss 5.739\n",
      "Ep 1 (Step 000150): Train loss 5.564, Val loss 5.692\n",
      "Ep 1 (Step 000160): Train loss 5.563, Val loss 5.679\n",
      "Ep 1 (Step 000170): Train loss 5.551, Val loss 5.625\n",
      "Ep 1 (Step 000180): Train loss 5.506, Val loss 5.609\n",
      "Ep 1 (Step 000190): Train loss 5.500, Val loss 5.593\n",
      "Ep 1 (Step 000200): Train loss 5.466, Val loss 5.570\n",
      "Ep 1 (Step 000210): Train loss 5.550, Val loss 5.541\n",
      "Ep 1 (Step 000220): Train loss 5.408, Val loss 5.515\n",
      "Ep 1 (Step 000230): Train loss 5.413, Val loss 5.504\n",
      "Ep 1 (Step 000240): Train loss 5.453, Val loss 5.475\n",
      "Ep 1 (Step 000250): Train loss 5.455, Val loss 5.461\n",
      "Ep 1 (Step 000260): Train loss 5.296, Val loss 5.457\n",
      "Ep 1 (Step 000270): Train loss 5.350, Val loss 5.435\n",
      "Ep 1 (Step 000280): Train loss 5.316, Val loss 5.425\n",
      "Ep 1 (Step 000290): Train loss 5.302, Val loss 5.407\n",
      "Ep 1 (Step 000300): Train loss 5.269, Val loss 5.399\n",
      "Ep 1 (Step 000310): Train loss 5.249, Val loss 5.391\n",
      "Ep 1 (Step 000320): Train loss 5.257, Val loss 5.383\n",
      "Ep 1 (Step 000330): Train loss 5.263, Val loss 5.359\n",
      "Ep 1 (Step 000340): Train loss 5.251, Val loss 5.366\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3661\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.936, Val loss 8.941\n",
      "Ep 1 (Step 000010): Train loss 7.405, Val loss 7.442\n",
      "Ep 1 (Step 000020): Train loss 6.817, Val loss 6.770\n",
      "Ep 1 (Step 000030): Train loss 6.490, Val loss 6.453\n",
      "Ep 1 (Step 000040): Train loss 6.400, Val loss 6.373\n",
      "Ep 1 (Step 000050): Train loss 6.327, Val loss 6.325\n",
      "Ep 1 (Step 000060): Train loss 6.235, Val loss 6.240\n",
      "Ep 1 (Step 000070): Train loss 6.122, Val loss 6.142\n",
      "Ep 1 (Step 000080): Train loss 6.013, Val loss 6.049\n",
      "Ep 1 (Step 000090): Train loss 5.925, Val loss 6.004\n",
      "Ep 1 (Step 000100): Train loss 5.951, Val loss 5.932\n",
      "Ep 1 (Step 000110): Train loss 5.778, Val loss 5.864\n",
      "Ep 1 (Step 000120): Train loss 5.736, Val loss 5.832\n",
      "Ep 1 (Step 000130): Train loss 5.742, Val loss 5.780\n",
      "Ep 1 (Step 000140): Train loss 5.712, Val loss 5.731\n",
      "Ep 1 (Step 000150): Train loss 5.687, Val loss 5.707\n",
      "Ep 1 (Step 000160): Train loss 5.562, Val loss 5.666\n",
      "Ep 1 (Step 000170): Train loss 5.573, Val loss 5.641\n",
      "Ep 1 (Step 000180): Train loss 5.501, Val loss 5.604\n",
      "Ep 1 (Step 000190): Train loss 5.514, Val loss 5.575\n",
      "Ep 1 (Step 000200): Train loss 5.467, Val loss 5.543\n",
      "Ep 1 (Step 000210): Train loss 5.479, Val loss 5.510\n",
      "Ep 1 (Step 000220): Train loss 5.505, Val loss 5.506\n",
      "Ep 1 (Step 000230): Train loss 5.360, Val loss 5.494\n",
      "Ep 1 (Step 000240): Train loss 5.399, Val loss 5.473\n",
      "Ep 1 (Step 000250): Train loss 5.390, Val loss 5.453\n",
      "Ep 1 (Step 000260): Train loss 5.341, Val loss 5.431\n",
      "Ep 1 (Step 000270): Train loss 5.369, Val loss 5.421\n",
      "Ep 1 (Step 000280): Train loss 5.276, Val loss 5.404\n",
      "Ep 1 (Step 000290): Train loss 5.342, Val loss 5.385\n",
      "Ep 1 (Step 000300): Train loss 5.306, Val loss 5.373\n",
      "Ep 1 (Step 000310): Train loss 5.288, Val loss 5.359\n",
      "Ep 1 (Step 000320): Train loss 5.221, Val loss 5.363\n",
      "Ep 1 (Step 000330): Train loss 5.237, Val loss 5.341\n",
      "Ep 1 (Step 000340): Train loss 5.197, Val loss 5.333\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3332\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.666, Val loss 8.684\n",
      "Ep 1 (Step 000010): Train loss 6.881, Val loss 6.863\n",
      "Ep 1 (Step 000020): Train loss 6.510, Val loss 6.464\n",
      "Ep 1 (Step 000030): Train loss 6.517, Val loss 6.458\n",
      "Ep 1 (Step 000040): Train loss 6.302, Val loss 6.316\n",
      "Ep 1 (Step 000050): Train loss 6.106, Val loss 6.176\n",
      "Ep 1 (Step 000060): Train loss 6.077, Val loss 6.056\n",
      "Ep 1 (Step 000070): Train loss 6.003, Val loss 5.953\n",
      "Ep 1 (Step 000080): Train loss 5.749, Val loss 5.877\n",
      "Ep 1 (Step 000090): Train loss 5.767, Val loss 5.819\n",
      "Ep 1 (Step 000100): Train loss 5.715, Val loss 5.755\n",
      "Ep 1 (Step 000110): Train loss 5.559, Val loss 5.682\n",
      "Ep 1 (Step 000120): Train loss 5.588, Val loss 5.670\n",
      "Ep 1 (Step 000130): Train loss 5.544, Val loss 5.625\n",
      "Ep 1 (Step 000140): Train loss 5.400, Val loss 5.569\n",
      "Ep 1 (Step 000150): Train loss 5.489, Val loss 5.543\n",
      "Ep 1 (Step 000160): Train loss 5.483, Val loss 5.533\n",
      "Ep 1 (Step 000170): Train loss 5.418, Val loss 5.519\n",
      "Ep 1 (Step 000180): Train loss 5.476, Val loss 5.488\n",
      "Ep 1 (Step 000190): Train loss 5.342, Val loss 5.447\n",
      "Ep 1 (Step 000200): Train loss 5.364, Val loss 5.438\n",
      "Ep 1 (Step 000210): Train loss 5.372, Val loss 5.419\n",
      "Ep 1 (Step 000220): Train loss 5.319, Val loss 5.419\n",
      "Ep 1 (Step 000230): Train loss 5.296, Val loss 5.414\n",
      "Ep 1 (Step 000240): Train loss 5.191, Val loss 5.395\n",
      "Ep 1 (Step 000250): Train loss 5.267, Val loss 5.371\n",
      "Ep 1 (Step 000260): Train loss 5.259, Val loss 5.357\n",
      "Ep 1 (Step 000270): Train loss 5.194, Val loss 5.346\n",
      "Ep 1 (Step 000280): Train loss 5.151, Val loss 5.326\n",
      "Ep 1 (Step 000290): Train loss 5.123, Val loss 5.317\n",
      "Ep 1 (Step 000300): Train loss 5.107, Val loss 5.312\n",
      "Ep 1 (Step 000310): Train loss 5.298, Val loss 5.297\n",
      "Ep 1 (Step 000320): Train loss 5.096, Val loss 5.280\n",
      "Ep 1 (Step 000330): Train loss 5.185, Val loss 5.266\n",
      "Ep 1 (Step 000340): Train loss 5.096, Val loss 5.281\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2807\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.654, Val loss 8.617\n",
      "Ep 1 (Step 000010): Train loss 6.827, Val loss 6.809\n",
      "Ep 1 (Step 000020): Train loss 6.480, Val loss 6.484\n",
      "Ep 1 (Step 000030): Train loss 6.417, Val loss 6.485\n",
      "Ep 1 (Step 000040): Train loss 6.242, Val loss 6.312\n",
      "Ep 1 (Step 000050): Train loss 6.169, Val loss 6.186\n",
      "Ep 1 (Step 000060): Train loss 6.020, Val loss 6.078\n",
      "Ep 1 (Step 000070): Train loss 5.981, Val loss 5.970\n",
      "Ep 1 (Step 000080): Train loss 5.817, Val loss 5.870\n",
      "Ep 1 (Step 000090): Train loss 5.767, Val loss 5.790\n",
      "Ep 1 (Step 000100): Train loss 5.742, Val loss 5.759\n",
      "Ep 1 (Step 000110): Train loss 5.602, Val loss 5.694\n",
      "Ep 1 (Step 000120): Train loss 5.567, Val loss 5.674\n",
      "Ep 1 (Step 000130): Train loss 5.584, Val loss 5.621\n",
      "Ep 1 (Step 000140): Train loss 5.419, Val loss 5.595\n",
      "Ep 1 (Step 000150): Train loss 5.485, Val loss 5.547\n",
      "Ep 1 (Step 000160): Train loss 5.533, Val loss 5.513\n",
      "Ep 1 (Step 000170): Train loss 5.405, Val loss 5.502\n",
      "Ep 1 (Step 000180): Train loss 5.281, Val loss 5.476\n",
      "Ep 1 (Step 000190): Train loss 5.294, Val loss 5.459\n",
      "Ep 1 (Step 000200): Train loss 5.384, Val loss 5.443\n",
      "Ep 1 (Step 000210): Train loss 5.374, Val loss 5.430\n",
      "Ep 1 (Step 000220): Train loss 5.349, Val loss 5.408\n",
      "Ep 1 (Step 000230): Train loss 5.303, Val loss 5.391\n",
      "Ep 1 (Step 000240): Train loss 5.285, Val loss 5.374\n",
      "Ep 1 (Step 000250): Train loss 5.235, Val loss 5.362\n",
      "Ep 1 (Step 000260): Train loss 5.218, Val loss 5.333\n",
      "Ep 1 (Step 000270): Train loss 5.295, Val loss 5.320\n",
      "Ep 1 (Step 000280): Train loss 5.215, Val loss 5.333\n",
      "Ep 1 (Step 000290): Train loss 5.172, Val loss 5.323\n",
      "Ep 1 (Step 000300): Train loss 5.160, Val loss 5.316\n",
      "Ep 1 (Step 000310): Train loss 5.173, Val loss 5.303\n",
      "Ep 1 (Step 000320): Train loss 5.121, Val loss 5.291\n",
      "Ep 1 (Step 000330): Train loss 5.140, Val loss 5.269\n",
      "Ep 1 (Step 000340): Train loss 5.218, Val loss 5.257\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2568\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.690, Val loss 8.674\n",
      "Ep 1 (Step 000010): Train loss 6.907, Val loss 6.888\n",
      "Ep 1 (Step 000020): Train loss 6.423, Val loss 6.503\n",
      "Ep 1 (Step 000030): Train loss 6.384, Val loss 6.456\n",
      "Ep 1 (Step 000040): Train loss 6.357, Val loss 6.287\n",
      "Ep 1 (Step 000050): Train loss 6.193, Val loss 6.148\n",
      "Ep 1 (Step 000060): Train loss 5.966, Val loss 6.018\n",
      "Ep 1 (Step 000070): Train loss 5.913, Val loss 5.938\n",
      "Ep 1 (Step 000080): Train loss 5.741, Val loss 5.840\n",
      "Ep 1 (Step 000090): Train loss 5.753, Val loss 5.774\n",
      "Ep 1 (Step 000100): Train loss 5.623, Val loss 5.721\n",
      "Ep 1 (Step 000110): Train loss 5.595, Val loss 5.663\n",
      "Ep 1 (Step 000120): Train loss 5.626, Val loss 5.640\n",
      "Ep 1 (Step 000130): Train loss 5.463, Val loss 5.599\n",
      "Ep 1 (Step 000140): Train loss 5.452, Val loss 5.583\n",
      "Ep 1 (Step 000150): Train loss 5.513, Val loss 5.548\n",
      "Ep 1 (Step 000160): Train loss 5.446, Val loss 5.511\n",
      "Ep 1 (Step 000170): Train loss 5.430, Val loss 5.491\n",
      "Ep 1 (Step 000180): Train loss 5.329, Val loss 5.456\n",
      "Ep 1 (Step 000190): Train loss 5.392, Val loss 5.445\n",
      "Ep 1 (Step 000200): Train loss 5.278, Val loss 5.426\n",
      "Ep 1 (Step 000210): Train loss 5.316, Val loss 5.416\n",
      "Ep 1 (Step 000220): Train loss 5.342, Val loss 5.430\n",
      "Ep 1 (Step 000230): Train loss 5.339, Val loss 5.391\n",
      "Ep 1 (Step 000240): Train loss 5.239, Val loss 5.375\n",
      "Ep 1 (Step 000250): Train loss 5.227, Val loss 5.375\n",
      "Ep 1 (Step 000260): Train loss 5.220, Val loss 5.360\n",
      "Ep 1 (Step 000270): Train loss 5.267, Val loss 5.326\n",
      "Ep 1 (Step 000280): Train loss 5.152, Val loss 5.323\n",
      "Ep 1 (Step 000290): Train loss 5.195, Val loss 5.310\n",
      "Ep 1 (Step 000300): Train loss 5.143, Val loss 5.319\n",
      "Ep 1 (Step 000310): Train loss 5.171, Val loss 5.316\n",
      "Ep 1 (Step 000320): Train loss 5.154, Val loss 5.303\n",
      "Ep 1 (Step 000330): Train loss 5.060, Val loss 5.285\n",
      "Ep 1 (Step 000340): Train loss 5.178, Val loss 5.264\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2639\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.571, Val loss 8.563\n",
      "Ep 1 (Step 000010): Train loss 6.816, Val loss 6.811\n",
      "Ep 1 (Step 000020): Train loss 6.504, Val loss 6.456\n",
      "Ep 1 (Step 000030): Train loss 6.386, Val loss 6.425\n",
      "Ep 1 (Step 000040): Train loss 6.343, Val loss 6.265\n",
      "Ep 1 (Step 000050): Train loss 6.122, Val loss 6.146\n",
      "Ep 1 (Step 000060): Train loss 5.960, Val loss 6.011\n",
      "Ep 1 (Step 000070): Train loss 5.861, Val loss 5.938\n",
      "Ep 1 (Step 000080): Train loss 5.850, Val loss 5.858\n",
      "Ep 1 (Step 000090): Train loss 5.724, Val loss 5.778\n",
      "Ep 1 (Step 000100): Train loss 5.698, Val loss 5.746\n",
      "Ep 1 (Step 000110): Train loss 5.629, Val loss 5.669\n",
      "Ep 1 (Step 000120): Train loss 5.640, Val loss 5.642\n",
      "Ep 1 (Step 000130): Train loss 5.475, Val loss 5.610\n",
      "Ep 1 (Step 000140): Train loss 5.434, Val loss 5.565\n",
      "Ep 1 (Step 000150): Train loss 5.493, Val loss 5.554\n",
      "Ep 1 (Step 000160): Train loss 5.374, Val loss 5.510\n",
      "Ep 1 (Step 000170): Train loss 5.424, Val loss 5.499\n",
      "Ep 1 (Step 000180): Train loss 5.365, Val loss 5.467\n",
      "Ep 1 (Step 000190): Train loss 5.305, Val loss 5.459\n",
      "Ep 1 (Step 000200): Train loss 5.317, Val loss 5.428\n",
      "Ep 1 (Step 000210): Train loss 5.241, Val loss 5.410\n",
      "Ep 1 (Step 000220): Train loss 5.298, Val loss 5.390\n",
      "Ep 1 (Step 000230): Train loss 5.268, Val loss 5.358\n",
      "Ep 1 (Step 000240): Train loss 5.220, Val loss 5.356\n",
      "Ep 1 (Step 000250): Train loss 5.321, Val loss 5.326\n",
      "Ep 1 (Step 000260): Train loss 5.161, Val loss 5.320\n",
      "Ep 1 (Step 000270): Train loss 5.294, Val loss 5.305\n",
      "Ep 1 (Step 000280): Train loss 5.198, Val loss 5.300\n",
      "Ep 1 (Step 000290): Train loss 5.142, Val loss 5.298\n",
      "Ep 1 (Step 000300): Train loss 5.181, Val loss 5.261\n",
      "Ep 1 (Step 000310): Train loss 5.169, Val loss 5.277\n",
      "Ep 1 (Step 000320): Train loss 5.242, Val loss 5.250\n",
      "Ep 1 (Step 000330): Train loss 5.118, Val loss 5.241\n",
      "Ep 1 (Step 000340): Train loss 5.079, Val loss 5.231\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2312\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.662, Val loss 8.634\n",
      "Ep 1 (Step 000010): Train loss 6.894, Val loss 6.823\n",
      "Ep 1 (Step 000020): Train loss 6.480, Val loss 6.448\n",
      "Ep 1 (Step 000030): Train loss 6.402, Val loss 6.433\n",
      "Ep 1 (Step 000040): Train loss 6.365, Val loss 6.345\n",
      "Ep 1 (Step 000050): Train loss 6.215, Val loss 6.233\n",
      "Ep 1 (Step 000060): Train loss 6.062, Val loss 6.134\n",
      "Ep 1 (Step 000070): Train loss 5.916, Val loss 5.997\n",
      "Ep 1 (Step 000080): Train loss 5.841, Val loss 5.905\n",
      "Ep 1 (Step 000090): Train loss 5.769, Val loss 5.841\n",
      "Ep 1 (Step 000100): Train loss 5.659, Val loss 5.784\n",
      "Ep 1 (Step 000110): Train loss 5.718, Val loss 5.711\n",
      "Ep 1 (Step 000120): Train loss 5.603, Val loss 5.660\n",
      "Ep 1 (Step 000130): Train loss 5.582, Val loss 5.617\n",
      "Ep 1 (Step 000140): Train loss 5.529, Val loss 5.596\n",
      "Ep 1 (Step 000150): Train loss 5.575, Val loss 5.570\n",
      "Ep 1 (Step 000160): Train loss 5.389, Val loss 5.516\n",
      "Ep 1 (Step 000170): Train loss 5.438, Val loss 5.515\n",
      "Ep 1 (Step 000180): Train loss 5.447, Val loss 5.503\n",
      "Ep 1 (Step 000190): Train loss 5.326, Val loss 5.479\n",
      "Ep 1 (Step 000200): Train loss 5.405, Val loss 5.450\n",
      "Ep 1 (Step 000210): Train loss 5.325, Val loss 5.440\n",
      "Ep 1 (Step 000220): Train loss 5.309, Val loss 5.410\n",
      "Ep 1 (Step 000230): Train loss 5.322, Val loss 5.372\n",
      "Ep 1 (Step 000240): Train loss 5.202, Val loss 5.359\n",
      "Ep 1 (Step 000250): Train loss 5.222, Val loss 5.358\n",
      "Ep 1 (Step 000260): Train loss 5.220, Val loss 5.362\n",
      "Ep 1 (Step 000270): Train loss 5.237, Val loss 5.357\n",
      "Ep 1 (Step 000280): Train loss 5.174, Val loss 5.322\n",
      "Ep 1 (Step 000290): Train loss 5.144, Val loss 5.307\n",
      "Ep 1 (Step 000300): Train loss 5.155, Val loss 5.291\n",
      "Ep 1 (Step 000310): Train loss 5.160, Val loss 5.288\n",
      "Ep 1 (Step 000320): Train loss 5.134, Val loss 5.251\n",
      "Ep 1 (Step 000330): Train loss 5.174, Val loss 5.254\n",
      "Ep 1 (Step 000340): Train loss 5.250, Val loss 5.247\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2468\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.615, Val loss 8.592\n",
      "Ep 1 (Step 000010): Train loss 6.811, Val loss 6.803\n",
      "Ep 1 (Step 000020): Train loss 6.496, Val loss 6.456\n",
      "Ep 1 (Step 000030): Train loss 6.439, Val loss 6.444\n",
      "Ep 1 (Step 000040): Train loss 6.356, Val loss 6.329\n",
      "Ep 1 (Step 000050): Train loss 6.198, Val loss 6.211\n",
      "Ep 1 (Step 000060): Train loss 5.981, Val loss 6.070\n",
      "Ep 1 (Step 000070): Train loss 5.909, Val loss 5.956\n",
      "Ep 1 (Step 000080): Train loss 5.792, Val loss 5.866\n",
      "Ep 1 (Step 000090): Train loss 5.806, Val loss 5.797\n",
      "Ep 1 (Step 000100): Train loss 5.684, Val loss 5.739\n",
      "Ep 1 (Step 000110): Train loss 5.722, Val loss 5.693\n",
      "Ep 1 (Step 000120): Train loss 5.509, Val loss 5.653\n",
      "Ep 1 (Step 000130): Train loss 5.558, Val loss 5.605\n",
      "Ep 1 (Step 000140): Train loss 5.449, Val loss 5.553\n",
      "Ep 1 (Step 000150): Train loss 5.452, Val loss 5.527\n",
      "Ep 1 (Step 000160): Train loss 5.413, Val loss 5.503\n",
      "Ep 1 (Step 000170): Train loss 5.427, Val loss 5.480\n",
      "Ep 1 (Step 000180): Train loss 5.413, Val loss 5.462\n",
      "Ep 1 (Step 000190): Train loss 5.404, Val loss 5.445\n",
      "Ep 1 (Step 000200): Train loss 5.308, Val loss 5.443\n",
      "Ep 1 (Step 000210): Train loss 5.302, Val loss 5.414\n",
      "Ep 1 (Step 000220): Train loss 5.297, Val loss 5.396\n",
      "Ep 1 (Step 000230): Train loss 5.227, Val loss 5.397\n",
      "Ep 1 (Step 000240): Train loss 5.201, Val loss 5.361\n",
      "Ep 1 (Step 000250): Train loss 5.166, Val loss 5.331\n",
      "Ep 1 (Step 000260): Train loss 5.174, Val loss 5.325\n",
      "Ep 1 (Step 000270): Train loss 5.204, Val loss 5.302\n",
      "Ep 1 (Step 000280): Train loss 5.177, Val loss 5.287\n",
      "Ep 1 (Step 000290): Train loss 5.211, Val loss 5.292\n",
      "Ep 1 (Step 000300): Train loss 5.164, Val loss 5.279\n",
      "Ep 1 (Step 000310): Train loss 5.189, Val loss 5.264\n",
      "Ep 1 (Step 000320): Train loss 5.061, Val loss 5.242\n",
      "Ep 1 (Step 000330): Train loss 5.147, Val loss 5.239\n",
      "Ep 1 (Step 000340): Train loss 5.152, Val loss 5.236\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2364\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.870, Val loss 8.891\n",
      "Ep 1 (Step 000010): Train loss 7.396, Val loss 7.402\n",
      "Ep 1 (Step 000020): Train loss 6.759, Val loss 6.788\n",
      "Ep 1 (Step 000030): Train loss 6.495, Val loss 6.482\n",
      "Ep 1 (Step 000040): Train loss 6.387, Val loss 6.399\n",
      "Ep 1 (Step 000050): Train loss 6.306, Val loss 6.355\n",
      "Ep 1 (Step 000060): Train loss 6.204, Val loss 6.254\n",
      "Ep 1 (Step 000070): Train loss 6.172, Val loss 6.164\n",
      "Ep 1 (Step 000080): Train loss 6.044, Val loss 6.068\n",
      "Ep 1 (Step 000090): Train loss 5.944, Val loss 5.986\n",
      "Ep 1 (Step 000100): Train loss 5.945, Val loss 5.934\n",
      "Ep 1 (Step 000110): Train loss 5.854, Val loss 5.880\n",
      "Ep 1 (Step 000120): Train loss 5.813, Val loss 5.802\n",
      "Ep 1 (Step 000130): Train loss 5.718, Val loss 5.755\n",
      "Ep 1 (Step 000140): Train loss 5.592, Val loss 5.719\n",
      "Ep 1 (Step 000150): Train loss 5.671, Val loss 5.692\n",
      "Ep 1 (Step 000160): Train loss 5.567, Val loss 5.653\n",
      "Ep 1 (Step 000170): Train loss 5.605, Val loss 5.629\n",
      "Ep 1 (Step 000180): Train loss 5.557, Val loss 5.624\n",
      "Ep 1 (Step 000190): Train loss 5.488, Val loss 5.590\n",
      "Ep 1 (Step 000200): Train loss 5.465, Val loss 5.565\n",
      "Ep 1 (Step 000210): Train loss 5.438, Val loss 5.538\n",
      "Ep 1 (Step 000220): Train loss 5.391, Val loss 5.509\n",
      "Ep 1 (Step 000230): Train loss 5.447, Val loss 5.507\n",
      "Ep 1 (Step 000240): Train loss 5.394, Val loss 5.492\n",
      "Ep 1 (Step 000250): Train loss 5.348, Val loss 5.473\n",
      "Ep 1 (Step 000260): Train loss 5.354, Val loss 5.456\n",
      "Ep 1 (Step 000270): Train loss 5.281, Val loss 5.439\n",
      "Ep 1 (Step 000280): Train loss 5.332, Val loss 5.427\n",
      "Ep 1 (Step 000290): Train loss 5.276, Val loss 5.409\n",
      "Ep 1 (Step 000300): Train loss 5.269, Val loss 5.397\n",
      "Ep 1 (Step 000310): Train loss 5.354, Val loss 5.386\n",
      "Ep 1 (Step 000320): Train loss 5.250, Val loss 5.378\n",
      "Ep 1 (Step 000330): Train loss 5.207, Val loss 5.366\n",
      "Ep 1 (Step 000340): Train loss 5.304, Val loss 5.360\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3598\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.973, Val loss 8.976\n",
      "Ep 1 (Step 000010): Train loss 7.467, Val loss 7.472\n",
      "Ep 1 (Step 000020): Train loss 6.853, Val loss 6.821\n",
      "Ep 1 (Step 000030): Train loss 6.463, Val loss 6.487\n",
      "Ep 1 (Step 000040): Train loss 6.421, Val loss 6.385\n",
      "Ep 1 (Step 000050): Train loss 6.331, Val loss 6.325\n",
      "Ep 1 (Step 000060): Train loss 6.188, Val loss 6.244\n",
      "Ep 1 (Step 000070): Train loss 6.204, Val loss 6.142\n",
      "Ep 1 (Step 000080): Train loss 6.039, Val loss 6.050\n",
      "Ep 1 (Step 000090): Train loss 5.991, Val loss 5.976\n",
      "Ep 1 (Step 000100): Train loss 5.897, Val loss 5.915\n",
      "Ep 1 (Step 000110): Train loss 5.868, Val loss 5.873\n",
      "Ep 1 (Step 000120): Train loss 5.717, Val loss 5.843\n",
      "Ep 1 (Step 000130): Train loss 5.690, Val loss 5.782\n",
      "Ep 1 (Step 000140): Train loss 5.630, Val loss 5.753\n",
      "Ep 1 (Step 000150): Train loss 5.596, Val loss 5.712\n",
      "Ep 1 (Step 000160): Train loss 5.642, Val loss 5.675\n",
      "Ep 1 (Step 000170): Train loss 5.512, Val loss 5.651\n",
      "Ep 1 (Step 000180): Train loss 5.509, Val loss 5.626\n",
      "Ep 1 (Step 000190): Train loss 5.622, Val loss 5.590\n",
      "Ep 1 (Step 000200): Train loss 5.530, Val loss 5.569\n",
      "Ep 1 (Step 000210): Train loss 5.458, Val loss 5.536\n",
      "Ep 1 (Step 000220): Train loss 5.398, Val loss 5.528\n",
      "Ep 1 (Step 000230): Train loss 5.409, Val loss 5.500\n",
      "Ep 1 (Step 000240): Train loss 5.435, Val loss 5.488\n",
      "Ep 1 (Step 000250): Train loss 5.433, Val loss 5.470\n",
      "Ep 1 (Step 000260): Train loss 5.382, Val loss 5.479\n",
      "Ep 1 (Step 000270): Train loss 5.281, Val loss 5.441\n",
      "Ep 1 (Step 000280): Train loss 5.334, Val loss 5.440\n",
      "Ep 1 (Step 000290): Train loss 5.222, Val loss 5.423\n",
      "Ep 1 (Step 000300): Train loss 5.341, Val loss 5.424\n",
      "Ep 1 (Step 000310): Train loss 5.259, Val loss 5.383\n",
      "Ep 1 (Step 000320): Train loss 5.236, Val loss 5.396\n",
      "Ep 1 (Step 000330): Train loss 5.292, Val loss 5.366\n",
      "Ep 1 (Step 000340): Train loss 5.296, Val loss 5.362\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3619\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.878, Val loss 8.905\n",
      "Ep 1 (Step 000010): Train loss 7.415, Val loss 7.390\n",
      "Ep 1 (Step 000020): Train loss 6.823, Val loss 6.784\n",
      "Ep 1 (Step 000030): Train loss 6.490, Val loss 6.474\n",
      "Ep 1 (Step 000040): Train loss 6.429, Val loss 6.369\n",
      "Ep 1 (Step 000050): Train loss 6.407, Val loss 6.316\n",
      "Ep 1 (Step 000060): Train loss 6.178, Val loss 6.209\n",
      "Ep 1 (Step 000070): Train loss 6.065, Val loss 6.122\n",
      "Ep 1 (Step 000080): Train loss 6.056, Val loss 6.027\n",
      "Ep 1 (Step 000090): Train loss 5.902, Val loss 5.969\n",
      "Ep 1 (Step 000100): Train loss 5.885, Val loss 5.897\n",
      "Ep 1 (Step 000110): Train loss 5.752, Val loss 5.848\n",
      "Ep 1 (Step 000120): Train loss 5.759, Val loss 5.800\n",
      "Ep 1 (Step 000130): Train loss 5.646, Val loss 5.770\n",
      "Ep 1 (Step 000140): Train loss 5.813, Val loss 5.730\n",
      "Ep 1 (Step 000150): Train loss 5.584, Val loss 5.689\n",
      "Ep 1 (Step 000160): Train loss 5.562, Val loss 5.659\n",
      "Ep 1 (Step 000170): Train loss 5.609, Val loss 5.635\n",
      "Ep 1 (Step 000180): Train loss 5.512, Val loss 5.611\n",
      "Ep 1 (Step 000190): Train loss 5.540, Val loss 5.595\n",
      "Ep 1 (Step 000200): Train loss 5.461, Val loss 5.569\n",
      "Ep 1 (Step 000210): Train loss 5.472, Val loss 5.555\n",
      "Ep 1 (Step 000220): Train loss 5.508, Val loss 5.527\n",
      "Ep 1 (Step 000230): Train loss 5.499, Val loss 5.498\n",
      "Ep 1 (Step 000240): Train loss 5.322, Val loss 5.495\n",
      "Ep 1 (Step 000250): Train loss 5.399, Val loss 5.471\n",
      "Ep 1 (Step 000260): Train loss 5.322, Val loss 5.468\n",
      "Ep 1 (Step 000270): Train loss 5.325, Val loss 5.447\n",
      "Ep 1 (Step 000280): Train loss 5.380, Val loss 5.446\n",
      "Ep 1 (Step 000290): Train loss 5.323, Val loss 5.425\n",
      "Ep 1 (Step 000300): Train loss 5.369, Val loss 5.431\n",
      "Ep 1 (Step 000310): Train loss 5.215, Val loss 5.403\n",
      "Ep 1 (Step 000320): Train loss 5.359, Val loss 5.402\n",
      "Ep 1 (Step 000330): Train loss 5.165, Val loss 5.387\n",
      "Ep 1 (Step 000340): Train loss 5.233, Val loss 5.366\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3660\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.887, Val loss 8.870\n",
      "Ep 1 (Step 000010): Train loss 7.455, Val loss 7.408\n",
      "Ep 1 (Step 000020): Train loss 6.761, Val loss 6.755\n",
      "Ep 1 (Step 000030): Train loss 6.501, Val loss 6.450\n",
      "Ep 1 (Step 000040): Train loss 6.329, Val loss 6.372\n",
      "Ep 1 (Step 000050): Train loss 6.376, Val loss 6.317\n",
      "Ep 1 (Step 000060): Train loss 6.228, Val loss 6.216\n",
      "Ep 1 (Step 000070): Train loss 6.142, Val loss 6.110\n",
      "Ep 1 (Step 000080): Train loss 6.041, Val loss 6.028\n",
      "Ep 1 (Step 000090): Train loss 5.944, Val loss 5.964\n",
      "Ep 1 (Step 000100): Train loss 5.883, Val loss 5.891\n",
      "Ep 1 (Step 000110): Train loss 5.828, Val loss 5.844\n",
      "Ep 1 (Step 000120): Train loss 5.841, Val loss 5.802\n",
      "Ep 1 (Step 000130): Train loss 5.682, Val loss 5.754\n",
      "Ep 1 (Step 000140): Train loss 5.674, Val loss 5.745\n",
      "Ep 1 (Step 000150): Train loss 5.642, Val loss 5.694\n",
      "Ep 1 (Step 000160): Train loss 5.613, Val loss 5.673\n",
      "Ep 1 (Step 000170): Train loss 5.623, Val loss 5.634\n",
      "Ep 1 (Step 000180): Train loss 5.537, Val loss 5.619\n",
      "Ep 1 (Step 000190): Train loss 5.548, Val loss 5.582\n",
      "Ep 1 (Step 000200): Train loss 5.571, Val loss 5.561\n",
      "Ep 1 (Step 000210): Train loss 5.504, Val loss 5.523\n",
      "Ep 1 (Step 000220): Train loss 5.405, Val loss 5.508\n",
      "Ep 1 (Step 000230): Train loss 5.328, Val loss 5.487\n",
      "Ep 1 (Step 000240): Train loss 5.425, Val loss 5.463\n",
      "Ep 1 (Step 000250): Train loss 5.372, Val loss 5.462\n",
      "Ep 1 (Step 000260): Train loss 5.284, Val loss 5.438\n",
      "Ep 1 (Step 000270): Train loss 5.367, Val loss 5.415\n",
      "Ep 1 (Step 000280): Train loss 5.295, Val loss 5.413\n",
      "Ep 1 (Step 000290): Train loss 5.272, Val loss 5.401\n",
      "Ep 1 (Step 000300): Train loss 5.254, Val loss 5.384\n",
      "Ep 1 (Step 000310): Train loss 5.311, Val loss 5.381\n",
      "Ep 1 (Step 000320): Train loss 5.180, Val loss 5.352\n",
      "Ep 1 (Step 000330): Train loss 5.126, Val loss 5.353\n",
      "Ep 1 (Step 000340): Train loss 5.194, Val loss 5.331\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3306\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.904, Val loss 8.891\n",
      "Ep 1 (Step 000010): Train loss 7.323, Val loss 7.378\n",
      "Ep 1 (Step 000020): Train loss 6.731, Val loss 6.739\n",
      "Ep 1 (Step 000030): Train loss 6.498, Val loss 6.448\n",
      "Ep 1 (Step 000040): Train loss 6.355, Val loss 6.382\n",
      "Ep 1 (Step 000050): Train loss 6.377, Val loss 6.351\n",
      "Ep 1 (Step 000060): Train loss 6.308, Val loss 6.257\n",
      "Ep 1 (Step 000070): Train loss 6.115, Val loss 6.148\n",
      "Ep 1 (Step 000080): Train loss 6.064, Val loss 6.051\n",
      "Ep 1 (Step 000090): Train loss 5.931, Val loss 5.972\n",
      "Ep 1 (Step 000100): Train loss 5.860, Val loss 5.912\n",
      "Ep 1 (Step 000110): Train loss 5.861, Val loss 5.875\n",
      "Ep 1 (Step 000120): Train loss 5.771, Val loss 5.816\n",
      "Ep 1 (Step 000130): Train loss 5.741, Val loss 5.769\n",
      "Ep 1 (Step 000140): Train loss 5.631, Val loss 5.740\n",
      "Ep 1 (Step 000150): Train loss 5.617, Val loss 5.702\n",
      "Ep 1 (Step 000160): Train loss 5.584, Val loss 5.675\n",
      "Ep 1 (Step 000170): Train loss 5.549, Val loss 5.645\n",
      "Ep 1 (Step 000180): Train loss 5.571, Val loss 5.607\n",
      "Ep 1 (Step 000190): Train loss 5.513, Val loss 5.576\n",
      "Ep 1 (Step 000200): Train loss 5.456, Val loss 5.551\n",
      "Ep 1 (Step 000210): Train loss 5.387, Val loss 5.522\n",
      "Ep 1 (Step 000220): Train loss 5.497, Val loss 5.491\n",
      "Ep 1 (Step 000230): Train loss 5.436, Val loss 5.494\n",
      "Ep 1 (Step 000240): Train loss 5.408, Val loss 5.487\n",
      "Ep 1 (Step 000250): Train loss 5.297, Val loss 5.463\n",
      "Ep 1 (Step 000260): Train loss 5.263, Val loss 5.429\n",
      "Ep 1 (Step 000270): Train loss 5.307, Val loss 5.426\n",
      "Ep 1 (Step 000280): Train loss 5.315, Val loss 5.400\n",
      "Ep 1 (Step 000290): Train loss 5.261, Val loss 5.396\n",
      "Ep 1 (Step 000300): Train loss 5.350, Val loss 5.364\n",
      "Ep 1 (Step 000310): Train loss 5.271, Val loss 5.355\n",
      "Ep 1 (Step 000320): Train loss 5.179, Val loss 5.346\n",
      "Ep 1 (Step 000330): Train loss 5.204, Val loss 5.331\n",
      "Ep 1 (Step 000340): Train loss 5.168, Val loss 5.321\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3212\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.987, Val loss 8.946\n",
      "Ep 1 (Step 000010): Train loss 7.468, Val loss 7.393\n",
      "Ep 1 (Step 000020): Train loss 6.761, Val loss 6.714\n",
      "Ep 1 (Step 000030): Train loss 6.448, Val loss 6.431\n",
      "Ep 1 (Step 000040): Train loss 6.351, Val loss 6.360\n",
      "Ep 1 (Step 000050): Train loss 6.273, Val loss 6.330\n",
      "Ep 1 (Step 000060): Train loss 6.248, Val loss 6.239\n",
      "Ep 1 (Step 000070): Train loss 6.086, Val loss 6.116\n",
      "Ep 1 (Step 000080): Train loss 6.045, Val loss 6.020\n",
      "Ep 1 (Step 000090): Train loss 5.947, Val loss 5.952\n",
      "Ep 1 (Step 000100): Train loss 5.878, Val loss 5.895\n",
      "Ep 1 (Step 000110): Train loss 5.852, Val loss 5.858\n",
      "Ep 1 (Step 000120): Train loss 5.761, Val loss 5.794\n",
      "Ep 1 (Step 000130): Train loss 5.704, Val loss 5.763\n",
      "Ep 1 (Step 000140): Train loss 5.646, Val loss 5.750\n",
      "Ep 1 (Step 000150): Train loss 5.606, Val loss 5.712\n",
      "Ep 1 (Step 000160): Train loss 5.592, Val loss 5.657\n",
      "Ep 1 (Step 000170): Train loss 5.520, Val loss 5.638\n",
      "Ep 1 (Step 000180): Train loss 5.579, Val loss 5.612\n",
      "Ep 1 (Step 000190): Train loss 5.474, Val loss 5.585\n",
      "Ep 1 (Step 000200): Train loss 5.457, Val loss 5.561\n",
      "Ep 1 (Step 000210): Train loss 5.473, Val loss 5.538\n",
      "Ep 1 (Step 000220): Train loss 5.499, Val loss 5.511\n",
      "Ep 1 (Step 000230): Train loss 5.358, Val loss 5.494\n",
      "Ep 1 (Step 000240): Train loss 5.429, Val loss 5.477\n",
      "Ep 1 (Step 000250): Train loss 5.348, Val loss 5.440\n",
      "Ep 1 (Step 000260): Train loss 5.329, Val loss 5.441\n",
      "Ep 1 (Step 000270): Train loss 5.326, Val loss 5.427\n",
      "Ep 1 (Step 000280): Train loss 5.249, Val loss 5.413\n",
      "Ep 1 (Step 000290): Train loss 5.255, Val loss 5.403\n",
      "Ep 1 (Step 000300): Train loss 5.305, Val loss 5.399\n",
      "Ep 1 (Step 000310): Train loss 5.223, Val loss 5.383\n",
      "Ep 1 (Step 000320): Train loss 5.352, Val loss 5.363\n",
      "Ep 1 (Step 000330): Train loss 5.269, Val loss 5.353\n",
      "Ep 1 (Step 000340): Train loss 5.167, Val loss 5.328\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3281\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.642, Val loss 8.619\n",
      "Ep 1 (Step 000010): Train loss 6.819, Val loss 6.811\n",
      "Ep 1 (Step 000020): Train loss 6.433, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.472, Val loss 6.441\n",
      "Ep 1 (Step 000040): Train loss 6.342, Val loss 6.332\n",
      "Ep 1 (Step 000050): Train loss 6.146, Val loss 6.197\n",
      "Ep 1 (Step 000060): Train loss 6.041, Val loss 6.077\n",
      "Ep 1 (Step 000070): Train loss 6.038, Val loss 5.977\n",
      "Ep 1 (Step 000080): Train loss 5.900, Val loss 5.869\n",
      "Ep 1 (Step 000090): Train loss 5.691, Val loss 5.815\n",
      "Ep 1 (Step 000100): Train loss 5.807, Val loss 5.773\n",
      "Ep 1 (Step 000110): Train loss 5.665, Val loss 5.728\n",
      "Ep 1 (Step 000120): Train loss 5.574, Val loss 5.676\n",
      "Ep 1 (Step 000130): Train loss 5.602, Val loss 5.639\n",
      "Ep 1 (Step 000140): Train loss 5.425, Val loss 5.596\n",
      "Ep 1 (Step 000150): Train loss 5.413, Val loss 5.566\n",
      "Ep 1 (Step 000160): Train loss 5.515, Val loss 5.554\n",
      "Ep 1 (Step 000170): Train loss 5.348, Val loss 5.528\n",
      "Ep 1 (Step 000180): Train loss 5.436, Val loss 5.498\n",
      "Ep 1 (Step 000190): Train loss 5.454, Val loss 5.466\n",
      "Ep 1 (Step 000200): Train loss 5.309, Val loss 5.434\n",
      "Ep 1 (Step 000210): Train loss 5.348, Val loss 5.432\n",
      "Ep 1 (Step 000220): Train loss 5.243, Val loss 5.413\n",
      "Ep 1 (Step 000230): Train loss 5.288, Val loss 5.387\n",
      "Ep 1 (Step 000240): Train loss 5.335, Val loss 5.397\n",
      "Ep 1 (Step 000250): Train loss 5.257, Val loss 5.379\n",
      "Ep 1 (Step 000260): Train loss 5.220, Val loss 5.345\n",
      "Ep 1 (Step 000270): Train loss 5.229, Val loss 5.352\n",
      "Ep 1 (Step 000280): Train loss 5.190, Val loss 5.343\n",
      "Ep 1 (Step 000290): Train loss 5.168, Val loss 5.350\n",
      "Ep 1 (Step 000300): Train loss 5.226, Val loss 5.330\n",
      "Ep 1 (Step 000310): Train loss 5.161, Val loss 5.309\n",
      "Ep 1 (Step 000320): Train loss 5.216, Val loss 5.317\n",
      "Ep 1 (Step 000330): Train loss 5.240, Val loss 5.279\n",
      "Ep 1 (Step 000340): Train loss 5.129, Val loss 5.281\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2807\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.692, Val loss 8.687\n",
      "Ep 1 (Step 000010): Train loss 6.889, Val loss 6.847\n",
      "Ep 1 (Step 000020): Train loss 6.493, Val loss 6.490\n",
      "Ep 1 (Step 000030): Train loss 6.493, Val loss 6.462\n",
      "Ep 1 (Step 000040): Train loss 6.309, Val loss 6.378\n",
      "Ep 1 (Step 000050): Train loss 6.313, Val loss 6.254\n",
      "Ep 1 (Step 000060): Train loss 6.107, Val loss 6.161\n",
      "Ep 1 (Step 000070): Train loss 5.981, Val loss 6.008\n",
      "Ep 1 (Step 000080): Train loss 5.888, Val loss 5.913\n",
      "Ep 1 (Step 000090): Train loss 5.856, Val loss 5.836\n",
      "Ep 1 (Step 000100): Train loss 5.756, Val loss 5.757\n",
      "Ep 1 (Step 000110): Train loss 5.711, Val loss 5.721\n",
      "Ep 1 (Step 000120): Train loss 5.664, Val loss 5.680\n",
      "Ep 1 (Step 000130): Train loss 5.599, Val loss 5.646\n",
      "Ep 1 (Step 000140): Train loss 5.536, Val loss 5.610\n",
      "Ep 1 (Step 000150): Train loss 5.551, Val loss 5.584\n",
      "Ep 1 (Step 000160): Train loss 5.469, Val loss 5.553\n",
      "Ep 1 (Step 000170): Train loss 5.414, Val loss 5.525\n",
      "Ep 1 (Step 000180): Train loss 5.506, Val loss 5.507\n",
      "Ep 1 (Step 000190): Train loss 5.385, Val loss 5.481\n",
      "Ep 1 (Step 000200): Train loss 5.407, Val loss 5.455\n",
      "Ep 1 (Step 000210): Train loss 5.234, Val loss 5.443\n",
      "Ep 1 (Step 000220): Train loss 5.328, Val loss 5.423\n",
      "Ep 1 (Step 000230): Train loss 5.276, Val loss 5.408\n",
      "Ep 1 (Step 000240): Train loss 5.330, Val loss 5.396\n",
      "Ep 1 (Step 000250): Train loss 5.224, Val loss 5.380\n",
      "Ep 1 (Step 000260): Train loss 5.310, Val loss 5.364\n",
      "Ep 1 (Step 000270): Train loss 5.236, Val loss 5.354\n",
      "Ep 1 (Step 000280): Train loss 5.199, Val loss 5.369\n",
      "Ep 1 (Step 000290): Train loss 5.243, Val loss 5.336\n",
      "Ep 1 (Step 000300): Train loss 5.161, Val loss 5.333\n",
      "Ep 1 (Step 000310): Train loss 5.177, Val loss 5.311\n",
      "Ep 1 (Step 000320): Train loss 5.232, Val loss 5.308\n",
      "Ep 1 (Step 000330): Train loss 5.175, Val loss 5.301\n",
      "Ep 1 (Step 000340): Train loss 5.122, Val loss 5.287\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2870\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.684, Val loss 8.651\n",
      "Ep 1 (Step 000010): Train loss 6.896, Val loss 6.874\n",
      "Ep 1 (Step 000020): Train loss 6.517, Val loss 6.469\n",
      "Ep 1 (Step 000030): Train loss 6.455, Val loss 6.453\n",
      "Ep 1 (Step 000040): Train loss 6.397, Val loss 6.303\n",
      "Ep 1 (Step 000050): Train loss 6.123, Val loss 6.157\n",
      "Ep 1 (Step 000060): Train loss 6.029, Val loss 6.011\n",
      "Ep 1 (Step 000070): Train loss 5.905, Val loss 5.942\n",
      "Ep 1 (Step 000080): Train loss 5.781, Val loss 5.890\n",
      "Ep 1 (Step 000090): Train loss 5.692, Val loss 5.806\n",
      "Ep 1 (Step 000100): Train loss 5.732, Val loss 5.751\n",
      "Ep 1 (Step 000110): Train loss 5.636, Val loss 5.709\n",
      "Ep 1 (Step 000120): Train loss 5.580, Val loss 5.651\n",
      "Ep 1 (Step 000130): Train loss 5.550, Val loss 5.624\n",
      "Ep 1 (Step 000140): Train loss 5.475, Val loss 5.574\n",
      "Ep 1 (Step 000150): Train loss 5.453, Val loss 5.546\n",
      "Ep 1 (Step 000160): Train loss 5.426, Val loss 5.519\n",
      "Ep 1 (Step 000170): Train loss 5.439, Val loss 5.500\n",
      "Ep 1 (Step 000180): Train loss 5.378, Val loss 5.474\n",
      "Ep 1 (Step 000190): Train loss 5.435, Val loss 5.456\n",
      "Ep 1 (Step 000200): Train loss 5.320, Val loss 5.445\n",
      "Ep 1 (Step 000210): Train loss 5.307, Val loss 5.438\n",
      "Ep 1 (Step 000220): Train loss 5.359, Val loss 5.429\n",
      "Ep 1 (Step 000230): Train loss 5.292, Val loss 5.410\n",
      "Ep 1 (Step 000240): Train loss 5.328, Val loss 5.404\n",
      "Ep 1 (Step 000250): Train loss 5.261, Val loss 5.386\n",
      "Ep 1 (Step 000260): Train loss 5.271, Val loss 5.367\n",
      "Ep 1 (Step 000270): Train loss 5.165, Val loss 5.342\n",
      "Ep 1 (Step 000280): Train loss 5.185, Val loss 5.336\n",
      "Ep 1 (Step 000290): Train loss 5.181, Val loss 5.327\n",
      "Ep 1 (Step 000300): Train loss 5.187, Val loss 5.318\n",
      "Ep 1 (Step 000310): Train loss 5.229, Val loss 5.294\n",
      "Ep 1 (Step 000320): Train loss 5.198, Val loss 5.293\n",
      "Ep 1 (Step 000330): Train loss 5.194, Val loss 5.289\n",
      "Ep 1 (Step 000340): Train loss 5.156, Val loss 5.293\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2933\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.623, Val loss 8.587\n",
      "Ep 1 (Step 000010): Train loss 6.864, Val loss 6.790\n",
      "Ep 1 (Step 000020): Train loss 6.495, Val loss 6.447\n",
      "Ep 1 (Step 000030): Train loss 6.437, Val loss 6.443\n",
      "Ep 1 (Step 000040): Train loss 6.323, Val loss 6.315\n",
      "Ep 1 (Step 000050): Train loss 6.124, Val loss 6.175\n",
      "Ep 1 (Step 000060): Train loss 6.114, Val loss 6.051\n",
      "Ep 1 (Step 000070): Train loss 5.913, Val loss 5.959\n",
      "Ep 1 (Step 000080): Train loss 5.852, Val loss 5.883\n",
      "Ep 1 (Step 000090): Train loss 5.751, Val loss 5.786\n",
      "Ep 1 (Step 000100): Train loss 5.817, Val loss 5.725\n",
      "Ep 1 (Step 000110): Train loss 5.626, Val loss 5.672\n",
      "Ep 1 (Step 000120): Train loss 5.605, Val loss 5.640\n",
      "Ep 1 (Step 000130): Train loss 5.627, Val loss 5.608\n",
      "Ep 1 (Step 000140): Train loss 5.553, Val loss 5.573\n",
      "Ep 1 (Step 000150): Train loss 5.532, Val loss 5.534\n",
      "Ep 1 (Step 000160): Train loss 5.416, Val loss 5.521\n",
      "Ep 1 (Step 000170): Train loss 5.392, Val loss 5.501\n",
      "Ep 1 (Step 000180): Train loss 5.368, Val loss 5.483\n",
      "Ep 1 (Step 000190): Train loss 5.392, Val loss 5.459\n",
      "Ep 1 (Step 000200): Train loss 5.395, Val loss 5.437\n",
      "Ep 1 (Step 000210): Train loss 5.373, Val loss 5.406\n",
      "Ep 1 (Step 000220): Train loss 5.340, Val loss 5.387\n",
      "Ep 1 (Step 000230): Train loss 5.264, Val loss 5.373\n",
      "Ep 1 (Step 000240): Train loss 5.267, Val loss 5.363\n",
      "Ep 1 (Step 000250): Train loss 5.173, Val loss 5.353\n",
      "Ep 1 (Step 000260): Train loss 5.221, Val loss 5.319\n",
      "Ep 1 (Step 000270): Train loss 5.183, Val loss 5.322\n",
      "Ep 1 (Step 000280): Train loss 5.152, Val loss 5.295\n",
      "Ep 1 (Step 000290): Train loss 5.299, Val loss 5.280\n",
      "Ep 1 (Step 000300): Train loss 5.120, Val loss 5.279\n",
      "Ep 1 (Step 000310): Train loss 5.134, Val loss 5.289\n",
      "Ep 1 (Step 000320): Train loss 5.158, Val loss 5.243\n",
      "Ep 1 (Step 000330): Train loss 5.064, Val loss 5.241\n",
      "Ep 1 (Step 000340): Train loss 5.072, Val loss 5.233\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2327\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.604, Val loss 8.610\n",
      "Ep 1 (Step 000010): Train loss 6.861, Val loss 6.823\n",
      "Ep 1 (Step 000020): Train loss 6.495, Val loss 6.475\n",
      "Ep 1 (Step 000030): Train loss 6.469, Val loss 6.461\n",
      "Ep 1 (Step 000040): Train loss 6.352, Val loss 6.354\n",
      "Ep 1 (Step 000050): Train loss 6.282, Val loss 6.265\n",
      "Ep 1 (Step 000060): Train loss 6.104, Val loss 6.099\n",
      "Ep 1 (Step 000070): Train loss 6.029, Val loss 5.993\n",
      "Ep 1 (Step 000080): Train loss 5.852, Val loss 5.917\n",
      "Ep 1 (Step 000090): Train loss 5.798, Val loss 5.846\n",
      "Ep 1 (Step 000100): Train loss 5.654, Val loss 5.788\n",
      "Ep 1 (Step 000110): Train loss 5.692, Val loss 5.712\n",
      "Ep 1 (Step 000120): Train loss 5.553, Val loss 5.654\n",
      "Ep 1 (Step 000130): Train loss 5.602, Val loss 5.628\n",
      "Ep 1 (Step 000140): Train loss 5.600, Val loss 5.590\n",
      "Ep 1 (Step 000150): Train loss 5.441, Val loss 5.549\n",
      "Ep 1 (Step 000160): Train loss 5.500, Val loss 5.492\n",
      "Ep 1 (Step 000170): Train loss 5.359, Val loss 5.486\n",
      "Ep 1 (Step 000180): Train loss 5.393, Val loss 5.473\n",
      "Ep 1 (Step 000190): Train loss 5.379, Val loss 5.443\n",
      "Ep 1 (Step 000200): Train loss 5.360, Val loss 5.429\n",
      "Ep 1 (Step 000210): Train loss 5.264, Val loss 5.413\n",
      "Ep 1 (Step 000220): Train loss 5.311, Val loss 5.391\n",
      "Ep 1 (Step 000230): Train loss 5.298, Val loss 5.379\n",
      "Ep 1 (Step 000240): Train loss 5.304, Val loss 5.376\n",
      "Ep 1 (Step 000250): Train loss 5.283, Val loss 5.342\n",
      "Ep 1 (Step 000260): Train loss 5.265, Val loss 5.324\n",
      "Ep 1 (Step 000270): Train loss 5.192, Val loss 5.338\n",
      "Ep 1 (Step 000280): Train loss 5.208, Val loss 5.309\n",
      "Ep 1 (Step 000290): Train loss 5.193, Val loss 5.313\n",
      "Ep 1 (Step 000300): Train loss 5.216, Val loss 5.301\n",
      "Ep 1 (Step 000310): Train loss 5.074, Val loss 5.273\n",
      "Ep 1 (Step 000320): Train loss 5.152, Val loss 5.267\n",
      "Ep 1 (Step 000330): Train loss 5.136, Val loss 5.255\n",
      "Ep 1 (Step 000340): Train loss 5.112, Val loss 5.252\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2524\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.522, Val loss 8.554\n",
      "Ep 1 (Step 000010): Train loss 6.799, Val loss 6.800\n",
      "Ep 1 (Step 000020): Train loss 6.554, Val loss 6.481\n",
      "Ep 1 (Step 000030): Train loss 6.419, Val loss 6.448\n",
      "Ep 1 (Step 000040): Train loss 6.284, Val loss 6.305\n",
      "Ep 1 (Step 000050): Train loss 6.165, Val loss 6.201\n",
      "Ep 1 (Step 000060): Train loss 6.044, Val loss 6.060\n",
      "Ep 1 (Step 000070): Train loss 5.928, Val loss 5.958\n",
      "Ep 1 (Step 000080): Train loss 5.766, Val loss 5.857\n",
      "Ep 1 (Step 000090): Train loss 5.804, Val loss 5.790\n",
      "Ep 1 (Step 000100): Train loss 5.604, Val loss 5.716\n",
      "Ep 1 (Step 000110): Train loss 5.527, Val loss 5.667\n",
      "Ep 1 (Step 000120): Train loss 5.540, Val loss 5.661\n",
      "Ep 1 (Step 000130): Train loss 5.501, Val loss 5.602\n",
      "Ep 1 (Step 000140): Train loss 5.550, Val loss 5.553\n",
      "Ep 1 (Step 000150): Train loss 5.468, Val loss 5.524\n",
      "Ep 1 (Step 000160): Train loss 5.429, Val loss 5.492\n",
      "Ep 1 (Step 000170): Train loss 5.391, Val loss 5.475\n",
      "Ep 1 (Step 000180): Train loss 5.349, Val loss 5.452\n",
      "Ep 1 (Step 000190): Train loss 5.387, Val loss 5.428\n",
      "Ep 1 (Step 000200): Train loss 5.271, Val loss 5.414\n",
      "Ep 1 (Step 000210): Train loss 5.330, Val loss 5.389\n",
      "Ep 1 (Step 000220): Train loss 5.169, Val loss 5.387\n",
      "Ep 1 (Step 000230): Train loss 5.238, Val loss 5.370\n",
      "Ep 1 (Step 000240): Train loss 5.204, Val loss 5.357\n",
      "Ep 1 (Step 000250): Train loss 5.225, Val loss 5.353\n",
      "Ep 1 (Step 000260): Train loss 5.187, Val loss 5.327\n",
      "Ep 1 (Step 000270): Train loss 5.259, Val loss 5.308\n",
      "Ep 1 (Step 000280): Train loss 5.183, Val loss 5.310\n",
      "Ep 1 (Step 000290): Train loss 5.140, Val loss 5.317\n",
      "Ep 1 (Step 000300): Train loss 5.217, Val loss 5.303\n",
      "Ep 1 (Step 000310): Train loss 5.139, Val loss 5.277\n",
      "Ep 1 (Step 000320): Train loss 5.078, Val loss 5.258\n",
      "Ep 1 (Step 000330): Train loss 5.051, Val loss 5.241\n",
      "Ep 1 (Step 000340): Train loss 5.090, Val loss 5.229\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2286\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.895, Val loss 8.884\n",
      "Ep 1 (Step 000010): Train loss 7.391, Val loss 7.378\n",
      "Ep 1 (Step 000020): Train loss 6.787, Val loss 6.760\n",
      "Ep 1 (Step 000030): Train loss 6.485, Val loss 6.456\n",
      "Ep 1 (Step 000040): Train loss 6.302, Val loss 6.358\n",
      "Ep 1 (Step 000050): Train loss 6.315, Val loss 6.265\n",
      "Ep 1 (Step 000060): Train loss 6.123, Val loss 6.149\n",
      "Ep 1 (Step 000070): Train loss 6.121, Val loss 6.071\n",
      "Ep 1 (Step 000080): Train loss 5.961, Val loss 5.974\n",
      "Ep 1 (Step 000090): Train loss 5.905, Val loss 5.912\n",
      "Ep 1 (Step 000100): Train loss 5.791, Val loss 5.848\n",
      "Ep 1 (Step 000110): Train loss 5.757, Val loss 5.799\n",
      "Ep 1 (Step 000120): Train loss 5.741, Val loss 5.767\n",
      "Ep 1 (Step 000130): Train loss 5.650, Val loss 5.728\n",
      "Ep 1 (Step 000140): Train loss 5.704, Val loss 5.688\n",
      "Ep 1 (Step 000150): Train loss 5.570, Val loss 5.638\n",
      "Ep 1 (Step 000160): Train loss 5.561, Val loss 5.610\n",
      "Ep 1 (Step 000170): Train loss 5.456, Val loss 5.570\n",
      "Ep 1 (Step 000180): Train loss 5.401, Val loss 5.546\n",
      "Ep 1 (Step 000190): Train loss 5.481, Val loss 5.525\n",
      "Ep 1 (Step 000200): Train loss 5.437, Val loss 5.512\n",
      "Ep 1 (Step 000210): Train loss 5.390, Val loss 5.481\n",
      "Ep 1 (Step 000220): Train loss 5.391, Val loss 5.460\n",
      "Ep 1 (Step 000230): Train loss 5.432, Val loss 5.433\n",
      "Ep 1 (Step 000240): Train loss 5.348, Val loss 5.423\n",
      "Ep 1 (Step 000250): Train loss 5.360, Val loss 5.424\n",
      "Ep 1 (Step 000260): Train loss 5.308, Val loss 5.403\n",
      "Ep 1 (Step 000270): Train loss 5.320, Val loss 5.385\n",
      "Ep 1 (Step 000280): Train loss 5.359, Val loss 5.379\n",
      "Ep 1 (Step 000290): Train loss 5.175, Val loss 5.365\n",
      "Ep 1 (Step 000300): Train loss 5.227, Val loss 5.350\n",
      "Ep 1 (Step 000310): Train loss 5.200, Val loss 5.329\n",
      "Ep 1 (Step 000320): Train loss 5.158, Val loss 5.328\n",
      "Ep 1 (Step 000330): Train loss 5.183, Val loss 5.302\n",
      "Ep 1 (Step 000340): Train loss 5.199, Val loss 5.319\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3186\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.916, Val loss 8.909\n",
      "Ep 1 (Step 000010): Train loss 7.454, Val loss 7.424\n",
      "Ep 1 (Step 000020): Train loss 6.826, Val loss 6.760\n",
      "Ep 1 (Step 000030): Train loss 6.451, Val loss 6.458\n",
      "Ep 1 (Step 000040): Train loss 6.401, Val loss 6.369\n",
      "Ep 1 (Step 000050): Train loss 6.252, Val loss 6.285\n",
      "Ep 1 (Step 000060): Train loss 6.088, Val loss 6.138\n",
      "Ep 1 (Step 000070): Train loss 6.077, Val loss 6.067\n",
      "Ep 1 (Step 000080): Train loss 5.917, Val loss 5.974\n",
      "Ep 1 (Step 000090): Train loss 5.865, Val loss 5.911\n",
      "Ep 1 (Step 000100): Train loss 5.774, Val loss 5.856\n",
      "Ep 1 (Step 000110): Train loss 5.784, Val loss 5.809\n",
      "Ep 1 (Step 000120): Train loss 5.670, Val loss 5.756\n",
      "Ep 1 (Step 000130): Train loss 5.663, Val loss 5.709\n",
      "Ep 1 (Step 000140): Train loss 5.598, Val loss 5.669\n",
      "Ep 1 (Step 000150): Train loss 5.624, Val loss 5.633\n",
      "Ep 1 (Step 000160): Train loss 5.474, Val loss 5.611\n",
      "Ep 1 (Step 000170): Train loss 5.528, Val loss 5.569\n",
      "Ep 1 (Step 000180): Train loss 5.472, Val loss 5.548\n",
      "Ep 1 (Step 000190): Train loss 5.440, Val loss 5.530\n",
      "Ep 1 (Step 000200): Train loss 5.462, Val loss 5.500\n",
      "Ep 1 (Step 000210): Train loss 5.338, Val loss 5.486\n",
      "Ep 1 (Step 000220): Train loss 5.370, Val loss 5.472\n",
      "Ep 1 (Step 000230): Train loss 5.330, Val loss 5.461\n",
      "Ep 1 (Step 000240): Train loss 5.221, Val loss 5.420\n",
      "Ep 1 (Step 000250): Train loss 5.394, Val loss 5.412\n",
      "Ep 1 (Step 000260): Train loss 5.253, Val loss 5.409\n",
      "Ep 1 (Step 000270): Train loss 5.310, Val loss 5.384\n",
      "Ep 1 (Step 000280): Train loss 5.251, Val loss 5.361\n",
      "Ep 1 (Step 000290): Train loss 5.280, Val loss 5.350\n",
      "Ep 1 (Step 000300): Train loss 5.157, Val loss 5.339\n",
      "Ep 1 (Step 000310): Train loss 5.228, Val loss 5.332\n",
      "Ep 1 (Step 000320): Train loss 5.175, Val loss 5.325\n",
      "Ep 1 (Step 000330): Train loss 5.148, Val loss 5.316\n",
      "Ep 1 (Step 000340): Train loss 5.183, Val loss 5.301\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3009\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.908, Val loss 8.904\n",
      "Ep 1 (Step 000010): Train loss 7.449, Val loss 7.402\n",
      "Ep 1 (Step 000020): Train loss 6.784, Val loss 6.755\n",
      "Ep 1 (Step 000030): Train loss 6.531, Val loss 6.444\n",
      "Ep 1 (Step 000040): Train loss 6.321, Val loss 6.356\n",
      "Ep 1 (Step 000050): Train loss 6.274, Val loss 6.290\n",
      "Ep 1 (Step 000060): Train loss 6.157, Val loss 6.168\n",
      "Ep 1 (Step 000070): Train loss 6.074, Val loss 6.038\n",
      "Ep 1 (Step 000080): Train loss 5.950, Val loss 5.985\n",
      "Ep 1 (Step 000090): Train loss 5.978, Val loss 5.904\n",
      "Ep 1 (Step 000100): Train loss 5.796, Val loss 5.857\n",
      "Ep 1 (Step 000110): Train loss 5.826, Val loss 5.800\n",
      "Ep 1 (Step 000120): Train loss 5.700, Val loss 5.762\n",
      "Ep 1 (Step 000130): Train loss 5.606, Val loss 5.715\n",
      "Ep 1 (Step 000140): Train loss 5.605, Val loss 5.682\n",
      "Ep 1 (Step 000150): Train loss 5.533, Val loss 5.652\n",
      "Ep 1 (Step 000160): Train loss 5.558, Val loss 5.613\n",
      "Ep 1 (Step 000170): Train loss 5.497, Val loss 5.603\n",
      "Ep 1 (Step 000180): Train loss 5.506, Val loss 5.589\n",
      "Ep 1 (Step 000190): Train loss 5.447, Val loss 5.547\n",
      "Ep 1 (Step 000200): Train loss 5.425, Val loss 5.515\n",
      "Ep 1 (Step 000210): Train loss 5.328, Val loss 5.503\n",
      "Ep 1 (Step 000220): Train loss 5.425, Val loss 5.495\n",
      "Ep 1 (Step 000230): Train loss 5.356, Val loss 5.465\n",
      "Ep 1 (Step 000240): Train loss 5.399, Val loss 5.446\n",
      "Ep 1 (Step 000250): Train loss 5.352, Val loss 5.446\n",
      "Ep 1 (Step 000260): Train loss 5.318, Val loss 5.420\n",
      "Ep 1 (Step 000270): Train loss 5.213, Val loss 5.398\n",
      "Ep 1 (Step 000280): Train loss 5.178, Val loss 5.390\n",
      "Ep 1 (Step 000290): Train loss 5.214, Val loss 5.361\n",
      "Ep 1 (Step 000300): Train loss 5.346, Val loss 5.354\n",
      "Ep 1 (Step 000310): Train loss 5.146, Val loss 5.347\n",
      "Ep 1 (Step 000320): Train loss 5.166, Val loss 5.336\n",
      "Ep 1 (Step 000330): Train loss 5.218, Val loss 5.326\n",
      "Ep 1 (Step 000340): Train loss 5.121, Val loss 5.306\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3063\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.832, Val loss 8.817\n",
      "Ep 1 (Step 000010): Train loss 7.387, Val loss 7.385\n",
      "Ep 1 (Step 000020): Train loss 6.779, Val loss 6.751\n",
      "Ep 1 (Step 000030): Train loss 6.429, Val loss 6.453\n",
      "Ep 1 (Step 000040): Train loss 6.372, Val loss 6.370\n",
      "Ep 1 (Step 000050): Train loss 6.343, Val loss 6.289\n",
      "Ep 1 (Step 000060): Train loss 6.198, Val loss 6.170\n",
      "Ep 1 (Step 000070): Train loss 6.008, Val loss 6.052\n",
      "Ep 1 (Step 000080): Train loss 5.953, Val loss 5.960\n",
      "Ep 1 (Step 000090): Train loss 5.855, Val loss 5.915\n",
      "Ep 1 (Step 000100): Train loss 5.843, Val loss 5.843\n",
      "Ep 1 (Step 000110): Train loss 5.750, Val loss 5.781\n",
      "Ep 1 (Step 000120): Train loss 5.768, Val loss 5.728\n",
      "Ep 1 (Step 000130): Train loss 5.613, Val loss 5.700\n",
      "Ep 1 (Step 000140): Train loss 5.647, Val loss 5.661\n",
      "Ep 1 (Step 000150): Train loss 5.563, Val loss 5.641\n",
      "Ep 1 (Step 000160): Train loss 5.524, Val loss 5.624\n",
      "Ep 1 (Step 000170): Train loss 5.519, Val loss 5.589\n",
      "Ep 1 (Step 000180): Train loss 5.402, Val loss 5.545\n",
      "Ep 1 (Step 000190): Train loss 5.483, Val loss 5.536\n",
      "Ep 1 (Step 000200): Train loss 5.543, Val loss 5.516\n",
      "Ep 1 (Step 000210): Train loss 5.364, Val loss 5.487\n",
      "Ep 1 (Step 000220): Train loss 5.448, Val loss 5.464\n",
      "Ep 1 (Step 000230): Train loss 5.390, Val loss 5.434\n",
      "Ep 1 (Step 000240): Train loss 5.331, Val loss 5.423\n",
      "Ep 1 (Step 000250): Train loss 5.275, Val loss 5.408\n",
      "Ep 1 (Step 000260): Train loss 5.286, Val loss 5.383\n",
      "Ep 1 (Step 000270): Train loss 5.262, Val loss 5.366\n",
      "Ep 1 (Step 000280): Train loss 5.285, Val loss 5.357\n",
      "Ep 1 (Step 000290): Train loss 5.236, Val loss 5.351\n",
      "Ep 1 (Step 000300): Train loss 5.225, Val loss 5.337\n",
      "Ep 1 (Step 000310): Train loss 5.280, Val loss 5.315\n",
      "Ep 1 (Step 000320): Train loss 5.269, Val loss 5.320\n",
      "Ep 1 (Step 000330): Train loss 5.174, Val loss 5.311\n",
      "Ep 1 (Step 000340): Train loss 5.100, Val loss 5.282\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2816\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.897, Val loss 8.868\n",
      "Ep 1 (Step 000010): Train loss 7.399, Val loss 7.358\n",
      "Ep 1 (Step 000020): Train loss 6.747, Val loss 6.726\n",
      "Ep 1 (Step 000030): Train loss 6.476, Val loss 6.443\n",
      "Ep 1 (Step 000040): Train loss 6.346, Val loss 6.355\n",
      "Ep 1 (Step 000050): Train loss 6.311, Val loss 6.256\n",
      "Ep 1 (Step 000060): Train loss 6.094, Val loss 6.172\n",
      "Ep 1 (Step 000070): Train loss 6.010, Val loss 6.050\n",
      "Ep 1 (Step 000080): Train loss 5.934, Val loss 5.988\n",
      "Ep 1 (Step 000090): Train loss 5.932, Val loss 5.906\n",
      "Ep 1 (Step 000100): Train loss 5.864, Val loss 5.849\n",
      "Ep 1 (Step 000110): Train loss 5.659, Val loss 5.790\n",
      "Ep 1 (Step 000120): Train loss 5.743, Val loss 5.738\n",
      "Ep 1 (Step 000130): Train loss 5.606, Val loss 5.701\n",
      "Ep 1 (Step 000140): Train loss 5.599, Val loss 5.659\n",
      "Ep 1 (Step 000150): Train loss 5.558, Val loss 5.642\n",
      "Ep 1 (Step 000160): Train loss 5.562, Val loss 5.603\n",
      "Ep 1 (Step 000170): Train loss 5.491, Val loss 5.586\n",
      "Ep 1 (Step 000180): Train loss 5.427, Val loss 5.551\n",
      "Ep 1 (Step 000190): Train loss 5.503, Val loss 5.517\n",
      "Ep 1 (Step 000200): Train loss 5.492, Val loss 5.493\n",
      "Ep 1 (Step 000210): Train loss 5.335, Val loss 5.492\n",
      "Ep 1 (Step 000220): Train loss 5.445, Val loss 5.476\n",
      "Ep 1 (Step 000230): Train loss 5.297, Val loss 5.438\n",
      "Ep 1 (Step 000240): Train loss 5.375, Val loss 5.426\n",
      "Ep 1 (Step 000250): Train loss 5.370, Val loss 5.397\n",
      "Ep 1 (Step 000260): Train loss 5.351, Val loss 5.388\n",
      "Ep 1 (Step 000270): Train loss 5.242, Val loss 5.359\n",
      "Ep 1 (Step 000280): Train loss 5.235, Val loss 5.359\n",
      "Ep 1 (Step 000290): Train loss 5.171, Val loss 5.337\n",
      "Ep 1 (Step 000300): Train loss 5.179, Val loss 5.341\n",
      "Ep 1 (Step 000310): Train loss 5.198, Val loss 5.318\n",
      "Ep 1 (Step 000320): Train loss 5.189, Val loss 5.297\n",
      "Ep 1 (Step 000330): Train loss 5.153, Val loss 5.292\n",
      "Ep 1 (Step 000340): Train loss 5.201, Val loss 5.284\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2844\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.844, Val loss 8.816\n",
      "Ep 1 (Step 000010): Train loss 7.329, Val loss 7.330\n",
      "Ep 1 (Step 000020): Train loss 6.697, Val loss 6.716\n",
      "Ep 1 (Step 000030): Train loss 6.488, Val loss 6.436\n",
      "Ep 1 (Step 000040): Train loss 6.361, Val loss 6.342\n",
      "Ep 1 (Step 000050): Train loss 6.326, Val loss 6.290\n",
      "Ep 1 (Step 000060): Train loss 6.176, Val loss 6.155\n",
      "Ep 1 (Step 000070): Train loss 6.038, Val loss 6.043\n",
      "Ep 1 (Step 000080): Train loss 5.978, Val loss 5.968\n",
      "Ep 1 (Step 000090): Train loss 5.936, Val loss 5.914\n",
      "Ep 1 (Step 000100): Train loss 5.745, Val loss 5.861\n",
      "Ep 1 (Step 000110): Train loss 5.798, Val loss 5.803\n",
      "Ep 1 (Step 000120): Train loss 5.697, Val loss 5.756\n",
      "Ep 1 (Step 000130): Train loss 5.617, Val loss 5.711\n",
      "Ep 1 (Step 000140): Train loss 5.619, Val loss 5.687\n",
      "Ep 1 (Step 000150): Train loss 5.618, Val loss 5.654\n",
      "Ep 1 (Step 000160): Train loss 5.508, Val loss 5.607\n",
      "Ep 1 (Step 000170): Train loss 5.532, Val loss 5.586\n",
      "Ep 1 (Step 000180): Train loss 5.484, Val loss 5.554\n",
      "Ep 1 (Step 000190): Train loss 5.430, Val loss 5.537\n",
      "Ep 1 (Step 000200): Train loss 5.469, Val loss 5.500\n",
      "Ep 1 (Step 000210): Train loss 5.394, Val loss 5.473\n",
      "Ep 1 (Step 000220): Train loss 5.418, Val loss 5.464\n",
      "Ep 1 (Step 000230): Train loss 5.388, Val loss 5.455\n",
      "Ep 1 (Step 000240): Train loss 5.236, Val loss 5.420\n",
      "Ep 1 (Step 000250): Train loss 5.244, Val loss 5.411\n",
      "Ep 1 (Step 000260): Train loss 5.280, Val loss 5.391\n",
      "Ep 1 (Step 000270): Train loss 5.248, Val loss 5.378\n",
      "Ep 1 (Step 000280): Train loss 5.287, Val loss 5.373\n",
      "Ep 1 (Step 000290): Train loss 5.300, Val loss 5.355\n",
      "Ep 1 (Step 000300): Train loss 5.164, Val loss 5.345\n",
      "Ep 1 (Step 000310): Train loss 5.221, Val loss 5.343\n",
      "Ep 1 (Step 000320): Train loss 5.203, Val loss 5.331\n",
      "Ep 1 (Step 000330): Train loss 5.191, Val loss 5.313\n",
      "Ep 1 (Step 000340): Train loss 5.108, Val loss 5.293\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2927\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.581, Val loss 8.554\n",
      "Ep 1 (Step 000010): Train loss 6.833, Val loss 6.794\n",
      "Ep 1 (Step 000020): Train loss 6.482, Val loss 6.445\n",
      "Ep 1 (Step 000030): Train loss 6.414, Val loss 6.366\n",
      "Ep 1 (Step 000040): Train loss 6.278, Val loss 6.262\n",
      "Ep 1 (Step 000050): Train loss 6.008, Val loss 6.111\n",
      "Ep 1 (Step 000060): Train loss 5.933, Val loss 6.008\n",
      "Ep 1 (Step 000070): Train loss 5.861, Val loss 5.906\n",
      "Ep 1 (Step 000080): Train loss 5.896, Val loss 5.832\n",
      "Ep 1 (Step 000090): Train loss 5.713, Val loss 5.776\n",
      "Ep 1 (Step 000100): Train loss 5.609, Val loss 5.723\n",
      "Ep 1 (Step 000110): Train loss 5.614, Val loss 5.675\n",
      "Ep 1 (Step 000120): Train loss 5.550, Val loss 5.637\n",
      "Ep 1 (Step 000130): Train loss 5.508, Val loss 5.583\n",
      "Ep 1 (Step 000140): Train loss 5.483, Val loss 5.555\n",
      "Ep 1 (Step 000150): Train loss 5.472, Val loss 5.504\n",
      "Ep 1 (Step 000160): Train loss 5.383, Val loss 5.506\n",
      "Ep 1 (Step 000170): Train loss 5.392, Val loss 5.485\n",
      "Ep 1 (Step 000180): Train loss 5.428, Val loss 5.445\n",
      "Ep 1 (Step 000190): Train loss 5.362, Val loss 5.428\n",
      "Ep 1 (Step 000200): Train loss 5.215, Val loss 5.392\n",
      "Ep 1 (Step 000210): Train loss 5.286, Val loss 5.385\n",
      "Ep 1 (Step 000220): Train loss 5.264, Val loss 5.351\n",
      "Ep 1 (Step 000230): Train loss 5.241, Val loss 5.350\n",
      "Ep 1 (Step 000240): Train loss 5.207, Val loss 5.329\n",
      "Ep 1 (Step 000250): Train loss 5.094, Val loss 5.332\n",
      "Ep 1 (Step 000260): Train loss 5.184, Val loss 5.302\n",
      "Ep 1 (Step 000270): Train loss 5.163, Val loss 5.291\n",
      "Ep 1 (Step 000280): Train loss 5.159, Val loss 5.282\n",
      "Ep 1 (Step 000290): Train loss 5.151, Val loss 5.277\n",
      "Ep 1 (Step 000300): Train loss 5.197, Val loss 5.256\n",
      "Ep 1 (Step 000310): Train loss 5.169, Val loss 5.246\n",
      "Ep 1 (Step 000320): Train loss 5.050, Val loss 5.251\n",
      "Ep 1 (Step 000330): Train loss 5.033, Val loss 5.236\n",
      "Ep 1 (Step 000340): Train loss 5.100, Val loss 5.223\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2227\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.555, Val loss 8.547\n",
      "Ep 1 (Step 000010): Train loss 6.832, Val loss 6.826\n",
      "Ep 1 (Step 000020): Train loss 6.508, Val loss 6.444\n",
      "Ep 1 (Step 000030): Train loss 6.317, Val loss 6.397\n",
      "Ep 1 (Step 000040): Train loss 6.262, Val loss 6.234\n",
      "Ep 1 (Step 000050): Train loss 6.130, Val loss 6.094\n",
      "Ep 1 (Step 000060): Train loss 5.927, Val loss 5.978\n",
      "Ep 1 (Step 000070): Train loss 5.787, Val loss 5.880\n",
      "Ep 1 (Step 000080): Train loss 5.795, Val loss 5.812\n",
      "Ep 1 (Step 000090): Train loss 5.700, Val loss 5.746\n",
      "Ep 1 (Step 000100): Train loss 5.652, Val loss 5.704\n",
      "Ep 1 (Step 000110): Train loss 5.550, Val loss 5.636\n",
      "Ep 1 (Step 000120): Train loss 5.543, Val loss 5.635\n",
      "Ep 1 (Step 000130): Train loss 5.582, Val loss 5.575\n",
      "Ep 1 (Step 000140): Train loss 5.467, Val loss 5.562\n",
      "Ep 1 (Step 000150): Train loss 5.522, Val loss 5.536\n",
      "Ep 1 (Step 000160): Train loss 5.315, Val loss 5.491\n",
      "Ep 1 (Step 000170): Train loss 5.392, Val loss 5.457\n",
      "Ep 1 (Step 000180): Train loss 5.339, Val loss 5.430\n",
      "Ep 1 (Step 000190): Train loss 5.324, Val loss 5.410\n",
      "Ep 1 (Step 000200): Train loss 5.365, Val loss 5.418\n",
      "Ep 1 (Step 000210): Train loss 5.255, Val loss 5.364\n",
      "Ep 1 (Step 000220): Train loss 5.203, Val loss 5.356\n",
      "Ep 1 (Step 000230): Train loss 5.298, Val loss 5.330\n",
      "Ep 1 (Step 000240): Train loss 5.244, Val loss 5.303\n",
      "Ep 1 (Step 000250): Train loss 5.228, Val loss 5.291\n",
      "Ep 1 (Step 000260): Train loss 5.162, Val loss 5.293\n",
      "Ep 1 (Step 000270): Train loss 5.209, Val loss 5.289\n",
      "Ep 1 (Step 000280): Train loss 5.167, Val loss 5.279\n",
      "Ep 1 (Step 000290): Train loss 5.199, Val loss 5.259\n",
      "Ep 1 (Step 000300): Train loss 5.112, Val loss 5.278\n",
      "Ep 1 (Step 000310): Train loss 5.055, Val loss 5.239\n",
      "Ep 1 (Step 000320): Train loss 5.083, Val loss 5.241\n",
      "Ep 1 (Step 000330): Train loss 5.099, Val loss 5.230\n",
      "Ep 1 (Step 000340): Train loss 5.128, Val loss 5.218\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2180\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.568, Val loss 8.544\n",
      "Ep 1 (Step 000010): Train loss 6.843, Val loss 6.829\n",
      "Ep 1 (Step 000020): Train loss 6.481, Val loss 6.450\n",
      "Ep 1 (Step 000030): Train loss 6.340, Val loss 6.361\n",
      "Ep 1 (Step 000040): Train loss 6.177, Val loss 6.211\n",
      "Ep 1 (Step 000050): Train loss 6.082, Val loss 6.064\n",
      "Ep 1 (Step 000060): Train loss 5.945, Val loss 5.945\n",
      "Ep 1 (Step 000070): Train loss 5.833, Val loss 5.843\n",
      "Ep 1 (Step 000080): Train loss 5.768, Val loss 5.764\n",
      "Ep 1 (Step 000090): Train loss 5.696, Val loss 5.723\n",
      "Ep 1 (Step 000100): Train loss 5.576, Val loss 5.682\n",
      "Ep 1 (Step 000110): Train loss 5.538, Val loss 5.625\n",
      "Ep 1 (Step 000120): Train loss 5.567, Val loss 5.577\n",
      "Ep 1 (Step 000130): Train loss 5.451, Val loss 5.561\n",
      "Ep 1 (Step 000140): Train loss 5.506, Val loss 5.519\n",
      "Ep 1 (Step 000150): Train loss 5.484, Val loss 5.521\n",
      "Ep 1 (Step 000160): Train loss 5.408, Val loss 5.467\n",
      "Ep 1 (Step 000170): Train loss 5.360, Val loss 5.460\n",
      "Ep 1 (Step 000180): Train loss 5.408, Val loss 5.438\n",
      "Ep 1 (Step 000190): Train loss 5.257, Val loss 5.410\n",
      "Ep 1 (Step 000200): Train loss 5.292, Val loss 5.406\n",
      "Ep 1 (Step 000210): Train loss 5.270, Val loss 5.378\n",
      "Ep 1 (Step 000220): Train loss 5.302, Val loss 5.357\n",
      "Ep 1 (Step 000230): Train loss 5.167, Val loss 5.353\n",
      "Ep 1 (Step 000240): Train loss 5.200, Val loss 5.331\n",
      "Ep 1 (Step 000250): Train loss 5.149, Val loss 5.321\n",
      "Ep 1 (Step 000260): Train loss 5.201, Val loss 5.298\n",
      "Ep 1 (Step 000270): Train loss 5.168, Val loss 5.292\n",
      "Ep 1 (Step 000280): Train loss 5.144, Val loss 5.294\n",
      "Ep 1 (Step 000290): Train loss 5.060, Val loss 5.293\n",
      "Ep 1 (Step 000300): Train loss 5.085, Val loss 5.272\n",
      "Ep 1 (Step 000310): Train loss 5.053, Val loss 5.255\n",
      "Ep 1 (Step 000320): Train loss 5.024, Val loss 5.238\n",
      "Ep 1 (Step 000330): Train loss 5.135, Val loss 5.242\n",
      "Ep 1 (Step 000340): Train loss 5.030, Val loss 5.224\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2236\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.650, Val loss 8.605\n",
      "Ep 1 (Step 000010): Train loss 6.921, Val loss 6.861\n",
      "Ep 1 (Step 000020): Train loss 6.463, Val loss 6.423\n",
      "Ep 1 (Step 000030): Train loss 6.334, Val loss 6.377\n",
      "Ep 1 (Step 000040): Train loss 6.203, Val loss 6.207\n",
      "Ep 1 (Step 000050): Train loss 6.112, Val loss 6.105\n",
      "Ep 1 (Step 000060): Train loss 5.960, Val loss 5.931\n",
      "Ep 1 (Step 000070): Train loss 5.757, Val loss 5.837\n",
      "Ep 1 (Step 000080): Train loss 5.728, Val loss 5.785\n",
      "Ep 1 (Step 000090): Train loss 5.651, Val loss 5.737\n",
      "Ep 1 (Step 000100): Train loss 5.658, Val loss 5.663\n",
      "Ep 1 (Step 000110): Train loss 5.511, Val loss 5.626\n",
      "Ep 1 (Step 000120): Train loss 5.458, Val loss 5.571\n",
      "Ep 1 (Step 000130): Train loss 5.501, Val loss 5.538\n",
      "Ep 1 (Step 000140): Train loss 5.448, Val loss 5.519\n",
      "Ep 1 (Step 000150): Train loss 5.331, Val loss 5.492\n",
      "Ep 1 (Step 000160): Train loss 5.437, Val loss 5.451\n",
      "Ep 1 (Step 000170): Train loss 5.285, Val loss 5.430\n",
      "Ep 1 (Step 000180): Train loss 5.336, Val loss 5.427\n",
      "Ep 1 (Step 000190): Train loss 5.332, Val loss 5.406\n",
      "Ep 1 (Step 000200): Train loss 5.317, Val loss 5.379\n",
      "Ep 1 (Step 000210): Train loss 5.233, Val loss 5.379\n",
      "Ep 1 (Step 000220): Train loss 5.298, Val loss 5.362\n",
      "Ep 1 (Step 000230): Train loss 5.198, Val loss 5.343\n",
      "Ep 1 (Step 000240): Train loss 5.240, Val loss 5.345\n",
      "Ep 1 (Step 000250): Train loss 5.221, Val loss 5.317\n",
      "Ep 1 (Step 000260): Train loss 5.132, Val loss 5.286\n",
      "Ep 1 (Step 000270): Train loss 5.113, Val loss 5.286\n",
      "Ep 1 (Step 000280): Train loss 5.104, Val loss 5.276\n",
      "Ep 1 (Step 000290): Train loss 5.117, Val loss 5.271\n",
      "Ep 1 (Step 000300): Train loss 5.097, Val loss 5.259\n",
      "Ep 1 (Step 000310): Train loss 5.093, Val loss 5.237\n",
      "Ep 1 (Step 000320): Train loss 5.116, Val loss 5.224\n",
      "Ep 1 (Step 000330): Train loss 5.047, Val loss 5.225\n",
      "Ep 1 (Step 000340): Train loss 5.050, Val loss 5.201\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2009\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.680, Val loss 8.612\n",
      "Ep 1 (Step 000010): Train loss 6.880, Val loss 6.808\n",
      "Ep 1 (Step 000020): Train loss 6.457, Val loss 6.449\n",
      "Ep 1 (Step 000030): Train loss 6.330, Val loss 6.414\n",
      "Ep 1 (Step 000040): Train loss 6.262, Val loss 6.216\n",
      "Ep 1 (Step 000050): Train loss 6.063, Val loss 6.094\n",
      "Ep 1 (Step 000060): Train loss 5.949, Val loss 5.955\n",
      "Ep 1 (Step 000070): Train loss 5.786, Val loss 5.857\n",
      "Ep 1 (Step 000080): Train loss 5.738, Val loss 5.791\n",
      "Ep 1 (Step 000090): Train loss 5.613, Val loss 5.732\n",
      "Ep 1 (Step 000100): Train loss 5.585, Val loss 5.670\n",
      "Ep 1 (Step 000110): Train loss 5.600, Val loss 5.628\n",
      "Ep 1 (Step 000120): Train loss 5.588, Val loss 5.585\n",
      "Ep 1 (Step 000130): Train loss 5.524, Val loss 5.548\n",
      "Ep 1 (Step 000140): Train loss 5.426, Val loss 5.509\n",
      "Ep 1 (Step 000150): Train loss 5.455, Val loss 5.489\n",
      "Ep 1 (Step 000160): Train loss 5.388, Val loss 5.443\n",
      "Ep 1 (Step 000170): Train loss 5.377, Val loss 5.420\n",
      "Ep 1 (Step 000180): Train loss 5.306, Val loss 5.417\n",
      "Ep 1 (Step 000190): Train loss 5.312, Val loss 5.392\n",
      "Ep 1 (Step 000200): Train loss 5.352, Val loss 5.367\n",
      "Ep 1 (Step 000210): Train loss 5.203, Val loss 5.337\n",
      "Ep 1 (Step 000220): Train loss 5.290, Val loss 5.332\n",
      "Ep 1 (Step 000230): Train loss 5.197, Val loss 5.322\n",
      "Ep 1 (Step 000240): Train loss 5.222, Val loss 5.299\n",
      "Ep 1 (Step 000250): Train loss 5.148, Val loss 5.287\n",
      "Ep 1 (Step 000260): Train loss 5.209, Val loss 5.296\n",
      "Ep 1 (Step 000270): Train loss 5.127, Val loss 5.269\n",
      "Ep 1 (Step 000280): Train loss 5.086, Val loss 5.264\n",
      "Ep 1 (Step 000290): Train loss 5.123, Val loss 5.224\n",
      "Ep 1 (Step 000300): Train loss 5.078, Val loss 5.236\n",
      "Ep 1 (Step 000310): Train loss 5.081, Val loss 5.216\n",
      "Ep 1 (Step 000320): Train loss 5.119, Val loss 5.223\n",
      "Ep 1 (Step 000330): Train loss 5.027, Val loss 5.203\n",
      "Ep 1 (Step 000340): Train loss 5.044, Val loss 5.185\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1846\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.622, Val loss 8.582\n",
      "Ep 1 (Step 000010): Train loss 6.865, Val loss 6.816\n",
      "Ep 1 (Step 000020): Train loss 6.567, Val loss 6.467\n",
      "Ep 1 (Step 000030): Train loss 6.415, Val loss 6.441\n",
      "Ep 1 (Step 000040): Train loss 6.270, Val loss 6.267\n",
      "Ep 1 (Step 000050): Train loss 6.054, Val loss 6.121\n",
      "Ep 1 (Step 000060): Train loss 5.936, Val loss 5.982\n",
      "Ep 1 (Step 000070): Train loss 5.802, Val loss 5.865\n",
      "Ep 1 (Step 000080): Train loss 5.761, Val loss 5.807\n",
      "Ep 1 (Step 000090): Train loss 5.682, Val loss 5.739\n",
      "Ep 1 (Step 000100): Train loss 5.618, Val loss 5.678\n",
      "Ep 1 (Step 000110): Train loss 5.599, Val loss 5.633\n",
      "Ep 1 (Step 000120): Train loss 5.523, Val loss 5.594\n",
      "Ep 1 (Step 000130): Train loss 5.487, Val loss 5.539\n",
      "Ep 1 (Step 000140): Train loss 5.414, Val loss 5.513\n",
      "Ep 1 (Step 000150): Train loss 5.497, Val loss 5.488\n",
      "Ep 1 (Step 000160): Train loss 5.491, Val loss 5.468\n",
      "Ep 1 (Step 000170): Train loss 5.383, Val loss 5.425\n",
      "Ep 1 (Step 000180): Train loss 5.380, Val loss 5.419\n",
      "Ep 1 (Step 000190): Train loss 5.359, Val loss 5.405\n",
      "Ep 1 (Step 000200): Train loss 5.253, Val loss 5.387\n",
      "Ep 1 (Step 000210): Train loss 5.290, Val loss 5.369\n",
      "Ep 1 (Step 000220): Train loss 5.279, Val loss 5.376\n",
      "Ep 1 (Step 000230): Train loss 5.316, Val loss 5.347\n",
      "Ep 1 (Step 000240): Train loss 5.226, Val loss 5.324\n",
      "Ep 1 (Step 000250): Train loss 5.157, Val loss 5.313\n",
      "Ep 1 (Step 000260): Train loss 5.157, Val loss 5.323\n",
      "Ep 1 (Step 000270): Train loss 5.144, Val loss 5.280\n",
      "Ep 1 (Step 000280): Train loss 5.130, Val loss 5.269\n",
      "Ep 1 (Step 000290): Train loss 5.127, Val loss 5.268\n",
      "Ep 1 (Step 000300): Train loss 5.141, Val loss 5.243\n",
      "Ep 1 (Step 000310): Train loss 5.115, Val loss 5.241\n",
      "Ep 1 (Step 000320): Train loss 5.016, Val loss 5.231\n",
      "Ep 1 (Step 000330): Train loss 5.047, Val loss 5.218\n",
      "Ep 1 (Step 000340): Train loss 5.071, Val loss 5.202\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2024\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.848, Val loss 8.833\n",
      "Ep 1 (Step 000010): Train loss 7.425, Val loss 7.384\n",
      "Ep 1 (Step 000020): Train loss 6.852, Val loss 6.762\n",
      "Ep 1 (Step 000030): Train loss 6.514, Val loss 6.458\n",
      "Ep 1 (Step 000040): Train loss 6.338, Val loss 6.341\n",
      "Ep 1 (Step 000050): Train loss 6.237, Val loss 6.260\n",
      "Ep 1 (Step 000060): Train loss 6.142, Val loss 6.149\n",
      "Ep 1 (Step 000070): Train loss 6.043, Val loss 6.057\n",
      "Ep 1 (Step 000080): Train loss 5.962, Val loss 5.953\n",
      "Ep 1 (Step 000090): Train loss 5.878, Val loss 5.898\n",
      "Ep 1 (Step 000100): Train loss 5.810, Val loss 5.828\n",
      "Ep 1 (Step 000110): Train loss 5.792, Val loss 5.786\n",
      "Ep 1 (Step 000120): Train loss 5.726, Val loss 5.735\n",
      "Ep 1 (Step 000130): Train loss 5.574, Val loss 5.700\n",
      "Ep 1 (Step 000140): Train loss 5.659, Val loss 5.657\n",
      "Ep 1 (Step 000150): Train loss 5.591, Val loss 5.630\n",
      "Ep 1 (Step 000160): Train loss 5.524, Val loss 5.604\n",
      "Ep 1 (Step 000170): Train loss 5.492, Val loss 5.566\n",
      "Ep 1 (Step 000180): Train loss 5.489, Val loss 5.534\n",
      "Ep 1 (Step 000190): Train loss 5.409, Val loss 5.518\n",
      "Ep 1 (Step 000200): Train loss 5.415, Val loss 5.491\n",
      "Ep 1 (Step 000210): Train loss 5.402, Val loss 5.477\n",
      "Ep 1 (Step 000220): Train loss 5.406, Val loss 5.453\n",
      "Ep 1 (Step 000230): Train loss 5.338, Val loss 5.439\n",
      "Ep 1 (Step 000240): Train loss 5.411, Val loss 5.414\n",
      "Ep 1 (Step 000250): Train loss 5.326, Val loss 5.400\n",
      "Ep 1 (Step 000260): Train loss 5.242, Val loss 5.385\n",
      "Ep 1 (Step 000270): Train loss 5.310, Val loss 5.368\n",
      "Ep 1 (Step 000280): Train loss 5.255, Val loss 5.360\n",
      "Ep 1 (Step 000290): Train loss 5.211, Val loss 5.345\n",
      "Ep 1 (Step 000300): Train loss 5.170, Val loss 5.325\n",
      "Ep 1 (Step 000310): Train loss 5.294, Val loss 5.327\n",
      "Ep 1 (Step 000320): Train loss 5.170, Val loss 5.312\n",
      "Ep 1 (Step 000330): Train loss 5.187, Val loss 5.306\n",
      "Ep 1 (Step 000340): Train loss 5.141, Val loss 5.293\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2931\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.866, Val loss 8.858\n",
      "Ep 1 (Step 000010): Train loss 7.403, Val loss 7.366\n",
      "Ep 1 (Step 000020): Train loss 6.741, Val loss 6.739\n",
      "Ep 1 (Step 000030): Train loss 6.447, Val loss 6.465\n",
      "Ep 1 (Step 000040): Train loss 6.335, Val loss 6.384\n",
      "Ep 1 (Step 000050): Train loss 6.289, Val loss 6.300\n",
      "Ep 1 (Step 000060): Train loss 6.142, Val loss 6.163\n",
      "Ep 1 (Step 000070): Train loss 6.085, Val loss 6.076\n",
      "Ep 1 (Step 000080): Train loss 5.901, Val loss 5.989\n",
      "Ep 1 (Step 000090): Train loss 5.860, Val loss 5.919\n",
      "Ep 1 (Step 000100): Train loss 5.778, Val loss 5.854\n",
      "Ep 1 (Step 000110): Train loss 5.762, Val loss 5.797\n",
      "Ep 1 (Step 000120): Train loss 5.648, Val loss 5.759\n",
      "Ep 1 (Step 000130): Train loss 5.615, Val loss 5.714\n",
      "Ep 1 (Step 000140): Train loss 5.649, Val loss 5.690\n",
      "Ep 1 (Step 000150): Train loss 5.561, Val loss 5.647\n",
      "Ep 1 (Step 000160): Train loss 5.563, Val loss 5.603\n",
      "Ep 1 (Step 000170): Train loss 5.546, Val loss 5.579\n",
      "Ep 1 (Step 000180): Train loss 5.464, Val loss 5.568\n",
      "Ep 1 (Step 000190): Train loss 5.482, Val loss 5.530\n",
      "Ep 1 (Step 000200): Train loss 5.406, Val loss 5.501\n",
      "Ep 1 (Step 000210): Train loss 5.446, Val loss 5.479\n",
      "Ep 1 (Step 000220): Train loss 5.385, Val loss 5.461\n",
      "Ep 1 (Step 000230): Train loss 5.312, Val loss 5.439\n",
      "Ep 1 (Step 000240): Train loss 5.413, Val loss 5.433\n",
      "Ep 1 (Step 000250): Train loss 5.302, Val loss 5.421\n",
      "Ep 1 (Step 000260): Train loss 5.311, Val loss 5.404\n",
      "Ep 1 (Step 000270): Train loss 5.300, Val loss 5.378\n",
      "Ep 1 (Step 000280): Train loss 5.283, Val loss 5.367\n",
      "Ep 1 (Step 000290): Train loss 5.228, Val loss 5.347\n",
      "Ep 1 (Step 000300): Train loss 5.284, Val loss 5.355\n",
      "Ep 1 (Step 000310): Train loss 5.239, Val loss 5.346\n",
      "Ep 1 (Step 000320): Train loss 5.249, Val loss 5.330\n",
      "Ep 1 (Step 000330): Train loss 5.216, Val loss 5.328\n",
      "Ep 1 (Step 000340): Train loss 5.184, Val loss 5.312\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3120\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.900, Val loss 8.865\n",
      "Ep 1 (Step 000010): Train loss 7.448, Val loss 7.371\n",
      "Ep 1 (Step 000020): Train loss 6.776, Val loss 6.725\n",
      "Ep 1 (Step 000030): Train loss 6.496, Val loss 6.425\n",
      "Ep 1 (Step 000040): Train loss 6.403, Val loss 6.337\n",
      "Ep 1 (Step 000050): Train loss 6.195, Val loss 6.236\n",
      "Ep 1 (Step 000060): Train loss 6.103, Val loss 6.111\n",
      "Ep 1 (Step 000070): Train loss 6.074, Val loss 6.031\n",
      "Ep 1 (Step 000080): Train loss 5.945, Val loss 5.966\n",
      "Ep 1 (Step 000090): Train loss 5.865, Val loss 5.932\n",
      "Ep 1 (Step 000100): Train loss 5.769, Val loss 5.851\n",
      "Ep 1 (Step 000110): Train loss 5.669, Val loss 5.788\n",
      "Ep 1 (Step 000120): Train loss 5.736, Val loss 5.738\n",
      "Ep 1 (Step 000130): Train loss 5.666, Val loss 5.709\n",
      "Ep 1 (Step 000140): Train loss 5.575, Val loss 5.668\n",
      "Ep 1 (Step 000150): Train loss 5.591, Val loss 5.634\n",
      "Ep 1 (Step 000160): Train loss 5.553, Val loss 5.611\n",
      "Ep 1 (Step 000170): Train loss 5.605, Val loss 5.580\n",
      "Ep 1 (Step 000180): Train loss 5.479, Val loss 5.553\n",
      "Ep 1 (Step 000190): Train loss 5.489, Val loss 5.520\n",
      "Ep 1 (Step 000200): Train loss 5.452, Val loss 5.502\n",
      "Ep 1 (Step 000210): Train loss 5.327, Val loss 5.488\n",
      "Ep 1 (Step 000220): Train loss 5.346, Val loss 5.484\n",
      "Ep 1 (Step 000230): Train loss 5.446, Val loss 5.451\n",
      "Ep 1 (Step 000240): Train loss 5.360, Val loss 5.436\n",
      "Ep 1 (Step 000250): Train loss 5.304, Val loss 5.406\n",
      "Ep 1 (Step 000260): Train loss 5.323, Val loss 5.387\n",
      "Ep 1 (Step 000270): Train loss 5.375, Val loss 5.393\n",
      "Ep 1 (Step 000280): Train loss 5.255, Val loss 5.374\n",
      "Ep 1 (Step 000290): Train loss 5.315, Val loss 5.360\n",
      "Ep 1 (Step 000300): Train loss 5.201, Val loss 5.350\n",
      "Ep 1 (Step 000310): Train loss 5.206, Val loss 5.327\n",
      "Ep 1 (Step 000320): Train loss 5.126, Val loss 5.311\n",
      "Ep 1 (Step 000330): Train loss 5.207, Val loss 5.328\n",
      "Ep 1 (Step 000340): Train loss 5.151, Val loss 5.329\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3293\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.919, Val loss 8.917\n",
      "Ep 1 (Step 000010): Train loss 7.447, Val loss 7.387\n",
      "Ep 1 (Step 000020): Train loss 6.727, Val loss 6.733\n",
      "Ep 1 (Step 000030): Train loss 6.432, Val loss 6.430\n",
      "Ep 1 (Step 000040): Train loss 6.285, Val loss 6.353\n",
      "Ep 1 (Step 000050): Train loss 6.211, Val loss 6.277\n",
      "Ep 1 (Step 000060): Train loss 6.164, Val loss 6.143\n",
      "Ep 1 (Step 000070): Train loss 6.032, Val loss 6.035\n",
      "Ep 1 (Step 000080): Train loss 5.900, Val loss 5.967\n",
      "Ep 1 (Step 000090): Train loss 5.854, Val loss 5.911\n",
      "Ep 1 (Step 000100): Train loss 5.743, Val loss 5.845\n",
      "Ep 1 (Step 000110): Train loss 5.772, Val loss 5.812\n",
      "Ep 1 (Step 000120): Train loss 5.761, Val loss 5.746\n",
      "Ep 1 (Step 000130): Train loss 5.741, Val loss 5.707\n",
      "Ep 1 (Step 000140): Train loss 5.677, Val loss 5.674\n",
      "Ep 1 (Step 000150): Train loss 5.615, Val loss 5.642\n",
      "Ep 1 (Step 000160): Train loss 5.572, Val loss 5.601\n",
      "Ep 1 (Step 000170): Train loss 5.533, Val loss 5.581\n",
      "Ep 1 (Step 000180): Train loss 5.445, Val loss 5.547\n",
      "Ep 1 (Step 000190): Train loss 5.459, Val loss 5.524\n",
      "Ep 1 (Step 000200): Train loss 5.435, Val loss 5.498\n",
      "Ep 1 (Step 000210): Train loss 5.292, Val loss 5.467\n",
      "Ep 1 (Step 000220): Train loss 5.425, Val loss 5.454\n",
      "Ep 1 (Step 000230): Train loss 5.361, Val loss 5.429\n",
      "Ep 1 (Step 000240): Train loss 5.288, Val loss 5.426\n",
      "Ep 1 (Step 000250): Train loss 5.257, Val loss 5.411\n",
      "Ep 1 (Step 000260): Train loss 5.207, Val loss 5.383\n",
      "Ep 1 (Step 000270): Train loss 5.267, Val loss 5.377\n",
      "Ep 1 (Step 000280): Train loss 5.257, Val loss 5.355\n",
      "Ep 1 (Step 000290): Train loss 5.152, Val loss 5.342\n",
      "Ep 1 (Step 000300): Train loss 5.156, Val loss 5.330\n",
      "Ep 1 (Step 000310): Train loss 5.224, Val loss 5.319\n",
      "Ep 1 (Step 000320): Train loss 5.138, Val loss 5.304\n",
      "Ep 1 (Step 000330): Train loss 5.187, Val loss 5.294\n",
      "Ep 1 (Step 000340): Train loss 5.138, Val loss 5.278\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2778\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.885, Val loss 8.873\n",
      "Ep 1 (Step 000010): Train loss 7.462, Val loss 7.372\n",
      "Ep 1 (Step 000020): Train loss 6.750, Val loss 6.746\n",
      "Ep 1 (Step 000030): Train loss 6.479, Val loss 6.445\n",
      "Ep 1 (Step 000040): Train loss 6.353, Val loss 6.347\n",
      "Ep 1 (Step 000050): Train loss 6.274, Val loss 6.263\n",
      "Ep 1 (Step 000060): Train loss 6.180, Val loss 6.149\n",
      "Ep 1 (Step 000070): Train loss 6.032, Val loss 6.036\n",
      "Ep 1 (Step 000080): Train loss 5.890, Val loss 5.955\n",
      "Ep 1 (Step 000090): Train loss 5.893, Val loss 5.910\n",
      "Ep 1 (Step 000100): Train loss 5.820, Val loss 5.849\n",
      "Ep 1 (Step 000110): Train loss 5.714, Val loss 5.788\n",
      "Ep 1 (Step 000120): Train loss 5.607, Val loss 5.752\n",
      "Ep 1 (Step 000130): Train loss 5.595, Val loss 5.703\n",
      "Ep 1 (Step 000140): Train loss 5.572, Val loss 5.688\n",
      "Ep 1 (Step 000150): Train loss 5.514, Val loss 5.642\n",
      "Ep 1 (Step 000160): Train loss 5.579, Val loss 5.613\n",
      "Ep 1 (Step 000170): Train loss 5.527, Val loss 5.567\n",
      "Ep 1 (Step 000180): Train loss 5.470, Val loss 5.534\n",
      "Ep 1 (Step 000190): Train loss 5.533, Val loss 5.523\n",
      "Ep 1 (Step 000200): Train loss 5.395, Val loss 5.505\n",
      "Ep 1 (Step 000210): Train loss 5.485, Val loss 5.489\n",
      "Ep 1 (Step 000220): Train loss 5.349, Val loss 5.454\n",
      "Ep 1 (Step 000230): Train loss 5.334, Val loss 5.428\n",
      "Ep 1 (Step 000240): Train loss 5.305, Val loss 5.409\n",
      "Ep 1 (Step 000250): Train loss 5.307, Val loss 5.402\n",
      "Ep 1 (Step 000260): Train loss 5.269, Val loss 5.387\n",
      "Ep 1 (Step 000270): Train loss 5.251, Val loss 5.383\n",
      "Ep 1 (Step 000280): Train loss 5.251, Val loss 5.365\n",
      "Ep 1 (Step 000290): Train loss 5.200, Val loss 5.351\n",
      "Ep 1 (Step 000300): Train loss 5.245, Val loss 5.330\n",
      "Ep 1 (Step 000310): Train loss 5.180, Val loss 5.326\n",
      "Ep 1 (Step 000320): Train loss 5.237, Val loss 5.311\n",
      "Ep 1 (Step 000330): Train loss 5.237, Val loss 5.307\n",
      "Ep 1 (Step 000340): Train loss 5.163, Val loss 5.305\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3047\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.864, Val loss 8.839\n",
      "Ep 1 (Step 000010): Train loss 7.356, Val loss 7.333\n",
      "Ep 1 (Step 000020): Train loss 6.727, Val loss 6.716\n",
      "Ep 1 (Step 000030): Train loss 6.454, Val loss 6.432\n",
      "Ep 1 (Step 000040): Train loss 6.302, Val loss 6.335\n",
      "Ep 1 (Step 000050): Train loss 6.291, Val loss 6.237\n",
      "Ep 1 (Step 000060): Train loss 6.093, Val loss 6.135\n",
      "Ep 1 (Step 000070): Train loss 6.017, Val loss 6.027\n",
      "Ep 1 (Step 000080): Train loss 5.902, Val loss 5.941\n",
      "Ep 1 (Step 000090): Train loss 5.882, Val loss 5.894\n",
      "Ep 1 (Step 000100): Train loss 5.809, Val loss 5.835\n",
      "Ep 1 (Step 000110): Train loss 5.731, Val loss 5.786\n",
      "Ep 1 (Step 000120): Train loss 5.669, Val loss 5.744\n",
      "Ep 1 (Step 000130): Train loss 5.591, Val loss 5.693\n",
      "Ep 1 (Step 000140): Train loss 5.593, Val loss 5.663\n",
      "Ep 1 (Step 000150): Train loss 5.533, Val loss 5.629\n",
      "Ep 1 (Step 000160): Train loss 5.551, Val loss 5.585\n",
      "Ep 1 (Step 000170): Train loss 5.503, Val loss 5.570\n",
      "Ep 1 (Step 000180): Train loss 5.464, Val loss 5.528\n",
      "Ep 1 (Step 000190): Train loss 5.333, Val loss 5.518\n",
      "Ep 1 (Step 000200): Train loss 5.393, Val loss 5.501\n",
      "Ep 1 (Step 000210): Train loss 5.390, Val loss 5.485\n",
      "Ep 1 (Step 000220): Train loss 5.421, Val loss 5.466\n",
      "Ep 1 (Step 000230): Train loss 5.262, Val loss 5.431\n",
      "Ep 1 (Step 000240): Train loss 5.343, Val loss 5.397\n",
      "Ep 1 (Step 000250): Train loss 5.262, Val loss 5.389\n",
      "Ep 1 (Step 000260): Train loss 5.267, Val loss 5.373\n",
      "Ep 1 (Step 000270): Train loss 5.253, Val loss 5.371\n",
      "Ep 1 (Step 000280): Train loss 5.209, Val loss 5.351\n",
      "Ep 1 (Step 000290): Train loss 5.345, Val loss 5.331\n",
      "Ep 1 (Step 000300): Train loss 5.148, Val loss 5.318\n",
      "Ep 1 (Step 000310): Train loss 5.225, Val loss 5.304\n",
      "Ep 1 (Step 000320): Train loss 5.208, Val loss 5.309\n",
      "Ep 1 (Step 000330): Train loss 5.161, Val loss 5.309\n",
      "Ep 1 (Step 000340): Train loss 5.157, Val loss 5.288\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2880\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.610, Val loss 8.519\n",
      "Ep 1 (Step 000010): Train loss 6.810, Val loss 6.791\n",
      "Ep 1 (Step 000020): Train loss 6.474, Val loss 6.444\n",
      "Ep 1 (Step 000030): Train loss 6.400, Val loss 6.337\n",
      "Ep 1 (Step 000040): Train loss 6.149, Val loss 6.178\n",
      "Ep 1 (Step 000050): Train loss 6.011, Val loss 6.028\n",
      "Ep 1 (Step 000060): Train loss 5.890, Val loss 5.918\n",
      "Ep 1 (Step 000070): Train loss 5.855, Val loss 5.834\n",
      "Ep 1 (Step 000080): Train loss 5.705, Val loss 5.759\n",
      "Ep 1 (Step 000090): Train loss 5.684, Val loss 5.727\n",
      "Ep 1 (Step 000100): Train loss 5.659, Val loss 5.689\n",
      "Ep 1 (Step 000110): Train loss 5.510, Val loss 5.625\n",
      "Ep 1 (Step 000120): Train loss 5.587, Val loss 5.583\n",
      "Ep 1 (Step 000130): Train loss 5.513, Val loss 5.557\n",
      "Ep 1 (Step 000140): Train loss 5.436, Val loss 5.527\n",
      "Ep 1 (Step 000150): Train loss 5.463, Val loss 5.498\n",
      "Ep 1 (Step 000160): Train loss 5.468, Val loss 5.483\n",
      "Ep 1 (Step 000170): Train loss 5.375, Val loss 5.460\n",
      "Ep 1 (Step 000180): Train loss 5.409, Val loss 5.432\n",
      "Ep 1 (Step 000190): Train loss 5.296, Val loss 5.419\n",
      "Ep 1 (Step 000200): Train loss 5.289, Val loss 5.394\n",
      "Ep 1 (Step 000210): Train loss 5.240, Val loss 5.360\n",
      "Ep 1 (Step 000220): Train loss 5.252, Val loss 5.369\n",
      "Ep 1 (Step 000230): Train loss 5.228, Val loss 5.335\n",
      "Ep 1 (Step 000240): Train loss 5.230, Val loss 5.314\n",
      "Ep 1 (Step 000250): Train loss 5.213, Val loss 5.311\n",
      "Ep 1 (Step 000260): Train loss 5.190, Val loss 5.311\n",
      "Ep 1 (Step 000270): Train loss 5.202, Val loss 5.300\n",
      "Ep 1 (Step 000280): Train loss 5.057, Val loss 5.268\n",
      "Ep 1 (Step 000290): Train loss 5.105, Val loss 5.270\n",
      "Ep 1 (Step 000300): Train loss 5.283, Val loss 5.265\n",
      "Ep 1 (Step 000310): Train loss 5.125, Val loss 5.230\n",
      "Ep 1 (Step 000320): Train loss 5.085, Val loss 5.232\n",
      "Ep 1 (Step 000330): Train loss 4.985, Val loss 5.217\n",
      "Ep 1 (Step 000340): Train loss 5.095, Val loss 5.216\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2159\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.618, Val loss 8.588\n",
      "Ep 1 (Step 000010): Train loss 6.815, Val loss 6.834\n",
      "Ep 1 (Step 000020): Train loss 6.449, Val loss 6.478\n",
      "Ep 1 (Step 000030): Train loss 6.420, Val loss 6.439\n",
      "Ep 1 (Step 000040): Train loss 6.310, Val loss 6.287\n",
      "Ep 1 (Step 000050): Train loss 6.120, Val loss 6.137\n",
      "Ep 1 (Step 000060): Train loss 5.935, Val loss 6.029\n",
      "Ep 1 (Step 000070): Train loss 5.884, Val loss 5.926\n",
      "Ep 1 (Step 000080): Train loss 5.803, Val loss 5.848\n",
      "Ep 1 (Step 000090): Train loss 5.670, Val loss 5.764\n",
      "Ep 1 (Step 000100): Train loss 5.605, Val loss 5.693\n",
      "Ep 1 (Step 000110): Train loss 5.559, Val loss 5.662\n",
      "Ep 1 (Step 000120): Train loss 5.554, Val loss 5.605\n",
      "Ep 1 (Step 000130): Train loss 5.467, Val loss 5.596\n",
      "Ep 1 (Step 000140): Train loss 5.538, Val loss 5.542\n",
      "Ep 1 (Step 000150): Train loss 5.417, Val loss 5.508\n",
      "Ep 1 (Step 000160): Train loss 5.425, Val loss 5.501\n",
      "Ep 1 (Step 000170): Train loss 5.389, Val loss 5.460\n",
      "Ep 1 (Step 000180): Train loss 5.349, Val loss 5.477\n",
      "Ep 1 (Step 000190): Train loss 5.385, Val loss 5.437\n",
      "Ep 1 (Step 000200): Train loss 5.299, Val loss 5.415\n",
      "Ep 1 (Step 000210): Train loss 5.349, Val loss 5.406\n",
      "Ep 1 (Step 000220): Train loss 5.193, Val loss 5.374\n",
      "Ep 1 (Step 000230): Train loss 5.227, Val loss 5.357\n",
      "Ep 1 (Step 000240): Train loss 5.224, Val loss 5.339\n",
      "Ep 1 (Step 000250): Train loss 5.169, Val loss 5.347\n",
      "Ep 1 (Step 000260): Train loss 5.188, Val loss 5.317\n",
      "Ep 1 (Step 000270): Train loss 5.193, Val loss 5.306\n",
      "Ep 1 (Step 000280): Train loss 5.199, Val loss 5.299\n",
      "Ep 1 (Step 000290): Train loss 5.247, Val loss 5.289\n",
      "Ep 1 (Step 000300): Train loss 5.162, Val loss 5.294\n",
      "Ep 1 (Step 000310): Train loss 5.201, Val loss 5.267\n",
      "Ep 1 (Step 000320): Train loss 5.132, Val loss 5.236\n",
      "Ep 1 (Step 000330): Train loss 5.137, Val loss 5.232\n",
      "Ep 1 (Step 000340): Train loss 5.031, Val loss 5.232\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2317\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.624, Val loss 8.598\n",
      "Ep 1 (Step 000010): Train loss 6.815, Val loss 6.781\n",
      "Ep 1 (Step 000020): Train loss 6.516, Val loss 6.445\n",
      "Ep 1 (Step 000030): Train loss 6.383, Val loss 6.403\n",
      "Ep 1 (Step 000040): Train loss 6.222, Val loss 6.211\n",
      "Ep 1 (Step 000050): Train loss 6.034, Val loss 6.101\n",
      "Ep 1 (Step 000060): Train loss 5.957, Val loss 5.969\n",
      "Ep 1 (Step 000070): Train loss 5.752, Val loss 5.886\n",
      "Ep 1 (Step 000080): Train loss 5.766, Val loss 5.827\n",
      "Ep 1 (Step 000090): Train loss 5.630, Val loss 5.746\n",
      "Ep 1 (Step 000100): Train loss 5.589, Val loss 5.679\n",
      "Ep 1 (Step 000110): Train loss 5.590, Val loss 5.636\n",
      "Ep 1 (Step 000120): Train loss 5.544, Val loss 5.594\n",
      "Ep 1 (Step 000130): Train loss 5.377, Val loss 5.557\n",
      "Ep 1 (Step 000140): Train loss 5.502, Val loss 5.539\n",
      "Ep 1 (Step 000150): Train loss 5.412, Val loss 5.508\n",
      "Ep 1 (Step 000160): Train loss 5.339, Val loss 5.489\n",
      "Ep 1 (Step 000170): Train loss 5.412, Val loss 5.463\n",
      "Ep 1 (Step 000180): Train loss 5.285, Val loss 5.445\n",
      "Ep 1 (Step 000190): Train loss 5.241, Val loss 5.411\n",
      "Ep 1 (Step 000200): Train loss 5.320, Val loss 5.400\n",
      "Ep 1 (Step 000210): Train loss 5.307, Val loss 5.394\n",
      "Ep 1 (Step 000220): Train loss 5.260, Val loss 5.364\n",
      "Ep 1 (Step 000230): Train loss 5.233, Val loss 5.343\n",
      "Ep 1 (Step 000240): Train loss 5.238, Val loss 5.355\n",
      "Ep 1 (Step 000250): Train loss 5.229, Val loss 5.332\n",
      "Ep 1 (Step 000260): Train loss 5.206, Val loss 5.318\n",
      "Ep 1 (Step 000270): Train loss 5.117, Val loss 5.298\n",
      "Ep 1 (Step 000280): Train loss 5.147, Val loss 5.286\n",
      "Ep 1 (Step 000290): Train loss 5.060, Val loss 5.265\n",
      "Ep 1 (Step 000300): Train loss 5.173, Val loss 5.261\n",
      "Ep 1 (Step 000310): Train loss 5.204, Val loss 5.254\n",
      "Ep 1 (Step 000320): Train loss 5.148, Val loss 5.249\n",
      "Ep 1 (Step 000330): Train loss 5.056, Val loss 5.243\n",
      "Ep 1 (Step 000340): Train loss 5.099, Val loss 5.230\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2302\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.649, Val loss 8.627\n",
      "Ep 1 (Step 000010): Train loss 6.830, Val loss 6.813\n",
      "Ep 1 (Step 000020): Train loss 6.475, Val loss 6.430\n",
      "Ep 1 (Step 000030): Train loss 6.394, Val loss 6.359\n",
      "Ep 1 (Step 000040): Train loss 6.250, Val loss 6.234\n",
      "Ep 1 (Step 000050): Train loss 6.073, Val loss 6.054\n",
      "Ep 1 (Step 000060): Train loss 5.893, Val loss 5.937\n",
      "Ep 1 (Step 000070): Train loss 5.873, Val loss 5.867\n",
      "Ep 1 (Step 000080): Train loss 5.705, Val loss 5.782\n",
      "Ep 1 (Step 000090): Train loss 5.722, Val loss 5.715\n",
      "Ep 1 (Step 000100): Train loss 5.619, Val loss 5.669\n",
      "Ep 1 (Step 000110): Train loss 5.659, Val loss 5.623\n",
      "Ep 1 (Step 000120): Train loss 5.498, Val loss 5.603\n",
      "Ep 1 (Step 000130): Train loss 5.422, Val loss 5.539\n",
      "Ep 1 (Step 000140): Train loss 5.455, Val loss 5.505\n",
      "Ep 1 (Step 000150): Train loss 5.502, Val loss 5.483\n",
      "Ep 1 (Step 000160): Train loss 5.407, Val loss 5.475\n",
      "Ep 1 (Step 000170): Train loss 5.439, Val loss 5.446\n",
      "Ep 1 (Step 000180): Train loss 5.314, Val loss 5.417\n",
      "Ep 1 (Step 000190): Train loss 5.346, Val loss 5.409\n",
      "Ep 1 (Step 000200): Train loss 5.174, Val loss 5.384\n",
      "Ep 1 (Step 000210): Train loss 5.252, Val loss 5.366\n",
      "Ep 1 (Step 000220): Train loss 5.171, Val loss 5.346\n",
      "Ep 1 (Step 000230): Train loss 5.143, Val loss 5.332\n",
      "Ep 1 (Step 000240): Train loss 5.185, Val loss 5.302\n",
      "Ep 1 (Step 000250): Train loss 5.196, Val loss 5.297\n",
      "Ep 1 (Step 000260): Train loss 5.181, Val loss 5.274\n",
      "Ep 1 (Step 000270): Train loss 5.136, Val loss 5.271\n",
      "Ep 1 (Step 000280): Train loss 5.120, Val loss 5.266\n",
      "Ep 1 (Step 000290): Train loss 5.127, Val loss 5.245\n",
      "Ep 1 (Step 000300): Train loss 5.053, Val loss 5.243\n",
      "Ep 1 (Step 000310): Train loss 5.102, Val loss 5.221\n",
      "Ep 1 (Step 000320): Train loss 5.081, Val loss 5.213\n",
      "Ep 1 (Step 000330): Train loss 5.038, Val loss 5.194\n",
      "Ep 1 (Step 000340): Train loss 5.121, Val loss 5.188\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1880\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.656, Val loss 8.616\n",
      "Ep 1 (Step 000010): Train loss 6.851, Val loss 6.821\n",
      "Ep 1 (Step 000020): Train loss 6.465, Val loss 6.453\n",
      "Ep 1 (Step 000030): Train loss 6.321, Val loss 6.381\n",
      "Ep 1 (Step 000040): Train loss 6.309, Val loss 6.248\n",
      "Ep 1 (Step 000050): Train loss 6.069, Val loss 6.075\n",
      "Ep 1 (Step 000060): Train loss 5.920, Val loss 5.935\n",
      "Ep 1 (Step 000070): Train loss 5.827, Val loss 5.856\n",
      "Ep 1 (Step 000080): Train loss 5.787, Val loss 5.797\n",
      "Ep 1 (Step 000090): Train loss 5.620, Val loss 5.713\n",
      "Ep 1 (Step 000100): Train loss 5.640, Val loss 5.661\n",
      "Ep 1 (Step 000110): Train loss 5.683, Val loss 5.652\n",
      "Ep 1 (Step 000120): Train loss 5.559, Val loss 5.584\n",
      "Ep 1 (Step 000130): Train loss 5.484, Val loss 5.551\n",
      "Ep 1 (Step 000140): Train loss 5.444, Val loss 5.523\n",
      "Ep 1 (Step 000150): Train loss 5.486, Val loss 5.486\n",
      "Ep 1 (Step 000160): Train loss 5.433, Val loss 5.480\n",
      "Ep 1 (Step 000170): Train loss 5.371, Val loss 5.438\n",
      "Ep 1 (Step 000180): Train loss 5.321, Val loss 5.414\n",
      "Ep 1 (Step 000190): Train loss 5.322, Val loss 5.402\n",
      "Ep 1 (Step 000200): Train loss 5.312, Val loss 5.396\n",
      "Ep 1 (Step 000210): Train loss 5.276, Val loss 5.380\n",
      "Ep 1 (Step 000220): Train loss 5.287, Val loss 5.363\n",
      "Ep 1 (Step 000230): Train loss 5.234, Val loss 5.337\n",
      "Ep 1 (Step 000240): Train loss 5.170, Val loss 5.315\n",
      "Ep 1 (Step 000250): Train loss 5.097, Val loss 5.303\n",
      "Ep 1 (Step 000260): Train loss 5.177, Val loss 5.285\n",
      "Ep 1 (Step 000270): Train loss 5.123, Val loss 5.291\n",
      "Ep 1 (Step 000280): Train loss 5.224, Val loss 5.287\n",
      "Ep 1 (Step 000290): Train loss 5.186, Val loss 5.254\n",
      "Ep 1 (Step 000300): Train loss 5.097, Val loss 5.257\n",
      "Ep 1 (Step 000310): Train loss 5.130, Val loss 5.247\n",
      "Ep 1 (Step 000320): Train loss 5.066, Val loss 5.234\n",
      "Ep 1 (Step 000330): Train loss 5.019, Val loss 5.222\n",
      "Ep 1 (Step 000340): Train loss 5.062, Val loss 5.195\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1949\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.640, Val loss 8.633\n",
      "Ep 1 (Step 000010): Train loss 6.815, Val loss 6.790\n",
      "Ep 1 (Step 000020): Train loss 6.452, Val loss 6.424\n",
      "Ep 1 (Step 000030): Train loss 6.367, Val loss 6.357\n",
      "Ep 1 (Step 000040): Train loss 6.256, Val loss 6.176\n",
      "Ep 1 (Step 000050): Train loss 6.101, Val loss 6.059\n",
      "Ep 1 (Step 000060): Train loss 5.877, Val loss 5.945\n",
      "Ep 1 (Step 000070): Train loss 5.844, Val loss 5.841\n",
      "Ep 1 (Step 000080): Train loss 5.764, Val loss 5.771\n",
      "Ep 1 (Step 000090): Train loss 5.750, Val loss 5.716\n",
      "Ep 1 (Step 000100): Train loss 5.585, Val loss 5.690\n",
      "Ep 1 (Step 000110): Train loss 5.472, Val loss 5.638\n",
      "Ep 1 (Step 000120): Train loss 5.471, Val loss 5.576\n",
      "Ep 1 (Step 000130): Train loss 5.429, Val loss 5.533\n",
      "Ep 1 (Step 000140): Train loss 5.445, Val loss 5.522\n",
      "Ep 1 (Step 000150): Train loss 5.455, Val loss 5.475\n",
      "Ep 1 (Step 000160): Train loss 5.419, Val loss 5.435\n",
      "Ep 1 (Step 000170): Train loss 5.420, Val loss 5.430\n",
      "Ep 1 (Step 000180): Train loss 5.232, Val loss 5.387\n",
      "Ep 1 (Step 000190): Train loss 5.217, Val loss 5.362\n",
      "Ep 1 (Step 000200): Train loss 5.263, Val loss 5.349\n",
      "Ep 1 (Step 000210): Train loss 5.228, Val loss 5.346\n",
      "Ep 1 (Step 000220): Train loss 5.208, Val loss 5.336\n",
      "Ep 1 (Step 000230): Train loss 5.269, Val loss 5.303\n",
      "Ep 1 (Step 000240): Train loss 5.130, Val loss 5.293\n",
      "Ep 1 (Step 000250): Train loss 5.153, Val loss 5.269\n",
      "Ep 1 (Step 000260): Train loss 5.153, Val loss 5.256\n",
      "Ep 1 (Step 000270): Train loss 5.153, Val loss 5.249\n",
      "Ep 1 (Step 000280): Train loss 5.021, Val loss 5.229\n",
      "Ep 1 (Step 000290): Train loss 5.200, Val loss 5.235\n",
      "Ep 1 (Step 000300): Train loss 5.130, Val loss 5.206\n",
      "Ep 1 (Step 000310): Train loss 5.056, Val loss 5.195\n",
      "Ep 1 (Step 000320): Train loss 5.154, Val loss 5.206\n",
      "Ep 1 (Step 000330): Train loss 5.081, Val loss 5.177\n",
      "Ep 1 (Step 000340): Train loss 5.066, Val loss 5.183\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1832\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.811, Val loss 8.769\n",
      "Ep 1 (Step 000010): Train loss 7.432, Val loss 7.367\n",
      "Ep 1 (Step 000020): Train loss 6.798, Val loss 6.734\n",
      "Ep 1 (Step 000030): Train loss 6.482, Val loss 6.447\n",
      "Ep 1 (Step 000040): Train loss 6.410, Val loss 6.353\n",
      "Ep 1 (Step 000050): Train loss 6.225, Val loss 6.276\n",
      "Ep 1 (Step 000060): Train loss 6.125, Val loss 6.178\n",
      "Ep 1 (Step 000070): Train loss 6.050, Val loss 6.099\n",
      "Ep 1 (Step 000080): Train loss 6.016, Val loss 6.013\n",
      "Ep 1 (Step 000090): Train loss 5.874, Val loss 5.953\n",
      "Ep 1 (Step 000100): Train loss 5.743, Val loss 5.876\n",
      "Ep 1 (Step 000110): Train loss 5.765, Val loss 5.841\n",
      "Ep 1 (Step 000120): Train loss 5.726, Val loss 5.786\n",
      "Ep 1 (Step 000130): Train loss 5.660, Val loss 5.741\n",
      "Ep 1 (Step 000140): Train loss 5.591, Val loss 5.729\n",
      "Ep 1 (Step 000150): Train loss 5.662, Val loss 5.670\n",
      "Ep 1 (Step 000160): Train loss 5.648, Val loss 5.652\n",
      "Ep 1 (Step 000170): Train loss 5.594, Val loss 5.624\n",
      "Ep 1 (Step 000180): Train loss 5.504, Val loss 5.594\n",
      "Ep 1 (Step 000190): Train loss 5.497, Val loss 5.566\n",
      "Ep 1 (Step 000200): Train loss 5.536, Val loss 5.547\n",
      "Ep 1 (Step 000210): Train loss 5.340, Val loss 5.511\n",
      "Ep 1 (Step 000220): Train loss 5.389, Val loss 5.510\n",
      "Ep 1 (Step 000230): Train loss 5.354, Val loss 5.481\n",
      "Ep 1 (Step 000240): Train loss 5.436, Val loss 5.458\n",
      "Ep 1 (Step 000250): Train loss 5.386, Val loss 5.475\n",
      "Ep 1 (Step 000260): Train loss 5.448, Val loss 5.438\n",
      "Ep 1 (Step 000270): Train loss 5.260, Val loss 5.432\n",
      "Ep 1 (Step 000280): Train loss 5.411, Val loss 5.421\n",
      "Ep 1 (Step 000290): Train loss 5.264, Val loss 5.394\n",
      "Ep 1 (Step 000300): Train loss 5.326, Val loss 5.381\n",
      "Ep 1 (Step 000310): Train loss 5.311, Val loss 5.372\n",
      "Ep 1 (Step 000320): Train loss 5.218, Val loss 5.362\n",
      "Ep 1 (Step 000330): Train loss 5.323, Val loss 5.357\n",
      "Ep 1 (Step 000340): Train loss 5.191, Val loss 5.342\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3420\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.787, Val loss 8.812\n",
      "Ep 1 (Step 000010): Train loss 7.409, Val loss 7.385\n",
      "Ep 1 (Step 000020): Train loss 6.815, Val loss 6.745\n",
      "Ep 1 (Step 000030): Train loss 6.429, Val loss 6.463\n",
      "Ep 1 (Step 000040): Train loss 6.346, Val loss 6.368\n",
      "Ep 1 (Step 000050): Train loss 6.251, Val loss 6.280\n",
      "Ep 1 (Step 000060): Train loss 6.190, Val loss 6.189\n",
      "Ep 1 (Step 000070): Train loss 6.107, Val loss 6.101\n",
      "Ep 1 (Step 000080): Train loss 6.067, Val loss 6.051\n",
      "Ep 1 (Step 000090): Train loss 5.907, Val loss 5.976\n",
      "Ep 1 (Step 000100): Train loss 5.853, Val loss 5.934\n",
      "Ep 1 (Step 000110): Train loss 5.742, Val loss 5.848\n",
      "Ep 1 (Step 000120): Train loss 5.662, Val loss 5.799\n",
      "Ep 1 (Step 000130): Train loss 5.736, Val loss 5.750\n",
      "Ep 1 (Step 000140): Train loss 5.688, Val loss 5.722\n",
      "Ep 1 (Step 000150): Train loss 5.669, Val loss 5.678\n",
      "Ep 1 (Step 000160): Train loss 5.531, Val loss 5.665\n",
      "Ep 1 (Step 000170): Train loss 5.613, Val loss 5.635\n",
      "Ep 1 (Step 000180): Train loss 5.439, Val loss 5.596\n",
      "Ep 1 (Step 000190): Train loss 5.513, Val loss 5.585\n",
      "Ep 1 (Step 000200): Train loss 5.533, Val loss 5.564\n",
      "Ep 1 (Step 000210): Train loss 5.441, Val loss 5.554\n",
      "Ep 1 (Step 000220): Train loss 5.426, Val loss 5.531\n",
      "Ep 1 (Step 000230): Train loss 5.400, Val loss 5.504\n",
      "Ep 1 (Step 000240): Train loss 5.331, Val loss 5.482\n",
      "Ep 1 (Step 000250): Train loss 5.370, Val loss 5.470\n",
      "Ep 1 (Step 000260): Train loss 5.439, Val loss 5.461\n",
      "Ep 1 (Step 000270): Train loss 5.331, Val loss 5.443\n",
      "Ep 1 (Step 000280): Train loss 5.307, Val loss 5.418\n",
      "Ep 1 (Step 000290): Train loss 5.301, Val loss 5.410\n",
      "Ep 1 (Step 000300): Train loss 5.290, Val loss 5.410\n",
      "Ep 1 (Step 000310): Train loss 5.254, Val loss 5.384\n",
      "Ep 1 (Step 000320): Train loss 5.268, Val loss 5.374\n",
      "Ep 1 (Step 000330): Train loss 5.195, Val loss 5.370\n",
      "Ep 1 (Step 000340): Train loss 5.216, Val loss 5.349\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3494\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.768, Val loss 8.755\n",
      "Ep 1 (Step 000010): Train loss 7.411, Val loss 7.405\n",
      "Ep 1 (Step 000020): Train loss 6.782, Val loss 6.769\n",
      "Ep 1 (Step 000030): Train loss 6.445, Val loss 6.459\n",
      "Ep 1 (Step 000040): Train loss 6.436, Val loss 6.360\n",
      "Ep 1 (Step 000050): Train loss 6.279, Val loss 6.300\n",
      "Ep 1 (Step 000060): Train loss 6.189, Val loss 6.198\n",
      "Ep 1 (Step 000070): Train loss 6.029, Val loss 6.097\n",
      "Ep 1 (Step 000080): Train loss 6.038, Val loss 6.048\n",
      "Ep 1 (Step 000090): Train loss 5.977, Val loss 5.965\n",
      "Ep 1 (Step 000100): Train loss 5.884, Val loss 5.903\n",
      "Ep 1 (Step 000110): Train loss 5.739, Val loss 5.848\n",
      "Ep 1 (Step 000120): Train loss 5.728, Val loss 5.801\n",
      "Ep 1 (Step 000130): Train loss 5.660, Val loss 5.739\n",
      "Ep 1 (Step 000140): Train loss 5.627, Val loss 5.716\n",
      "Ep 1 (Step 000150): Train loss 5.636, Val loss 5.678\n",
      "Ep 1 (Step 000160): Train loss 5.542, Val loss 5.654\n",
      "Ep 1 (Step 000170): Train loss 5.541, Val loss 5.619\n",
      "Ep 1 (Step 000180): Train loss 5.551, Val loss 5.604\n",
      "Ep 1 (Step 000190): Train loss 5.448, Val loss 5.569\n",
      "Ep 1 (Step 000200): Train loss 5.475, Val loss 5.570\n",
      "Ep 1 (Step 000210): Train loss 5.511, Val loss 5.565\n",
      "Ep 1 (Step 000220): Train loss 5.405, Val loss 5.546\n",
      "Ep 1 (Step 000230): Train loss 5.424, Val loss 5.499\n",
      "Ep 1 (Step 000240): Train loss 5.367, Val loss 5.487\n",
      "Ep 1 (Step 000250): Train loss 5.370, Val loss 5.476\n",
      "Ep 1 (Step 000260): Train loss 5.354, Val loss 5.455\n",
      "Ep 1 (Step 000270): Train loss 5.347, Val loss 5.443\n",
      "Ep 1 (Step 000280): Train loss 5.296, Val loss 5.431\n",
      "Ep 1 (Step 000290): Train loss 5.247, Val loss 5.421\n",
      "Ep 1 (Step 000300): Train loss 5.322, Val loss 5.400\n",
      "Ep 1 (Step 000310): Train loss 5.257, Val loss 5.390\n",
      "Ep 1 (Step 000320): Train loss 5.336, Val loss 5.388\n",
      "Ep 1 (Step 000330): Train loss 5.215, Val loss 5.372\n",
      "Ep 1 (Step 000340): Train loss 5.137, Val loss 5.362\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3619\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.773, Val loss 8.729\n",
      "Ep 1 (Step 000010): Train loss 7.373, Val loss 7.364\n",
      "Ep 1 (Step 000020): Train loss 6.731, Val loss 6.737\n",
      "Ep 1 (Step 000030): Train loss 6.480, Val loss 6.442\n",
      "Ep 1 (Step 000040): Train loss 6.403, Val loss 6.376\n",
      "Ep 1 (Step 000050): Train loss 6.269, Val loss 6.309\n",
      "Ep 1 (Step 000060): Train loss 6.207, Val loss 6.178\n",
      "Ep 1 (Step 000070): Train loss 6.084, Val loss 6.067\n",
      "Ep 1 (Step 000080): Train loss 6.010, Val loss 5.998\n",
      "Ep 1 (Step 000090): Train loss 5.877, Val loss 5.911\n",
      "Ep 1 (Step 000100): Train loss 5.931, Val loss 5.839\n",
      "Ep 1 (Step 000110): Train loss 5.827, Val loss 5.794\n",
      "Ep 1 (Step 000120): Train loss 5.744, Val loss 5.746\n",
      "Ep 1 (Step 000130): Train loss 5.740, Val loss 5.754\n",
      "Ep 1 (Step 000140): Train loss 5.698, Val loss 5.697\n",
      "Ep 1 (Step 000150): Train loss 5.649, Val loss 5.649\n",
      "Ep 1 (Step 000160): Train loss 5.577, Val loss 5.633\n",
      "Ep 1 (Step 000170): Train loss 5.574, Val loss 5.605\n",
      "Ep 1 (Step 000180): Train loss 5.622, Val loss 5.587\n",
      "Ep 1 (Step 000190): Train loss 5.511, Val loss 5.557\n",
      "Ep 1 (Step 000200): Train loss 5.473, Val loss 5.541\n",
      "Ep 1 (Step 000210): Train loss 5.447, Val loss 5.519\n",
      "Ep 1 (Step 000220): Train loss 5.420, Val loss 5.490\n",
      "Ep 1 (Step 000230): Train loss 5.314, Val loss 5.469\n",
      "Ep 1 (Step 000240): Train loss 5.385, Val loss 5.468\n",
      "Ep 1 (Step 000250): Train loss 5.252, Val loss 5.447\n",
      "Ep 1 (Step 000260): Train loss 5.355, Val loss 5.443\n",
      "Ep 1 (Step 000270): Train loss 5.310, Val loss 5.436\n",
      "Ep 1 (Step 000280): Train loss 5.249, Val loss 5.410\n",
      "Ep 1 (Step 000290): Train loss 5.260, Val loss 5.398\n",
      "Ep 1 (Step 000300): Train loss 5.153, Val loss 5.376\n",
      "Ep 1 (Step 000310): Train loss 5.235, Val loss 5.364\n",
      "Ep 1 (Step 000320): Train loss 5.274, Val loss 5.353\n",
      "Ep 1 (Step 000330): Train loss 5.231, Val loss 5.334\n",
      "Ep 1 (Step 000340): Train loss 5.259, Val loss 5.335\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3354\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.803, Val loss 8.800\n",
      "Ep 1 (Step 000010): Train loss 7.443, Val loss 7.406\n",
      "Ep 1 (Step 000020): Train loss 6.724, Val loss 6.754\n",
      "Ep 1 (Step 000030): Train loss 6.457, Val loss 6.452\n",
      "Ep 1 (Step 000040): Train loss 6.424, Val loss 6.382\n",
      "Ep 1 (Step 000050): Train loss 6.320, Val loss 6.309\n",
      "Ep 1 (Step 000060): Train loss 6.152, Val loss 6.186\n",
      "Ep 1 (Step 000070): Train loss 6.098, Val loss 6.080\n",
      "Ep 1 (Step 000080): Train loss 5.921, Val loss 6.002\n",
      "Ep 1 (Step 000090): Train loss 5.902, Val loss 5.935\n",
      "Ep 1 (Step 000100): Train loss 5.839, Val loss 5.878\n",
      "Ep 1 (Step 000110): Train loss 5.722, Val loss 5.808\n",
      "Ep 1 (Step 000120): Train loss 5.702, Val loss 5.776\n",
      "Ep 1 (Step 000130): Train loss 5.704, Val loss 5.715\n",
      "Ep 1 (Step 000140): Train loss 5.649, Val loss 5.689\n",
      "Ep 1 (Step 000150): Train loss 5.628, Val loss 5.670\n",
      "Ep 1 (Step 000160): Train loss 5.576, Val loss 5.650\n",
      "Ep 1 (Step 000170): Train loss 5.503, Val loss 5.601\n",
      "Ep 1 (Step 000180): Train loss 5.471, Val loss 5.572\n",
      "Ep 1 (Step 000190): Train loss 5.520, Val loss 5.556\n",
      "Ep 1 (Step 000200): Train loss 5.498, Val loss 5.529\n",
      "Ep 1 (Step 000210): Train loss 5.344, Val loss 5.511\n",
      "Ep 1 (Step 000220): Train loss 5.411, Val loss 5.491\n",
      "Ep 1 (Step 000230): Train loss 5.417, Val loss 5.470\n",
      "Ep 1 (Step 000240): Train loss 5.323, Val loss 5.458\n",
      "Ep 1 (Step 000250): Train loss 5.331, Val loss 5.442\n",
      "Ep 1 (Step 000260): Train loss 5.329, Val loss 5.434\n",
      "Ep 1 (Step 000270): Train loss 5.373, Val loss 5.410\n",
      "Ep 1 (Step 000280): Train loss 5.267, Val loss 5.389\n",
      "Ep 1 (Step 000290): Train loss 5.236, Val loss 5.382\n",
      "Ep 1 (Step 000300): Train loss 5.258, Val loss 5.378\n",
      "Ep 1 (Step 000310): Train loss 5.238, Val loss 5.365\n",
      "Ep 1 (Step 000320): Train loss 5.188, Val loss 5.347\n",
      "Ep 1 (Step 000330): Train loss 5.232, Val loss 5.336\n",
      "Ep 1 (Step 000340): Train loss 5.264, Val loss 5.329\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3294\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.784, Val loss 8.751\n",
      "Ep 1 (Step 000010): Train loss 7.420, Val loss 7.377\n",
      "Ep 1 (Step 000020): Train loss 6.770, Val loss 6.741\n",
      "Ep 1 (Step 000030): Train loss 6.490, Val loss 6.455\n",
      "Ep 1 (Step 000040): Train loss 6.344, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.298, Val loss 6.275\n",
      "Ep 1 (Step 000060): Train loss 6.195, Val loss 6.164\n",
      "Ep 1 (Step 000070): Train loss 5.986, Val loss 6.059\n",
      "Ep 1 (Step 000080): Train loss 6.003, Val loss 5.999\n",
      "Ep 1 (Step 000090): Train loss 5.888, Val loss 5.900\n",
      "Ep 1 (Step 000100): Train loss 5.799, Val loss 5.871\n",
      "Ep 1 (Step 000110): Train loss 5.869, Val loss 5.827\n",
      "Ep 1 (Step 000120): Train loss 5.685, Val loss 5.766\n",
      "Ep 1 (Step 000130): Train loss 5.719, Val loss 5.732\n",
      "Ep 1 (Step 000140): Train loss 5.605, Val loss 5.692\n",
      "Ep 1 (Step 000150): Train loss 5.575, Val loss 5.649\n",
      "Ep 1 (Step 000160): Train loss 5.559, Val loss 5.635\n",
      "Ep 1 (Step 000170): Train loss 5.490, Val loss 5.604\n",
      "Ep 1 (Step 000180): Train loss 5.435, Val loss 5.575\n",
      "Ep 1 (Step 000190): Train loss 5.497, Val loss 5.549\n",
      "Ep 1 (Step 000200): Train loss 5.468, Val loss 5.530\n",
      "Ep 1 (Step 000210): Train loss 5.514, Val loss 5.512\n",
      "Ep 1 (Step 000220): Train loss 5.423, Val loss 5.500\n",
      "Ep 1 (Step 000230): Train loss 5.301, Val loss 5.473\n",
      "Ep 1 (Step 000240): Train loss 5.372, Val loss 5.462\n",
      "Ep 1 (Step 000250): Train loss 5.350, Val loss 5.425\n",
      "Ep 1 (Step 000260): Train loss 5.363, Val loss 5.432\n",
      "Ep 1 (Step 000270): Train loss 5.334, Val loss 5.404\n",
      "Ep 1 (Step 000280): Train loss 5.349, Val loss 5.391\n",
      "Ep 1 (Step 000290): Train loss 5.327, Val loss 5.374\n",
      "Ep 1 (Step 000300): Train loss 5.211, Val loss 5.355\n",
      "Ep 1 (Step 000310): Train loss 5.287, Val loss 5.352\n",
      "Ep 1 (Step 000320): Train loss 5.183, Val loss 5.336\n",
      "Ep 1 (Step 000330): Train loss 5.267, Val loss 5.340\n",
      "Ep 1 (Step 000340): Train loss 5.192, Val loss 5.309\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3093\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.552, Val loss 8.496\n",
      "Ep 1 (Step 000010): Train loss 6.849, Val loss 6.824\n",
      "Ep 1 (Step 000020): Train loss 6.538, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.456, Val loss 6.413\n",
      "Ep 1 (Step 000040): Train loss 6.306, Val loss 6.283\n",
      "Ep 1 (Step 000050): Train loss 6.143, Val loss 6.160\n",
      "Ep 1 (Step 000060): Train loss 6.046, Val loss 6.034\n",
      "Ep 1 (Step 000070): Train loss 5.908, Val loss 5.972\n",
      "Ep 1 (Step 000080): Train loss 5.899, Val loss 5.894\n",
      "Ep 1 (Step 000090): Train loss 5.727, Val loss 5.806\n",
      "Ep 1 (Step 000100): Train loss 5.717, Val loss 5.745\n",
      "Ep 1 (Step 000110): Train loss 5.606, Val loss 5.696\n",
      "Ep 1 (Step 000120): Train loss 5.575, Val loss 5.667\n",
      "Ep 1 (Step 000130): Train loss 5.600, Val loss 5.623\n",
      "Ep 1 (Step 000140): Train loss 5.542, Val loss 5.614\n",
      "Ep 1 (Step 000150): Train loss 5.487, Val loss 5.581\n",
      "Ep 1 (Step 000160): Train loss 5.508, Val loss 5.550\n",
      "Ep 1 (Step 000170): Train loss 5.412, Val loss 5.524\n",
      "Ep 1 (Step 000180): Train loss 5.369, Val loss 5.496\n",
      "Ep 1 (Step 000190): Train loss 5.347, Val loss 5.473\n",
      "Ep 1 (Step 000200): Train loss 5.459, Val loss 5.452\n",
      "Ep 1 (Step 000210): Train loss 5.284, Val loss 5.457\n",
      "Ep 1 (Step 000220): Train loss 5.364, Val loss 5.451\n",
      "Ep 1 (Step 000230): Train loss 5.329, Val loss 5.410\n",
      "Ep 1 (Step 000240): Train loss 5.401, Val loss 5.393\n",
      "Ep 1 (Step 000250): Train loss 5.314, Val loss 5.384\n",
      "Ep 1 (Step 000260): Train loss 5.268, Val loss 5.377\n",
      "Ep 1 (Step 000270): Train loss 5.252, Val loss 5.368\n",
      "Ep 1 (Step 000280): Train loss 5.214, Val loss 5.357\n",
      "Ep 1 (Step 000290): Train loss 5.177, Val loss 5.352\n",
      "Ep 1 (Step 000300): Train loss 5.233, Val loss 5.337\n",
      "Ep 1 (Step 000310): Train loss 5.299, Val loss 5.318\n",
      "Ep 1 (Step 000320): Train loss 5.190, Val loss 5.304\n",
      "Ep 1 (Step 000330): Train loss 5.216, Val loss 5.292\n",
      "Ep 1 (Step 000340): Train loss 5.102, Val loss 5.289\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2886\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.485, Val loss 8.477\n",
      "Ep 1 (Step 000010): Train loss 6.851, Val loss 6.816\n",
      "Ep 1 (Step 000020): Train loss 6.467, Val loss 6.438\n",
      "Ep 1 (Step 000030): Train loss 6.462, Val loss 6.439\n",
      "Ep 1 (Step 000040): Train loss 6.285, Val loss 6.288\n",
      "Ep 1 (Step 000050): Train loss 6.080, Val loss 6.129\n",
      "Ep 1 (Step 000060): Train loss 5.959, Val loss 6.009\n",
      "Ep 1 (Step 000070): Train loss 5.879, Val loss 5.930\n",
      "Ep 1 (Step 000080): Train loss 5.868, Val loss 5.842\n",
      "Ep 1 (Step 000090): Train loss 5.737, Val loss 5.781\n",
      "Ep 1 (Step 000100): Train loss 5.703, Val loss 5.737\n",
      "Ep 1 (Step 000110): Train loss 5.666, Val loss 5.686\n",
      "Ep 1 (Step 000120): Train loss 5.604, Val loss 5.648\n",
      "Ep 1 (Step 000130): Train loss 5.553, Val loss 5.611\n",
      "Ep 1 (Step 000140): Train loss 5.545, Val loss 5.553\n",
      "Ep 1 (Step 000150): Train loss 5.441, Val loss 5.545\n",
      "Ep 1 (Step 000160): Train loss 5.438, Val loss 5.519\n",
      "Ep 1 (Step 000170): Train loss 5.517, Val loss 5.511\n",
      "Ep 1 (Step 000180): Train loss 5.436, Val loss 5.477\n",
      "Ep 1 (Step 000190): Train loss 5.404, Val loss 5.468\n",
      "Ep 1 (Step 000200): Train loss 5.425, Val loss 5.447\n",
      "Ep 1 (Step 000210): Train loss 5.347, Val loss 5.436\n",
      "Ep 1 (Step 000220): Train loss 5.284, Val loss 5.429\n",
      "Ep 1 (Step 000230): Train loss 5.237, Val loss 5.414\n",
      "Ep 1 (Step 000240): Train loss 5.336, Val loss 5.414\n",
      "Ep 1 (Step 000250): Train loss 5.274, Val loss 5.401\n",
      "Ep 1 (Step 000260): Train loss 5.288, Val loss 5.377\n",
      "Ep 1 (Step 000270): Train loss 5.254, Val loss 5.364\n",
      "Ep 1 (Step 000280): Train loss 5.283, Val loss 5.346\n",
      "Ep 1 (Step 000290): Train loss 5.128, Val loss 5.340\n",
      "Ep 1 (Step 000300): Train loss 5.168, Val loss 5.316\n",
      "Ep 1 (Step 000310): Train loss 5.164, Val loss 5.313\n",
      "Ep 1 (Step 000320): Train loss 5.238, Val loss 5.306\n",
      "Ep 1 (Step 000330): Train loss 5.215, Val loss 5.278\n",
      "Ep 1 (Step 000340): Train loss 5.206, Val loss 5.279\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2787\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.562, Val loss 8.524\n",
      "Ep 1 (Step 000010): Train loss 6.785, Val loss 6.837\n",
      "Ep 1 (Step 000020): Train loss 6.425, Val loss 6.451\n",
      "Ep 1 (Step 000030): Train loss 6.436, Val loss 6.415\n",
      "Ep 1 (Step 000040): Train loss 6.451, Val loss 6.431\n",
      "Ep 1 (Step 000050): Train loss 6.155, Val loss 6.161\n",
      "Ep 1 (Step 000060): Train loss 5.990, Val loss 6.038\n",
      "Ep 1 (Step 000070): Train loss 5.939, Val loss 5.948\n",
      "Ep 1 (Step 000080): Train loss 5.849, Val loss 5.861\n",
      "Ep 1 (Step 000090): Train loss 5.924, Val loss 5.823\n",
      "Ep 1 (Step 000100): Train loss 5.695, Val loss 5.752\n",
      "Ep 1 (Step 000110): Train loss 5.705, Val loss 5.731\n",
      "Ep 1 (Step 000120): Train loss 5.566, Val loss 5.668\n",
      "Ep 1 (Step 000130): Train loss 5.593, Val loss 5.650\n",
      "Ep 1 (Step 000140): Train loss 5.491, Val loss 5.602\n",
      "Ep 1 (Step 000150): Train loss 5.541, Val loss 5.583\n",
      "Ep 1 (Step 000160): Train loss 5.442, Val loss 5.537\n",
      "Ep 1 (Step 000170): Train loss 5.488, Val loss 5.531\n",
      "Ep 1 (Step 000180): Train loss 5.388, Val loss 5.506\n",
      "Ep 1 (Step 000190): Train loss 5.367, Val loss 5.479\n",
      "Ep 1 (Step 000200): Train loss 5.331, Val loss 5.471\n",
      "Ep 1 (Step 000210): Train loss 5.387, Val loss 5.456\n",
      "Ep 1 (Step 000220): Train loss 5.341, Val loss 5.451\n",
      "Ep 1 (Step 000230): Train loss 5.304, Val loss 5.414\n",
      "Ep 1 (Step 000240): Train loss 5.249, Val loss 5.395\n",
      "Ep 1 (Step 000250): Train loss 5.236, Val loss 5.411\n",
      "Ep 1 (Step 000260): Train loss 5.321, Val loss 5.383\n",
      "Ep 1 (Step 000270): Train loss 5.176, Val loss 5.380\n",
      "Ep 1 (Step 000280): Train loss 5.250, Val loss 5.342\n",
      "Ep 1 (Step 000290): Train loss 5.339, Val loss 5.333\n",
      "Ep 1 (Step 000300): Train loss 5.181, Val loss 5.317\n",
      "Ep 1 (Step 000310): Train loss 5.166, Val loss 5.312\n",
      "Ep 1 (Step 000320): Train loss 5.270, Val loss 5.312\n",
      "Ep 1 (Step 000330): Train loss 5.185, Val loss 5.299\n",
      "Ep 1 (Step 000340): Train loss 5.155, Val loss 5.306\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3063\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.552, Val loss 8.505\n",
      "Ep 1 (Step 000010): Train loss 6.816, Val loss 6.779\n",
      "Ep 1 (Step 000020): Train loss 6.415, Val loss 6.438\n",
      "Ep 1 (Step 000030): Train loss 6.454, Val loss 6.416\n",
      "Ep 1 (Step 000040): Train loss 6.285, Val loss 6.295\n",
      "Ep 1 (Step 000050): Train loss 6.206, Val loss 6.241\n",
      "Ep 1 (Step 000060): Train loss 6.056, Val loss 6.083\n",
      "Ep 1 (Step 000070): Train loss 5.995, Val loss 5.974\n",
      "Ep 1 (Step 000080): Train loss 5.802, Val loss 5.900\n",
      "Ep 1 (Step 000090): Train loss 5.773, Val loss 5.840\n",
      "Ep 1 (Step 000100): Train loss 5.653, Val loss 5.756\n",
      "Ep 1 (Step 000110): Train loss 5.721, Val loss 5.730\n",
      "Ep 1 (Step 000120): Train loss 5.684, Val loss 5.668\n",
      "Ep 1 (Step 000130): Train loss 5.622, Val loss 5.610\n",
      "Ep 1 (Step 000140): Train loss 5.557, Val loss 5.572\n",
      "Ep 1 (Step 000150): Train loss 5.453, Val loss 5.535\n",
      "Ep 1 (Step 000160): Train loss 5.461, Val loss 5.512\n",
      "Ep 1 (Step 000170): Train loss 5.408, Val loss 5.500\n",
      "Ep 1 (Step 000180): Train loss 5.443, Val loss 5.489\n",
      "Ep 1 (Step 000190): Train loss 5.416, Val loss 5.460\n",
      "Ep 1 (Step 000200): Train loss 5.436, Val loss 5.436\n",
      "Ep 1 (Step 000210): Train loss 5.306, Val loss 5.435\n",
      "Ep 1 (Step 000220): Train loss 5.302, Val loss 5.398\n",
      "Ep 1 (Step 000230): Train loss 5.310, Val loss 5.380\n",
      "Ep 1 (Step 000240): Train loss 5.371, Val loss 5.392\n",
      "Ep 1 (Step 000250): Train loss 5.284, Val loss 5.340\n",
      "Ep 1 (Step 000260): Train loss 5.225, Val loss 5.331\n",
      "Ep 1 (Step 000270): Train loss 5.201, Val loss 5.327\n",
      "Ep 1 (Step 000280): Train loss 5.149, Val loss 5.320\n",
      "Ep 1 (Step 000290): Train loss 5.176, Val loss 5.314\n",
      "Ep 1 (Step 000300): Train loss 5.206, Val loss 5.289\n",
      "Ep 1 (Step 000310): Train loss 5.154, Val loss 5.278\n",
      "Ep 1 (Step 000320): Train loss 5.109, Val loss 5.273\n",
      "Ep 1 (Step 000330): Train loss 5.140, Val loss 5.270\n",
      "Ep 1 (Step 000340): Train loss 5.119, Val loss 5.245\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2448\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.591, Val loss 8.535\n",
      "Ep 1 (Step 000010): Train loss 6.890, Val loss 6.804\n",
      "Ep 1 (Step 000020): Train loss 6.472, Val loss 6.471\n",
      "Ep 1 (Step 000030): Train loss 6.383, Val loss 6.442\n",
      "Ep 1 (Step 000040): Train loss 6.273, Val loss 6.324\n",
      "Ep 1 (Step 000050): Train loss 6.194, Val loss 6.173\n",
      "Ep 1 (Step 000060): Train loss 6.003, Val loss 6.054\n",
      "Ep 1 (Step 000070): Train loss 5.890, Val loss 5.959\n",
      "Ep 1 (Step 000080): Train loss 5.816, Val loss 5.866\n",
      "Ep 1 (Step 000090): Train loss 5.735, Val loss 5.808\n",
      "Ep 1 (Step 000100): Train loss 5.715, Val loss 5.747\n",
      "Ep 1 (Step 000110): Train loss 5.621, Val loss 5.685\n",
      "Ep 1 (Step 000120): Train loss 5.567, Val loss 5.645\n",
      "Ep 1 (Step 000130): Train loss 5.660, Val loss 5.621\n",
      "Ep 1 (Step 000140): Train loss 5.549, Val loss 5.611\n",
      "Ep 1 (Step 000150): Train loss 5.482, Val loss 5.565\n",
      "Ep 1 (Step 000160): Train loss 5.462, Val loss 5.540\n",
      "Ep 1 (Step 000170): Train loss 5.473, Val loss 5.509\n",
      "Ep 1 (Step 000180): Train loss 5.406, Val loss 5.478\n",
      "Ep 1 (Step 000190): Train loss 5.397, Val loss 5.466\n",
      "Ep 1 (Step 000200): Train loss 5.339, Val loss 5.450\n",
      "Ep 1 (Step 000210): Train loss 5.283, Val loss 5.443\n",
      "Ep 1 (Step 000220): Train loss 5.351, Val loss 5.414\n",
      "Ep 1 (Step 000230): Train loss 5.340, Val loss 5.412\n",
      "Ep 1 (Step 000240): Train loss 5.330, Val loss 5.383\n",
      "Ep 1 (Step 000250): Train loss 5.168, Val loss 5.355\n",
      "Ep 1 (Step 000260): Train loss 5.195, Val loss 5.350\n",
      "Ep 1 (Step 000270): Train loss 5.186, Val loss 5.325\n",
      "Ep 1 (Step 000280): Train loss 5.217, Val loss 5.325\n",
      "Ep 1 (Step 000290): Train loss 5.203, Val loss 5.309\n",
      "Ep 1 (Step 000300): Train loss 5.182, Val loss 5.309\n",
      "Ep 1 (Step 000310): Train loss 5.165, Val loss 5.285\n",
      "Ep 1 (Step 000320): Train loss 5.140, Val loss 5.280\n",
      "Ep 1 (Step 000330): Train loss 5.023, Val loss 5.254\n",
      "Ep 1 (Step 000340): Train loss 5.024, Val loss 5.255\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2550\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.520, Val loss 8.496\n",
      "Ep 1 (Step 000010): Train loss 6.877, Val loss 6.821\n",
      "Ep 1 (Step 000020): Train loss 6.493, Val loss 6.446\n",
      "Ep 1 (Step 000030): Train loss 6.472, Val loss 6.389\n",
      "Ep 1 (Step 000040): Train loss 6.273, Val loss 6.245\n",
      "Ep 1 (Step 000050): Train loss 6.161, Val loss 6.126\n",
      "Ep 1 (Step 000060): Train loss 5.974, Val loss 6.012\n",
      "Ep 1 (Step 000070): Train loss 5.899, Val loss 5.915\n",
      "Ep 1 (Step 000080): Train loss 5.821, Val loss 5.841\n",
      "Ep 1 (Step 000090): Train loss 5.746, Val loss 5.794\n",
      "Ep 1 (Step 000100): Train loss 5.678, Val loss 5.725\n",
      "Ep 1 (Step 000110): Train loss 5.618, Val loss 5.670\n",
      "Ep 1 (Step 000120): Train loss 5.579, Val loss 5.629\n",
      "Ep 1 (Step 000130): Train loss 5.552, Val loss 5.586\n",
      "Ep 1 (Step 000140): Train loss 5.460, Val loss 5.574\n",
      "Ep 1 (Step 000150): Train loss 5.450, Val loss 5.567\n",
      "Ep 1 (Step 000160): Train loss 5.471, Val loss 5.491\n",
      "Ep 1 (Step 000170): Train loss 5.488, Val loss 5.500\n",
      "Ep 1 (Step 000180): Train loss 5.348, Val loss 5.453\n",
      "Ep 1 (Step 000190): Train loss 5.341, Val loss 5.434\n",
      "Ep 1 (Step 000200): Train loss 5.349, Val loss 5.434\n",
      "Ep 1 (Step 000210): Train loss 5.330, Val loss 5.435\n",
      "Ep 1 (Step 000220): Train loss 5.232, Val loss 5.399\n",
      "Ep 1 (Step 000230): Train loss 5.272, Val loss 5.385\n",
      "Ep 1 (Step 000240): Train loss 5.216, Val loss 5.362\n",
      "Ep 1 (Step 000250): Train loss 5.213, Val loss 5.354\n",
      "Ep 1 (Step 000260): Train loss 5.309, Val loss 5.343\n",
      "Ep 1 (Step 000270): Train loss 5.314, Val loss 5.345\n",
      "Ep 1 (Step 000280): Train loss 5.274, Val loss 5.327\n",
      "Ep 1 (Step 000290): Train loss 5.222, Val loss 5.320\n",
      "Ep 1 (Step 000300): Train loss 5.104, Val loss 5.315\n",
      "Ep 1 (Step 000310): Train loss 5.103, Val loss 5.301\n",
      "Ep 1 (Step 000320): Train loss 5.148, Val loss 5.289\n",
      "Ep 1 (Step 000330): Train loss 5.083, Val loss 5.260\n",
      "Ep 1 (Step 000340): Train loss 5.125, Val loss 5.269\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2691\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.798, Val loss 8.774\n",
      "Ep 1 (Step 000010): Train loss 7.415, Val loss 7.402\n",
      "Ep 1 (Step 000020): Train loss 6.769, Val loss 6.749\n",
      "Ep 1 (Step 000030): Train loss 6.459, Val loss 6.453\n",
      "Ep 1 (Step 000040): Train loss 6.358, Val loss 6.376\n",
      "Ep 1 (Step 000050): Train loss 6.304, Val loss 6.309\n",
      "Ep 1 (Step 000060): Train loss 6.181, Val loss 6.207\n",
      "Ep 1 (Step 000070): Train loss 6.114, Val loss 6.129\n",
      "Ep 1 (Step 000080): Train loss 6.027, Val loss 6.008\n",
      "Ep 1 (Step 000090): Train loss 5.870, Val loss 5.951\n",
      "Ep 1 (Step 000100): Train loss 5.872, Val loss 5.880\n",
      "Ep 1 (Step 000110): Train loss 5.881, Val loss 5.863\n",
      "Ep 1 (Step 000120): Train loss 5.763, Val loss 5.781\n",
      "Ep 1 (Step 000130): Train loss 5.662, Val loss 5.739\n",
      "Ep 1 (Step 000140): Train loss 5.659, Val loss 5.696\n",
      "Ep 1 (Step 000150): Train loss 5.641, Val loss 5.660\n",
      "Ep 1 (Step 000160): Train loss 5.592, Val loss 5.642\n",
      "Ep 1 (Step 000170): Train loss 5.569, Val loss 5.623\n",
      "Ep 1 (Step 000180): Train loss 5.597, Val loss 5.599\n",
      "Ep 1 (Step 000190): Train loss 5.475, Val loss 5.565\n",
      "Ep 1 (Step 000200): Train loss 5.425, Val loss 5.542\n",
      "Ep 1 (Step 000210): Train loss 5.429, Val loss 5.514\n",
      "Ep 1 (Step 000220): Train loss 5.366, Val loss 5.514\n",
      "Ep 1 (Step 000230): Train loss 5.411, Val loss 5.474\n",
      "Ep 1 (Step 000240): Train loss 5.400, Val loss 5.462\n",
      "Ep 1 (Step 000250): Train loss 5.384, Val loss 5.449\n",
      "Ep 1 (Step 000260): Train loss 5.392, Val loss 5.437\n",
      "Ep 1 (Step 000270): Train loss 5.305, Val loss 5.417\n",
      "Ep 1 (Step 000280): Train loss 5.286, Val loss 5.411\n",
      "Ep 1 (Step 000290): Train loss 5.349, Val loss 5.394\n",
      "Ep 1 (Step 000300): Train loss 5.268, Val loss 5.390\n",
      "Ep 1 (Step 000310): Train loss 5.332, Val loss 5.375\n",
      "Ep 1 (Step 000320): Train loss 5.304, Val loss 5.356\n",
      "Ep 1 (Step 000330): Train loss 5.268, Val loss 5.351\n",
      "Ep 1 (Step 000340): Train loss 5.206, Val loss 5.346\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3458\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.818, Val loss 8.780\n",
      "Ep 1 (Step 000010): Train loss 7.375, Val loss 7.366\n",
      "Ep 1 (Step 000020): Train loss 6.786, Val loss 6.726\n",
      "Ep 1 (Step 000030): Train loss 6.464, Val loss 6.432\n",
      "Ep 1 (Step 000040): Train loss 6.355, Val loss 6.366\n",
      "Ep 1 (Step 000050): Train loss 6.295, Val loss 6.301\n",
      "Ep 1 (Step 000060): Train loss 6.157, Val loss 6.166\n",
      "Ep 1 (Step 000070): Train loss 6.117, Val loss 6.078\n",
      "Ep 1 (Step 000080): Train loss 6.009, Val loss 5.996\n",
      "Ep 1 (Step 000090): Train loss 5.939, Val loss 5.942\n",
      "Ep 1 (Step 000100): Train loss 5.819, Val loss 5.881\n",
      "Ep 1 (Step 000110): Train loss 5.828, Val loss 5.808\n",
      "Ep 1 (Step 000120): Train loss 5.749, Val loss 5.788\n",
      "Ep 1 (Step 000130): Train loss 5.673, Val loss 5.743\n",
      "Ep 1 (Step 000140): Train loss 5.621, Val loss 5.721\n",
      "Ep 1 (Step 000150): Train loss 5.625, Val loss 5.682\n",
      "Ep 1 (Step 000160): Train loss 5.561, Val loss 5.638\n",
      "Ep 1 (Step 000170): Train loss 5.597, Val loss 5.609\n",
      "Ep 1 (Step 000180): Train loss 5.514, Val loss 5.582\n",
      "Ep 1 (Step 000190): Train loss 5.449, Val loss 5.568\n",
      "Ep 1 (Step 000200): Train loss 5.457, Val loss 5.532\n",
      "Ep 1 (Step 000210): Train loss 5.514, Val loss 5.523\n",
      "Ep 1 (Step 000220): Train loss 5.312, Val loss 5.488\n",
      "Ep 1 (Step 000230): Train loss 5.399, Val loss 5.472\n",
      "Ep 1 (Step 000240): Train loss 5.372, Val loss 5.457\n",
      "Ep 1 (Step 000250): Train loss 5.377, Val loss 5.446\n",
      "Ep 1 (Step 000260): Train loss 5.325, Val loss 5.432\n",
      "Ep 1 (Step 000270): Train loss 5.408, Val loss 5.414\n",
      "Ep 1 (Step 000280): Train loss 5.307, Val loss 5.411\n",
      "Ep 1 (Step 000290): Train loss 5.352, Val loss 5.404\n",
      "Ep 1 (Step 000300): Train loss 5.266, Val loss 5.373\n",
      "Ep 1 (Step 000310): Train loss 5.306, Val loss 5.373\n",
      "Ep 1 (Step 000320): Train loss 5.241, Val loss 5.368\n",
      "Ep 1 (Step 000330): Train loss 5.285, Val loss 5.357\n",
      "Ep 1 (Step 000340): Train loss 5.194, Val loss 5.332\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3321\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.772, Val loss 8.791\n",
      "Ep 1 (Step 000010): Train loss 7.391, Val loss 7.380\n",
      "Ep 1 (Step 000020): Train loss 6.792, Val loss 6.747\n",
      "Ep 1 (Step 000030): Train loss 6.434, Val loss 6.443\n",
      "Ep 1 (Step 000040): Train loss 6.300, Val loss 6.367\n",
      "Ep 1 (Step 000050): Train loss 6.282, Val loss 6.283\n",
      "Ep 1 (Step 000060): Train loss 6.126, Val loss 6.191\n",
      "Ep 1 (Step 000070): Train loss 6.097, Val loss 6.107\n",
      "Ep 1 (Step 000080): Train loss 5.988, Val loss 6.003\n",
      "Ep 1 (Step 000090): Train loss 5.880, Val loss 5.939\n",
      "Ep 1 (Step 000100): Train loss 5.810, Val loss 5.876\n",
      "Ep 1 (Step 000110): Train loss 5.794, Val loss 5.810\n",
      "Ep 1 (Step 000120): Train loss 5.731, Val loss 5.778\n",
      "Ep 1 (Step 000130): Train loss 5.697, Val loss 5.738\n",
      "Ep 1 (Step 000140): Train loss 5.693, Val loss 5.706\n",
      "Ep 1 (Step 000150): Train loss 5.611, Val loss 5.682\n",
      "Ep 1 (Step 000160): Train loss 5.609, Val loss 5.639\n",
      "Ep 1 (Step 000170): Train loss 5.558, Val loss 5.617\n",
      "Ep 1 (Step 000180): Train loss 5.471, Val loss 5.594\n",
      "Ep 1 (Step 000190): Train loss 5.612, Val loss 5.571\n",
      "Ep 1 (Step 000200): Train loss 5.483, Val loss 5.551\n",
      "Ep 1 (Step 000210): Train loss 5.442, Val loss 5.516\n",
      "Ep 1 (Step 000220): Train loss 5.469, Val loss 5.493\n",
      "Ep 1 (Step 000230): Train loss 5.381, Val loss 5.477\n",
      "Ep 1 (Step 000240): Train loss 5.346, Val loss 5.450\n",
      "Ep 1 (Step 000250): Train loss 5.391, Val loss 5.434\n",
      "Ep 1 (Step 000260): Train loss 5.296, Val loss 5.423\n",
      "Ep 1 (Step 000270): Train loss 5.260, Val loss 5.408\n",
      "Ep 1 (Step 000280): Train loss 5.264, Val loss 5.401\n",
      "Ep 1 (Step 000290): Train loss 5.299, Val loss 5.392\n",
      "Ep 1 (Step 000300): Train loss 5.236, Val loss 5.388\n",
      "Ep 1 (Step 000310): Train loss 5.336, Val loss 5.381\n",
      "Ep 1 (Step 000320): Train loss 5.344, Val loss 5.377\n",
      "Ep 1 (Step 000330): Train loss 5.326, Val loss 5.361\n",
      "Ep 1 (Step 000340): Train loss 5.258, Val loss 5.342\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3422\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.821, Val loss 8.816\n",
      "Ep 1 (Step 000010): Train loss 7.402, Val loss 7.402\n",
      "Ep 1 (Step 000020): Train loss 6.733, Val loss 6.757\n",
      "Ep 1 (Step 000030): Train loss 6.456, Val loss 6.457\n",
      "Ep 1 (Step 000040): Train loss 6.373, Val loss 6.361\n",
      "Ep 1 (Step 000050): Train loss 6.297, Val loss 6.285\n",
      "Ep 1 (Step 000060): Train loss 6.274, Val loss 6.221\n",
      "Ep 1 (Step 000070): Train loss 6.103, Val loss 6.083\n",
      "Ep 1 (Step 000080): Train loss 6.001, Val loss 5.982\n",
      "Ep 1 (Step 000090): Train loss 5.942, Val loss 5.947\n",
      "Ep 1 (Step 000100): Train loss 5.895, Val loss 5.892\n",
      "Ep 1 (Step 000110): Train loss 5.845, Val loss 5.840\n",
      "Ep 1 (Step 000120): Train loss 5.764, Val loss 5.794\n",
      "Ep 1 (Step 000130): Train loss 5.638, Val loss 5.738\n",
      "Ep 1 (Step 000140): Train loss 5.739, Val loss 5.725\n",
      "Ep 1 (Step 000150): Train loss 5.642, Val loss 5.689\n",
      "Ep 1 (Step 000160): Train loss 5.563, Val loss 5.655\n",
      "Ep 1 (Step 000170): Train loss 5.542, Val loss 5.613\n",
      "Ep 1 (Step 000180): Train loss 5.545, Val loss 5.588\n",
      "Ep 1 (Step 000190): Train loss 5.489, Val loss 5.564\n",
      "Ep 1 (Step 000200): Train loss 5.439, Val loss 5.545\n",
      "Ep 1 (Step 000210): Train loss 5.447, Val loss 5.504\n",
      "Ep 1 (Step 000220): Train loss 5.475, Val loss 5.489\n",
      "Ep 1 (Step 000230): Train loss 5.423, Val loss 5.486\n",
      "Ep 1 (Step 000240): Train loss 5.294, Val loss 5.454\n",
      "Ep 1 (Step 000250): Train loss 5.288, Val loss 5.438\n",
      "Ep 1 (Step 000260): Train loss 5.270, Val loss 5.425\n",
      "Ep 1 (Step 000270): Train loss 5.308, Val loss 5.409\n",
      "Ep 1 (Step 000280): Train loss 5.369, Val loss 5.388\n",
      "Ep 1 (Step 000290): Train loss 5.381, Val loss 5.374\n",
      "Ep 1 (Step 000300): Train loss 5.315, Val loss 5.371\n",
      "Ep 1 (Step 000310): Train loss 5.222, Val loss 5.348\n",
      "Ep 1 (Step 000320): Train loss 5.251, Val loss 5.318\n",
      "Ep 1 (Step 000330): Train loss 5.277, Val loss 5.311\n",
      "Ep 1 (Step 000340): Train loss 5.168, Val loss 5.306\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3057\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.805, Val loss 8.762\n",
      "Ep 1 (Step 000010): Train loss 7.443, Val loss 7.403\n",
      "Ep 1 (Step 000020): Train loss 6.752, Val loss 6.730\n",
      "Ep 1 (Step 000030): Train loss 6.485, Val loss 6.436\n",
      "Ep 1 (Step 000040): Train loss 6.403, Val loss 6.364\n",
      "Ep 1 (Step 000050): Train loss 6.320, Val loss 6.321\n",
      "Ep 1 (Step 000060): Train loss 6.251, Val loss 6.216\n",
      "Ep 1 (Step 000070): Train loss 6.086, Val loss 6.099\n",
      "Ep 1 (Step 000080): Train loss 5.985, Val loss 6.025\n",
      "Ep 1 (Step 000090): Train loss 5.903, Val loss 5.949\n",
      "Ep 1 (Step 000100): Train loss 5.857, Val loss 5.883\n",
      "Ep 1 (Step 000110): Train loss 5.827, Val loss 5.844\n",
      "Ep 1 (Step 000120): Train loss 5.745, Val loss 5.805\n",
      "Ep 1 (Step 000130): Train loss 5.749, Val loss 5.763\n",
      "Ep 1 (Step 000140): Train loss 5.649, Val loss 5.721\n",
      "Ep 1 (Step 000150): Train loss 5.575, Val loss 5.666\n",
      "Ep 1 (Step 000160): Train loss 5.574, Val loss 5.635\n",
      "Ep 1 (Step 000170): Train loss 5.500, Val loss 5.603\n",
      "Ep 1 (Step 000180): Train loss 5.536, Val loss 5.566\n",
      "Ep 1 (Step 000190): Train loss 5.503, Val loss 5.561\n",
      "Ep 1 (Step 000200): Train loss 5.474, Val loss 5.526\n",
      "Ep 1 (Step 000210): Train loss 5.509, Val loss 5.523\n",
      "Ep 1 (Step 000220): Train loss 5.441, Val loss 5.495\n",
      "Ep 1 (Step 000230): Train loss 5.464, Val loss 5.469\n",
      "Ep 1 (Step 000240): Train loss 5.348, Val loss 5.466\n",
      "Ep 1 (Step 000250): Train loss 5.394, Val loss 5.453\n",
      "Ep 1 (Step 000260): Train loss 5.351, Val loss 5.432\n",
      "Ep 1 (Step 000270): Train loss 5.312, Val loss 5.414\n",
      "Ep 1 (Step 000280): Train loss 5.149, Val loss 5.399\n",
      "Ep 1 (Step 000290): Train loss 5.298, Val loss 5.399\n",
      "Ep 1 (Step 000300): Train loss 5.243, Val loss 5.381\n",
      "Ep 1 (Step 000310): Train loss 5.264, Val loss 5.374\n",
      "Ep 1 (Step 000320): Train loss 5.241, Val loss 5.358\n",
      "Ep 1 (Step 000330): Train loss 5.129, Val loss 5.344\n",
      "Ep 1 (Step 000340): Train loss 5.227, Val loss 5.331\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3307\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.796, Val loss 8.766\n",
      "Ep 1 (Step 000010): Train loss 7.392, Val loss 7.347\n",
      "Ep 1 (Step 000020): Train loss 6.719, Val loss 6.712\n",
      "Ep 1 (Step 000030): Train loss 6.485, Val loss 6.436\n",
      "Ep 1 (Step 000040): Train loss 6.383, Val loss 6.355\n",
      "Ep 1 (Step 000050): Train loss 6.230, Val loss 6.304\n",
      "Ep 1 (Step 000060): Train loss 6.158, Val loss 6.216\n",
      "Ep 1 (Step 000070): Train loss 6.124, Val loss 6.096\n",
      "Ep 1 (Step 000080): Train loss 5.966, Val loss 6.027\n",
      "Ep 1 (Step 000090): Train loss 5.933, Val loss 5.932\n",
      "Ep 1 (Step 000100): Train loss 5.892, Val loss 5.890\n",
      "Ep 1 (Step 000110): Train loss 5.809, Val loss 5.833\n",
      "Ep 1 (Step 000120): Train loss 5.670, Val loss 5.787\n",
      "Ep 1 (Step 000130): Train loss 5.753, Val loss 5.755\n",
      "Ep 1 (Step 000140): Train loss 5.599, Val loss 5.726\n",
      "Ep 1 (Step 000150): Train loss 5.612, Val loss 5.683\n",
      "Ep 1 (Step 000160): Train loss 5.593, Val loss 5.649\n",
      "Ep 1 (Step 000170): Train loss 5.528, Val loss 5.622\n",
      "Ep 1 (Step 000180): Train loss 5.494, Val loss 5.588\n",
      "Ep 1 (Step 000190): Train loss 5.485, Val loss 5.550\n",
      "Ep 1 (Step 000200): Train loss 5.456, Val loss 5.534\n",
      "Ep 1 (Step 000210): Train loss 5.438, Val loss 5.516\n",
      "Ep 1 (Step 000220): Train loss 5.433, Val loss 5.505\n",
      "Ep 1 (Step 000230): Train loss 5.417, Val loss 5.479\n",
      "Ep 1 (Step 000240): Train loss 5.351, Val loss 5.454\n",
      "Ep 1 (Step 000250): Train loss 5.341, Val loss 5.456\n",
      "Ep 1 (Step 000260): Train loss 5.255, Val loss 5.418\n",
      "Ep 1 (Step 000270): Train loss 5.367, Val loss 5.414\n",
      "Ep 1 (Step 000280): Train loss 5.281, Val loss 5.382\n",
      "Ep 1 (Step 000290): Train loss 5.280, Val loss 5.372\n",
      "Ep 1 (Step 000300): Train loss 5.264, Val loss 5.371\n",
      "Ep 1 (Step 000310): Train loss 5.263, Val loss 5.352\n",
      "Ep 1 (Step 000320): Train loss 5.301, Val loss 5.361\n",
      "Ep 1 (Step 000330): Train loss 5.267, Val loss 5.345\n",
      "Ep 1 (Step 000340): Train loss 5.172, Val loss 5.328\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3281\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.524, Val loss 8.520\n",
      "Ep 1 (Step 000010): Train loss 6.811, Val loss 6.817\n",
      "Ep 1 (Step 000020): Train loss 6.499, Val loss 6.436\n",
      "Ep 1 (Step 000030): Train loss 6.454, Val loss 6.424\n",
      "Ep 1 (Step 000040): Train loss 6.212, Val loss 6.308\n",
      "Ep 1 (Step 000050): Train loss 6.138, Val loss 6.179\n",
      "Ep 1 (Step 000060): Train loss 6.030, Val loss 6.048\n",
      "Ep 1 (Step 000070): Train loss 5.933, Val loss 5.951\n",
      "Ep 1 (Step 000080): Train loss 5.866, Val loss 5.867\n",
      "Ep 1 (Step 000090): Train loss 5.744, Val loss 5.776\n",
      "Ep 1 (Step 000100): Train loss 5.682, Val loss 5.722\n",
      "Ep 1 (Step 000110): Train loss 5.664, Val loss 5.712\n",
      "Ep 1 (Step 000120): Train loss 5.589, Val loss 5.676\n",
      "Ep 1 (Step 000130): Train loss 5.517, Val loss 5.640\n",
      "Ep 1 (Step 000140): Train loss 5.468, Val loss 5.582\n",
      "Ep 1 (Step 000150): Train loss 5.511, Val loss 5.559\n",
      "Ep 1 (Step 000160): Train loss 5.466, Val loss 5.527\n",
      "Ep 1 (Step 000170): Train loss 5.458, Val loss 5.508\n",
      "Ep 1 (Step 000180): Train loss 5.398, Val loss 5.470\n",
      "Ep 1 (Step 000190): Train loss 5.387, Val loss 5.479\n",
      "Ep 1 (Step 000200): Train loss 5.375, Val loss 5.438\n",
      "Ep 1 (Step 000210): Train loss 5.412, Val loss 5.456\n",
      "Ep 1 (Step 000220): Train loss 5.325, Val loss 5.424\n",
      "Ep 1 (Step 000230): Train loss 5.254, Val loss 5.412\n",
      "Ep 1 (Step 000240): Train loss 5.292, Val loss 5.396\n",
      "Ep 1 (Step 000250): Train loss 5.314, Val loss 5.369\n",
      "Ep 1 (Step 000260): Train loss 5.331, Val loss 5.370\n",
      "Ep 1 (Step 000270): Train loss 5.323, Val loss 5.356\n",
      "Ep 1 (Step 000280): Train loss 5.174, Val loss 5.339\n",
      "Ep 1 (Step 000290): Train loss 5.240, Val loss 5.352\n",
      "Ep 1 (Step 000300): Train loss 5.207, Val loss 5.324\n",
      "Ep 1 (Step 000310): Train loss 5.121, Val loss 5.309\n",
      "Ep 1 (Step 000320): Train loss 5.170, Val loss 5.296\n",
      "Ep 1 (Step 000330): Train loss 5.165, Val loss 5.282\n",
      "Ep 1 (Step 000340): Train loss 5.209, Val loss 5.275\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2748\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.528, Val loss 8.504\n",
      "Ep 1 (Step 000010): Train loss 6.861, Val loss 6.841\n",
      "Ep 1 (Step 000020): Train loss 6.446, Val loss 6.454\n",
      "Ep 1 (Step 000030): Train loss 6.391, Val loss 6.386\n",
      "Ep 1 (Step 000040): Train loss 6.291, Val loss 6.262\n",
      "Ep 1 (Step 000050): Train loss 6.102, Val loss 6.103\n",
      "Ep 1 (Step 000060): Train loss 6.019, Val loss 6.021\n",
      "Ep 1 (Step 000070): Train loss 5.883, Val loss 5.913\n",
      "Ep 1 (Step 000080): Train loss 5.828, Val loss 5.827\n",
      "Ep 1 (Step 000090): Train loss 5.722, Val loss 5.783\n",
      "Ep 1 (Step 000100): Train loss 5.570, Val loss 5.707\n",
      "Ep 1 (Step 000110): Train loss 5.630, Val loss 5.673\n",
      "Ep 1 (Step 000120): Train loss 5.541, Val loss 5.665\n",
      "Ep 1 (Step 000130): Train loss 5.591, Val loss 5.594\n",
      "Ep 1 (Step 000140): Train loss 5.457, Val loss 5.551\n",
      "Ep 1 (Step 000150): Train loss 5.460, Val loss 5.531\n",
      "Ep 1 (Step 000160): Train loss 5.475, Val loss 5.521\n",
      "Ep 1 (Step 000170): Train loss 5.358, Val loss 5.504\n",
      "Ep 1 (Step 000180): Train loss 5.440, Val loss 5.499\n",
      "Ep 1 (Step 000190): Train loss 5.371, Val loss 5.483\n",
      "Ep 1 (Step 000200): Train loss 5.430, Val loss 5.452\n",
      "Ep 1 (Step 000210): Train loss 5.302, Val loss 5.431\n",
      "Ep 1 (Step 000220): Train loss 5.349, Val loss 5.412\n",
      "Ep 1 (Step 000230): Train loss 5.355, Val loss 5.406\n",
      "Ep 1 (Step 000240): Train loss 5.353, Val loss 5.397\n",
      "Ep 1 (Step 000250): Train loss 5.288, Val loss 5.374\n",
      "Ep 1 (Step 000260): Train loss 5.326, Val loss 5.368\n",
      "Ep 1 (Step 000270): Train loss 5.355, Val loss 5.353\n",
      "Ep 1 (Step 000280): Train loss 5.219, Val loss 5.342\n",
      "Ep 1 (Step 000290): Train loss 5.185, Val loss 5.352\n",
      "Ep 1 (Step 000300): Train loss 5.173, Val loss 5.357\n",
      "Ep 1 (Step 000310): Train loss 5.196, Val loss 5.318\n",
      "Ep 1 (Step 000320): Train loss 5.160, Val loss 5.304\n",
      "Ep 1 (Step 000330): Train loss 5.256, Val loss 5.301\n",
      "Ep 1 (Step 000340): Train loss 5.206, Val loss 5.274\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2745\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.608, Val loss 8.575\n",
      "Ep 1 (Step 000010): Train loss 6.836, Val loss 6.839\n",
      "Ep 1 (Step 000020): Train loss 6.512, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.517, Val loss 6.419\n",
      "Ep 1 (Step 000040): Train loss 6.299, Val loss 6.282\n",
      "Ep 1 (Step 000050): Train loss 6.210, Val loss 6.186\n",
      "Ep 1 (Step 000060): Train loss 5.998, Val loss 6.050\n",
      "Ep 1 (Step 000070): Train loss 5.878, Val loss 5.936\n",
      "Ep 1 (Step 000080): Train loss 5.822, Val loss 5.888\n",
      "Ep 1 (Step 000090): Train loss 5.701, Val loss 5.825\n",
      "Ep 1 (Step 000100): Train loss 5.720, Val loss 5.780\n",
      "Ep 1 (Step 000110): Train loss 5.686, Val loss 5.735\n",
      "Ep 1 (Step 000120): Train loss 5.613, Val loss 5.683\n",
      "Ep 1 (Step 000130): Train loss 5.535, Val loss 5.646\n",
      "Ep 1 (Step 000140): Train loss 5.581, Val loss 5.617\n",
      "Ep 1 (Step 000150): Train loss 5.516, Val loss 5.578\n",
      "Ep 1 (Step 000160): Train loss 5.388, Val loss 5.556\n",
      "Ep 1 (Step 000170): Train loss 5.377, Val loss 5.529\n",
      "Ep 1 (Step 000180): Train loss 5.349, Val loss 5.499\n",
      "Ep 1 (Step 000190): Train loss 5.376, Val loss 5.470\n",
      "Ep 1 (Step 000200): Train loss 5.362, Val loss 5.456\n",
      "Ep 1 (Step 000210): Train loss 5.310, Val loss 5.442\n",
      "Ep 1 (Step 000220): Train loss 5.368, Val loss 5.434\n",
      "Ep 1 (Step 000230): Train loss 5.299, Val loss 5.421\n",
      "Ep 1 (Step 000240): Train loss 5.363, Val loss 5.408\n",
      "Ep 1 (Step 000250): Train loss 5.281, Val loss 5.406\n",
      "Ep 1 (Step 000260): Train loss 5.258, Val loss 5.371\n",
      "Ep 1 (Step 000270): Train loss 5.218, Val loss 5.357\n",
      "Ep 1 (Step 000280): Train loss 5.293, Val loss 5.338\n",
      "Ep 1 (Step 000290): Train loss 5.246, Val loss 5.339\n",
      "Ep 1 (Step 000300): Train loss 5.238, Val loss 5.313\n",
      "Ep 1 (Step 000310): Train loss 5.234, Val loss 5.310\n",
      "Ep 1 (Step 000320): Train loss 5.207, Val loss 5.295\n",
      "Ep 1 (Step 000330): Train loss 5.168, Val loss 5.286\n",
      "Ep 1 (Step 000340): Train loss 5.236, Val loss 5.274\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2736\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.568, Val loss 8.553\n",
      "Ep 1 (Step 000010): Train loss 6.847, Val loss 6.835\n",
      "Ep 1 (Step 000020): Train loss 6.469, Val loss 6.456\n",
      "Ep 1 (Step 000030): Train loss 6.408, Val loss 6.397\n",
      "Ep 1 (Step 000040): Train loss 6.227, Val loss 6.234\n",
      "Ep 1 (Step 000050): Train loss 6.104, Val loss 6.155\n",
      "Ep 1 (Step 000060): Train loss 6.047, Val loss 6.044\n",
      "Ep 1 (Step 000070): Train loss 5.911, Val loss 5.934\n",
      "Ep 1 (Step 000080): Train loss 5.846, Val loss 5.865\n",
      "Ep 1 (Step 000090): Train loss 5.716, Val loss 5.821\n",
      "Ep 1 (Step 000100): Train loss 5.677, Val loss 5.755\n",
      "Ep 1 (Step 000110): Train loss 5.635, Val loss 5.688\n",
      "Ep 1 (Step 000120): Train loss 5.613, Val loss 5.653\n",
      "Ep 1 (Step 000130): Train loss 5.510, Val loss 5.617\n",
      "Ep 1 (Step 000140): Train loss 5.526, Val loss 5.591\n",
      "Ep 1 (Step 000150): Train loss 5.498, Val loss 5.564\n",
      "Ep 1 (Step 000160): Train loss 5.477, Val loss 5.529\n",
      "Ep 1 (Step 000170): Train loss 5.410, Val loss 5.503\n",
      "Ep 1 (Step 000180): Train loss 5.398, Val loss 5.465\n",
      "Ep 1 (Step 000190): Train loss 5.380, Val loss 5.437\n",
      "Ep 1 (Step 000200): Train loss 5.368, Val loss 5.437\n",
      "Ep 1 (Step 000210): Train loss 5.402, Val loss 5.434\n",
      "Ep 1 (Step 000220): Train loss 5.299, Val loss 5.400\n",
      "Ep 1 (Step 000230): Train loss 5.238, Val loss 5.385\n",
      "Ep 1 (Step 000240): Train loss 5.287, Val loss 5.369\n",
      "Ep 1 (Step 000250): Train loss 5.295, Val loss 5.336\n",
      "Ep 1 (Step 000260): Train loss 5.202, Val loss 5.318\n",
      "Ep 1 (Step 000270): Train loss 5.300, Val loss 5.316\n",
      "Ep 1 (Step 000280): Train loss 5.229, Val loss 5.296\n",
      "Ep 1 (Step 000290): Train loss 5.179, Val loss 5.306\n",
      "Ep 1 (Step 000300): Train loss 5.158, Val loss 5.303\n",
      "Ep 1 (Step 000310): Train loss 5.173, Val loss 5.261\n",
      "Ep 1 (Step 000320): Train loss 5.133, Val loss 5.252\n",
      "Ep 1 (Step 000330): Train loss 5.130, Val loss 5.247\n",
      "Ep 1 (Step 000340): Train loss 5.027, Val loss 5.244\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2440\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.592, Val loss 8.596\n",
      "Ep 1 (Step 000010): Train loss 6.879, Val loss 6.845\n",
      "Ep 1 (Step 000020): Train loss 6.519, Val loss 6.462\n",
      "Ep 1 (Step 000030): Train loss 6.454, Val loss 6.428\n",
      "Ep 1 (Step 000040): Train loss 6.371, Val loss 6.282\n",
      "Ep 1 (Step 000050): Train loss 6.175, Val loss 6.139\n",
      "Ep 1 (Step 000060): Train loss 6.069, Val loss 6.026\n",
      "Ep 1 (Step 000070): Train loss 5.969, Val loss 5.933\n",
      "Ep 1 (Step 000080): Train loss 5.807, Val loss 5.863\n",
      "Ep 1 (Step 000090): Train loss 5.711, Val loss 5.794\n",
      "Ep 1 (Step 000100): Train loss 5.659, Val loss 5.757\n",
      "Ep 1 (Step 000110): Train loss 5.608, Val loss 5.718\n",
      "Ep 1 (Step 000120): Train loss 5.687, Val loss 5.661\n",
      "Ep 1 (Step 000130): Train loss 5.475, Val loss 5.611\n",
      "Ep 1 (Step 000140): Train loss 5.509, Val loss 5.576\n",
      "Ep 1 (Step 000150): Train loss 5.521, Val loss 5.540\n",
      "Ep 1 (Step 000160): Train loss 5.451, Val loss 5.534\n",
      "Ep 1 (Step 000170): Train loss 5.357, Val loss 5.496\n",
      "Ep 1 (Step 000180): Train loss 5.431, Val loss 5.485\n",
      "Ep 1 (Step 000190): Train loss 5.353, Val loss 5.450\n",
      "Ep 1 (Step 000200): Train loss 5.338, Val loss 5.437\n",
      "Ep 1 (Step 000210): Train loss 5.331, Val loss 5.417\n",
      "Ep 1 (Step 000220): Train loss 5.425, Val loss 5.398\n",
      "Ep 1 (Step 000230): Train loss 5.291, Val loss 5.378\n",
      "Ep 1 (Step 000240): Train loss 5.245, Val loss 5.370\n",
      "Ep 1 (Step 000250): Train loss 5.204, Val loss 5.368\n",
      "Ep 1 (Step 000260): Train loss 5.200, Val loss 5.349\n",
      "Ep 1 (Step 000270): Train loss 5.178, Val loss 5.340\n",
      "Ep 1 (Step 000280): Train loss 5.203, Val loss 5.335\n",
      "Ep 1 (Step 000290): Train loss 5.229, Val loss 5.333\n",
      "Ep 1 (Step 000300): Train loss 5.153, Val loss 5.326\n",
      "Ep 1 (Step 000310): Train loss 5.125, Val loss 5.286\n",
      "Ep 1 (Step 000320): Train loss 5.160, Val loss 5.284\n",
      "Ep 1 (Step 000330): Train loss 5.104, Val loss 5.270\n",
      "Ep 1 (Step 000340): Train loss 5.095, Val loss 5.270\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2702\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.444, Val loss 8.450\n",
      "Ep 1 (Step 000010): Train loss 6.877, Val loss 6.801\n",
      "Ep 1 (Step 000020): Train loss 6.475, Val loss 6.444\n",
      "Ep 1 (Step 000030): Train loss 6.343, Val loss 6.406\n",
      "Ep 1 (Step 000040): Train loss 6.375, Val loss 6.265\n",
      "Ep 1 (Step 000050): Train loss 6.139, Val loss 6.153\n",
      "Ep 1 (Step 000060): Train loss 5.994, Val loss 6.040\n",
      "Ep 1 (Step 000070): Train loss 5.874, Val loss 5.918\n",
      "Ep 1 (Step 000080): Train loss 5.886, Val loss 5.857\n",
      "Ep 1 (Step 000090): Train loss 5.823, Val loss 5.789\n",
      "Ep 1 (Step 000100): Train loss 5.638, Val loss 5.750\n",
      "Ep 1 (Step 000110): Train loss 5.634, Val loss 5.687\n",
      "Ep 1 (Step 000120): Train loss 5.572, Val loss 5.646\n",
      "Ep 1 (Step 000130): Train loss 5.621, Val loss 5.574\n",
      "Ep 1 (Step 000140): Train loss 5.487, Val loss 5.526\n",
      "Ep 1 (Step 000150): Train loss 5.500, Val loss 5.528\n",
      "Ep 1 (Step 000160): Train loss 5.404, Val loss 5.499\n",
      "Ep 1 (Step 000170): Train loss 5.461, Val loss 5.485\n",
      "Ep 1 (Step 000180): Train loss 5.273, Val loss 5.483\n",
      "Ep 1 (Step 000190): Train loss 5.384, Val loss 5.471\n",
      "Ep 1 (Step 000200): Train loss 5.308, Val loss 5.430\n",
      "Ep 1 (Step 000210): Train loss 5.334, Val loss 5.409\n",
      "Ep 1 (Step 000220): Train loss 5.365, Val loss 5.386\n",
      "Ep 1 (Step 000230): Train loss 5.290, Val loss 5.396\n",
      "Ep 1 (Step 000240): Train loss 5.318, Val loss 5.358\n",
      "Ep 1 (Step 000250): Train loss 5.221, Val loss 5.359\n",
      "Ep 1 (Step 000260): Train loss 5.273, Val loss 5.350\n",
      "Ep 1 (Step 000270): Train loss 5.322, Val loss 5.348\n",
      "Ep 1 (Step 000280): Train loss 5.159, Val loss 5.327\n",
      "Ep 1 (Step 000290): Train loss 5.196, Val loss 5.305\n",
      "Ep 1 (Step 000300): Train loss 5.224, Val loss 5.304\n",
      "Ep 1 (Step 000310): Train loss 5.197, Val loss 5.287\n",
      "Ep 1 (Step 000320): Train loss 5.213, Val loss 5.287\n",
      "Ep 1 (Step 000330): Train loss 5.084, Val loss 5.253\n",
      "Ep 1 (Step 000340): Train loss 5.146, Val loss 5.260\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2599\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.787, Val loss 8.768\n",
      "Ep 1 (Step 000010): Train loss 7.398, Val loss 7.374\n",
      "Ep 1 (Step 000020): Train loss 6.794, Val loss 6.746\n",
      "Ep 1 (Step 000030): Train loss 6.478, Val loss 6.438\n",
      "Ep 1 (Step 000040): Train loss 6.301, Val loss 6.309\n",
      "Ep 1 (Step 000050): Train loss 6.256, Val loss 6.207\n",
      "Ep 1 (Step 000060): Train loss 6.133, Val loss 6.078\n",
      "Ep 1 (Step 000070): Train loss 6.020, Val loss 6.001\n",
      "Ep 1 (Step 000080): Train loss 5.938, Val loss 5.949\n",
      "Ep 1 (Step 000090): Train loss 5.921, Val loss 5.865\n",
      "Ep 1 (Step 000100): Train loss 5.830, Val loss 5.827\n",
      "Ep 1 (Step 000110): Train loss 5.732, Val loss 5.762\n",
      "Ep 1 (Step 000120): Train loss 5.680, Val loss 5.730\n",
      "Ep 1 (Step 000130): Train loss 5.720, Val loss 5.699\n",
      "Ep 1 (Step 000140): Train loss 5.540, Val loss 5.658\n",
      "Ep 1 (Step 000150): Train loss 5.592, Val loss 5.612\n",
      "Ep 1 (Step 000160): Train loss 5.581, Val loss 5.596\n",
      "Ep 1 (Step 000170): Train loss 5.514, Val loss 5.587\n",
      "Ep 1 (Step 000180): Train loss 5.439, Val loss 5.537\n",
      "Ep 1 (Step 000190): Train loss 5.460, Val loss 5.514\n",
      "Ep 1 (Step 000200): Train loss 5.447, Val loss 5.487\n",
      "Ep 1 (Step 000210): Train loss 5.405, Val loss 5.467\n",
      "Ep 1 (Step 000220): Train loss 5.430, Val loss 5.449\n",
      "Ep 1 (Step 000230): Train loss 5.286, Val loss 5.448\n",
      "Ep 1 (Step 000240): Train loss 5.286, Val loss 5.434\n",
      "Ep 1 (Step 000250): Train loss 5.243, Val loss 5.418\n",
      "Ep 1 (Step 000260): Train loss 5.313, Val loss 5.398\n",
      "Ep 1 (Step 000270): Train loss 5.285, Val loss 5.389\n",
      "Ep 1 (Step 000280): Train loss 5.226, Val loss 5.366\n",
      "Ep 1 (Step 000290): Train loss 5.170, Val loss 5.350\n",
      "Ep 1 (Step 000300): Train loss 5.267, Val loss 5.337\n",
      "Ep 1 (Step 000310): Train loss 5.166, Val loss 5.319\n",
      "Ep 1 (Step 000320): Train loss 5.087, Val loss 5.318\n",
      "Ep 1 (Step 000330): Train loss 5.282, Val loss 5.323\n",
      "Ep 1 (Step 000340): Train loss 5.179, Val loss 5.302\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3017\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.808, Val loss 8.769\n",
      "Ep 1 (Step 000010): Train loss 7.400, Val loss 7.387\n",
      "Ep 1 (Step 000020): Train loss 6.750, Val loss 6.748\n",
      "Ep 1 (Step 000030): Train loss 6.458, Val loss 6.449\n",
      "Ep 1 (Step 000040): Train loss 6.324, Val loss 6.347\n",
      "Ep 1 (Step 000050): Train loss 6.237, Val loss 6.254\n",
      "Ep 1 (Step 000060): Train loss 6.085, Val loss 6.119\n",
      "Ep 1 (Step 000070): Train loss 5.986, Val loss 6.033\n",
      "Ep 1 (Step 000080): Train loss 5.921, Val loss 5.943\n",
      "Ep 1 (Step 000090): Train loss 5.840, Val loss 5.882\n",
      "Ep 1 (Step 000100): Train loss 5.811, Val loss 5.820\n",
      "Ep 1 (Step 000110): Train loss 5.758, Val loss 5.792\n",
      "Ep 1 (Step 000120): Train loss 5.707, Val loss 5.760\n",
      "Ep 1 (Step 000130): Train loss 5.576, Val loss 5.707\n",
      "Ep 1 (Step 000140): Train loss 5.658, Val loss 5.669\n",
      "Ep 1 (Step 000150): Train loss 5.571, Val loss 5.628\n",
      "Ep 1 (Step 000160): Train loss 5.530, Val loss 5.594\n",
      "Ep 1 (Step 000170): Train loss 5.451, Val loss 5.558\n",
      "Ep 1 (Step 000180): Train loss 5.451, Val loss 5.528\n",
      "Ep 1 (Step 000190): Train loss 5.401, Val loss 5.497\n",
      "Ep 1 (Step 000200): Train loss 5.472, Val loss 5.477\n",
      "Ep 1 (Step 000210): Train loss 5.394, Val loss 5.465\n",
      "Ep 1 (Step 000220): Train loss 5.342, Val loss 5.435\n",
      "Ep 1 (Step 000230): Train loss 5.342, Val loss 5.426\n",
      "Ep 1 (Step 000240): Train loss 5.388, Val loss 5.413\n",
      "Ep 1 (Step 000250): Train loss 5.261, Val loss 5.376\n",
      "Ep 1 (Step 000260): Train loss 5.271, Val loss 5.373\n",
      "Ep 1 (Step 000270): Train loss 5.225, Val loss 5.359\n",
      "Ep 1 (Step 000280): Train loss 5.232, Val loss 5.349\n",
      "Ep 1 (Step 000290): Train loss 5.165, Val loss 5.340\n",
      "Ep 1 (Step 000300): Train loss 5.298, Val loss 5.332\n",
      "Ep 1 (Step 000310): Train loss 5.184, Val loss 5.328\n",
      "Ep 1 (Step 000320): Train loss 5.224, Val loss 5.299\n",
      "Ep 1 (Step 000330): Train loss 5.145, Val loss 5.292\n",
      "Ep 1 (Step 000340): Train loss 5.201, Val loss 5.284\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2844\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.861, Val loss 8.821\n",
      "Ep 1 (Step 000010): Train loss 7.454, Val loss 7.398\n",
      "Ep 1 (Step 000020): Train loss 6.804, Val loss 6.756\n",
      "Ep 1 (Step 000030): Train loss 6.455, Val loss 6.441\n",
      "Ep 1 (Step 000040): Train loss 6.385, Val loss 6.340\n",
      "Ep 1 (Step 000050): Train loss 6.233, Val loss 6.247\n",
      "Ep 1 (Step 000060): Train loss 6.105, Val loss 6.125\n",
      "Ep 1 (Step 000070): Train loss 6.020, Val loss 6.006\n",
      "Ep 1 (Step 000080): Train loss 5.850, Val loss 5.942\n",
      "Ep 1 (Step 000090): Train loss 5.908, Val loss 5.858\n",
      "Ep 1 (Step 000100): Train loss 5.679, Val loss 5.807\n",
      "Ep 1 (Step 000110): Train loss 5.724, Val loss 5.765\n",
      "Ep 1 (Step 000120): Train loss 5.763, Val loss 5.741\n",
      "Ep 1 (Step 000130): Train loss 5.611, Val loss 5.684\n",
      "Ep 1 (Step 000140): Train loss 5.580, Val loss 5.637\n",
      "Ep 1 (Step 000150): Train loss 5.545, Val loss 5.618\n",
      "Ep 1 (Step 000160): Train loss 5.502, Val loss 5.605\n",
      "Ep 1 (Step 000170): Train loss 5.519, Val loss 5.572\n",
      "Ep 1 (Step 000180): Train loss 5.505, Val loss 5.550\n",
      "Ep 1 (Step 000190): Train loss 5.516, Val loss 5.530\n",
      "Ep 1 (Step 000200): Train loss 5.450, Val loss 5.512\n",
      "Ep 1 (Step 000210): Train loss 5.374, Val loss 5.475\n",
      "Ep 1 (Step 000220): Train loss 5.374, Val loss 5.467\n",
      "Ep 1 (Step 000230): Train loss 5.435, Val loss 5.451\n",
      "Ep 1 (Step 000240): Train loss 5.319, Val loss 5.422\n",
      "Ep 1 (Step 000250): Train loss 5.320, Val loss 5.414\n",
      "Ep 1 (Step 000260): Train loss 5.292, Val loss 5.399\n",
      "Ep 1 (Step 000270): Train loss 5.270, Val loss 5.385\n",
      "Ep 1 (Step 000280): Train loss 5.332, Val loss 5.371\n",
      "Ep 1 (Step 000290): Train loss 5.136, Val loss 5.368\n",
      "Ep 1 (Step 000300): Train loss 5.329, Val loss 5.360\n",
      "Ep 1 (Step 000310): Train loss 5.263, Val loss 5.332\n",
      "Ep 1 (Step 000320): Train loss 5.262, Val loss 5.320\n",
      "Ep 1 (Step 000330): Train loss 5.152, Val loss 5.315\n",
      "Ep 1 (Step 000340): Train loss 5.135, Val loss 5.298\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2978\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.785, Val loss 8.766\n",
      "Ep 1 (Step 000010): Train loss 7.443, Val loss 7.377\n",
      "Ep 1 (Step 000020): Train loss 6.756, Val loss 6.740\n",
      "Ep 1 (Step 000030): Train loss 6.413, Val loss 6.422\n",
      "Ep 1 (Step 000040): Train loss 6.380, Val loss 6.339\n",
      "Ep 1 (Step 000050): Train loss 6.246, Val loss 6.224\n",
      "Ep 1 (Step 000060): Train loss 6.122, Val loss 6.110\n",
      "Ep 1 (Step 000070): Train loss 6.028, Val loss 6.010\n",
      "Ep 1 (Step 000080): Train loss 5.851, Val loss 5.953\n",
      "Ep 1 (Step 000090): Train loss 5.802, Val loss 5.870\n",
      "Ep 1 (Step 000100): Train loss 5.731, Val loss 5.805\n",
      "Ep 1 (Step 000110): Train loss 5.757, Val loss 5.749\n",
      "Ep 1 (Step 000120): Train loss 5.735, Val loss 5.736\n",
      "Ep 1 (Step 000130): Train loss 5.664, Val loss 5.677\n",
      "Ep 1 (Step 000140): Train loss 5.519, Val loss 5.656\n",
      "Ep 1 (Step 000150): Train loss 5.565, Val loss 5.616\n",
      "Ep 1 (Step 000160): Train loss 5.543, Val loss 5.579\n",
      "Ep 1 (Step 000170): Train loss 5.451, Val loss 5.563\n",
      "Ep 1 (Step 000180): Train loss 5.470, Val loss 5.532\n",
      "Ep 1 (Step 000190): Train loss 5.480, Val loss 5.509\n",
      "Ep 1 (Step 000200): Train loss 5.457, Val loss 5.485\n",
      "Ep 1 (Step 000210): Train loss 5.362, Val loss 5.453\n",
      "Ep 1 (Step 000220): Train loss 5.348, Val loss 5.452\n",
      "Ep 1 (Step 000230): Train loss 5.380, Val loss 5.422\n",
      "Ep 1 (Step 000240): Train loss 5.319, Val loss 5.391\n",
      "Ep 1 (Step 000250): Train loss 5.305, Val loss 5.408\n",
      "Ep 1 (Step 000260): Train loss 5.257, Val loss 5.386\n",
      "Ep 1 (Step 000270): Train loss 5.237, Val loss 5.363\n",
      "Ep 1 (Step 000280): Train loss 5.276, Val loss 5.351\n",
      "Ep 1 (Step 000290): Train loss 5.205, Val loss 5.355\n",
      "Ep 1 (Step 000300): Train loss 5.315, Val loss 5.345\n",
      "Ep 1 (Step 000310): Train loss 5.284, Val loss 5.315\n",
      "Ep 1 (Step 000320): Train loss 5.132, Val loss 5.302\n",
      "Ep 1 (Step 000330): Train loss 5.165, Val loss 5.298\n",
      "Ep 1 (Step 000340): Train loss 5.153, Val loss 5.280\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2802\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.816, Val loss 8.778\n",
      "Ep 1 (Step 000010): Train loss 7.389, Val loss 7.362\n",
      "Ep 1 (Step 000020): Train loss 6.742, Val loss 6.707\n",
      "Ep 1 (Step 000030): Train loss 6.451, Val loss 6.426\n",
      "Ep 1 (Step 000040): Train loss 6.380, Val loss 6.348\n",
      "Ep 1 (Step 000050): Train loss 6.266, Val loss 6.251\n",
      "Ep 1 (Step 000060): Train loss 6.259, Val loss 6.136\n",
      "Ep 1 (Step 000070): Train loss 6.017, Val loss 6.047\n",
      "Ep 1 (Step 000080): Train loss 5.980, Val loss 5.966\n",
      "Ep 1 (Step 000090): Train loss 5.912, Val loss 5.892\n",
      "Ep 1 (Step 000100): Train loss 5.853, Val loss 5.836\n",
      "Ep 1 (Step 000110): Train loss 5.724, Val loss 5.774\n",
      "Ep 1 (Step 000120): Train loss 5.708, Val loss 5.762\n",
      "Ep 1 (Step 000130): Train loss 5.578, Val loss 5.719\n",
      "Ep 1 (Step 000140): Train loss 5.504, Val loss 5.677\n",
      "Ep 1 (Step 000150): Train loss 5.610, Val loss 5.637\n",
      "Ep 1 (Step 000160): Train loss 5.558, Val loss 5.612\n",
      "Ep 1 (Step 000170): Train loss 5.487, Val loss 5.574\n",
      "Ep 1 (Step 000180): Train loss 5.496, Val loss 5.532\n",
      "Ep 1 (Step 000190): Train loss 5.451, Val loss 5.519\n",
      "Ep 1 (Step 000200): Train loss 5.424, Val loss 5.482\n",
      "Ep 1 (Step 000210): Train loss 5.389, Val loss 5.476\n",
      "Ep 1 (Step 000220): Train loss 5.351, Val loss 5.452\n",
      "Ep 1 (Step 000230): Train loss 5.347, Val loss 5.440\n",
      "Ep 1 (Step 000240): Train loss 5.360, Val loss 5.414\n",
      "Ep 1 (Step 000250): Train loss 5.326, Val loss 5.404\n",
      "Ep 1 (Step 000260): Train loss 5.252, Val loss 5.388\n",
      "Ep 1 (Step 000270): Train loss 5.275, Val loss 5.376\n",
      "Ep 1 (Step 000280): Train loss 5.251, Val loss 5.362\n",
      "Ep 1 (Step 000290): Train loss 5.206, Val loss 5.351\n",
      "Ep 1 (Step 000300): Train loss 5.231, Val loss 5.335\n",
      "Ep 1 (Step 000310): Train loss 5.157, Val loss 5.291\n",
      "Ep 1 (Step 000320): Train loss 5.142, Val loss 5.293\n",
      "Ep 1 (Step 000330): Train loss 5.253, Val loss 5.276\n",
      "Ep 1 (Step 000340): Train loss 5.152, Val loss 5.259\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2588\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.714, Val loss 8.691\n",
      "Ep 1 (Step 000010): Train loss 7.341, Val loss 7.302\n",
      "Ep 1 (Step 000020): Train loss 6.713, Val loss 6.679\n",
      "Ep 1 (Step 000030): Train loss 6.443, Val loss 6.421\n",
      "Ep 1 (Step 000040): Train loss 6.324, Val loss 6.326\n",
      "Ep 1 (Step 000050): Train loss 6.196, Val loss 6.229\n",
      "Ep 1 (Step 000060): Train loss 6.080, Val loss 6.095\n",
      "Ep 1 (Step 000070): Train loss 6.005, Val loss 6.008\n",
      "Ep 1 (Step 000080): Train loss 5.927, Val loss 5.944\n",
      "Ep 1 (Step 000090): Train loss 5.840, Val loss 5.883\n",
      "Ep 1 (Step 000100): Train loss 5.776, Val loss 5.823\n",
      "Ep 1 (Step 000110): Train loss 5.698, Val loss 5.767\n",
      "Ep 1 (Step 000120): Train loss 5.614, Val loss 5.740\n",
      "Ep 1 (Step 000130): Train loss 5.640, Val loss 5.702\n",
      "Ep 1 (Step 000140): Train loss 5.604, Val loss 5.658\n",
      "Ep 1 (Step 000150): Train loss 5.529, Val loss 5.617\n",
      "Ep 1 (Step 000160): Train loss 5.486, Val loss 5.598\n",
      "Ep 1 (Step 000170): Train loss 5.459, Val loss 5.576\n",
      "Ep 1 (Step 000180): Train loss 5.419, Val loss 5.541\n",
      "Ep 1 (Step 000190): Train loss 5.466, Val loss 5.506\n",
      "Ep 1 (Step 000200): Train loss 5.347, Val loss 5.479\n",
      "Ep 1 (Step 000210): Train loss 5.356, Val loss 5.458\n",
      "Ep 1 (Step 000220): Train loss 5.409, Val loss 5.441\n",
      "Ep 1 (Step 000230): Train loss 5.350, Val loss 5.423\n",
      "Ep 1 (Step 000240): Train loss 5.328, Val loss 5.409\n",
      "Ep 1 (Step 000250): Train loss 5.269, Val loss 5.420\n",
      "Ep 1 (Step 000260): Train loss 5.194, Val loss 5.382\n",
      "Ep 1 (Step 000270): Train loss 5.175, Val loss 5.372\n",
      "Ep 1 (Step 000280): Train loss 5.310, Val loss 5.359\n",
      "Ep 1 (Step 000290): Train loss 5.255, Val loss 5.344\n",
      "Ep 1 (Step 000300): Train loss 5.191, Val loss 5.331\n",
      "Ep 1 (Step 000310): Train loss 5.237, Val loss 5.321\n",
      "Ep 1 (Step 000320): Train loss 5.229, Val loss 5.302\n",
      "Ep 1 (Step 000330): Train loss 5.125, Val loss 5.306\n",
      "Ep 1 (Step 000340): Train loss 5.065, Val loss 5.280\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2802\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.575, Val loss 8.525\n",
      "Ep 1 (Step 000010): Train loss 6.842, Val loss 6.832\n",
      "Ep 1 (Step 000020): Train loss 6.453, Val loss 6.434\n",
      "Ep 1 (Step 000030): Train loss 6.348, Val loss 6.355\n",
      "Ep 1 (Step 000040): Train loss 6.229, Val loss 6.156\n",
      "Ep 1 (Step 000050): Train loss 6.038, Val loss 6.057\n",
      "Ep 1 (Step 000060): Train loss 5.886, Val loss 5.924\n",
      "Ep 1 (Step 000070): Train loss 5.762, Val loss 5.853\n",
      "Ep 1 (Step 000080): Train loss 5.688, Val loss 5.793\n",
      "Ep 1 (Step 000090): Train loss 5.696, Val loss 5.726\n",
      "Ep 1 (Step 000100): Train loss 5.555, Val loss 5.683\n",
      "Ep 1 (Step 000110): Train loss 5.565, Val loss 5.635\n",
      "Ep 1 (Step 000120): Train loss 5.632, Val loss 5.605\n",
      "Ep 1 (Step 000130): Train loss 5.574, Val loss 5.572\n",
      "Ep 1 (Step 000140): Train loss 5.454, Val loss 5.548\n",
      "Ep 1 (Step 000150): Train loss 5.383, Val loss 5.516\n",
      "Ep 1 (Step 000160): Train loss 5.507, Val loss 5.473\n",
      "Ep 1 (Step 000170): Train loss 5.340, Val loss 5.467\n",
      "Ep 1 (Step 000180): Train loss 5.357, Val loss 5.454\n",
      "Ep 1 (Step 000190): Train loss 5.375, Val loss 5.443\n",
      "Ep 1 (Step 000200): Train loss 5.363, Val loss 5.402\n",
      "Ep 1 (Step 000210): Train loss 5.287, Val loss 5.396\n",
      "Ep 1 (Step 000220): Train loss 5.269, Val loss 5.369\n",
      "Ep 1 (Step 000230): Train loss 5.226, Val loss 5.369\n",
      "Ep 1 (Step 000240): Train loss 5.261, Val loss 5.365\n",
      "Ep 1 (Step 000250): Train loss 5.141, Val loss 5.369\n",
      "Ep 1 (Step 000260): Train loss 5.218, Val loss 5.340\n",
      "Ep 1 (Step 000270): Train loss 5.207, Val loss 5.328\n",
      "Ep 1 (Step 000280): Train loss 5.161, Val loss 5.312\n",
      "Ep 1 (Step 000290): Train loss 5.214, Val loss 5.298\n",
      "Ep 1 (Step 000300): Train loss 5.182, Val loss 5.259\n",
      "Ep 1 (Step 000310): Train loss 5.088, Val loss 5.254\n",
      "Ep 1 (Step 000320): Train loss 5.145, Val loss 5.265\n",
      "Ep 1 (Step 000330): Train loss 5.199, Val loss 5.246\n",
      "Ep 1 (Step 000340): Train loss 5.142, Val loss 5.232\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2317\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.610, Val loss 8.558\n",
      "Ep 1 (Step 000010): Train loss 6.907, Val loss 6.858\n",
      "Ep 1 (Step 000020): Train loss 6.465, Val loss 6.430\n",
      "Ep 1 (Step 000030): Train loss 6.332, Val loss 6.346\n",
      "Ep 1 (Step 000040): Train loss 6.228, Val loss 6.201\n",
      "Ep 1 (Step 000050): Train loss 6.125, Val loss 6.087\n",
      "Ep 1 (Step 000060): Train loss 5.987, Val loss 6.010\n",
      "Ep 1 (Step 000070): Train loss 5.892, Val loss 5.900\n",
      "Ep 1 (Step 000080): Train loss 5.698, Val loss 5.807\n",
      "Ep 1 (Step 000090): Train loss 5.760, Val loss 5.767\n",
      "Ep 1 (Step 000100): Train loss 5.645, Val loss 5.717\n",
      "Ep 1 (Step 000110): Train loss 5.531, Val loss 5.665\n",
      "Ep 1 (Step 000120): Train loss 5.610, Val loss 5.634\n",
      "Ep 1 (Step 000130): Train loss 5.508, Val loss 5.572\n",
      "Ep 1 (Step 000140): Train loss 5.514, Val loss 5.547\n",
      "Ep 1 (Step 000150): Train loss 5.485, Val loss 5.528\n",
      "Ep 1 (Step 000160): Train loss 5.473, Val loss 5.481\n",
      "Ep 1 (Step 000170): Train loss 5.417, Val loss 5.461\n",
      "Ep 1 (Step 000180): Train loss 5.438, Val loss 5.447\n",
      "Ep 1 (Step 000190): Train loss 5.352, Val loss 5.438\n",
      "Ep 1 (Step 000200): Train loss 5.384, Val loss 5.415\n",
      "Ep 1 (Step 000210): Train loss 5.344, Val loss 5.389\n",
      "Ep 1 (Step 000220): Train loss 5.295, Val loss 5.393\n",
      "Ep 1 (Step 000230): Train loss 5.281, Val loss 5.373\n",
      "Ep 1 (Step 000240): Train loss 5.307, Val loss 5.352\n",
      "Ep 1 (Step 000250): Train loss 5.202, Val loss 5.349\n",
      "Ep 1 (Step 000260): Train loss 5.186, Val loss 5.349\n",
      "Ep 1 (Step 000270): Train loss 5.217, Val loss 5.353\n",
      "Ep 1 (Step 000280): Train loss 5.203, Val loss 5.324\n",
      "Ep 1 (Step 000290): Train loss 5.310, Val loss 5.317\n",
      "Ep 1 (Step 000300): Train loss 5.118, Val loss 5.299\n",
      "Ep 1 (Step 000310): Train loss 5.166, Val loss 5.300\n",
      "Ep 1 (Step 000320): Train loss 5.170, Val loss 5.268\n",
      "Ep 1 (Step 000330): Train loss 5.088, Val loss 5.271\n",
      "Ep 1 (Step 000340): Train loss 5.153, Val loss 5.258\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2583\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.519, Val loss 8.532\n",
      "Ep 1 (Step 000010): Train loss 6.762, Val loss 6.808\n",
      "Ep 1 (Step 000020): Train loss 6.463, Val loss 6.432\n",
      "Ep 1 (Step 000030): Train loss 6.385, Val loss 6.332\n",
      "Ep 1 (Step 000040): Train loss 6.181, Val loss 6.194\n",
      "Ep 1 (Step 000050): Train loss 6.102, Val loss 6.104\n",
      "Ep 1 (Step 000060): Train loss 5.897, Val loss 5.947\n",
      "Ep 1 (Step 000070): Train loss 5.782, Val loss 5.872\n",
      "Ep 1 (Step 000080): Train loss 5.781, Val loss 5.782\n",
      "Ep 1 (Step 000090): Train loss 5.641, Val loss 5.756\n",
      "Ep 1 (Step 000100): Train loss 5.638, Val loss 5.701\n",
      "Ep 1 (Step 000110): Train loss 5.597, Val loss 5.622\n",
      "Ep 1 (Step 000120): Train loss 5.656, Val loss 5.606\n",
      "Ep 1 (Step 000130): Train loss 5.519, Val loss 5.574\n",
      "Ep 1 (Step 000140): Train loss 5.509, Val loss 5.542\n",
      "Ep 1 (Step 000150): Train loss 5.425, Val loss 5.526\n",
      "Ep 1 (Step 000160): Train loss 5.381, Val loss 5.513\n",
      "Ep 1 (Step 000170): Train loss 5.337, Val loss 5.478\n",
      "Ep 1 (Step 000180): Train loss 5.392, Val loss 5.455\n",
      "Ep 1 (Step 000190): Train loss 5.376, Val loss 5.425\n",
      "Ep 1 (Step 000200): Train loss 5.334, Val loss 5.420\n",
      "Ep 1 (Step 000210): Train loss 5.323, Val loss 5.401\n",
      "Ep 1 (Step 000220): Train loss 5.286, Val loss 5.378\n",
      "Ep 1 (Step 000230): Train loss 5.335, Val loss 5.356\n",
      "Ep 1 (Step 000240): Train loss 5.242, Val loss 5.373\n",
      "Ep 1 (Step 000250): Train loss 5.190, Val loss 5.345\n",
      "Ep 1 (Step 000260): Train loss 5.259, Val loss 5.324\n",
      "Ep 1 (Step 000270): Train loss 5.276, Val loss 5.323\n",
      "Ep 1 (Step 000280): Train loss 5.272, Val loss 5.299\n",
      "Ep 1 (Step 000290): Train loss 5.218, Val loss 5.273\n",
      "Ep 1 (Step 000300): Train loss 5.202, Val loss 5.271\n",
      "Ep 1 (Step 000310): Train loss 5.199, Val loss 5.277\n",
      "Ep 1 (Step 000320): Train loss 5.170, Val loss 5.259\n",
      "Ep 1 (Step 000330): Train loss 5.070, Val loss 5.256\n",
      "Ep 1 (Step 000340): Train loss 5.116, Val loss 5.263\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2630\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.562, Val loss 8.526\n",
      "Ep 1 (Step 000010): Train loss 6.837, Val loss 6.814\n",
      "Ep 1 (Step 000020): Train loss 6.428, Val loss 6.430\n",
      "Ep 1 (Step 000030): Train loss 6.389, Val loss 6.347\n",
      "Ep 1 (Step 000040): Train loss 6.151, Val loss 6.226\n",
      "Ep 1 (Step 000050): Train loss 6.093, Val loss 6.075\n",
      "Ep 1 (Step 000060): Train loss 5.968, Val loss 5.975\n",
      "Ep 1 (Step 000070): Train loss 5.785, Val loss 5.880\n",
      "Ep 1 (Step 000080): Train loss 5.756, Val loss 5.792\n",
      "Ep 1 (Step 000090): Train loss 5.793, Val loss 5.761\n",
      "Ep 1 (Step 000100): Train loss 5.685, Val loss 5.694\n",
      "Ep 1 (Step 000110): Train loss 5.615, Val loss 5.642\n",
      "Ep 1 (Step 000120): Train loss 5.515, Val loss 5.608\n",
      "Ep 1 (Step 000130): Train loss 5.549, Val loss 5.557\n",
      "Ep 1 (Step 000140): Train loss 5.459, Val loss 5.544\n",
      "Ep 1 (Step 000150): Train loss 5.397, Val loss 5.510\n",
      "Ep 1 (Step 000160): Train loss 5.435, Val loss 5.504\n",
      "Ep 1 (Step 000170): Train loss 5.334, Val loss 5.470\n",
      "Ep 1 (Step 000180): Train loss 5.377, Val loss 5.417\n",
      "Ep 1 (Step 000190): Train loss 5.331, Val loss 5.423\n",
      "Ep 1 (Step 000200): Train loss 5.327, Val loss 5.392\n",
      "Ep 1 (Step 000210): Train loss 5.275, Val loss 5.384\n",
      "Ep 1 (Step 000220): Train loss 5.236, Val loss 5.365\n",
      "Ep 1 (Step 000230): Train loss 5.206, Val loss 5.360\n",
      "Ep 1 (Step 000240): Train loss 5.315, Val loss 5.338\n",
      "Ep 1 (Step 000250): Train loss 5.276, Val loss 5.308\n",
      "Ep 1 (Step 000260): Train loss 5.183, Val loss 5.319\n",
      "Ep 1 (Step 000270): Train loss 5.170, Val loss 5.297\n",
      "Ep 1 (Step 000280): Train loss 5.139, Val loss 5.290\n",
      "Ep 1 (Step 000290): Train loss 5.157, Val loss 5.269\n",
      "Ep 1 (Step 000300): Train loss 5.163, Val loss 5.247\n",
      "Ep 1 (Step 000310): Train loss 5.150, Val loss 5.250\n",
      "Ep 1 (Step 000320): Train loss 5.161, Val loss 5.245\n",
      "Ep 1 (Step 000330): Train loss 5.263, Val loss 5.234\n",
      "Ep 1 (Step 000340): Train loss 5.088, Val loss 5.218\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2185\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.553, Val loss 8.541\n",
      "Ep 1 (Step 000010): Train loss 6.863, Val loss 6.859\n",
      "Ep 1 (Step 000020): Train loss 6.433, Val loss 6.453\n",
      "Ep 1 (Step 000030): Train loss 6.313, Val loss 6.376\n",
      "Ep 1 (Step 000040): Train loss 6.238, Val loss 6.211\n",
      "Ep 1 (Step 000050): Train loss 6.016, Val loss 6.070\n",
      "Ep 1 (Step 000060): Train loss 6.034, Val loss 5.971\n",
      "Ep 1 (Step 000070): Train loss 5.800, Val loss 5.887\n",
      "Ep 1 (Step 000080): Train loss 5.736, Val loss 5.813\n",
      "Ep 1 (Step 000090): Train loss 5.678, Val loss 5.761\n",
      "Ep 1 (Step 000100): Train loss 5.636, Val loss 5.703\n",
      "Ep 1 (Step 000110): Train loss 5.563, Val loss 5.641\n",
      "Ep 1 (Step 000120): Train loss 5.607, Val loss 5.603\n",
      "Ep 1 (Step 000130): Train loss 5.504, Val loss 5.580\n",
      "Ep 1 (Step 000140): Train loss 5.466, Val loss 5.542\n",
      "Ep 1 (Step 000150): Train loss 5.393, Val loss 5.509\n",
      "Ep 1 (Step 000160): Train loss 5.442, Val loss 5.493\n",
      "Ep 1 (Step 000170): Train loss 5.372, Val loss 5.491\n",
      "Ep 1 (Step 000180): Train loss 5.327, Val loss 5.440\n",
      "Ep 1 (Step 000190): Train loss 5.286, Val loss 5.407\n",
      "Ep 1 (Step 000200): Train loss 5.243, Val loss 5.391\n",
      "Ep 1 (Step 000210): Train loss 5.275, Val loss 5.373\n",
      "Ep 1 (Step 000220): Train loss 5.143, Val loss 5.364\n",
      "Ep 1 (Step 000230): Train loss 5.252, Val loss 5.341\n",
      "Ep 1 (Step 000240): Train loss 5.216, Val loss 5.314\n",
      "Ep 1 (Step 000250): Train loss 5.184, Val loss 5.303\n",
      "Ep 1 (Step 000260): Train loss 5.222, Val loss 5.298\n",
      "Ep 1 (Step 000270): Train loss 5.088, Val loss 5.296\n",
      "Ep 1 (Step 000280): Train loss 5.127, Val loss 5.277\n",
      "Ep 1 (Step 000290): Train loss 5.038, Val loss 5.257\n",
      "Ep 1 (Step 000300): Train loss 5.114, Val loss 5.260\n",
      "Ep 1 (Step 000310): Train loss 5.132, Val loss 5.242\n",
      "Ep 1 (Step 000320): Train loss 5.113, Val loss 5.240\n",
      "Ep 1 (Step 000330): Train loss 5.128, Val loss 5.223\n",
      "Ep 1 (Step 000340): Train loss 5.062, Val loss 5.216\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2163\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.535, Val loss 8.506\n",
      "Ep 1 (Step 000010): Train loss 6.905, Val loss 6.844\n",
      "Ep 1 (Step 000020): Train loss 6.491, Val loss 6.440\n",
      "Ep 1 (Step 000030): Train loss 6.443, Val loss 6.403\n",
      "Ep 1 (Step 000040): Train loss 6.192, Val loss 6.229\n",
      "Ep 1 (Step 000050): Train loss 6.156, Val loss 6.087\n",
      "Ep 1 (Step 000060): Train loss 5.952, Val loss 5.975\n",
      "Ep 1 (Step 000070): Train loss 5.837, Val loss 5.902\n",
      "Ep 1 (Step 000080): Train loss 5.743, Val loss 5.813\n",
      "Ep 1 (Step 000090): Train loss 5.642, Val loss 5.752\n",
      "Ep 1 (Step 000100): Train loss 5.649, Val loss 5.701\n",
      "Ep 1 (Step 000110): Train loss 5.601, Val loss 5.663\n",
      "Ep 1 (Step 000120): Train loss 5.551, Val loss 5.589\n",
      "Ep 1 (Step 000130): Train loss 5.505, Val loss 5.554\n",
      "Ep 1 (Step 000140): Train loss 5.502, Val loss 5.556\n",
      "Ep 1 (Step 000150): Train loss 5.411, Val loss 5.499\n",
      "Ep 1 (Step 000160): Train loss 5.443, Val loss 5.472\n",
      "Ep 1 (Step 000170): Train loss 5.455, Val loss 5.457\n",
      "Ep 1 (Step 000180): Train loss 5.344, Val loss 5.468\n",
      "Ep 1 (Step 000190): Train loss 5.371, Val loss 5.434\n",
      "Ep 1 (Step 000200): Train loss 5.326, Val loss 5.413\n",
      "Ep 1 (Step 000210): Train loss 5.258, Val loss 5.401\n",
      "Ep 1 (Step 000220): Train loss 5.321, Val loss 5.380\n",
      "Ep 1 (Step 000230): Train loss 5.291, Val loss 5.361\n",
      "Ep 1 (Step 000240): Train loss 5.242, Val loss 5.341\n",
      "Ep 1 (Step 000250): Train loss 5.283, Val loss 5.320\n",
      "Ep 1 (Step 000260): Train loss 5.209, Val loss 5.290\n",
      "Ep 1 (Step 000270): Train loss 5.206, Val loss 5.291\n",
      "Ep 1 (Step 000280): Train loss 5.156, Val loss 5.260\n",
      "Ep 1 (Step 000290): Train loss 5.104, Val loss 5.255\n",
      "Ep 1 (Step 000300): Train loss 5.201, Val loss 5.254\n",
      "Ep 1 (Step 000310): Train loss 5.062, Val loss 5.263\n",
      "Ep 1 (Step 000320): Train loss 5.158, Val loss 5.244\n",
      "Ep 1 (Step 000330): Train loss 5.043, Val loss 5.230\n",
      "Ep 1 (Step 000340): Train loss 5.060, Val loss 5.215\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2152\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.731, Val loss 8.739\n",
      "Ep 1 (Step 000010): Train loss 7.390, Val loss 7.351\n",
      "Ep 1 (Step 000020): Train loss 6.778, Val loss 6.722\n",
      "Ep 1 (Step 000030): Train loss 6.441, Val loss 6.419\n",
      "Ep 1 (Step 000040): Train loss 6.238, Val loss 6.320\n",
      "Ep 1 (Step 000050): Train loss 6.244, Val loss 6.219\n",
      "Ep 1 (Step 000060): Train loss 6.141, Val loss 6.099\n",
      "Ep 1 (Step 000070): Train loss 6.041, Val loss 6.010\n",
      "Ep 1 (Step 000080): Train loss 5.980, Val loss 5.952\n",
      "Ep 1 (Step 000090): Train loss 5.810, Val loss 5.862\n",
      "Ep 1 (Step 000100): Train loss 5.763, Val loss 5.786\n",
      "Ep 1 (Step 000110): Train loss 5.748, Val loss 5.752\n",
      "Ep 1 (Step 000120): Train loss 5.704, Val loss 5.715\n",
      "Ep 1 (Step 000130): Train loss 5.589, Val loss 5.683\n",
      "Ep 1 (Step 000140): Train loss 5.627, Val loss 5.642\n",
      "Ep 1 (Step 000150): Train loss 5.636, Val loss 5.598\n",
      "Ep 1 (Step 000160): Train loss 5.505, Val loss 5.567\n",
      "Ep 1 (Step 000170): Train loss 5.540, Val loss 5.540\n",
      "Ep 1 (Step 000180): Train loss 5.506, Val loss 5.511\n",
      "Ep 1 (Step 000190): Train loss 5.466, Val loss 5.495\n",
      "Ep 1 (Step 000200): Train loss 5.392, Val loss 5.481\n",
      "Ep 1 (Step 000210): Train loss 5.412, Val loss 5.460\n",
      "Ep 1 (Step 000220): Train loss 5.353, Val loss 5.447\n",
      "Ep 1 (Step 000230): Train loss 5.345, Val loss 5.445\n",
      "Ep 1 (Step 000240): Train loss 5.290, Val loss 5.425\n",
      "Ep 1 (Step 000250): Train loss 5.273, Val loss 5.405\n",
      "Ep 1 (Step 000260): Train loss 5.262, Val loss 5.422\n",
      "Ep 1 (Step 000270): Train loss 5.254, Val loss 5.388\n",
      "Ep 1 (Step 000280): Train loss 5.202, Val loss 5.382\n",
      "Ep 1 (Step 000290): Train loss 5.333, Val loss 5.362\n",
      "Ep 1 (Step 000300): Train loss 5.274, Val loss 5.364\n",
      "Ep 1 (Step 000310): Train loss 5.216, Val loss 5.347\n",
      "Ep 1 (Step 000320): Train loss 5.276, Val loss 5.317\n",
      "Ep 1 (Step 000330): Train loss 5.193, Val loss 5.311\n",
      "Ep 1 (Step 000340): Train loss 5.143, Val loss 5.303\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.3025\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.822, Val loss 8.768\n",
      "Ep 1 (Step 000010): Train loss 7.509, Val loss 7.436\n",
      "Ep 1 (Step 000020): Train loss 6.803, Val loss 6.785\n",
      "Ep 1 (Step 000030): Train loss 6.498, Val loss 6.454\n",
      "Ep 1 (Step 000040): Train loss 6.346, Val loss 6.333\n",
      "Ep 1 (Step 000050): Train loss 6.214, Val loss 6.224\n",
      "Ep 1 (Step 000060): Train loss 6.158, Val loss 6.108\n",
      "Ep 1 (Step 000070): Train loss 6.000, Val loss 5.998\n",
      "Ep 1 (Step 000080): Train loss 5.981, Val loss 5.941\n",
      "Ep 1 (Step 000090): Train loss 5.833, Val loss 5.880\n",
      "Ep 1 (Step 000100): Train loss 5.758, Val loss 5.823\n",
      "Ep 1 (Step 000110): Train loss 5.765, Val loss 5.788\n",
      "Ep 1 (Step 000120): Train loss 5.660, Val loss 5.731\n",
      "Ep 1 (Step 000130): Train loss 5.669, Val loss 5.701\n",
      "Ep 1 (Step 000140): Train loss 5.538, Val loss 5.667\n",
      "Ep 1 (Step 000150): Train loss 5.609, Val loss 5.636\n",
      "Ep 1 (Step 000160): Train loss 5.563, Val loss 5.585\n",
      "Ep 1 (Step 000170): Train loss 5.524, Val loss 5.560\n",
      "Ep 1 (Step 000180): Train loss 5.431, Val loss 5.552\n",
      "Ep 1 (Step 000190): Train loss 5.434, Val loss 5.519\n",
      "Ep 1 (Step 000200): Train loss 5.442, Val loss 5.498\n",
      "Ep 1 (Step 000210): Train loss 5.337, Val loss 5.483\n",
      "Ep 1 (Step 000220): Train loss 5.302, Val loss 5.460\n",
      "Ep 1 (Step 000230): Train loss 5.344, Val loss 5.442\n",
      "Ep 1 (Step 000240): Train loss 5.301, Val loss 5.425\n",
      "Ep 1 (Step 000250): Train loss 5.234, Val loss 5.418\n",
      "Ep 1 (Step 000260): Train loss 5.265, Val loss 5.398\n",
      "Ep 1 (Step 000270): Train loss 5.276, Val loss 5.399\n",
      "Ep 1 (Step 000280): Train loss 5.181, Val loss 5.385\n",
      "Ep 1 (Step 000290): Train loss 5.221, Val loss 5.367\n",
      "Ep 1 (Step 000300): Train loss 5.230, Val loss 5.362\n",
      "Ep 1 (Step 000310): Train loss 5.194, Val loss 5.349\n",
      "Ep 1 (Step 000320): Train loss 5.164, Val loss 5.338\n",
      "Ep 1 (Step 000330): Train loss 5.194, Val loss 5.329\n",
      "Ep 1 (Step 000340): Train loss 5.201, Val loss 5.314\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3136\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.715, Val loss 8.689\n",
      "Ep 1 (Step 000010): Train loss 7.343, Val loss 7.343\n",
      "Ep 1 (Step 000020): Train loss 6.751, Val loss 6.723\n",
      "Ep 1 (Step 000030): Train loss 6.483, Val loss 6.443\n",
      "Ep 1 (Step 000040): Train loss 6.451, Val loss 6.347\n",
      "Ep 1 (Step 000050): Train loss 6.258, Val loss 6.240\n",
      "Ep 1 (Step 000060): Train loss 6.014, Val loss 6.122\n",
      "Ep 1 (Step 000070): Train loss 5.979, Val loss 6.012\n",
      "Ep 1 (Step 000080): Train loss 5.889, Val loss 5.943\n",
      "Ep 1 (Step 000090): Train loss 5.863, Val loss 5.884\n",
      "Ep 1 (Step 000100): Train loss 5.798, Val loss 5.816\n",
      "Ep 1 (Step 000110): Train loss 5.743, Val loss 5.775\n",
      "Ep 1 (Step 000120): Train loss 5.686, Val loss 5.750\n",
      "Ep 1 (Step 000130): Train loss 5.657, Val loss 5.698\n",
      "Ep 1 (Step 000140): Train loss 5.673, Val loss 5.641\n",
      "Ep 1 (Step 000150): Train loss 5.556, Val loss 5.607\n",
      "Ep 1 (Step 000160): Train loss 5.533, Val loss 5.579\n",
      "Ep 1 (Step 000170): Train loss 5.465, Val loss 5.567\n",
      "Ep 1 (Step 000180): Train loss 5.482, Val loss 5.535\n",
      "Ep 1 (Step 000190): Train loss 5.430, Val loss 5.530\n",
      "Ep 1 (Step 000200): Train loss 5.389, Val loss 5.527\n",
      "Ep 1 (Step 000210): Train loss 5.355, Val loss 5.473\n",
      "Ep 1 (Step 000220): Train loss 5.356, Val loss 5.454\n",
      "Ep 1 (Step 000230): Train loss 5.365, Val loss 5.450\n",
      "Ep 1 (Step 000240): Train loss 5.338, Val loss 5.423\n",
      "Ep 1 (Step 000250): Train loss 5.372, Val loss 5.403\n",
      "Ep 1 (Step 000260): Train loss 5.207, Val loss 5.405\n",
      "Ep 1 (Step 000270): Train loss 5.289, Val loss 5.404\n",
      "Ep 1 (Step 000280): Train loss 5.220, Val loss 5.400\n",
      "Ep 1 (Step 000290): Train loss 5.179, Val loss 5.372\n",
      "Ep 1 (Step 000300): Train loss 5.249, Val loss 5.346\n",
      "Ep 1 (Step 000310): Train loss 5.197, Val loss 5.339\n",
      "Ep 1 (Step 000320): Train loss 5.215, Val loss 5.326\n",
      "Ep 1 (Step 000330): Train loss 5.203, Val loss 5.320\n",
      "Ep 1 (Step 000340): Train loss 5.107, Val loss 5.294\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2937\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.774, Val loss 8.760\n",
      "Ep 1 (Step 000010): Train loss 7.376, Val loss 7.346\n",
      "Ep 1 (Step 000020): Train loss 6.699, Val loss 6.699\n",
      "Ep 1 (Step 000030): Train loss 6.456, Val loss 6.393\n",
      "Ep 1 (Step 000040): Train loss 6.369, Val loss 6.283\n",
      "Ep 1 (Step 000050): Train loss 6.127, Val loss 6.163\n",
      "Ep 1 (Step 000060): Train loss 5.997, Val loss 6.049\n",
      "Ep 1 (Step 000070): Train loss 6.005, Val loss 5.955\n",
      "Ep 1 (Step 000080): Train loss 5.835, Val loss 5.881\n",
      "Ep 1 (Step 000090): Train loss 5.847, Val loss 5.841\n",
      "Ep 1 (Step 000100): Train loss 5.786, Val loss 5.805\n",
      "Ep 1 (Step 000110): Train loss 5.647, Val loss 5.755\n",
      "Ep 1 (Step 000120): Train loss 5.718, Val loss 5.692\n",
      "Ep 1 (Step 000130): Train loss 5.585, Val loss 5.658\n",
      "Ep 1 (Step 000140): Train loss 5.510, Val loss 5.627\n",
      "Ep 1 (Step 000150): Train loss 5.496, Val loss 5.597\n",
      "Ep 1 (Step 000160): Train loss 5.555, Val loss 5.566\n",
      "Ep 1 (Step 000170): Train loss 5.484, Val loss 5.531\n",
      "Ep 1 (Step 000180): Train loss 5.486, Val loss 5.520\n",
      "Ep 1 (Step 000190): Train loss 5.329, Val loss 5.481\n",
      "Ep 1 (Step 000200): Train loss 5.385, Val loss 5.469\n",
      "Ep 1 (Step 000210): Train loss 5.392, Val loss 5.448\n",
      "Ep 1 (Step 000220): Train loss 5.358, Val loss 5.425\n",
      "Ep 1 (Step 000230): Train loss 5.307, Val loss 5.402\n",
      "Ep 1 (Step 000240): Train loss 5.279, Val loss 5.394\n",
      "Ep 1 (Step 000250): Train loss 5.347, Val loss 5.384\n",
      "Ep 1 (Step 000260): Train loss 5.299, Val loss 5.371\n",
      "Ep 1 (Step 000270): Train loss 5.262, Val loss 5.341\n",
      "Ep 1 (Step 000280): Train loss 5.270, Val loss 5.329\n",
      "Ep 1 (Step 000290): Train loss 5.167, Val loss 5.320\n",
      "Ep 1 (Step 000300): Train loss 5.190, Val loss 5.306\n",
      "Ep 1 (Step 000310): Train loss 5.201, Val loss 5.292\n",
      "Ep 1 (Step 000320): Train loss 5.151, Val loss 5.292\n",
      "Ep 1 (Step 000330): Train loss 5.111, Val loss 5.269\n",
      "Ep 1 (Step 000340): Train loss 5.141, Val loss 5.265\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2653\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.750, Val loss 8.714\n",
      "Ep 1 (Step 000010): Train loss 7.422, Val loss 7.332\n",
      "Ep 1 (Step 000020): Train loss 6.699, Val loss 6.691\n",
      "Ep 1 (Step 000030): Train loss 6.462, Val loss 6.417\n",
      "Ep 1 (Step 000040): Train loss 6.343, Val loss 6.330\n",
      "Ep 1 (Step 000050): Train loss 6.232, Val loss 6.212\n",
      "Ep 1 (Step 000060): Train loss 6.119, Val loss 6.121\n",
      "Ep 1 (Step 000070): Train loss 6.005, Val loss 6.022\n",
      "Ep 1 (Step 000080): Train loss 5.928, Val loss 5.919\n",
      "Ep 1 (Step 000090): Train loss 5.834, Val loss 5.853\n",
      "Ep 1 (Step 000100): Train loss 5.750, Val loss 5.799\n",
      "Ep 1 (Step 000110): Train loss 5.772, Val loss 5.746\n",
      "Ep 1 (Step 000120): Train loss 5.542, Val loss 5.704\n",
      "Ep 1 (Step 000130): Train loss 5.624, Val loss 5.654\n",
      "Ep 1 (Step 000140): Train loss 5.514, Val loss 5.639\n",
      "Ep 1 (Step 000150): Train loss 5.547, Val loss 5.602\n",
      "Ep 1 (Step 000160): Train loss 5.566, Val loss 5.576\n",
      "Ep 1 (Step 000170): Train loss 5.532, Val loss 5.544\n",
      "Ep 1 (Step 000180): Train loss 5.461, Val loss 5.531\n",
      "Ep 1 (Step 000190): Train loss 5.332, Val loss 5.484\n",
      "Ep 1 (Step 000200): Train loss 5.454, Val loss 5.474\n",
      "Ep 1 (Step 000210): Train loss 5.395, Val loss 5.448\n",
      "Ep 1 (Step 000220): Train loss 5.368, Val loss 5.446\n",
      "Ep 1 (Step 000230): Train loss 5.273, Val loss 5.422\n",
      "Ep 1 (Step 000240): Train loss 5.262, Val loss 5.408\n",
      "Ep 1 (Step 000250): Train loss 5.261, Val loss 5.377\n",
      "Ep 1 (Step 000260): Train loss 5.183, Val loss 5.360\n",
      "Ep 1 (Step 000270): Train loss 5.212, Val loss 5.358\n",
      "Ep 1 (Step 000280): Train loss 5.270, Val loss 5.342\n",
      "Ep 1 (Step 000290): Train loss 5.248, Val loss 5.342\n",
      "Ep 1 (Step 000300): Train loss 5.170, Val loss 5.326\n",
      "Ep 1 (Step 000310): Train loss 5.161, Val loss 5.320\n",
      "Ep 1 (Step 000320): Train loss 5.109, Val loss 5.304\n",
      "Ep 1 (Step 000330): Train loss 5.153, Val loss 5.289\n",
      "Ep 1 (Step 000340): Train loss 5.127, Val loss 5.271\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2713\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.769, Val loss 8.742\n",
      "Ep 1 (Step 000010): Train loss 7.428, Val loss 7.342\n",
      "Ep 1 (Step 000020): Train loss 6.749, Val loss 6.701\n",
      "Ep 1 (Step 000030): Train loss 6.481, Val loss 6.403\n",
      "Ep 1 (Step 000040): Train loss 6.335, Val loss 6.320\n",
      "Ep 1 (Step 000050): Train loss 6.202, Val loss 6.204\n",
      "Ep 1 (Step 000060): Train loss 6.017, Val loss 6.102\n",
      "Ep 1 (Step 000070): Train loss 5.969, Val loss 6.016\n",
      "Ep 1 (Step 000080): Train loss 5.895, Val loss 5.940\n",
      "Ep 1 (Step 000090): Train loss 5.842, Val loss 5.852\n",
      "Ep 1 (Step 000100): Train loss 5.768, Val loss 5.795\n",
      "Ep 1 (Step 000110): Train loss 5.775, Val loss 5.748\n",
      "Ep 1 (Step 000120): Train loss 5.737, Val loss 5.719\n",
      "Ep 1 (Step 000130): Train loss 5.643, Val loss 5.679\n",
      "Ep 1 (Step 000140): Train loss 5.570, Val loss 5.645\n",
      "Ep 1 (Step 000150): Train loss 5.566, Val loss 5.615\n",
      "Ep 1 (Step 000160): Train loss 5.451, Val loss 5.577\n",
      "Ep 1 (Step 000170): Train loss 5.448, Val loss 5.550\n",
      "Ep 1 (Step 000180): Train loss 5.381, Val loss 5.530\n",
      "Ep 1 (Step 000190): Train loss 5.379, Val loss 5.510\n",
      "Ep 1 (Step 000200): Train loss 5.346, Val loss 5.479\n",
      "Ep 1 (Step 000210): Train loss 5.423, Val loss 5.461\n",
      "Ep 1 (Step 000220): Train loss 5.354, Val loss 5.432\n",
      "Ep 1 (Step 000230): Train loss 5.378, Val loss 5.432\n",
      "Ep 1 (Step 000240): Train loss 5.282, Val loss 5.416\n",
      "Ep 1 (Step 000250): Train loss 5.364, Val loss 5.393\n",
      "Ep 1 (Step 000260): Train loss 5.217, Val loss 5.374\n",
      "Ep 1 (Step 000270): Train loss 5.187, Val loss 5.368\n",
      "Ep 1 (Step 000280): Train loss 5.262, Val loss 5.350\n",
      "Ep 1 (Step 000290): Train loss 5.228, Val loss 5.346\n",
      "Ep 1 (Step 000300): Train loss 5.258, Val loss 5.337\n",
      "Ep 1 (Step 000310): Train loss 5.213, Val loss 5.324\n",
      "Ep 1 (Step 000320): Train loss 5.205, Val loss 5.303\n",
      "Ep 1 (Step 000330): Train loss 5.181, Val loss 5.285\n",
      "Ep 1 (Step 000340): Train loss 5.217, Val loss 5.298\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2983\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.521, Val loss 8.501\n",
      "Ep 1 (Step 000010): Train loss 6.825, Val loss 6.806\n",
      "Ep 1 (Step 000020): Train loss 6.433, Val loss 6.429\n",
      "Ep 1 (Step 000030): Train loss 6.317, Val loss 6.335\n",
      "Ep 1 (Step 000040): Train loss 6.236, Val loss 6.197\n",
      "Ep 1 (Step 000050): Train loss 6.029, Val loss 6.068\n",
      "Ep 1 (Step 000060): Train loss 5.940, Val loss 5.974\n",
      "Ep 1 (Step 000070): Train loss 5.895, Val loss 5.864\n",
      "Ep 1 (Step 000080): Train loss 5.760, Val loss 5.755\n",
      "Ep 1 (Step 000090): Train loss 5.749, Val loss 5.736\n",
      "Ep 1 (Step 000100): Train loss 5.656, Val loss 5.672\n",
      "Ep 1 (Step 000110): Train loss 5.499, Val loss 5.617\n",
      "Ep 1 (Step 000120): Train loss 5.616, Val loss 5.579\n",
      "Ep 1 (Step 000130): Train loss 5.461, Val loss 5.555\n",
      "Ep 1 (Step 000140): Train loss 5.454, Val loss 5.524\n",
      "Ep 1 (Step 000150): Train loss 5.409, Val loss 5.482\n",
      "Ep 1 (Step 000160): Train loss 5.369, Val loss 5.467\n",
      "Ep 1 (Step 000170): Train loss 5.512, Val loss 5.443\n",
      "Ep 1 (Step 000180): Train loss 5.444, Val loss 5.426\n",
      "Ep 1 (Step 000190): Train loss 5.350, Val loss 5.414\n",
      "Ep 1 (Step 000200): Train loss 5.294, Val loss 5.402\n",
      "Ep 1 (Step 000210): Train loss 5.271, Val loss 5.394\n",
      "Ep 1 (Step 000220): Train loss 5.229, Val loss 5.366\n",
      "Ep 1 (Step 000230): Train loss 5.175, Val loss 5.365\n",
      "Ep 1 (Step 000240): Train loss 5.288, Val loss 5.357\n",
      "Ep 1 (Step 000250): Train loss 5.277, Val loss 5.356\n",
      "Ep 1 (Step 000260): Train loss 5.238, Val loss 5.326\n",
      "Ep 1 (Step 000270): Train loss 5.215, Val loss 5.300\n",
      "Ep 1 (Step 000280): Train loss 5.241, Val loss 5.305\n",
      "Ep 1 (Step 000290): Train loss 5.132, Val loss 5.299\n",
      "Ep 1 (Step 000300): Train loss 5.173, Val loss 5.284\n",
      "Ep 1 (Step 000310): Train loss 5.226, Val loss 5.288\n",
      "Ep 1 (Step 000320): Train loss 5.137, Val loss 5.274\n",
      "Ep 1 (Step 000330): Train loss 5.051, Val loss 5.258\n",
      "Ep 1 (Step 000340): Train loss 5.190, Val loss 5.241\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2409\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.587, Val loss 8.571\n",
      "Ep 1 (Step 000010): Train loss 6.859, Val loss 6.839\n",
      "Ep 1 (Step 000020): Train loss 6.423, Val loss 6.448\n",
      "Ep 1 (Step 000030): Train loss 6.338, Val loss 6.337\n",
      "Ep 1 (Step 000040): Train loss 6.187, Val loss 6.148\n",
      "Ep 1 (Step 000050): Train loss 6.083, Val loss 6.015\n",
      "Ep 1 (Step 000060): Train loss 5.890, Val loss 5.957\n",
      "Ep 1 (Step 000070): Train loss 5.821, Val loss 5.841\n",
      "Ep 1 (Step 000080): Train loss 5.867, Val loss 5.769\n",
      "Ep 1 (Step 000090): Train loss 5.706, Val loss 5.719\n",
      "Ep 1 (Step 000100): Train loss 5.662, Val loss 5.685\n",
      "Ep 1 (Step 000110): Train loss 5.664, Val loss 5.640\n",
      "Ep 1 (Step 000120): Train loss 5.501, Val loss 5.609\n",
      "Ep 1 (Step 000130): Train loss 5.582, Val loss 5.564\n",
      "Ep 1 (Step 000140): Train loss 5.453, Val loss 5.542\n",
      "Ep 1 (Step 000150): Train loss 5.462, Val loss 5.538\n",
      "Ep 1 (Step 000160): Train loss 5.422, Val loss 5.481\n",
      "Ep 1 (Step 000170): Train loss 5.360, Val loss 5.459\n",
      "Ep 1 (Step 000180): Train loss 5.327, Val loss 5.437\n",
      "Ep 1 (Step 000190): Train loss 5.290, Val loss 5.430\n",
      "Ep 1 (Step 000200): Train loss 5.375, Val loss 5.398\n",
      "Ep 1 (Step 000210): Train loss 5.246, Val loss 5.394\n",
      "Ep 1 (Step 000220): Train loss 5.316, Val loss 5.370\n",
      "Ep 1 (Step 000230): Train loss 5.234, Val loss 5.339\n",
      "Ep 1 (Step 000240): Train loss 5.319, Val loss 5.344\n",
      "Ep 1 (Step 000250): Train loss 5.126, Val loss 5.333\n",
      "Ep 1 (Step 000260): Train loss 5.208, Val loss 5.326\n",
      "Ep 1 (Step 000270): Train loss 5.269, Val loss 5.334\n",
      "Ep 1 (Step 000280): Train loss 5.200, Val loss 5.296\n",
      "Ep 1 (Step 000290): Train loss 5.132, Val loss 5.293\n",
      "Ep 1 (Step 000300): Train loss 5.192, Val loss 5.288\n",
      "Ep 1 (Step 000310): Train loss 5.172, Val loss 5.271\n",
      "Ep 1 (Step 000320): Train loss 5.106, Val loss 5.244\n",
      "Ep 1 (Step 000330): Train loss 5.154, Val loss 5.233\n",
      "Ep 1 (Step 000340): Train loss 5.055, Val loss 5.224\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2235\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.544, Val loss 8.490\n",
      "Ep 1 (Step 000010): Train loss 6.841, Val loss 6.812\n",
      "Ep 1 (Step 000020): Train loss 6.459, Val loss 6.452\n",
      "Ep 1 (Step 000030): Train loss 6.312, Val loss 6.311\n",
      "Ep 1 (Step 000040): Train loss 6.167, Val loss 6.162\n",
      "Ep 1 (Step 000050): Train loss 5.982, Val loss 6.039\n",
      "Ep 1 (Step 000060): Train loss 5.890, Val loss 5.932\n",
      "Ep 1 (Step 000070): Train loss 5.765, Val loss 5.839\n",
      "Ep 1 (Step 000080): Train loss 5.699, Val loss 5.796\n",
      "Ep 1 (Step 000090): Train loss 5.658, Val loss 5.746\n",
      "Ep 1 (Step 000100): Train loss 5.653, Val loss 5.669\n",
      "Ep 1 (Step 000110): Train loss 5.531, Val loss 5.634\n",
      "Ep 1 (Step 000120): Train loss 5.512, Val loss 5.594\n",
      "Ep 1 (Step 000130): Train loss 5.546, Val loss 5.560\n",
      "Ep 1 (Step 000140): Train loss 5.419, Val loss 5.547\n",
      "Ep 1 (Step 000150): Train loss 5.378, Val loss 5.501\n",
      "Ep 1 (Step 000160): Train loss 5.422, Val loss 5.497\n",
      "Ep 1 (Step 000170): Train loss 5.475, Val loss 5.477\n",
      "Ep 1 (Step 000180): Train loss 5.317, Val loss 5.439\n",
      "Ep 1 (Step 000190): Train loss 5.376, Val loss 5.424\n",
      "Ep 1 (Step 000200): Train loss 5.272, Val loss 5.407\n",
      "Ep 1 (Step 000210): Train loss 5.418, Val loss 5.415\n",
      "Ep 1 (Step 000220): Train loss 5.338, Val loss 5.376\n",
      "Ep 1 (Step 000230): Train loss 5.274, Val loss 5.376\n",
      "Ep 1 (Step 000240): Train loss 5.168, Val loss 5.352\n",
      "Ep 1 (Step 000250): Train loss 5.200, Val loss 5.332\n",
      "Ep 1 (Step 000260): Train loss 5.230, Val loss 5.345\n",
      "Ep 1 (Step 000270): Train loss 5.214, Val loss 5.323\n",
      "Ep 1 (Step 000280): Train loss 5.136, Val loss 5.293\n",
      "Ep 1 (Step 000290): Train loss 5.180, Val loss 5.293\n",
      "Ep 1 (Step 000300): Train loss 5.161, Val loss 5.254\n",
      "Ep 1 (Step 000310): Train loss 5.128, Val loss 5.263\n",
      "Ep 1 (Step 000320): Train loss 5.165, Val loss 5.261\n",
      "Ep 1 (Step 000330): Train loss 5.146, Val loss 5.241\n",
      "Ep 1 (Step 000340): Train loss 5.153, Val loss 5.231\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2314\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.556, Val loss 8.541\n",
      "Ep 1 (Step 000010): Train loss 6.875, Val loss 6.825\n",
      "Ep 1 (Step 000020): Train loss 6.479, Val loss 6.444\n",
      "Ep 1 (Step 000030): Train loss 6.389, Val loss 6.381\n",
      "Ep 1 (Step 000040): Train loss 6.310, Val loss 6.280\n",
      "Ep 1 (Step 000050): Train loss 6.044, Val loss 6.099\n",
      "Ep 1 (Step 000060): Train loss 5.997, Val loss 6.027\n",
      "Ep 1 (Step 000070): Train loss 5.891, Val loss 5.922\n",
      "Ep 1 (Step 000080): Train loss 5.793, Val loss 5.829\n",
      "Ep 1 (Step 000090): Train loss 5.738, Val loss 5.773\n",
      "Ep 1 (Step 000100): Train loss 5.666, Val loss 5.734\n",
      "Ep 1 (Step 000110): Train loss 5.623, Val loss 5.679\n",
      "Ep 1 (Step 000120): Train loss 5.634, Val loss 5.639\n",
      "Ep 1 (Step 000130): Train loss 5.565, Val loss 5.585\n",
      "Ep 1 (Step 000140): Train loss 5.419, Val loss 5.572\n",
      "Ep 1 (Step 000150): Train loss 5.428, Val loss 5.533\n",
      "Ep 1 (Step 000160): Train loss 5.461, Val loss 5.505\n",
      "Ep 1 (Step 000170): Train loss 5.429, Val loss 5.462\n",
      "Ep 1 (Step 000180): Train loss 5.263, Val loss 5.454\n",
      "Ep 1 (Step 000190): Train loss 5.303, Val loss 5.416\n",
      "Ep 1 (Step 000200): Train loss 5.218, Val loss 5.402\n",
      "Ep 1 (Step 000210): Train loss 5.365, Val loss 5.389\n",
      "Ep 1 (Step 000220): Train loss 5.221, Val loss 5.366\n",
      "Ep 1 (Step 000230): Train loss 5.294, Val loss 5.352\n",
      "Ep 1 (Step 000240): Train loss 5.187, Val loss 5.348\n",
      "Ep 1 (Step 000250): Train loss 5.246, Val loss 5.319\n",
      "Ep 1 (Step 000260): Train loss 5.206, Val loss 5.309\n",
      "Ep 1 (Step 000270): Train loss 5.127, Val loss 5.301\n",
      "Ep 1 (Step 000280): Train loss 5.180, Val loss 5.302\n",
      "Ep 1 (Step 000290): Train loss 5.168, Val loss 5.262\n",
      "Ep 1 (Step 000300): Train loss 5.097, Val loss 5.263\n",
      "Ep 1 (Step 000310): Train loss 5.131, Val loss 5.247\n",
      "Ep 1 (Step 000320): Train loss 5.075, Val loss 5.229\n",
      "Ep 1 (Step 000330): Train loss 5.133, Val loss 5.227\n",
      "Ep 1 (Step 000340): Train loss 4.998, Val loss 5.216\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2160\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.554, Val loss 8.497\n",
      "Ep 1 (Step 000010): Train loss 6.926, Val loss 6.857\n",
      "Ep 1 (Step 000020): Train loss 6.457, Val loss 6.445\n",
      "Ep 1 (Step 000030): Train loss 6.393, Val loss 6.371\n",
      "Ep 1 (Step 000040): Train loss 6.248, Val loss 6.221\n",
      "Ep 1 (Step 000050): Train loss 6.076, Val loss 6.110\n",
      "Ep 1 (Step 000060): Train loss 5.936, Val loss 6.004\n",
      "Ep 1 (Step 000070): Train loss 5.902, Val loss 5.891\n",
      "Ep 1 (Step 000080): Train loss 5.782, Val loss 5.845\n",
      "Ep 1 (Step 000090): Train loss 5.657, Val loss 5.762\n",
      "Ep 1 (Step 000100): Train loss 5.633, Val loss 5.704\n",
      "Ep 1 (Step 000110): Train loss 5.619, Val loss 5.640\n",
      "Ep 1 (Step 000120): Train loss 5.526, Val loss 5.598\n",
      "Ep 1 (Step 000130): Train loss 5.513, Val loss 5.552\n",
      "Ep 1 (Step 000140): Train loss 5.519, Val loss 5.529\n",
      "Ep 1 (Step 000150): Train loss 5.376, Val loss 5.511\n",
      "Ep 1 (Step 000160): Train loss 5.380, Val loss 5.487\n",
      "Ep 1 (Step 000170): Train loss 5.433, Val loss 5.456\n",
      "Ep 1 (Step 000180): Train loss 5.409, Val loss 5.441\n",
      "Ep 1 (Step 000190): Train loss 5.274, Val loss 5.396\n",
      "Ep 1 (Step 000200): Train loss 5.377, Val loss 5.389\n",
      "Ep 1 (Step 000210): Train loss 5.248, Val loss 5.370\n",
      "Ep 1 (Step 000220): Train loss 5.282, Val loss 5.350\n",
      "Ep 1 (Step 000230): Train loss 5.183, Val loss 5.355\n",
      "Ep 1 (Step 000240): Train loss 5.142, Val loss 5.321\n",
      "Ep 1 (Step 000250): Train loss 5.228, Val loss 5.317\n",
      "Ep 1 (Step 000260): Train loss 5.166, Val loss 5.299\n",
      "Ep 1 (Step 000270): Train loss 5.256, Val loss 5.298\n",
      "Ep 1 (Step 000280): Train loss 5.173, Val loss 5.280\n",
      "Ep 1 (Step 000290): Train loss 5.164, Val loss 5.268\n",
      "Ep 1 (Step 000300): Train loss 5.139, Val loss 5.268\n",
      "Ep 1 (Step 000310): Train loss 5.149, Val loss 5.238\n",
      "Ep 1 (Step 000320): Train loss 5.200, Val loss 5.229\n",
      "Ep 1 (Step 000330): Train loss 5.115, Val loss 5.223\n",
      "Ep 1 (Step 000340): Train loss 5.098, Val loss 5.210\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2100\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.562, Val loss 8.520\n",
      "Ep 1 (Step 000010): Train loss 6.859, Val loss 6.799\n",
      "Ep 1 (Step 000020): Train loss 6.450, Val loss 6.434\n",
      "Ep 1 (Step 000030): Train loss 6.453, Val loss 6.406\n",
      "Ep 1 (Step 000040): Train loss 6.204, Val loss 6.228\n",
      "Ep 1 (Step 000050): Train loss 6.087, Val loss 6.127\n",
      "Ep 1 (Step 000060): Train loss 5.975, Val loss 5.993\n",
      "Ep 1 (Step 000070): Train loss 5.859, Val loss 5.918\n",
      "Ep 1 (Step 000080): Train loss 5.758, Val loss 5.827\n",
      "Ep 1 (Step 000090): Train loss 5.730, Val loss 5.807\n",
      "Ep 1 (Step 000100): Train loss 5.631, Val loss 5.697\n",
      "Ep 1 (Step 000110): Train loss 5.604, Val loss 5.647\n",
      "Ep 1 (Step 000120): Train loss 5.563, Val loss 5.607\n",
      "Ep 1 (Step 000130): Train loss 5.475, Val loss 5.589\n",
      "Ep 1 (Step 000140): Train loss 5.484, Val loss 5.569\n",
      "Ep 1 (Step 000150): Train loss 5.358, Val loss 5.517\n",
      "Ep 1 (Step 000160): Train loss 5.386, Val loss 5.501\n",
      "Ep 1 (Step 000170): Train loss 5.404, Val loss 5.468\n",
      "Ep 1 (Step 000180): Train loss 5.451, Val loss 5.435\n",
      "Ep 1 (Step 000190): Train loss 5.364, Val loss 5.417\n",
      "Ep 1 (Step 000200): Train loss 5.351, Val loss 5.400\n",
      "Ep 1 (Step 000210): Train loss 5.316, Val loss 5.401\n",
      "Ep 1 (Step 000220): Train loss 5.304, Val loss 5.387\n",
      "Ep 1 (Step 000230): Train loss 5.204, Val loss 5.362\n",
      "Ep 1 (Step 000240): Train loss 5.313, Val loss 5.360\n",
      "Ep 1 (Step 000250): Train loss 5.164, Val loss 5.325\n",
      "Ep 1 (Step 000260): Train loss 5.064, Val loss 5.320\n",
      "Ep 1 (Step 000270): Train loss 5.112, Val loss 5.319\n",
      "Ep 1 (Step 000280): Train loss 5.165, Val loss 5.291\n",
      "Ep 1 (Step 000290): Train loss 5.164, Val loss 5.271\n",
      "Ep 1 (Step 000300): Train loss 5.125, Val loss 5.276\n",
      "Ep 1 (Step 000310): Train loss 5.177, Val loss 5.266\n",
      "Ep 1 (Step 000320): Train loss 5.178, Val loss 5.250\n",
      "Ep 1 (Step 000330): Train loss 5.127, Val loss 5.235\n",
      "Ep 1 (Step 000340): Train loss 5.116, Val loss 5.245\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 512, 'n_heads': 16, 'n_layers': 16, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2454\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.938, Val loss 8.930\n",
      "Ep 1 (Step 000010): Train loss 7.097, Val loss 7.054\n",
      "Ep 1 (Step 000020): Train loss 6.512, Val loss 6.485\n",
      "Ep 1 (Step 000030): Train loss 6.386, Val loss 6.412\n",
      "Ep 1 (Step 000040): Train loss 6.428, Val loss 6.383\n",
      "Ep 1 (Step 000050): Train loss 6.245, Val loss 6.257\n",
      "Ep 1 (Step 000060): Train loss 6.011, Val loss 6.139\n",
      "Ep 1 (Step 000070): Train loss 5.978, Val loss 6.033\n",
      "Ep 1 (Step 000080): Train loss 5.963, Val loss 5.921\n",
      "Ep 1 (Step 000090): Train loss 5.855, Val loss 5.878\n",
      "Ep 1 (Step 000100): Train loss 5.806, Val loss 5.807\n",
      "Ep 1 (Step 000110): Train loss 5.645, Val loss 5.729\n",
      "Ep 1 (Step 000120): Train loss 5.644, Val loss 5.692\n",
      "Ep 1 (Step 000130): Train loss 5.643, Val loss 5.666\n",
      "Ep 1 (Step 000140): Train loss 5.567, Val loss 5.619\n",
      "Ep 1 (Step 000150): Train loss 5.468, Val loss 5.580\n",
      "Ep 1 (Step 000160): Train loss 5.524, Val loss 5.553\n",
      "Ep 1 (Step 000170): Train loss 5.383, Val loss 5.527\n",
      "Ep 1 (Step 000180): Train loss 5.412, Val loss 5.497\n",
      "Ep 1 (Step 000190): Train loss 5.402, Val loss 5.485\n",
      "Ep 1 (Step 000200): Train loss 5.351, Val loss 5.457\n",
      "Ep 1 (Step 000210): Train loss 5.400, Val loss 5.430\n",
      "Ep 1 (Step 000220): Train loss 5.229, Val loss 5.414\n",
      "Ep 1 (Step 000230): Train loss 5.306, Val loss 5.402\n",
      "Ep 1 (Step 000240): Train loss 5.321, Val loss 5.382\n",
      "Ep 1 (Step 000250): Train loss 5.294, Val loss 5.402\n",
      "Ep 1 (Step 000260): Train loss 5.257, Val loss 5.389\n",
      "Ep 1 (Step 000270): Train loss 5.215, Val loss 5.354\n",
      "Ep 1 (Step 000280): Train loss 5.210, Val loss 5.350\n",
      "Ep 1 (Step 000290): Train loss 5.160, Val loss 5.326\n",
      "Ep 1 (Step 000300): Train loss 5.225, Val loss 5.326\n",
      "Ep 1 (Step 000310): Train loss 5.109, Val loss 5.324\n",
      "Ep 1 (Step 000320): Train loss 5.124, Val loss 5.286\n",
      "Ep 1 (Step 000330): Train loss 5.129, Val loss 5.303\n",
      "Ep 1 (Step 000340): Train loss 5.088, Val loss 5.282\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2820\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.989, Val loss 8.969\n",
      "Ep 1 (Step 000010): Train loss 7.105, Val loss 7.047\n",
      "Ep 1 (Step 000020): Train loss 6.496, Val loss 6.485\n",
      "Ep 1 (Step 000030): Train loss 6.457, Val loss 6.403\n",
      "Ep 1 (Step 000040): Train loss 6.339, Val loss 6.364\n",
      "Ep 1 (Step 000050): Train loss 6.278, Val loss 6.252\n",
      "Ep 1 (Step 000060): Train loss 6.122, Val loss 6.111\n",
      "Ep 1 (Step 000070): Train loss 6.034, Val loss 6.019\n",
      "Ep 1 (Step 000080): Train loss 5.851, Val loss 5.928\n",
      "Ep 1 (Step 000090): Train loss 5.843, Val loss 5.859\n",
      "Ep 1 (Step 000100): Train loss 5.790, Val loss 5.791\n",
      "Ep 1 (Step 000110): Train loss 5.619, Val loss 5.737\n",
      "Ep 1 (Step 000120): Train loss 5.633, Val loss 5.691\n",
      "Ep 1 (Step 000130): Train loss 5.605, Val loss 5.661\n",
      "Ep 1 (Step 000140): Train loss 5.505, Val loss 5.629\n",
      "Ep 1 (Step 000150): Train loss 5.499, Val loss 5.576\n",
      "Ep 1 (Step 000160): Train loss 5.414, Val loss 5.566\n",
      "Ep 1 (Step 000170): Train loss 5.302, Val loss 5.545\n",
      "Ep 1 (Step 000180): Train loss 5.374, Val loss 5.507\n",
      "Ep 1 (Step 000190): Train loss 5.406, Val loss 5.482\n",
      "Ep 1 (Step 000200): Train loss 5.358, Val loss 5.470\n",
      "Ep 1 (Step 000210): Train loss 5.420, Val loss 5.454\n",
      "Ep 1 (Step 000220): Train loss 5.327, Val loss 5.426\n",
      "Ep 1 (Step 000230): Train loss 5.322, Val loss 5.415\n",
      "Ep 1 (Step 000240): Train loss 5.270, Val loss 5.386\n",
      "Ep 1 (Step 000250): Train loss 5.246, Val loss 5.370\n",
      "Ep 1 (Step 000260): Train loss 5.218, Val loss 5.362\n",
      "Ep 1 (Step 000270): Train loss 5.179, Val loss 5.363\n",
      "Ep 1 (Step 000280): Train loss 5.252, Val loss 5.354\n",
      "Ep 1 (Step 000290): Train loss 5.219, Val loss 5.338\n",
      "Ep 1 (Step 000300): Train loss 5.184, Val loss 5.325\n",
      "Ep 1 (Step 000310): Train loss 5.252, Val loss 5.315\n",
      "Ep 1 (Step 000320): Train loss 5.189, Val loss 5.323\n",
      "Ep 1 (Step 000330): Train loss 5.104, Val loss 5.294\n",
      "Ep 1 (Step 000340): Train loss 5.038, Val loss 5.303\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3026\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.989, Val loss 8.975\n",
      "Ep 1 (Step 000010): Train loss 7.065, Val loss 7.053\n",
      "Ep 1 (Step 000020): Train loss 6.536, Val loss 6.527\n",
      "Ep 1 (Step 000030): Train loss 6.421, Val loss 6.431\n",
      "Ep 1 (Step 000040): Train loss 6.411, Val loss 6.379\n",
      "Ep 1 (Step 000050): Train loss 6.190, Val loss 6.234\n",
      "Ep 1 (Step 000060): Train loss 6.141, Val loss 6.122\n",
      "Ep 1 (Step 000070): Train loss 6.068, Val loss 6.032\n",
      "Ep 1 (Step 000080): Train loss 5.928, Val loss 5.944\n",
      "Ep 1 (Step 000090): Train loss 5.803, Val loss 5.899\n",
      "Ep 1 (Step 000100): Train loss 5.714, Val loss 5.825\n",
      "Ep 1 (Step 000110): Train loss 5.712, Val loss 5.760\n",
      "Ep 1 (Step 000120): Train loss 5.613, Val loss 5.696\n",
      "Ep 1 (Step 000130): Train loss 5.553, Val loss 5.653\n",
      "Ep 1 (Step 000140): Train loss 5.537, Val loss 5.613\n",
      "Ep 1 (Step 000150): Train loss 5.535, Val loss 5.590\n",
      "Ep 1 (Step 000160): Train loss 5.432, Val loss 5.572\n",
      "Ep 1 (Step 000170): Train loss 5.402, Val loss 5.527\n",
      "Ep 1 (Step 000180): Train loss 5.364, Val loss 5.507\n",
      "Ep 1 (Step 000190): Train loss 5.423, Val loss 5.471\n",
      "Ep 1 (Step 000200): Train loss 5.379, Val loss 5.462\n",
      "Ep 1 (Step 000210): Train loss 5.463, Val loss 5.433\n",
      "Ep 1 (Step 000220): Train loss 5.240, Val loss 5.431\n",
      "Ep 1 (Step 000230): Train loss 5.310, Val loss 5.423\n",
      "Ep 1 (Step 000240): Train loss 5.285, Val loss 5.417\n",
      "Ep 1 (Step 000250): Train loss 5.263, Val loss 5.388\n",
      "Ep 1 (Step 000260): Train loss 5.184, Val loss 5.387\n",
      "Ep 1 (Step 000270): Train loss 5.232, Val loss 5.345\n",
      "Ep 1 (Step 000280): Train loss 5.181, Val loss 5.354\n",
      "Ep 1 (Step 000290): Train loss 5.175, Val loss 5.335\n",
      "Ep 1 (Step 000300): Train loss 5.242, Val loss 5.331\n",
      "Ep 1 (Step 000310): Train loss 5.133, Val loss 5.306\n",
      "Ep 1 (Step 000320): Train loss 5.117, Val loss 5.290\n",
      "Ep 1 (Step 000330): Train loss 5.083, Val loss 5.270\n",
      "Ep 1 (Step 000340): Train loss 5.111, Val loss 5.272\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2722\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.984, Val loss 8.971\n",
      "Ep 1 (Step 000010): Train loss 7.075, Val loss 7.046\n",
      "Ep 1 (Step 000020): Train loss 6.561, Val loss 6.535\n",
      "Ep 1 (Step 000030): Train loss 6.446, Val loss 6.472\n",
      "Ep 1 (Step 000040): Train loss 6.393, Val loss 6.416\n",
      "Ep 1 (Step 000050): Train loss 6.308, Val loss 6.311\n",
      "Ep 1 (Step 000060): Train loss 6.131, Val loss 6.174\n",
      "Ep 1 (Step 000070): Train loss 6.078, Val loss 6.057\n",
      "Ep 1 (Step 000080): Train loss 5.905, Val loss 5.989\n",
      "Ep 1 (Step 000090): Train loss 5.844, Val loss 5.895\n",
      "Ep 1 (Step 000100): Train loss 5.785, Val loss 5.819\n",
      "Ep 1 (Step 000110): Train loss 5.722, Val loss 5.783\n",
      "Ep 1 (Step 000120): Train loss 5.636, Val loss 5.711\n",
      "Ep 1 (Step 000130): Train loss 5.709, Val loss 5.672\n",
      "Ep 1 (Step 000140): Train loss 5.572, Val loss 5.639\n",
      "Ep 1 (Step 000150): Train loss 5.475, Val loss 5.596\n",
      "Ep 1 (Step 000160): Train loss 5.428, Val loss 5.555\n",
      "Ep 1 (Step 000170): Train loss 5.501, Val loss 5.530\n",
      "Ep 1 (Step 000180): Train loss 5.469, Val loss 5.519\n",
      "Ep 1 (Step 000190): Train loss 5.425, Val loss 5.490\n",
      "Ep 1 (Step 000200): Train loss 5.278, Val loss 5.448\n",
      "Ep 1 (Step 000210): Train loss 5.370, Val loss 5.427\n",
      "Ep 1 (Step 000220): Train loss 5.332, Val loss 5.410\n",
      "Ep 1 (Step 000230): Train loss 5.350, Val loss 5.406\n",
      "Ep 1 (Step 000240): Train loss 5.305, Val loss 5.378\n",
      "Ep 1 (Step 000250): Train loss 5.099, Val loss 5.371\n",
      "Ep 1 (Step 000260): Train loss 5.192, Val loss 5.342\n",
      "Ep 1 (Step 000270): Train loss 5.177, Val loss 5.340\n",
      "Ep 1 (Step 000280): Train loss 5.272, Val loss 5.318\n",
      "Ep 1 (Step 000290): Train loss 5.129, Val loss 5.330\n",
      "Ep 1 (Step 000300): Train loss 5.135, Val loss 5.299\n",
      "Ep 1 (Step 000310): Train loss 5.101, Val loss 5.293\n",
      "Ep 1 (Step 000320): Train loss 5.088, Val loss 5.284\n",
      "Ep 1 (Step 000330): Train loss 5.127, Val loss 5.267\n",
      "Ep 1 (Step 000340): Train loss 5.095, Val loss 5.259\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2586\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.996, Val loss 8.975\n",
      "Ep 1 (Step 000010): Train loss 7.085, Val loss 7.060\n",
      "Ep 1 (Step 000020): Train loss 6.542, Val loss 6.515\n",
      "Ep 1 (Step 000030): Train loss 6.460, Val loss 6.419\n",
      "Ep 1 (Step 000040): Train loss 6.429, Val loss 6.395\n",
      "Ep 1 (Step 000050): Train loss 6.345, Val loss 6.302\n",
      "Ep 1 (Step 000060): Train loss 6.086, Val loss 6.157\n",
      "Ep 1 (Step 000070): Train loss 5.988, Val loss 6.042\n",
      "Ep 1 (Step 000080): Train loss 5.977, Val loss 5.979\n",
      "Ep 1 (Step 000090): Train loss 5.869, Val loss 5.895\n",
      "Ep 1 (Step 000100): Train loss 5.714, Val loss 5.849\n",
      "Ep 1 (Step 000110): Train loss 5.720, Val loss 5.777\n",
      "Ep 1 (Step 000120): Train loss 5.721, Val loss 5.732\n",
      "Ep 1 (Step 000130): Train loss 5.514, Val loss 5.663\n",
      "Ep 1 (Step 000140): Train loss 5.609, Val loss 5.626\n",
      "Ep 1 (Step 000150): Train loss 5.475, Val loss 5.605\n",
      "Ep 1 (Step 000160): Train loss 5.542, Val loss 5.551\n",
      "Ep 1 (Step 000170): Train loss 5.430, Val loss 5.522\n",
      "Ep 1 (Step 000180): Train loss 5.410, Val loss 5.493\n",
      "Ep 1 (Step 000190): Train loss 5.410, Val loss 5.472\n",
      "Ep 1 (Step 000200): Train loss 5.398, Val loss 5.457\n",
      "Ep 1 (Step 000210): Train loss 5.283, Val loss 5.422\n",
      "Ep 1 (Step 000220): Train loss 5.323, Val loss 5.419\n",
      "Ep 1 (Step 000230): Train loss 5.284, Val loss 5.407\n",
      "Ep 1 (Step 000240): Train loss 5.343, Val loss 5.383\n",
      "Ep 1 (Step 000250): Train loss 5.277, Val loss 5.379\n",
      "Ep 1 (Step 000260): Train loss 5.311, Val loss 5.350\n",
      "Ep 1 (Step 000270): Train loss 5.247, Val loss 5.336\n",
      "Ep 1 (Step 000280): Train loss 5.116, Val loss 5.336\n",
      "Ep 1 (Step 000290): Train loss 5.187, Val loss 5.320\n",
      "Ep 1 (Step 000300): Train loss 5.202, Val loss 5.307\n",
      "Ep 1 (Step 000310): Train loss 5.146, Val loss 5.293\n",
      "Ep 1 (Step 000320): Train loss 5.122, Val loss 5.267\n",
      "Ep 1 (Step 000330): Train loss 5.097, Val loss 5.264\n",
      "Ep 1 (Step 000340): Train loss 5.092, Val loss 5.257\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2565\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.966, Val loss 8.974\n",
      "Ep 1 (Step 000010): Train loss 7.065, Val loss 7.039\n",
      "Ep 1 (Step 000020): Train loss 6.525, Val loss 6.517\n",
      "Ep 1 (Step 000030): Train loss 6.481, Val loss 6.428\n",
      "Ep 1 (Step 000040): Train loss 6.384, Val loss 6.384\n",
      "Ep 1 (Step 000050): Train loss 6.260, Val loss 6.307\n",
      "Ep 1 (Step 000060): Train loss 6.173, Val loss 6.167\n",
      "Ep 1 (Step 000070): Train loss 6.001, Val loss 6.057\n",
      "Ep 1 (Step 000080): Train loss 5.912, Val loss 5.952\n",
      "Ep 1 (Step 000090): Train loss 5.741, Val loss 5.866\n",
      "Ep 1 (Step 000100): Train loss 5.657, Val loss 5.796\n",
      "Ep 1 (Step 000110): Train loss 5.674, Val loss 5.749\n",
      "Ep 1 (Step 000120): Train loss 5.622, Val loss 5.690\n",
      "Ep 1 (Step 000130): Train loss 5.555, Val loss 5.651\n",
      "Ep 1 (Step 000140): Train loss 5.523, Val loss 5.638\n",
      "Ep 1 (Step 000150): Train loss 5.544, Val loss 5.609\n",
      "Ep 1 (Step 000160): Train loss 5.507, Val loss 5.559\n",
      "Ep 1 (Step 000170): Train loss 5.332, Val loss 5.530\n",
      "Ep 1 (Step 000180): Train loss 5.360, Val loss 5.510\n",
      "Ep 1 (Step 000190): Train loss 5.348, Val loss 5.471\n",
      "Ep 1 (Step 000200): Train loss 5.347, Val loss 5.441\n",
      "Ep 1 (Step 000210): Train loss 5.303, Val loss 5.444\n",
      "Ep 1 (Step 000220): Train loss 5.182, Val loss 5.411\n",
      "Ep 1 (Step 000230): Train loss 5.239, Val loss 5.403\n",
      "Ep 1 (Step 000240): Train loss 5.313, Val loss 5.396\n",
      "Ep 1 (Step 000250): Train loss 5.196, Val loss 5.370\n",
      "Ep 1 (Step 000260): Train loss 5.153, Val loss 5.339\n",
      "Ep 1 (Step 000270): Train loss 5.228, Val loss 5.343\n",
      "Ep 1 (Step 000280): Train loss 5.178, Val loss 5.317\n",
      "Ep 1 (Step 000290): Train loss 5.156, Val loss 5.327\n",
      "Ep 1 (Step 000300): Train loss 5.249, Val loss 5.307\n",
      "Ep 1 (Step 000310): Train loss 5.190, Val loss 5.318\n",
      "Ep 1 (Step 000320): Train loss 5.050, Val loss 5.289\n",
      "Ep 1 (Step 000330): Train loss 5.210, Val loss 5.288\n",
      "Ep 1 (Step 000340): Train loss 5.131, Val loss 5.265\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2652\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.644, Val loss 8.624\n",
      "Ep 1 (Step 000010): Train loss 6.575, Val loss 6.574\n",
      "Ep 1 (Step 000020): Train loss 6.632, Val loss 6.534\n",
      "Ep 1 (Step 000030): Train loss 6.419, Val loss 6.364\n",
      "Ep 1 (Step 000040): Train loss 6.256, Val loss 6.180\n",
      "Ep 1 (Step 000050): Train loss 6.036, Val loss 6.033\n",
      "Ep 1 (Step 000060): Train loss 5.790, Val loss 5.924\n",
      "Ep 1 (Step 000070): Train loss 5.731, Val loss 5.794\n",
      "Ep 1 (Step 000080): Train loss 5.684, Val loss 5.719\n",
      "Ep 1 (Step 000090): Train loss 5.628, Val loss 5.659\n",
      "Ep 1 (Step 000100): Train loss 5.620, Val loss 5.612\n",
      "Ep 1 (Step 000110): Train loss 5.502, Val loss 5.568\n",
      "Ep 1 (Step 000120): Train loss 5.496, Val loss 5.549\n",
      "Ep 1 (Step 000130): Train loss 5.399, Val loss 5.493\n",
      "Ep 1 (Step 000140): Train loss 5.407, Val loss 5.462\n",
      "Ep 1 (Step 000150): Train loss 5.338, Val loss 5.437\n",
      "Ep 1 (Step 000160): Train loss 5.353, Val loss 5.397\n",
      "Ep 1 (Step 000170): Train loss 5.316, Val loss 5.391\n",
      "Ep 1 (Step 000180): Train loss 5.307, Val loss 5.393\n",
      "Ep 1 (Step 000190): Train loss 5.172, Val loss 5.373\n",
      "Ep 1 (Step 000200): Train loss 5.139, Val loss 5.362\n",
      "Ep 1 (Step 000210): Train loss 5.273, Val loss 5.366\n",
      "Ep 1 (Step 000220): Train loss 5.145, Val loss 5.329\n",
      "Ep 1 (Step 000230): Train loss 5.151, Val loss 5.307\n",
      "Ep 1 (Step 000240): Train loss 5.087, Val loss 5.317\n",
      "Ep 1 (Step 000250): Train loss 5.045, Val loss 5.299\n",
      "Ep 1 (Step 000260): Train loss 5.103, Val loss 5.301\n",
      "Ep 1 (Step 000270): Train loss 5.091, Val loss 5.273\n",
      "Ep 1 (Step 000280): Train loss 5.147, Val loss 5.281\n",
      "Ep 1 (Step 000290): Train loss 5.180, Val loss 5.259\n",
      "Ep 1 (Step 000300): Train loss 5.027, Val loss 5.268\n",
      "Ep 1 (Step 000310): Train loss 5.080, Val loss 5.253\n",
      "Ep 1 (Step 000320): Train loss 5.052, Val loss 5.254\n",
      "Ep 1 (Step 000330): Train loss 5.058, Val loss 5.247\n",
      "Ep 1 (Step 000340): Train loss 5.093, Val loss 5.221\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2209\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.652, Val loss 8.636\n",
      "Ep 1 (Step 000010): Train loss 6.618, Val loss 6.577\n",
      "Ep 1 (Step 000020): Train loss 6.508, Val loss 6.540\n",
      "Ep 1 (Step 000030): Train loss 6.447, Val loss 6.429\n",
      "Ep 1 (Step 000040): Train loss 6.197, Val loss 6.165\n",
      "Ep 1 (Step 000050): Train loss 5.996, Val loss 6.053\n",
      "Ep 1 (Step 000060): Train loss 5.882, Val loss 5.914\n",
      "Ep 1 (Step 000070): Train loss 5.712, Val loss 5.828\n",
      "Ep 1 (Step 000080): Train loss 5.782, Val loss 5.790\n",
      "Ep 1 (Step 000090): Train loss 5.616, Val loss 5.674\n",
      "Ep 1 (Step 000100): Train loss 5.545, Val loss 5.643\n",
      "Ep 1 (Step 000110): Train loss 5.522, Val loss 5.602\n",
      "Ep 1 (Step 000120): Train loss 5.405, Val loss 5.564\n",
      "Ep 1 (Step 000130): Train loss 5.461, Val loss 5.508\n",
      "Ep 1 (Step 000140): Train loss 5.359, Val loss 5.463\n",
      "Ep 1 (Step 000150): Train loss 5.367, Val loss 5.460\n",
      "Ep 1 (Step 000160): Train loss 5.414, Val loss 5.435\n",
      "Ep 1 (Step 000170): Train loss 5.365, Val loss 5.431\n",
      "Ep 1 (Step 000180): Train loss 5.376, Val loss 5.403\n",
      "Ep 1 (Step 000190): Train loss 5.289, Val loss 5.393\n",
      "Ep 1 (Step 000200): Train loss 5.224, Val loss 5.373\n",
      "Ep 1 (Step 000210): Train loss 5.261, Val loss 5.350\n",
      "Ep 1 (Step 000220): Train loss 5.244, Val loss 5.344\n",
      "Ep 1 (Step 000230): Train loss 5.167, Val loss 5.322\n",
      "Ep 1 (Step 000240): Train loss 5.154, Val loss 5.336\n",
      "Ep 1 (Step 000250): Train loss 5.153, Val loss 5.303\n",
      "Ep 1 (Step 000260): Train loss 5.118, Val loss 5.312\n",
      "Ep 1 (Step 000270): Train loss 5.198, Val loss 5.299\n",
      "Ep 1 (Step 000280): Train loss 5.083, Val loss 5.292\n",
      "Ep 1 (Step 000290): Train loss 5.101, Val loss 5.269\n",
      "Ep 1 (Step 000300): Train loss 5.065, Val loss 5.254\n",
      "Ep 1 (Step 000310): Train loss 5.036, Val loss 5.238\n",
      "Ep 1 (Step 000320): Train loss 5.037, Val loss 5.220\n",
      "Ep 1 (Step 000330): Train loss 5.068, Val loss 5.223\n",
      "Ep 1 (Step 000340): Train loss 5.019, Val loss 5.218\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2182\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.694, Val loss 8.681\n",
      "Ep 1 (Step 000010): Train loss 6.600, Val loss 6.574\n",
      "Ep 1 (Step 000020): Train loss 6.580, Val loss 6.516\n",
      "Ep 1 (Step 000030): Train loss 6.356, Val loss 6.393\n",
      "Ep 1 (Step 000040): Train loss 6.299, Val loss 6.272\n",
      "Ep 1 (Step 000050): Train loss 6.022, Val loss 6.066\n",
      "Ep 1 (Step 000060): Train loss 5.894, Val loss 5.921\n",
      "Ep 1 (Step 000070): Train loss 5.788, Val loss 5.843\n",
      "Ep 1 (Step 000080): Train loss 5.611, Val loss 5.774\n",
      "Ep 1 (Step 000090): Train loss 5.653, Val loss 5.697\n",
      "Ep 1 (Step 000100): Train loss 5.481, Val loss 5.631\n",
      "Ep 1 (Step 000110): Train loss 5.480, Val loss 5.599\n",
      "Ep 1 (Step 000120): Train loss 5.413, Val loss 5.568\n",
      "Ep 1 (Step 000130): Train loss 5.418, Val loss 5.525\n",
      "Ep 1 (Step 000140): Train loss 5.355, Val loss 5.484\n",
      "Ep 1 (Step 000150): Train loss 5.459, Val loss 5.460\n",
      "Ep 1 (Step 000160): Train loss 5.302, Val loss 5.434\n",
      "Ep 1 (Step 000170): Train loss 5.327, Val loss 5.409\n",
      "Ep 1 (Step 000180): Train loss 5.283, Val loss 5.398\n",
      "Ep 1 (Step 000190): Train loss 5.336, Val loss 5.371\n",
      "Ep 1 (Step 000200): Train loss 5.247, Val loss 5.368\n",
      "Ep 1 (Step 000210): Train loss 5.264, Val loss 5.375\n",
      "Ep 1 (Step 000220): Train loss 5.258, Val loss 5.323\n",
      "Ep 1 (Step 000230): Train loss 5.188, Val loss 5.319\n",
      "Ep 1 (Step 000240): Train loss 5.227, Val loss 5.332\n",
      "Ep 1 (Step 000250): Train loss 5.127, Val loss 5.298\n",
      "Ep 1 (Step 000260): Train loss 5.073, Val loss 5.287\n",
      "Ep 1 (Step 000270): Train loss 5.139, Val loss 5.293\n",
      "Ep 1 (Step 000280): Train loss 5.098, Val loss 5.263\n",
      "Ep 1 (Step 000290): Train loss 5.141, Val loss 5.242\n",
      "Ep 1 (Step 000300): Train loss 5.204, Val loss 5.285\n",
      "Ep 1 (Step 000310): Train loss 5.237, Val loss 5.260\n",
      "Ep 1 (Step 000320): Train loss 5.121, Val loss 5.262\n",
      "Ep 1 (Step 000330): Train loss 5.058, Val loss 5.239\n",
      "Ep 1 (Step 000340): Train loss 5.074, Val loss 5.214\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2140\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.670, Val loss 8.639\n",
      "Ep 1 (Step 000010): Train loss 6.573, Val loss 6.579\n",
      "Ep 1 (Step 000020): Train loss 6.631, Val loss 6.571\n",
      "Ep 1 (Step 000030): Train loss 6.399, Val loss 6.436\n",
      "Ep 1 (Step 000040): Train loss 6.236, Val loss 6.276\n",
      "Ep 1 (Step 000050): Train loss 6.017, Val loss 6.051\n",
      "Ep 1 (Step 000060): Train loss 5.895, Val loss 5.950\n",
      "Ep 1 (Step 000070): Train loss 5.782, Val loss 5.842\n",
      "Ep 1 (Step 000080): Train loss 5.682, Val loss 5.754\n",
      "Ep 1 (Step 000090): Train loss 5.513, Val loss 5.677\n",
      "Ep 1 (Step 000100): Train loss 5.547, Val loss 5.621\n",
      "Ep 1 (Step 000110): Train loss 5.527, Val loss 5.571\n",
      "Ep 1 (Step 000120): Train loss 5.369, Val loss 5.556\n",
      "Ep 1 (Step 000130): Train loss 5.442, Val loss 5.491\n",
      "Ep 1 (Step 000140): Train loss 5.351, Val loss 5.446\n",
      "Ep 1 (Step 000150): Train loss 5.263, Val loss 5.453\n",
      "Ep 1 (Step 000160): Train loss 5.318, Val loss 5.400\n",
      "Ep 1 (Step 000170): Train loss 5.265, Val loss 5.403\n",
      "Ep 1 (Step 000180): Train loss 5.223, Val loss 5.383\n",
      "Ep 1 (Step 000190): Train loss 5.278, Val loss 5.378\n",
      "Ep 1 (Step 000200): Train loss 5.225, Val loss 5.324\n",
      "Ep 1 (Step 000210): Train loss 5.245, Val loss 5.330\n",
      "Ep 1 (Step 000220): Train loss 5.185, Val loss 5.314\n",
      "Ep 1 (Step 000230): Train loss 5.135, Val loss 5.281\n",
      "Ep 1 (Step 000240): Train loss 5.126, Val loss 5.289\n",
      "Ep 1 (Step 000250): Train loss 5.126, Val loss 5.262\n",
      "Ep 1 (Step 000260): Train loss 5.158, Val loss 5.252\n",
      "Ep 1 (Step 000270): Train loss 5.106, Val loss 5.227\n",
      "Ep 1 (Step 000280): Train loss 5.052, Val loss 5.226\n",
      "Ep 1 (Step 000290): Train loss 5.070, Val loss 5.257\n",
      "Ep 1 (Step 000300): Train loss 5.105, Val loss 5.226\n",
      "Ep 1 (Step 000310): Train loss 4.932, Val loss 5.194\n",
      "Ep 1 (Step 000320): Train loss 5.045, Val loss 5.183\n",
      "Ep 1 (Step 000330): Train loss 5.053, Val loss 5.178\n",
      "Ep 1 (Step 000340): Train loss 5.026, Val loss 5.178\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1775\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.719, Val loss 8.677\n",
      "Ep 1 (Step 000010): Train loss 6.613, Val loss 6.563\n",
      "Ep 1 (Step 000020): Train loss 6.533, Val loss 6.533\n",
      "Ep 1 (Step 000030): Train loss 6.447, Val loss 6.438\n",
      "Ep 1 (Step 000040): Train loss 6.154, Val loss 6.234\n",
      "Ep 1 (Step 000050): Train loss 5.962, Val loss 6.038\n",
      "Ep 1 (Step 000060): Train loss 5.910, Val loss 5.946\n",
      "Ep 1 (Step 000070): Train loss 5.776, Val loss 5.864\n",
      "Ep 1 (Step 000080): Train loss 5.699, Val loss 5.779\n",
      "Ep 1 (Step 000090): Train loss 5.620, Val loss 5.672\n",
      "Ep 1 (Step 000100): Train loss 5.547, Val loss 5.619\n",
      "Ep 1 (Step 000110): Train loss 5.450, Val loss 5.567\n",
      "Ep 1 (Step 000120): Train loss 5.490, Val loss 5.556\n",
      "Ep 1 (Step 000130): Train loss 5.536, Val loss 5.499\n",
      "Ep 1 (Step 000140): Train loss 5.373, Val loss 5.472\n",
      "Ep 1 (Step 000150): Train loss 5.388, Val loss 5.481\n",
      "Ep 1 (Step 000160): Train loss 5.339, Val loss 5.454\n",
      "Ep 1 (Step 000170): Train loss 5.281, Val loss 5.439\n",
      "Ep 1 (Step 000180): Train loss 5.292, Val loss 5.402\n",
      "Ep 1 (Step 000190): Train loss 5.251, Val loss 5.364\n",
      "Ep 1 (Step 000200): Train loss 5.202, Val loss 5.365\n",
      "Ep 1 (Step 000210): Train loss 5.240, Val loss 5.336\n",
      "Ep 1 (Step 000220): Train loss 5.224, Val loss 5.303\n",
      "Ep 1 (Step 000230): Train loss 5.153, Val loss 5.306\n",
      "Ep 1 (Step 000240): Train loss 5.171, Val loss 5.305\n",
      "Ep 1 (Step 000250): Train loss 5.175, Val loss 5.283\n",
      "Ep 1 (Step 000260): Train loss 5.192, Val loss 5.264\n",
      "Ep 1 (Step 000270): Train loss 5.040, Val loss 5.251\n",
      "Ep 1 (Step 000280): Train loss 5.054, Val loss 5.240\n",
      "Ep 1 (Step 000290): Train loss 5.087, Val loss 5.236\n",
      "Ep 1 (Step 000300): Train loss 5.035, Val loss 5.229\n",
      "Ep 1 (Step 000310): Train loss 5.067, Val loss 5.195\n",
      "Ep 1 (Step 000320): Train loss 4.961, Val loss 5.199\n",
      "Ep 1 (Step 000330): Train loss 5.003, Val loss 5.201\n",
      "Ep 1 (Step 000340): Train loss 5.094, Val loss 5.206\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2060\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.656, Val loss 8.638\n",
      "Ep 1 (Step 000010): Train loss 6.581, Val loss 6.606\n",
      "Ep 1 (Step 000020): Train loss 6.530, Val loss 6.529\n",
      "Ep 1 (Step 000030): Train loss 6.474, Val loss 6.413\n",
      "Ep 1 (Step 000040): Train loss 6.159, Val loss 6.246\n",
      "Ep 1 (Step 000050): Train loss 5.990, Val loss 6.052\n",
      "Ep 1 (Step 000060): Train loss 5.942, Val loss 5.934\n",
      "Ep 1 (Step 000070): Train loss 5.846, Val loss 5.817\n",
      "Ep 1 (Step 000080): Train loss 5.650, Val loss 5.770\n",
      "Ep 1 (Step 000090): Train loss 5.569, Val loss 5.678\n",
      "Ep 1 (Step 000100): Train loss 5.560, Val loss 5.638\n",
      "Ep 1 (Step 000110): Train loss 5.505, Val loss 5.571\n",
      "Ep 1 (Step 000120): Train loss 5.485, Val loss 5.540\n",
      "Ep 1 (Step 000130): Train loss 5.453, Val loss 5.492\n",
      "Ep 1 (Step 000140): Train loss 5.369, Val loss 5.505\n",
      "Ep 1 (Step 000150): Train loss 5.305, Val loss 5.471\n",
      "Ep 1 (Step 000160): Train loss 5.329, Val loss 5.429\n",
      "Ep 1 (Step 000170): Train loss 5.263, Val loss 5.429\n",
      "Ep 1 (Step 000180): Train loss 5.312, Val loss 5.380\n",
      "Ep 1 (Step 000190): Train loss 5.192, Val loss 5.378\n",
      "Ep 1 (Step 000200): Train loss 5.198, Val loss 5.376\n",
      "Ep 1 (Step 000210): Train loss 5.232, Val loss 5.326\n",
      "Ep 1 (Step 000220): Train loss 5.227, Val loss 5.317\n",
      "Ep 1 (Step 000230): Train loss 5.174, Val loss 5.319\n",
      "Ep 1 (Step 000240): Train loss 5.164, Val loss 5.326\n",
      "Ep 1 (Step 000250): Train loss 5.157, Val loss 5.287\n",
      "Ep 1 (Step 000260): Train loss 5.116, Val loss 5.256\n",
      "Ep 1 (Step 000270): Train loss 5.118, Val loss 5.241\n",
      "Ep 1 (Step 000280): Train loss 5.156, Val loss 5.243\n",
      "Ep 1 (Step 000290): Train loss 5.092, Val loss 5.231\n",
      "Ep 1 (Step 000300): Train loss 5.120, Val loss 5.236\n",
      "Ep 1 (Step 000310): Train loss 5.031, Val loss 5.214\n",
      "Ep 1 (Step 000320): Train loss 4.995, Val loss 5.197\n",
      "Ep 1 (Step 000330): Train loss 5.080, Val loss 5.217\n",
      "Ep 1 (Step 000340): Train loss 5.009, Val loss 5.203\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2032\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.963, Val loss 8.950\n",
      "Ep 1 (Step 000010): Train loss 7.049, Val loss 7.043\n",
      "Ep 1 (Step 000020): Train loss 6.539, Val loss 6.513\n",
      "Ep 1 (Step 000030): Train loss 6.416, Val loss 6.431\n",
      "Ep 1 (Step 000040): Train loss 6.422, Val loss 6.375\n",
      "Ep 1 (Step 000050): Train loss 6.279, Val loss 6.279\n",
      "Ep 1 (Step 000060): Train loss 6.090, Val loss 6.139\n",
      "Ep 1 (Step 000070): Train loss 6.019, Val loss 6.045\n",
      "Ep 1 (Step 000080): Train loss 5.898, Val loss 5.944\n",
      "Ep 1 (Step 000090): Train loss 5.865, Val loss 5.889\n",
      "Ep 1 (Step 000100): Train loss 5.780, Val loss 5.815\n",
      "Ep 1 (Step 000110): Train loss 5.715, Val loss 5.756\n",
      "Ep 1 (Step 000120): Train loss 5.686, Val loss 5.717\n",
      "Ep 1 (Step 000130): Train loss 5.525, Val loss 5.674\n",
      "Ep 1 (Step 000140): Train loss 5.551, Val loss 5.633\n",
      "Ep 1 (Step 000150): Train loss 5.468, Val loss 5.599\n",
      "Ep 1 (Step 000160): Train loss 5.514, Val loss 5.585\n",
      "Ep 1 (Step 000170): Train loss 5.458, Val loss 5.551\n",
      "Ep 1 (Step 000180): Train loss 5.493, Val loss 5.522\n",
      "Ep 1 (Step 000190): Train loss 5.389, Val loss 5.499\n",
      "Ep 1 (Step 000200): Train loss 5.379, Val loss 5.485\n",
      "Ep 1 (Step 000210): Train loss 5.353, Val loss 5.464\n",
      "Ep 1 (Step 000220): Train loss 5.340, Val loss 5.441\n",
      "Ep 1 (Step 000230): Train loss 5.311, Val loss 5.417\n",
      "Ep 1 (Step 000240): Train loss 5.228, Val loss 5.415\n",
      "Ep 1 (Step 000250): Train loss 5.215, Val loss 5.390\n",
      "Ep 1 (Step 000260): Train loss 5.330, Val loss 5.374\n",
      "Ep 1 (Step 000270): Train loss 5.220, Val loss 5.350\n",
      "Ep 1 (Step 000280): Train loss 5.256, Val loss 5.347\n",
      "Ep 1 (Step 000290): Train loss 5.202, Val loss 5.330\n",
      "Ep 1 (Step 000300): Train loss 5.188, Val loss 5.332\n",
      "Ep 1 (Step 000310): Train loss 5.210, Val loss 5.313\n",
      "Ep 1 (Step 000320): Train loss 5.155, Val loss 5.328\n",
      "Ep 1 (Step 000330): Train loss 5.017, Val loss 5.316\n",
      "Ep 1 (Step 000340): Train loss 5.058, Val loss 5.292\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2917\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.976, Val loss 8.975\n",
      "Ep 1 (Step 000010): Train loss 7.039, Val loss 7.053\n",
      "Ep 1 (Step 000020): Train loss 6.587, Val loss 6.538\n",
      "Ep 1 (Step 000030): Train loss 6.449, Val loss 6.431\n",
      "Ep 1 (Step 000040): Train loss 6.398, Val loss 6.408\n",
      "Ep 1 (Step 000050): Train loss 6.302, Val loss 6.312\n",
      "Ep 1 (Step 000060): Train loss 6.142, Val loss 6.136\n",
      "Ep 1 (Step 000070): Train loss 5.984, Val loss 6.036\n",
      "Ep 1 (Step 000080): Train loss 5.964, Val loss 5.940\n",
      "Ep 1 (Step 000090): Train loss 5.891, Val loss 5.911\n",
      "Ep 1 (Step 000100): Train loss 5.785, Val loss 5.826\n",
      "Ep 1 (Step 000110): Train loss 5.738, Val loss 5.774\n",
      "Ep 1 (Step 000120): Train loss 5.680, Val loss 5.707\n",
      "Ep 1 (Step 000130): Train loss 5.602, Val loss 5.668\n",
      "Ep 1 (Step 000140): Train loss 5.515, Val loss 5.636\n",
      "Ep 1 (Step 000150): Train loss 5.506, Val loss 5.594\n",
      "Ep 1 (Step 000160): Train loss 5.508, Val loss 5.565\n",
      "Ep 1 (Step 000170): Train loss 5.476, Val loss 5.542\n",
      "Ep 1 (Step 000180): Train loss 5.472, Val loss 5.510\n",
      "Ep 1 (Step 000190): Train loss 5.380, Val loss 5.507\n",
      "Ep 1 (Step 000200): Train loss 5.336, Val loss 5.464\n",
      "Ep 1 (Step 000210): Train loss 5.404, Val loss 5.455\n",
      "Ep 1 (Step 000220): Train loss 5.359, Val loss 5.438\n",
      "Ep 1 (Step 000230): Train loss 5.251, Val loss 5.414\n",
      "Ep 1 (Step 000240): Train loss 5.316, Val loss 5.418\n",
      "Ep 1 (Step 000250): Train loss 5.147, Val loss 5.377\n",
      "Ep 1 (Step 000260): Train loss 5.165, Val loss 5.385\n",
      "Ep 1 (Step 000270): Train loss 5.238, Val loss 5.370\n",
      "Ep 1 (Step 000280): Train loss 5.244, Val loss 5.363\n",
      "Ep 1 (Step 000290): Train loss 5.118, Val loss 5.360\n",
      "Ep 1 (Step 000300): Train loss 5.214, Val loss 5.328\n",
      "Ep 1 (Step 000310): Train loss 5.204, Val loss 5.329\n",
      "Ep 1 (Step 000320): Train loss 5.097, Val loss 5.317\n",
      "Ep 1 (Step 000330): Train loss 5.151, Val loss 5.319\n",
      "Ep 1 (Step 000340): Train loss 5.163, Val loss 5.307\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.3073\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.006, Val loss 8.980\n",
      "Ep 1 (Step 000010): Train loss 7.057, Val loss 7.049\n",
      "Ep 1 (Step 000020): Train loss 6.554, Val loss 6.518\n",
      "Ep 1 (Step 000030): Train loss 6.487, Val loss 6.422\n",
      "Ep 1 (Step 000040): Train loss 6.388, Val loss 6.378\n",
      "Ep 1 (Step 000050): Train loss 6.309, Val loss 6.280\n",
      "Ep 1 (Step 000060): Train loss 6.086, Val loss 6.135\n",
      "Ep 1 (Step 000070): Train loss 6.006, Val loss 6.018\n",
      "Ep 1 (Step 000080): Train loss 5.963, Val loss 5.937\n",
      "Ep 1 (Step 000090): Train loss 5.855, Val loss 5.873\n",
      "Ep 1 (Step 000100): Train loss 5.793, Val loss 5.819\n",
      "Ep 1 (Step 000110): Train loss 5.699, Val loss 5.734\n",
      "Ep 1 (Step 000120): Train loss 5.611, Val loss 5.701\n",
      "Ep 1 (Step 000130): Train loss 5.614, Val loss 5.667\n",
      "Ep 1 (Step 000140): Train loss 5.576, Val loss 5.627\n",
      "Ep 1 (Step 000150): Train loss 5.518, Val loss 5.582\n",
      "Ep 1 (Step 000160): Train loss 5.504, Val loss 5.557\n",
      "Ep 1 (Step 000170): Train loss 5.414, Val loss 5.533\n",
      "Ep 1 (Step 000180): Train loss 5.411, Val loss 5.516\n",
      "Ep 1 (Step 000190): Train loss 5.394, Val loss 5.487\n",
      "Ep 1 (Step 000200): Train loss 5.356, Val loss 5.457\n",
      "Ep 1 (Step 000210): Train loss 5.401, Val loss 5.445\n",
      "Ep 1 (Step 000220): Train loss 5.342, Val loss 5.415\n",
      "Ep 1 (Step 000230): Train loss 5.193, Val loss 5.401\n",
      "Ep 1 (Step 000240): Train loss 5.212, Val loss 5.369\n",
      "Ep 1 (Step 000250): Train loss 5.342, Val loss 5.358\n",
      "Ep 1 (Step 000260): Train loss 5.311, Val loss 5.358\n",
      "Ep 1 (Step 000270): Train loss 5.208, Val loss 5.357\n",
      "Ep 1 (Step 000280): Train loss 5.189, Val loss 5.346\n",
      "Ep 1 (Step 000290): Train loss 5.197, Val loss 5.334\n",
      "Ep 1 (Step 000300): Train loss 5.305, Val loss 5.319\n",
      "Ep 1 (Step 000310): Train loss 5.132, Val loss 5.307\n",
      "Ep 1 (Step 000320): Train loss 5.280, Val loss 5.308\n",
      "Ep 1 (Step 000330): Train loss 5.108, Val loss 5.269\n",
      "Ep 1 (Step 000340): Train loss 5.104, Val loss 5.291\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2907\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.965, Val loss 8.963\n",
      "Ep 1 (Step 000010): Train loss 7.046, Val loss 6.990\n",
      "Ep 1 (Step 000020): Train loss 6.550, Val loss 6.499\n",
      "Ep 1 (Step 000030): Train loss 6.506, Val loss 6.428\n",
      "Ep 1 (Step 000040): Train loss 6.405, Val loss 6.398\n",
      "Ep 1 (Step 000050): Train loss 6.331, Val loss 6.321\n",
      "Ep 1 (Step 000060): Train loss 6.210, Val loss 6.179\n",
      "Ep 1 (Step 000070): Train loss 6.061, Val loss 6.033\n",
      "Ep 1 (Step 000080): Train loss 5.983, Val loss 5.947\n",
      "Ep 1 (Step 000090): Train loss 5.774, Val loss 5.855\n",
      "Ep 1 (Step 000100): Train loss 5.827, Val loss 5.794\n",
      "Ep 1 (Step 000110): Train loss 5.647, Val loss 5.721\n",
      "Ep 1 (Step 000120): Train loss 5.634, Val loss 5.682\n",
      "Ep 1 (Step 000130): Train loss 5.668, Val loss 5.647\n",
      "Ep 1 (Step 000140): Train loss 5.616, Val loss 5.617\n",
      "Ep 1 (Step 000150): Train loss 5.566, Val loss 5.589\n",
      "Ep 1 (Step 000160): Train loss 5.477, Val loss 5.551\n",
      "Ep 1 (Step 000170): Train loss 5.459, Val loss 5.528\n",
      "Ep 1 (Step 000180): Train loss 5.378, Val loss 5.484\n",
      "Ep 1 (Step 000190): Train loss 5.381, Val loss 5.476\n",
      "Ep 1 (Step 000200): Train loss 5.416, Val loss 5.452\n",
      "Ep 1 (Step 000210): Train loss 5.319, Val loss 5.416\n",
      "Ep 1 (Step 000220): Train loss 5.322, Val loss 5.405\n",
      "Ep 1 (Step 000230): Train loss 5.306, Val loss 5.408\n",
      "Ep 1 (Step 000240): Train loss 5.311, Val loss 5.400\n",
      "Ep 1 (Step 000250): Train loss 5.227, Val loss 5.365\n",
      "Ep 1 (Step 000260): Train loss 5.285, Val loss 5.348\n",
      "Ep 1 (Step 000270): Train loss 5.103, Val loss 5.343\n",
      "Ep 1 (Step 000280): Train loss 5.170, Val loss 5.346\n",
      "Ep 1 (Step 000290): Train loss 5.238, Val loss 5.317\n",
      "Ep 1 (Step 000300): Train loss 5.089, Val loss 5.299\n",
      "Ep 1 (Step 000310): Train loss 5.170, Val loss 5.290\n",
      "Ep 1 (Step 000320): Train loss 5.169, Val loss 5.290\n",
      "Ep 1 (Step 000330): Train loss 5.171, Val loss 5.267\n",
      "Ep 1 (Step 000340): Train loss 5.118, Val loss 5.255\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2551\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 9.004, Val loss 9.004\n",
      "Ep 1 (Step 000010): Train loss 7.067, Val loss 7.052\n",
      "Ep 1 (Step 000020): Train loss 6.510, Val loss 6.510\n",
      "Ep 1 (Step 000030): Train loss 6.432, Val loss 6.426\n",
      "Ep 1 (Step 000040): Train loss 6.370, Val loss 6.385\n",
      "Ep 1 (Step 000050): Train loss 6.352, Val loss 6.312\n",
      "Ep 1 (Step 000060): Train loss 6.243, Val loss 6.160\n",
      "Ep 1 (Step 000070): Train loss 6.016, Val loss 6.051\n",
      "Ep 1 (Step 000080): Train loss 5.840, Val loss 5.953\n",
      "Ep 1 (Step 000090): Train loss 5.793, Val loss 5.882\n",
      "Ep 1 (Step 000100): Train loss 5.842, Val loss 5.794\n",
      "Ep 1 (Step 000110): Train loss 5.737, Val loss 5.751\n",
      "Ep 1 (Step 000120): Train loss 5.656, Val loss 5.694\n",
      "Ep 1 (Step 000130): Train loss 5.570, Val loss 5.662\n",
      "Ep 1 (Step 000140): Train loss 5.597, Val loss 5.613\n",
      "Ep 1 (Step 000150): Train loss 5.475, Val loss 5.575\n",
      "Ep 1 (Step 000160): Train loss 5.490, Val loss 5.557\n",
      "Ep 1 (Step 000170): Train loss 5.410, Val loss 5.515\n",
      "Ep 1 (Step 000180): Train loss 5.393, Val loss 5.484\n",
      "Ep 1 (Step 000190): Train loss 5.329, Val loss 5.470\n",
      "Ep 1 (Step 000200): Train loss 5.458, Val loss 5.451\n",
      "Ep 1 (Step 000210): Train loss 5.384, Val loss 5.453\n",
      "Ep 1 (Step 000220): Train loss 5.267, Val loss 5.408\n",
      "Ep 1 (Step 000230): Train loss 5.288, Val loss 5.380\n",
      "Ep 1 (Step 000240): Train loss 5.298, Val loss 5.369\n",
      "Ep 1 (Step 000250): Train loss 5.237, Val loss 5.354\n",
      "Ep 1 (Step 000260): Train loss 5.250, Val loss 5.353\n",
      "Ep 1 (Step 000270): Train loss 5.223, Val loss 5.341\n",
      "Ep 1 (Step 000280): Train loss 5.170, Val loss 5.323\n",
      "Ep 1 (Step 000290): Train loss 5.186, Val loss 5.306\n",
      "Ep 1 (Step 000300): Train loss 5.138, Val loss 5.302\n",
      "Ep 1 (Step 000310): Train loss 5.119, Val loss 5.285\n",
      "Ep 1 (Step 000320): Train loss 5.158, Val loss 5.265\n",
      "Ep 1 (Step 000330): Train loss 5.174, Val loss 5.270\n",
      "Ep 1 (Step 000340): Train loss 5.134, Val loss 5.247\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2471\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.955, Val loss 8.948\n",
      "Ep 1 (Step 000010): Train loss 7.059, Val loss 7.060\n",
      "Ep 1 (Step 000020): Train loss 6.545, Val loss 6.535\n",
      "Ep 1 (Step 000030): Train loss 6.462, Val loss 6.446\n",
      "Ep 1 (Step 000040): Train loss 6.339, Val loss 6.412\n",
      "Ep 1 (Step 000050): Train loss 6.378, Val loss 6.302\n",
      "Ep 1 (Step 000060): Train loss 6.133, Val loss 6.174\n",
      "Ep 1 (Step 000070): Train loss 5.940, Val loss 6.064\n",
      "Ep 1 (Step 000080): Train loss 5.949, Val loss 5.960\n",
      "Ep 1 (Step 000090): Train loss 5.854, Val loss 5.888\n",
      "Ep 1 (Step 000100): Train loss 5.767, Val loss 5.833\n",
      "Ep 1 (Step 000110): Train loss 5.742, Val loss 5.760\n",
      "Ep 1 (Step 000120): Train loss 5.574, Val loss 5.703\n",
      "Ep 1 (Step 000130): Train loss 5.593, Val loss 5.662\n",
      "Ep 1 (Step 000140): Train loss 5.512, Val loss 5.608\n",
      "Ep 1 (Step 000150): Train loss 5.546, Val loss 5.587\n",
      "Ep 1 (Step 000160): Train loss 5.600, Val loss 5.559\n",
      "Ep 1 (Step 000170): Train loss 5.476, Val loss 5.547\n",
      "Ep 1 (Step 000180): Train loss 5.404, Val loss 5.533\n",
      "Ep 1 (Step 000190): Train loss 5.300, Val loss 5.523\n",
      "Ep 1 (Step 000200): Train loss 5.329, Val loss 5.480\n",
      "Ep 1 (Step 000210): Train loss 5.402, Val loss 5.460\n",
      "Ep 1 (Step 000220): Train loss 5.255, Val loss 5.427\n",
      "Ep 1 (Step 000230): Train loss 5.341, Val loss 5.426\n",
      "Ep 1 (Step 000240): Train loss 5.226, Val loss 5.386\n",
      "Ep 1 (Step 000250): Train loss 5.287, Val loss 5.378\n",
      "Ep 1 (Step 000260): Train loss 5.177, Val loss 5.372\n",
      "Ep 1 (Step 000270): Train loss 5.177, Val loss 5.347\n",
      "Ep 1 (Step 000280): Train loss 5.236, Val loss 5.332\n",
      "Ep 1 (Step 000290): Train loss 5.247, Val loss 5.324\n",
      "Ep 1 (Step 000300): Train loss 5.162, Val loss 5.314\n",
      "Ep 1 (Step 000310): Train loss 5.207, Val loss 5.291\n",
      "Ep 1 (Step 000320): Train loss 5.113, Val loss 5.281\n",
      "Ep 1 (Step 000330): Train loss 5.087, Val loss 5.262\n",
      "Ep 1 (Step 000340): Train loss 5.101, Val loss 5.258\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2585\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.635, Val loss 8.631\n",
      "Ep 1 (Step 000010): Train loss 6.590, Val loss 6.569\n",
      "Ep 1 (Step 000020): Train loss 6.515, Val loss 6.516\n",
      "Ep 1 (Step 000030): Train loss 6.493, Val loss 6.422\n",
      "Ep 1 (Step 000040): Train loss 6.178, Val loss 6.240\n",
      "Ep 1 (Step 000050): Train loss 6.024, Val loss 6.091\n",
      "Ep 1 (Step 000060): Train loss 5.913, Val loss 5.926\n",
      "Ep 1 (Step 000070): Train loss 5.852, Val loss 5.860\n",
      "Ep 1 (Step 000080): Train loss 5.664, Val loss 5.775\n",
      "Ep 1 (Step 000090): Train loss 5.611, Val loss 5.700\n",
      "Ep 1 (Step 000100): Train loss 5.665, Val loss 5.631\n",
      "Ep 1 (Step 000110): Train loss 5.560, Val loss 5.573\n",
      "Ep 1 (Step 000120): Train loss 5.496, Val loss 5.549\n",
      "Ep 1 (Step 000130): Train loss 5.553, Val loss 5.544\n",
      "Ep 1 (Step 000140): Train loss 5.461, Val loss 5.496\n",
      "Ep 1 (Step 000150): Train loss 5.419, Val loss 5.471\n",
      "Ep 1 (Step 000160): Train loss 5.348, Val loss 5.460\n",
      "Ep 1 (Step 000170): Train loss 5.231, Val loss 5.436\n",
      "Ep 1 (Step 000180): Train loss 5.351, Val loss 5.406\n",
      "Ep 1 (Step 000190): Train loss 5.308, Val loss 5.425\n",
      "Ep 1 (Step 000200): Train loss 5.245, Val loss 5.370\n",
      "Ep 1 (Step 000210): Train loss 5.213, Val loss 5.345\n",
      "Ep 1 (Step 000220): Train loss 5.227, Val loss 5.350\n",
      "Ep 1 (Step 000230): Train loss 5.175, Val loss 5.328\n",
      "Ep 1 (Step 000240): Train loss 5.221, Val loss 5.317\n",
      "Ep 1 (Step 000250): Train loss 5.172, Val loss 5.299\n",
      "Ep 1 (Step 000260): Train loss 5.080, Val loss 5.291\n",
      "Ep 1 (Step 000270): Train loss 5.222, Val loss 5.308\n",
      "Ep 1 (Step 000280): Train loss 5.136, Val loss 5.286\n",
      "Ep 1 (Step 000290): Train loss 5.076, Val loss 5.283\n",
      "Ep 1 (Step 000300): Train loss 5.134, Val loss 5.270\n",
      "Ep 1 (Step 000310): Train loss 5.149, Val loss 5.256\n",
      "Ep 1 (Step 000320): Train loss 5.095, Val loss 5.253\n",
      "Ep 1 (Step 000330): Train loss 5.089, Val loss 5.228\n",
      "Ep 1 (Step 000340): Train loss 5.010, Val loss 5.241\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2411\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.714, Val loss 8.709\n",
      "Ep 1 (Step 000010): Train loss 6.578, Val loss 6.559\n",
      "Ep 1 (Step 000020): Train loss 6.631, Val loss 6.526\n",
      "Ep 1 (Step 000030): Train loss 6.314, Val loss 6.337\n",
      "Ep 1 (Step 000040): Train loss 6.286, Val loss 6.141\n",
      "Ep 1 (Step 000050): Train loss 6.004, Val loss 6.042\n",
      "Ep 1 (Step 000060): Train loss 5.798, Val loss 5.924\n",
      "Ep 1 (Step 000070): Train loss 5.729, Val loss 5.841\n",
      "Ep 1 (Step 000080): Train loss 5.733, Val loss 5.750\n",
      "Ep 1 (Step 000090): Train loss 5.600, Val loss 5.678\n",
      "Ep 1 (Step 000100): Train loss 5.472, Val loss 5.625\n",
      "Ep 1 (Step 000110): Train loss 5.446, Val loss 5.574\n",
      "Ep 1 (Step 000120): Train loss 5.502, Val loss 5.551\n",
      "Ep 1 (Step 000130): Train loss 5.472, Val loss 5.521\n",
      "Ep 1 (Step 000140): Train loss 5.355, Val loss 5.498\n",
      "Ep 1 (Step 000150): Train loss 5.295, Val loss 5.480\n",
      "Ep 1 (Step 000160): Train loss 5.372, Val loss 5.440\n",
      "Ep 1 (Step 000170): Train loss 5.271, Val loss 5.446\n",
      "Ep 1 (Step 000180): Train loss 5.303, Val loss 5.415\n",
      "Ep 1 (Step 000190): Train loss 5.205, Val loss 5.406\n",
      "Ep 1 (Step 000200): Train loss 5.249, Val loss 5.396\n",
      "Ep 1 (Step 000210): Train loss 5.228, Val loss 5.362\n",
      "Ep 1 (Step 000220): Train loss 5.194, Val loss 5.346\n",
      "Ep 1 (Step 000230): Train loss 5.197, Val loss 5.333\n",
      "Ep 1 (Step 000240): Train loss 5.028, Val loss 5.335\n",
      "Ep 1 (Step 000250): Train loss 5.110, Val loss 5.312\n",
      "Ep 1 (Step 000260): Train loss 5.259, Val loss 5.317\n",
      "Ep 1 (Step 000270): Train loss 5.144, Val loss 5.313\n",
      "Ep 1 (Step 000280): Train loss 5.171, Val loss 5.284\n",
      "Ep 1 (Step 000290): Train loss 5.175, Val loss 5.278\n",
      "Ep 1 (Step 000300): Train loss 5.151, Val loss 5.244\n",
      "Ep 1 (Step 000310): Train loss 5.052, Val loss 5.245\n",
      "Ep 1 (Step 000320): Train loss 5.034, Val loss 5.223\n",
      "Ep 1 (Step 000330): Train loss 5.120, Val loss 5.216\n",
      "Ep 1 (Step 000340): Train loss 5.065, Val loss 5.207\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2066\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.673, Val loss 8.654\n",
      "Ep 1 (Step 000010): Train loss 6.613, Val loss 6.567\n",
      "Ep 1 (Step 000020): Train loss 6.548, Val loss 6.539\n",
      "Ep 1 (Step 000030): Train loss 6.410, Val loss 6.372\n",
      "Ep 1 (Step 000040): Train loss 6.114, Val loss 6.153\n",
      "Ep 1 (Step 000050): Train loss 6.044, Val loss 6.044\n",
      "Ep 1 (Step 000060): Train loss 5.913, Val loss 5.937\n",
      "Ep 1 (Step 000070): Train loss 5.779, Val loss 5.801\n",
      "Ep 1 (Step 000080): Train loss 5.579, Val loss 5.721\n",
      "Ep 1 (Step 000090): Train loss 5.601, Val loss 5.692\n",
      "Ep 1 (Step 000100): Train loss 5.561, Val loss 5.639\n",
      "Ep 1 (Step 000110): Train loss 5.536, Val loss 5.584\n",
      "Ep 1 (Step 000120): Train loss 5.498, Val loss 5.522\n",
      "Ep 1 (Step 000130): Train loss 5.304, Val loss 5.514\n",
      "Ep 1 (Step 000140): Train loss 5.440, Val loss 5.503\n",
      "Ep 1 (Step 000150): Train loss 5.383, Val loss 5.475\n",
      "Ep 1 (Step 000160): Train loss 5.321, Val loss 5.446\n",
      "Ep 1 (Step 000170): Train loss 5.253, Val loss 5.431\n",
      "Ep 1 (Step 000180): Train loss 5.270, Val loss 5.397\n",
      "Ep 1 (Step 000190): Train loss 5.325, Val loss 5.393\n",
      "Ep 1 (Step 000200): Train loss 5.233, Val loss 5.402\n",
      "Ep 1 (Step 000210): Train loss 5.176, Val loss 5.368\n",
      "Ep 1 (Step 000220): Train loss 5.181, Val loss 5.339\n",
      "Ep 1 (Step 000230): Train loss 5.167, Val loss 5.354\n",
      "Ep 1 (Step 000240): Train loss 5.198, Val loss 5.351\n",
      "Ep 1 (Step 000250): Train loss 5.193, Val loss 5.327\n",
      "Ep 1 (Step 000260): Train loss 5.095, Val loss 5.322\n",
      "Ep 1 (Step 000270): Train loss 4.991, Val loss 5.296\n",
      "Ep 1 (Step 000280): Train loss 5.128, Val loss 5.277\n",
      "Ep 1 (Step 000290): Train loss 5.065, Val loss 5.279\n",
      "Ep 1 (Step 000300): Train loss 5.061, Val loss 5.254\n",
      "Ep 1 (Step 000310): Train loss 5.184, Val loss 5.247\n",
      "Ep 1 (Step 000320): Train loss 5.149, Val loss 5.269\n",
      "Ep 1 (Step 000330): Train loss 5.002, Val loss 5.251\n",
      "Ep 1 (Step 000340): Train loss 4.991, Val loss 5.209\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2091\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.635, Val loss 8.622\n",
      "Ep 1 (Step 000010): Train loss 6.546, Val loss 6.572\n",
      "Ep 1 (Step 000020): Train loss 6.501, Val loss 6.491\n",
      "Ep 1 (Step 000030): Train loss 6.431, Val loss 6.395\n",
      "Ep 1 (Step 000040): Train loss 6.218, Val loss 6.195\n",
      "Ep 1 (Step 000050): Train loss 5.993, Val loss 6.031\n",
      "Ep 1 (Step 000060): Train loss 5.844, Val loss 5.899\n",
      "Ep 1 (Step 000070): Train loss 5.772, Val loss 5.806\n",
      "Ep 1 (Step 000080): Train loss 5.679, Val loss 5.728\n",
      "Ep 1 (Step 000090): Train loss 5.605, Val loss 5.674\n",
      "Ep 1 (Step 000100): Train loss 5.557, Val loss 5.591\n",
      "Ep 1 (Step 000110): Train loss 5.519, Val loss 5.586\n",
      "Ep 1 (Step 000120): Train loss 5.426, Val loss 5.555\n",
      "Ep 1 (Step 000130): Train loss 5.387, Val loss 5.495\n",
      "Ep 1 (Step 000140): Train loss 5.341, Val loss 5.456\n",
      "Ep 1 (Step 000150): Train loss 5.387, Val loss 5.452\n",
      "Ep 1 (Step 000160): Train loss 5.362, Val loss 5.423\n",
      "Ep 1 (Step 000170): Train loss 5.218, Val loss 5.398\n",
      "Ep 1 (Step 000180): Train loss 5.248, Val loss 5.379\n",
      "Ep 1 (Step 000190): Train loss 5.229, Val loss 5.375\n",
      "Ep 1 (Step 000200): Train loss 5.199, Val loss 5.351\n",
      "Ep 1 (Step 000210): Train loss 5.212, Val loss 5.335\n",
      "Ep 1 (Step 000220): Train loss 5.163, Val loss 5.326\n",
      "Ep 1 (Step 000230): Train loss 5.120, Val loss 5.314\n",
      "Ep 1 (Step 000240): Train loss 5.213, Val loss 5.323\n",
      "Ep 1 (Step 000250): Train loss 5.160, Val loss 5.295\n",
      "Ep 1 (Step 000260): Train loss 5.239, Val loss 5.256\n",
      "Ep 1 (Step 000270): Train loss 5.068, Val loss 5.276\n",
      "Ep 1 (Step 000280): Train loss 5.016, Val loss 5.239\n",
      "Ep 1 (Step 000290): Train loss 4.981, Val loss 5.240\n",
      "Ep 1 (Step 000300): Train loss 5.008, Val loss 5.193\n",
      "Ep 1 (Step 000310): Train loss 5.079, Val loss 5.193\n",
      "Ep 1 (Step 000320): Train loss 5.039, Val loss 5.200\n",
      "Ep 1 (Step 000330): Train loss 5.062, Val loss 5.180\n",
      "Ep 1 (Step 000340): Train loss 5.045, Val loss 5.183\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1829\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.595, Val loss 8.547\n",
      "Ep 1 (Step 000010): Train loss 6.622, Val loss 6.556\n",
      "Ep 1 (Step 000020): Train loss 6.571, Val loss 6.490\n",
      "Ep 1 (Step 000030): Train loss 6.407, Val loss 6.397\n",
      "Ep 1 (Step 000040): Train loss 6.171, Val loss 6.201\n",
      "Ep 1 (Step 000050): Train loss 5.942, Val loss 5.995\n",
      "Ep 1 (Step 000060): Train loss 5.858, Val loss 5.887\n",
      "Ep 1 (Step 000070): Train loss 5.809, Val loss 5.822\n",
      "Ep 1 (Step 000080): Train loss 5.697, Val loss 5.725\n",
      "Ep 1 (Step 000090): Train loss 5.624, Val loss 5.659\n",
      "Ep 1 (Step 000100): Train loss 5.566, Val loss 5.623\n",
      "Ep 1 (Step 000110): Train loss 5.563, Val loss 5.583\n",
      "Ep 1 (Step 000120): Train loss 5.454, Val loss 5.532\n",
      "Ep 1 (Step 000130): Train loss 5.418, Val loss 5.482\n",
      "Ep 1 (Step 000140): Train loss 5.468, Val loss 5.456\n",
      "Ep 1 (Step 000150): Train loss 5.357, Val loss 5.439\n",
      "Ep 1 (Step 000160): Train loss 5.324, Val loss 5.430\n",
      "Ep 1 (Step 000170): Train loss 5.277, Val loss 5.439\n",
      "Ep 1 (Step 000180): Train loss 5.295, Val loss 5.388\n",
      "Ep 1 (Step 000190): Train loss 5.318, Val loss 5.383\n",
      "Ep 1 (Step 000200): Train loss 5.176, Val loss 5.350\n",
      "Ep 1 (Step 000210): Train loss 5.180, Val loss 5.339\n",
      "Ep 1 (Step 000220): Train loss 5.170, Val loss 5.291\n",
      "Ep 1 (Step 000230): Train loss 5.193, Val loss 5.312\n",
      "Ep 1 (Step 000240): Train loss 5.078, Val loss 5.286\n",
      "Ep 1 (Step 000250): Train loss 5.151, Val loss 5.308\n",
      "Ep 1 (Step 000260): Train loss 5.069, Val loss 5.266\n",
      "Ep 1 (Step 000270): Train loss 5.077, Val loss 5.243\n",
      "Ep 1 (Step 000280): Train loss 5.076, Val loss 5.224\n",
      "Ep 1 (Step 000290): Train loss 5.072, Val loss 5.222\n",
      "Ep 1 (Step 000300): Train loss 5.038, Val loss 5.209\n",
      "Ep 1 (Step 000310): Train loss 5.062, Val loss 5.225\n",
      "Ep 1 (Step 000320): Train loss 4.998, Val loss 5.204\n",
      "Ep 1 (Step 000330): Train loss 4.963, Val loss 5.190\n",
      "Ep 1 (Step 000340): Train loss 4.985, Val loss 5.182\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1825\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.667, Val loss 8.622\n",
      "Ep 1 (Step 000010): Train loss 6.604, Val loss 6.585\n",
      "Ep 1 (Step 000020): Train loss 6.528, Val loss 6.519\n",
      "Ep 1 (Step 000030): Train loss 6.410, Val loss 6.401\n",
      "Ep 1 (Step 000040): Train loss 6.231, Val loss 6.214\n",
      "Ep 1 (Step 000050): Train loss 6.006, Val loss 6.050\n",
      "Ep 1 (Step 000060): Train loss 5.936, Val loss 5.966\n",
      "Ep 1 (Step 000070): Train loss 5.749, Val loss 5.805\n",
      "Ep 1 (Step 000080): Train loss 5.613, Val loss 5.729\n",
      "Ep 1 (Step 000090): Train loss 5.594, Val loss 5.662\n",
      "Ep 1 (Step 000100): Train loss 5.462, Val loss 5.616\n",
      "Ep 1 (Step 000110): Train loss 5.470, Val loss 5.580\n",
      "Ep 1 (Step 000120): Train loss 5.510, Val loss 5.566\n",
      "Ep 1 (Step 000130): Train loss 5.409, Val loss 5.520\n",
      "Ep 1 (Step 000140): Train loss 5.295, Val loss 5.455\n",
      "Ep 1 (Step 000150): Train loss 5.285, Val loss 5.475\n",
      "Ep 1 (Step 000160): Train loss 5.383, Val loss 5.435\n",
      "Ep 1 (Step 000170): Train loss 5.310, Val loss 5.421\n",
      "Ep 1 (Step 000180): Train loss 5.244, Val loss 5.383\n",
      "Ep 1 (Step 000190): Train loss 5.226, Val loss 5.370\n",
      "Ep 1 (Step 000200): Train loss 5.184, Val loss 5.356\n",
      "Ep 1 (Step 000210): Train loss 5.210, Val loss 5.361\n",
      "Ep 1 (Step 000220): Train loss 5.146, Val loss 5.323\n",
      "Ep 1 (Step 000230): Train loss 5.181, Val loss 5.300\n",
      "Ep 1 (Step 000240): Train loss 5.217, Val loss 5.296\n",
      "Ep 1 (Step 000250): Train loss 5.148, Val loss 5.292\n",
      "Ep 1 (Step 000260): Train loss 5.115, Val loss 5.326\n",
      "Ep 1 (Step 000270): Train loss 5.038, Val loss 5.250\n",
      "Ep 1 (Step 000280): Train loss 5.041, Val loss 5.246\n",
      "Ep 1 (Step 000290): Train loss 5.038, Val loss 5.231\n",
      "Ep 1 (Step 000300): Train loss 5.133, Val loss 5.221\n",
      "Ep 1 (Step 000310): Train loss 5.037, Val loss 5.210\n",
      "Ep 1 (Step 000320): Train loss 5.029, Val loss 5.208\n",
      "Ep 1 (Step 000330): Train loss 4.979, Val loss 5.183\n",
      "Ep 1 (Step 000340): Train loss 4.986, Val loss 5.158\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1583\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.957, Val loss 8.925\n",
      "Ep 1 (Step 000010): Train loss 7.004, Val loss 7.003\n",
      "Ep 1 (Step 000020): Train loss 6.442, Val loss 6.491\n",
      "Ep 1 (Step 000030): Train loss 6.392, Val loss 6.399\n",
      "Ep 1 (Step 000040): Train loss 6.335, Val loss 6.307\n",
      "Ep 1 (Step 000050): Train loss 6.190, Val loss 6.174\n",
      "Ep 1 (Step 000060): Train loss 6.036, Val loss 6.064\n",
      "Ep 1 (Step 000070): Train loss 5.920, Val loss 5.960\n",
      "Ep 1 (Step 000080): Train loss 5.804, Val loss 5.904\n",
      "Ep 1 (Step 000090): Train loss 5.752, Val loss 5.824\n",
      "Ep 1 (Step 000100): Train loss 5.724, Val loss 5.745\n",
      "Ep 1 (Step 000110): Train loss 5.564, Val loss 5.696\n",
      "Ep 1 (Step 000120): Train loss 5.470, Val loss 5.633\n",
      "Ep 1 (Step 000130): Train loss 5.524, Val loss 5.597\n",
      "Ep 1 (Step 000140): Train loss 5.507, Val loss 5.561\n",
      "Ep 1 (Step 000150): Train loss 5.404, Val loss 5.517\n",
      "Ep 1 (Step 000160): Train loss 5.439, Val loss 5.506\n",
      "Ep 1 (Step 000170): Train loss 5.418, Val loss 5.473\n",
      "Ep 1 (Step 000180): Train loss 5.388, Val loss 5.458\n",
      "Ep 1 (Step 000190): Train loss 5.339, Val loss 5.422\n",
      "Ep 1 (Step 000200): Train loss 5.266, Val loss 5.413\n",
      "Ep 1 (Step 000210): Train loss 5.331, Val loss 5.399\n",
      "Ep 1 (Step 000220): Train loss 5.217, Val loss 5.383\n",
      "Ep 1 (Step 000230): Train loss 5.249, Val loss 5.364\n",
      "Ep 1 (Step 000240): Train loss 5.138, Val loss 5.356\n",
      "Ep 1 (Step 000250): Train loss 5.197, Val loss 5.335\n",
      "Ep 1 (Step 000260): Train loss 5.261, Val loss 5.322\n",
      "Ep 1 (Step 000270): Train loss 5.159, Val loss 5.308\n",
      "Ep 1 (Step 000280): Train loss 5.189, Val loss 5.297\n",
      "Ep 1 (Step 000290): Train loss 5.198, Val loss 5.285\n",
      "Ep 1 (Step 000300): Train loss 5.138, Val loss 5.276\n",
      "Ep 1 (Step 000310): Train loss 5.157, Val loss 5.280\n",
      "Ep 1 (Step 000320): Train loss 5.077, Val loss 5.256\n",
      "Ep 1 (Step 000330): Train loss 5.013, Val loss 5.255\n",
      "Ep 1 (Step 000340): Train loss 5.032, Val loss 5.237\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2367\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.955, Val loss 8.938\n",
      "Ep 1 (Step 000010): Train loss 7.036, Val loss 6.986\n",
      "Ep 1 (Step 000020): Train loss 6.518, Val loss 6.487\n",
      "Ep 1 (Step 000030): Train loss 6.411, Val loss 6.421\n",
      "Ep 1 (Step 000040): Train loss 6.398, Val loss 6.357\n",
      "Ep 1 (Step 000050): Train loss 6.147, Val loss 6.152\n",
      "Ep 1 (Step 000060): Train loss 5.970, Val loss 6.072\n",
      "Ep 1 (Step 000070): Train loss 6.000, Val loss 5.956\n",
      "Ep 1 (Step 000080): Train loss 5.784, Val loss 5.862\n",
      "Ep 1 (Step 000090): Train loss 5.789, Val loss 5.788\n",
      "Ep 1 (Step 000100): Train loss 5.634, Val loss 5.736\n",
      "Ep 1 (Step 000110): Train loss 5.604, Val loss 5.675\n",
      "Ep 1 (Step 000120): Train loss 5.524, Val loss 5.622\n",
      "Ep 1 (Step 000130): Train loss 5.437, Val loss 5.597\n",
      "Ep 1 (Step 000140): Train loss 5.610, Val loss 5.551\n",
      "Ep 1 (Step 000150): Train loss 5.393, Val loss 5.514\n",
      "Ep 1 (Step 000160): Train loss 5.459, Val loss 5.511\n",
      "Ep 1 (Step 000170): Train loss 5.402, Val loss 5.481\n",
      "Ep 1 (Step 000180): Train loss 5.406, Val loss 5.451\n",
      "Ep 1 (Step 000190): Train loss 5.219, Val loss 5.419\n",
      "Ep 1 (Step 000200): Train loss 5.324, Val loss 5.398\n",
      "Ep 1 (Step 000210): Train loss 5.260, Val loss 5.388\n",
      "Ep 1 (Step 000220): Train loss 5.224, Val loss 5.376\n",
      "Ep 1 (Step 000230): Train loss 5.232, Val loss 5.368\n",
      "Ep 1 (Step 000240): Train loss 5.162, Val loss 5.333\n",
      "Ep 1 (Step 000250): Train loss 5.188, Val loss 5.323\n",
      "Ep 1 (Step 000260): Train loss 5.156, Val loss 5.310\n",
      "Ep 1 (Step 000270): Train loss 5.191, Val loss 5.302\n",
      "Ep 1 (Step 000280): Train loss 5.102, Val loss 5.292\n",
      "Ep 1 (Step 000290): Train loss 5.074, Val loss 5.288\n",
      "Ep 1 (Step 000300): Train loss 5.119, Val loss 5.276\n",
      "Ep 1 (Step 000310): Train loss 5.130, Val loss 5.270\n",
      "Ep 1 (Step 000320): Train loss 5.106, Val loss 5.258\n",
      "Ep 1 (Step 000330): Train loss 5.141, Val loss 5.254\n",
      "Ep 1 (Step 000340): Train loss 5.020, Val loss 5.264\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2636\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.960, Val loss 8.954\n",
      "Ep 1 (Step 000010): Train loss 7.061, Val loss 7.032\n",
      "Ep 1 (Step 000020): Train loss 6.576, Val loss 6.495\n",
      "Ep 1 (Step 000030): Train loss 6.427, Val loss 6.404\n",
      "Ep 1 (Step 000040): Train loss 6.291, Val loss 6.329\n",
      "Ep 1 (Step 000050): Train loss 6.155, Val loss 6.153\n",
      "Ep 1 (Step 000060): Train loss 5.973, Val loss 6.064\n",
      "Ep 1 (Step 000070): Train loss 5.873, Val loss 5.954\n",
      "Ep 1 (Step 000080): Train loss 5.840, Val loss 5.874\n",
      "Ep 1 (Step 000090): Train loss 5.823, Val loss 5.823\n",
      "Ep 1 (Step 000100): Train loss 5.623, Val loss 5.743\n",
      "Ep 1 (Step 000110): Train loss 5.604, Val loss 5.682\n",
      "Ep 1 (Step 000120): Train loss 5.606, Val loss 5.648\n",
      "Ep 1 (Step 000130): Train loss 5.568, Val loss 5.603\n",
      "Ep 1 (Step 000140): Train loss 5.539, Val loss 5.573\n",
      "Ep 1 (Step 000150): Train loss 5.423, Val loss 5.531\n",
      "Ep 1 (Step 000160): Train loss 5.384, Val loss 5.494\n",
      "Ep 1 (Step 000170): Train loss 5.384, Val loss 5.468\n",
      "Ep 1 (Step 000180): Train loss 5.272, Val loss 5.444\n",
      "Ep 1 (Step 000190): Train loss 5.327, Val loss 5.431\n",
      "Ep 1 (Step 000200): Train loss 5.328, Val loss 5.428\n",
      "Ep 1 (Step 000210): Train loss 5.327, Val loss 5.405\n",
      "Ep 1 (Step 000220): Train loss 5.307, Val loss 5.372\n",
      "Ep 1 (Step 000230): Train loss 5.363, Val loss 5.365\n",
      "Ep 1 (Step 000240): Train loss 5.269, Val loss 5.352\n",
      "Ep 1 (Step 000250): Train loss 5.270, Val loss 5.343\n",
      "Ep 1 (Step 000260): Train loss 5.136, Val loss 5.325\n",
      "Ep 1 (Step 000270): Train loss 5.204, Val loss 5.311\n",
      "Ep 1 (Step 000280): Train loss 5.156, Val loss 5.288\n",
      "Ep 1 (Step 000290): Train loss 5.077, Val loss 5.283\n",
      "Ep 1 (Step 000300): Train loss 5.141, Val loss 5.294\n",
      "Ep 1 (Step 000310): Train loss 5.034, Val loss 5.270\n",
      "Ep 1 (Step 000320): Train loss 5.057, Val loss 5.256\n",
      "Ep 1 (Step 000330): Train loss 5.069, Val loss 5.258\n",
      "Ep 1 (Step 000340): Train loss 5.089, Val loss 5.235\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2354\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.967, Val loss 8.969\n",
      "Ep 1 (Step 000010): Train loss 6.987, Val loss 7.008\n",
      "Ep 1 (Step 000020): Train loss 6.543, Val loss 6.509\n",
      "Ep 1 (Step 000030): Train loss 6.414, Val loss 6.423\n",
      "Ep 1 (Step 000040): Train loss 6.423, Val loss 6.380\n",
      "Ep 1 (Step 000050): Train loss 6.214, Val loss 6.224\n",
      "Ep 1 (Step 000060): Train loss 6.112, Val loss 6.065\n",
      "Ep 1 (Step 000070): Train loss 5.910, Val loss 5.997\n",
      "Ep 1 (Step 000080): Train loss 5.834, Val loss 5.882\n",
      "Ep 1 (Step 000090): Train loss 5.727, Val loss 5.809\n",
      "Ep 1 (Step 000100): Train loss 5.699, Val loss 5.746\n",
      "Ep 1 (Step 000110): Train loss 5.610, Val loss 5.686\n",
      "Ep 1 (Step 000120): Train loss 5.542, Val loss 5.660\n",
      "Ep 1 (Step 000130): Train loss 5.523, Val loss 5.596\n",
      "Ep 1 (Step 000140): Train loss 5.512, Val loss 5.581\n",
      "Ep 1 (Step 000150): Train loss 5.437, Val loss 5.548\n",
      "Ep 1 (Step 000160): Train loss 5.349, Val loss 5.495\n",
      "Ep 1 (Step 000170): Train loss 5.360, Val loss 5.466\n",
      "Ep 1 (Step 000180): Train loss 5.301, Val loss 5.440\n",
      "Ep 1 (Step 000190): Train loss 5.294, Val loss 5.418\n",
      "Ep 1 (Step 000200): Train loss 5.306, Val loss 5.410\n",
      "Ep 1 (Step 000210): Train loss 5.281, Val loss 5.378\n",
      "Ep 1 (Step 000220): Train loss 5.338, Val loss 5.359\n",
      "Ep 1 (Step 000230): Train loss 5.249, Val loss 5.350\n",
      "Ep 1 (Step 000240): Train loss 5.252, Val loss 5.326\n",
      "Ep 1 (Step 000250): Train loss 5.210, Val loss 5.317\n",
      "Ep 1 (Step 000260): Train loss 5.140, Val loss 5.290\n",
      "Ep 1 (Step 000270): Train loss 5.181, Val loss 5.288\n",
      "Ep 1 (Step 000280): Train loss 5.106, Val loss 5.272\n",
      "Ep 1 (Step 000290): Train loss 5.077, Val loss 5.265\n",
      "Ep 1 (Step 000300): Train loss 5.176, Val loss 5.244\n",
      "Ep 1 (Step 000310): Train loss 5.048, Val loss 5.237\n",
      "Ep 1 (Step 000320): Train loss 5.127, Val loss 5.230\n",
      "Ep 1 (Step 000330): Train loss 4.998, Val loss 5.221\n",
      "Ep 1 (Step 000340): Train loss 5.029, Val loss 5.214\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2140\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.982, Val loss 8.978\n",
      "Ep 1 (Step 000010): Train loss 6.959, Val loss 6.987\n",
      "Ep 1 (Step 000020): Train loss 6.458, Val loss 6.511\n",
      "Ep 1 (Step 000030): Train loss 6.368, Val loss 6.420\n",
      "Ep 1 (Step 000040): Train loss 6.366, Val loss 6.361\n",
      "Ep 1 (Step 000050): Train loss 6.260, Val loss 6.229\n",
      "Ep 1 (Step 000060): Train loss 6.054, Val loss 6.086\n",
      "Ep 1 (Step 000070): Train loss 6.010, Val loss 5.992\n",
      "Ep 1 (Step 000080): Train loss 5.906, Val loss 5.888\n",
      "Ep 1 (Step 000090): Train loss 5.765, Val loss 5.846\n",
      "Ep 1 (Step 000100): Train loss 5.710, Val loss 5.764\n",
      "Ep 1 (Step 000110): Train loss 5.709, Val loss 5.698\n",
      "Ep 1 (Step 000120): Train loss 5.547, Val loss 5.651\n",
      "Ep 1 (Step 000130): Train loss 5.520, Val loss 5.624\n",
      "Ep 1 (Step 000140): Train loss 5.513, Val loss 5.577\n",
      "Ep 1 (Step 000150): Train loss 5.451, Val loss 5.549\n",
      "Ep 1 (Step 000160): Train loss 5.369, Val loss 5.513\n",
      "Ep 1 (Step 000170): Train loss 5.427, Val loss 5.466\n",
      "Ep 1 (Step 000180): Train loss 5.329, Val loss 5.449\n",
      "Ep 1 (Step 000190): Train loss 5.201, Val loss 5.427\n",
      "Ep 1 (Step 000200): Train loss 5.249, Val loss 5.399\n",
      "Ep 1 (Step 000210): Train loss 5.340, Val loss 5.365\n",
      "Ep 1 (Step 000220): Train loss 5.247, Val loss 5.359\n",
      "Ep 1 (Step 000230): Train loss 5.259, Val loss 5.366\n",
      "Ep 1 (Step 000240): Train loss 5.229, Val loss 5.359\n",
      "Ep 1 (Step 000250): Train loss 5.149, Val loss 5.339\n",
      "Ep 1 (Step 000260): Train loss 5.196, Val loss 5.322\n",
      "Ep 1 (Step 000270): Train loss 5.147, Val loss 5.295\n",
      "Ep 1 (Step 000280): Train loss 5.168, Val loss 5.277\n",
      "Ep 1 (Step 000290): Train loss 5.118, Val loss 5.246\n",
      "Ep 1 (Step 000300): Train loss 5.056, Val loss 5.242\n",
      "Ep 1 (Step 000310): Train loss 5.110, Val loss 5.235\n",
      "Ep 1 (Step 000320): Train loss 5.008, Val loss 5.233\n",
      "Ep 1 (Step 000330): Train loss 5.061, Val loss 5.213\n",
      "Ep 1 (Step 000340): Train loss 5.053, Val loss 5.202\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2025\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.949, Val loss 8.923\n",
      "Ep 1 (Step 000010): Train loss 7.017, Val loss 7.018\n",
      "Ep 1 (Step 000020): Train loss 6.527, Val loss 6.509\n",
      "Ep 1 (Step 000030): Train loss 6.376, Val loss 6.400\n",
      "Ep 1 (Step 000040): Train loss 6.300, Val loss 6.350\n",
      "Ep 1 (Step 000050): Train loss 6.145, Val loss 6.174\n",
      "Ep 1 (Step 000060): Train loss 6.065, Val loss 6.033\n",
      "Ep 1 (Step 000070): Train loss 5.901, Val loss 5.934\n",
      "Ep 1 (Step 000080): Train loss 5.787, Val loss 5.866\n",
      "Ep 1 (Step 000090): Train loss 5.700, Val loss 5.778\n",
      "Ep 1 (Step 000100): Train loss 5.715, Val loss 5.719\n",
      "Ep 1 (Step 000110): Train loss 5.607, Val loss 5.693\n",
      "Ep 1 (Step 000120): Train loss 5.590, Val loss 5.632\n",
      "Ep 1 (Step 000130): Train loss 5.477, Val loss 5.587\n",
      "Ep 1 (Step 000140): Train loss 5.416, Val loss 5.549\n",
      "Ep 1 (Step 000150): Train loss 5.392, Val loss 5.520\n",
      "Ep 1 (Step 000160): Train loss 5.390, Val loss 5.482\n",
      "Ep 1 (Step 000170): Train loss 5.322, Val loss 5.457\n",
      "Ep 1 (Step 000180): Train loss 5.288, Val loss 5.432\n",
      "Ep 1 (Step 000190): Train loss 5.289, Val loss 5.421\n",
      "Ep 1 (Step 000200): Train loss 5.222, Val loss 5.383\n",
      "Ep 1 (Step 000210): Train loss 5.217, Val loss 5.363\n",
      "Ep 1 (Step 000220): Train loss 5.195, Val loss 5.340\n",
      "Ep 1 (Step 000230): Train loss 5.247, Val loss 5.322\n",
      "Ep 1 (Step 000240): Train loss 5.187, Val loss 5.306\n",
      "Ep 1 (Step 000250): Train loss 5.225, Val loss 5.287\n",
      "Ep 1 (Step 000260): Train loss 5.179, Val loss 5.274\n",
      "Ep 1 (Step 000270): Train loss 5.123, Val loss 5.265\n",
      "Ep 1 (Step 000280): Train loss 5.108, Val loss 5.281\n",
      "Ep 1 (Step 000290): Train loss 5.118, Val loss 5.252\n",
      "Ep 1 (Step 000300): Train loss 5.094, Val loss 5.257\n",
      "Ep 1 (Step 000310): Train loss 5.053, Val loss 5.216\n",
      "Ep 1 (Step 000320): Train loss 4.997, Val loss 5.214\n",
      "Ep 1 (Step 000330): Train loss 5.084, Val loss 5.206\n",
      "Ep 1 (Step 000340): Train loss 5.072, Val loss 5.212\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2115\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.616, Val loss 8.604\n",
      "Ep 1 (Step 000010): Train loss 6.606, Val loss 6.566\n",
      "Ep 1 (Step 000020): Train loss 6.490, Val loss 6.512\n",
      "Ep 1 (Step 000030): Train loss 6.210, Val loss 6.279\n",
      "Ep 1 (Step 000040): Train loss 6.088, Val loss 6.110\n",
      "Ep 1 (Step 000050): Train loss 5.923, Val loss 5.959\n",
      "Ep 1 (Step 000060): Train loss 5.817, Val loss 5.847\n",
      "Ep 1 (Step 000070): Train loss 5.650, Val loss 5.738\n",
      "Ep 1 (Step 000080): Train loss 5.588, Val loss 5.663\n",
      "Ep 1 (Step 000090): Train loss 5.607, Val loss 5.618\n",
      "Ep 1 (Step 000100): Train loss 5.439, Val loss 5.552\n",
      "Ep 1 (Step 000110): Train loss 5.425, Val loss 5.512\n",
      "Ep 1 (Step 000120): Train loss 5.400, Val loss 5.474\n",
      "Ep 1 (Step 000130): Train loss 5.369, Val loss 5.457\n",
      "Ep 1 (Step 000140): Train loss 5.258, Val loss 5.446\n",
      "Ep 1 (Step 000150): Train loss 5.301, Val loss 5.410\n",
      "Ep 1 (Step 000160): Train loss 5.278, Val loss 5.405\n",
      "Ep 1 (Step 000170): Train loss 5.153, Val loss 5.367\n",
      "Ep 1 (Step 000180): Train loss 5.260, Val loss 5.349\n",
      "Ep 1 (Step 000190): Train loss 5.293, Val loss 5.346\n",
      "Ep 1 (Step 000200): Train loss 5.173, Val loss 5.340\n",
      "Ep 1 (Step 000210): Train loss 5.202, Val loss 5.339\n",
      "Ep 1 (Step 000220): Train loss 5.197, Val loss 5.290\n",
      "Ep 1 (Step 000230): Train loss 5.066, Val loss 5.276\n",
      "Ep 1 (Step 000240): Train loss 5.061, Val loss 5.275\n",
      "Ep 1 (Step 000250): Train loss 5.128, Val loss 5.266\n",
      "Ep 1 (Step 000260): Train loss 4.998, Val loss 5.232\n",
      "Ep 1 (Step 000270): Train loss 5.049, Val loss 5.248\n",
      "Ep 1 (Step 000280): Train loss 5.106, Val loss 5.232\n",
      "Ep 1 (Step 000290): Train loss 4.980, Val loss 5.224\n",
      "Ep 1 (Step 000300): Train loss 5.060, Val loss 5.204\n",
      "Ep 1 (Step 000310): Train loss 4.936, Val loss 5.185\n",
      "Ep 1 (Step 000320): Train loss 4.898, Val loss 5.192\n",
      "Ep 1 (Step 000330): Train loss 4.935, Val loss 5.165\n",
      "Ep 1 (Step 000340): Train loss 4.975, Val loss 5.177\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1768\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.662, Val loss 8.620\n",
      "Ep 1 (Step 000010): Train loss 6.562, Val loss 6.561\n",
      "Ep 1 (Step 000020): Train loss 6.439, Val loss 6.503\n",
      "Ep 1 (Step 000030): Train loss 6.274, Val loss 6.273\n",
      "Ep 1 (Step 000040): Train loss 6.101, Val loss 6.117\n",
      "Ep 1 (Step 000050): Train loss 5.844, Val loss 5.923\n",
      "Ep 1 (Step 000060): Train loss 5.815, Val loss 5.822\n",
      "Ep 1 (Step 000070): Train loss 5.725, Val loss 5.763\n",
      "Ep 1 (Step 000080): Train loss 5.604, Val loss 5.688\n",
      "Ep 1 (Step 000090): Train loss 5.562, Val loss 5.636\n",
      "Ep 1 (Step 000100): Train loss 5.560, Val loss 5.582\n",
      "Ep 1 (Step 000110): Train loss 5.463, Val loss 5.550\n",
      "Ep 1 (Step 000120): Train loss 5.403, Val loss 5.504\n",
      "Ep 1 (Step 000130): Train loss 5.424, Val loss 5.481\n",
      "Ep 1 (Step 000140): Train loss 5.244, Val loss 5.441\n",
      "Ep 1 (Step 000150): Train loss 5.284, Val loss 5.431\n",
      "Ep 1 (Step 000160): Train loss 5.322, Val loss 5.388\n",
      "Ep 1 (Step 000170): Train loss 5.261, Val loss 5.376\n",
      "Ep 1 (Step 000180): Train loss 5.284, Val loss 5.330\n",
      "Ep 1 (Step 000190): Train loss 5.153, Val loss 5.336\n",
      "Ep 1 (Step 000200): Train loss 5.213, Val loss 5.337\n",
      "Ep 1 (Step 000210): Train loss 5.262, Val loss 5.321\n",
      "Ep 1 (Step 000220): Train loss 4.989, Val loss 5.319\n",
      "Ep 1 (Step 000230): Train loss 5.108, Val loss 5.295\n",
      "Ep 1 (Step 000240): Train loss 5.117, Val loss 5.291\n",
      "Ep 1 (Step 000250): Train loss 5.059, Val loss 5.291\n",
      "Ep 1 (Step 000260): Train loss 5.105, Val loss 5.258\n",
      "Ep 1 (Step 000270): Train loss 5.056, Val loss 5.242\n",
      "Ep 1 (Step 000280): Train loss 5.165, Val loss 5.225\n",
      "Ep 1 (Step 000290): Train loss 5.042, Val loss 5.234\n",
      "Ep 1 (Step 000300): Train loss 5.129, Val loss 5.199\n",
      "Ep 1 (Step 000310): Train loss 5.003, Val loss 5.199\n",
      "Ep 1 (Step 000320): Train loss 4.987, Val loss 5.211\n",
      "Ep 1 (Step 000330): Train loss 5.043, Val loss 5.198\n",
      "Ep 1 (Step 000340): Train loss 5.017, Val loss 5.180\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1803\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.636, Val loss 8.613\n",
      "Ep 1 (Step 000010): Train loss 6.559, Val loss 6.564\n",
      "Ep 1 (Step 000020): Train loss 6.503, Val loss 6.493\n",
      "Ep 1 (Step 000030): Train loss 6.314, Val loss 6.199\n",
      "Ep 1 (Step 000040): Train loss 6.133, Val loss 6.068\n",
      "Ep 1 (Step 000050): Train loss 5.824, Val loss 5.901\n",
      "Ep 1 (Step 000060): Train loss 5.759, Val loss 5.802\n",
      "Ep 1 (Step 000070): Train loss 5.674, Val loss 5.733\n",
      "Ep 1 (Step 000080): Train loss 5.592, Val loss 5.643\n",
      "Ep 1 (Step 000090): Train loss 5.558, Val loss 5.601\n",
      "Ep 1 (Step 000100): Train loss 5.385, Val loss 5.563\n",
      "Ep 1 (Step 000110): Train loss 5.428, Val loss 5.529\n",
      "Ep 1 (Step 000120): Train loss 5.311, Val loss 5.489\n",
      "Ep 1 (Step 000130): Train loss 5.403, Val loss 5.473\n",
      "Ep 1 (Step 000140): Train loss 5.382, Val loss 5.447\n",
      "Ep 1 (Step 000150): Train loss 5.255, Val loss 5.426\n",
      "Ep 1 (Step 000160): Train loss 5.274, Val loss 5.417\n",
      "Ep 1 (Step 000170): Train loss 5.168, Val loss 5.413\n",
      "Ep 1 (Step 000180): Train loss 5.148, Val loss 5.364\n",
      "Ep 1 (Step 000190): Train loss 5.303, Val loss 5.350\n",
      "Ep 1 (Step 000200): Train loss 5.076, Val loss 5.341\n",
      "Ep 1 (Step 000210): Train loss 5.196, Val loss 5.310\n",
      "Ep 1 (Step 000220): Train loss 5.085, Val loss 5.315\n",
      "Ep 1 (Step 000230): Train loss 5.211, Val loss 5.310\n",
      "Ep 1 (Step 000240): Train loss 5.174, Val loss 5.294\n",
      "Ep 1 (Step 000250): Train loss 5.086, Val loss 5.268\n",
      "Ep 1 (Step 000260): Train loss 5.069, Val loss 5.249\n",
      "Ep 1 (Step 000270): Train loss 5.074, Val loss 5.254\n",
      "Ep 1 (Step 000280): Train loss 5.097, Val loss 5.229\n",
      "Ep 1 (Step 000290): Train loss 5.001, Val loss 5.226\n",
      "Ep 1 (Step 000300): Train loss 5.060, Val loss 5.219\n",
      "Ep 1 (Step 000310): Train loss 5.021, Val loss 5.208\n",
      "Ep 1 (Step 000320): Train loss 4.950, Val loss 5.191\n",
      "Ep 1 (Step 000330): Train loss 4.936, Val loss 5.185\n",
      "Ep 1 (Step 000340): Train loss 4.958, Val loss 5.166\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1660\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.575, Val loss 8.564\n",
      "Ep 1 (Step 000010): Train loss 6.586, Val loss 6.618\n",
      "Ep 1 (Step 000020): Train loss 6.468, Val loss 6.512\n",
      "Ep 1 (Step 000030): Train loss 6.321, Val loss 6.283\n",
      "Ep 1 (Step 000040): Train loss 6.057, Val loss 6.093\n",
      "Ep 1 (Step 000050): Train loss 5.924, Val loss 5.968\n",
      "Ep 1 (Step 000060): Train loss 5.857, Val loss 5.808\n",
      "Ep 1 (Step 000070): Train loss 5.742, Val loss 5.750\n",
      "Ep 1 (Step 000080): Train loss 5.631, Val loss 5.649\n",
      "Ep 1 (Step 000090): Train loss 5.442, Val loss 5.595\n",
      "Ep 1 (Step 000100): Train loss 5.503, Val loss 5.576\n",
      "Ep 1 (Step 000110): Train loss 5.390, Val loss 5.511\n",
      "Ep 1 (Step 000120): Train loss 5.393, Val loss 5.473\n",
      "Ep 1 (Step 000130): Train loss 5.332, Val loss 5.436\n",
      "Ep 1 (Step 000140): Train loss 5.316, Val loss 5.400\n",
      "Ep 1 (Step 000150): Train loss 5.165, Val loss 5.365\n",
      "Ep 1 (Step 000160): Train loss 5.358, Val loss 5.377\n",
      "Ep 1 (Step 000170): Train loss 5.164, Val loss 5.364\n",
      "Ep 1 (Step 000180): Train loss 5.231, Val loss 5.340\n",
      "Ep 1 (Step 000190): Train loss 5.158, Val loss 5.328\n",
      "Ep 1 (Step 000200): Train loss 5.161, Val loss 5.308\n",
      "Ep 1 (Step 000210): Train loss 5.231, Val loss 5.323\n",
      "Ep 1 (Step 000220): Train loss 5.043, Val loss 5.308\n",
      "Ep 1 (Step 000230): Train loss 5.076, Val loss 5.269\n",
      "Ep 1 (Step 000240): Train loss 5.117, Val loss 5.262\n",
      "Ep 1 (Step 000250): Train loss 5.026, Val loss 5.240\n",
      "Ep 1 (Step 000260): Train loss 5.047, Val loss 5.222\n",
      "Ep 1 (Step 000270): Train loss 4.948, Val loss 5.211\n",
      "Ep 1 (Step 000280): Train loss 4.988, Val loss 5.231\n",
      "Ep 1 (Step 000290): Train loss 4.991, Val loss 5.201\n",
      "Ep 1 (Step 000300): Train loss 4.967, Val loss 5.185\n",
      "Ep 1 (Step 000310): Train loss 4.919, Val loss 5.158\n",
      "Ep 1 (Step 000320): Train loss 5.068, Val loss 5.154\n",
      "Ep 1 (Step 000330): Train loss 4.904, Val loss 5.152\n",
      "Ep 1 (Step 000340): Train loss 4.932, Val loss 5.131\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1312\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.712, Val loss 8.688\n",
      "Ep 1 (Step 000010): Train loss 6.609, Val loss 6.575\n",
      "Ep 1 (Step 000020): Train loss 6.452, Val loss 6.495\n",
      "Ep 1 (Step 000030): Train loss 6.390, Val loss 6.372\n",
      "Ep 1 (Step 000040): Train loss 6.179, Val loss 6.165\n",
      "Ep 1 (Step 000050): Train loss 5.968, Val loss 6.002\n",
      "Ep 1 (Step 000060): Train loss 5.842, Val loss 5.871\n",
      "Ep 1 (Step 000070): Train loss 5.696, Val loss 5.756\n",
      "Ep 1 (Step 000080): Train loss 5.633, Val loss 5.671\n",
      "Ep 1 (Step 000090): Train loss 5.549, Val loss 5.623\n",
      "Ep 1 (Step 000100): Train loss 5.538, Val loss 5.594\n",
      "Ep 1 (Step 000110): Train loss 5.443, Val loss 5.555\n",
      "Ep 1 (Step 000120): Train loss 5.303, Val loss 5.498\n",
      "Ep 1 (Step 000130): Train loss 5.306, Val loss 5.444\n",
      "Ep 1 (Step 000140): Train loss 5.282, Val loss 5.424\n",
      "Ep 1 (Step 000150): Train loss 5.357, Val loss 5.418\n",
      "Ep 1 (Step 000160): Train loss 5.304, Val loss 5.377\n",
      "Ep 1 (Step 000170): Train loss 5.308, Val loss 5.357\n",
      "Ep 1 (Step 000180): Train loss 5.187, Val loss 5.346\n",
      "Ep 1 (Step 000190): Train loss 5.151, Val loss 5.310\n",
      "Ep 1 (Step 000200): Train loss 5.217, Val loss 5.291\n",
      "Ep 1 (Step 000210): Train loss 5.123, Val loss 5.264\n",
      "Ep 1 (Step 000220): Train loss 5.139, Val loss 5.243\n",
      "Ep 1 (Step 000230): Train loss 5.023, Val loss 5.252\n",
      "Ep 1 (Step 000240): Train loss 5.142, Val loss 5.236\n",
      "Ep 1 (Step 000250): Train loss 5.032, Val loss 5.225\n",
      "Ep 1 (Step 000260): Train loss 5.012, Val loss 5.237\n",
      "Ep 1 (Step 000270): Train loss 5.062, Val loss 5.223\n",
      "Ep 1 (Step 000280): Train loss 5.005, Val loss 5.210\n",
      "Ep 1 (Step 000290): Train loss 4.997, Val loss 5.185\n",
      "Ep 1 (Step 000300): Train loss 4.993, Val loss 5.180\n",
      "Ep 1 (Step 000310): Train loss 4.999, Val loss 5.174\n",
      "Ep 1 (Step 000320): Train loss 4.958, Val loss 5.172\n",
      "Ep 1 (Step 000330): Train loss 5.010, Val loss 5.148\n",
      "Ep 1 (Step 000340): Train loss 5.003, Val loss 5.133\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1332\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.611, Val loss 8.550\n",
      "Ep 1 (Step 000010): Train loss 6.610, Val loss 6.552\n",
      "Ep 1 (Step 000020): Train loss 6.558, Val loss 6.498\n",
      "Ep 1 (Step 000030): Train loss 6.403, Val loss 6.358\n",
      "Ep 1 (Step 000040): Train loss 6.176, Val loss 6.136\n",
      "Ep 1 (Step 000050): Train loss 5.925, Val loss 6.005\n",
      "Ep 1 (Step 000060): Train loss 5.887, Val loss 5.873\n",
      "Ep 1 (Step 000070): Train loss 5.726, Val loss 5.770\n",
      "Ep 1 (Step 000080): Train loss 5.748, Val loss 5.678\n",
      "Ep 1 (Step 000090): Train loss 5.598, Val loss 5.635\n",
      "Ep 1 (Step 000100): Train loss 5.529, Val loss 5.559\n",
      "Ep 1 (Step 000110): Train loss 5.510, Val loss 5.525\n",
      "Ep 1 (Step 000120): Train loss 5.397, Val loss 5.479\n",
      "Ep 1 (Step 000130): Train loss 5.333, Val loss 5.470\n",
      "Ep 1 (Step 000140): Train loss 5.344, Val loss 5.427\n",
      "Ep 1 (Step 000150): Train loss 5.270, Val loss 5.385\n",
      "Ep 1 (Step 000160): Train loss 5.308, Val loss 5.374\n",
      "Ep 1 (Step 000170): Train loss 5.166, Val loss 5.353\n",
      "Ep 1 (Step 000180): Train loss 5.299, Val loss 5.344\n",
      "Ep 1 (Step 000190): Train loss 5.210, Val loss 5.316\n",
      "Ep 1 (Step 000200): Train loss 5.122, Val loss 5.298\n",
      "Ep 1 (Step 000210): Train loss 5.233, Val loss 5.260\n",
      "Ep 1 (Step 000220): Train loss 5.134, Val loss 5.292\n",
      "Ep 1 (Step 000230): Train loss 5.132, Val loss 5.268\n",
      "Ep 1 (Step 000240): Train loss 5.079, Val loss 5.255\n",
      "Ep 1 (Step 000250): Train loss 5.110, Val loss 5.245\n",
      "Ep 1 (Step 000260): Train loss 5.081, Val loss 5.207\n",
      "Ep 1 (Step 000270): Train loss 5.027, Val loss 5.213\n",
      "Ep 1 (Step 000280): Train loss 4.995, Val loss 5.195\n",
      "Ep 1 (Step 000290): Train loss 5.006, Val loss 5.212\n",
      "Ep 1 (Step 000300): Train loss 5.007, Val loss 5.177\n",
      "Ep 1 (Step 000310): Train loss 4.914, Val loss 5.150\n",
      "Ep 1 (Step 000320): Train loss 4.957, Val loss 5.147\n",
      "Ep 1 (Step 000330): Train loss 4.873, Val loss 5.166\n",
      "Ep 1 (Step 000340): Train loss 4.928, Val loss 5.142\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1416\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.955, Val loss 8.963\n",
      "Ep 1 (Step 000010): Train loss 7.040, Val loss 7.025\n",
      "Ep 1 (Step 000020): Train loss 6.534, Val loss 6.486\n",
      "Ep 1 (Step 000030): Train loss 6.400, Val loss 6.425\n",
      "Ep 1 (Step 000040): Train loss 6.339, Val loss 6.393\n",
      "Ep 1 (Step 000050): Train loss 6.141, Val loss 6.218\n",
      "Ep 1 (Step 000060): Train loss 6.070, Val loss 6.074\n",
      "Ep 1 (Step 000070): Train loss 5.933, Val loss 5.983\n",
      "Ep 1 (Step 000080): Train loss 5.796, Val loss 5.900\n",
      "Ep 1 (Step 000090): Train loss 5.799, Val loss 5.817\n",
      "Ep 1 (Step 000100): Train loss 5.702, Val loss 5.775\n",
      "Ep 1 (Step 000110): Train loss 5.635, Val loss 5.708\n",
      "Ep 1 (Step 000120): Train loss 5.504, Val loss 5.652\n",
      "Ep 1 (Step 000130): Train loss 5.458, Val loss 5.616\n",
      "Ep 1 (Step 000140): Train loss 5.470, Val loss 5.565\n",
      "Ep 1 (Step 000150): Train loss 5.380, Val loss 5.541\n",
      "Ep 1 (Step 000160): Train loss 5.420, Val loss 5.502\n",
      "Ep 1 (Step 000170): Train loss 5.347, Val loss 5.478\n",
      "Ep 1 (Step 000180): Train loss 5.272, Val loss 5.451\n",
      "Ep 1 (Step 000190): Train loss 5.329, Val loss 5.429\n",
      "Ep 1 (Step 000200): Train loss 5.246, Val loss 5.408\n",
      "Ep 1 (Step 000210): Train loss 5.329, Val loss 5.401\n",
      "Ep 1 (Step 000220): Train loss 5.195, Val loss 5.396\n",
      "Ep 1 (Step 000230): Train loss 5.271, Val loss 5.379\n",
      "Ep 1 (Step 000240): Train loss 5.148, Val loss 5.331\n",
      "Ep 1 (Step 000250): Train loss 5.162, Val loss 5.321\n",
      "Ep 1 (Step 000260): Train loss 5.198, Val loss 5.314\n",
      "Ep 1 (Step 000270): Train loss 5.147, Val loss 5.306\n",
      "Ep 1 (Step 000280): Train loss 5.129, Val loss 5.293\n",
      "Ep 1 (Step 000290): Train loss 5.134, Val loss 5.273\n",
      "Ep 1 (Step 000300): Train loss 5.146, Val loss 5.253\n",
      "Ep 1 (Step 000310): Train loss 5.119, Val loss 5.265\n",
      "Ep 1 (Step 000320): Train loss 5.128, Val loss 5.254\n",
      "Ep 1 (Step 000330): Train loss 5.048, Val loss 5.272\n",
      "Ep 1 (Step 000340): Train loss 5.043, Val loss 5.242\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2416\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.984, Val loss 8.960\n",
      "Ep 1 (Step 000010): Train loss 7.036, Val loss 6.990\n",
      "Ep 1 (Step 000020): Train loss 6.492, Val loss 6.470\n",
      "Ep 1 (Step 000030): Train loss 6.380, Val loss 6.372\n",
      "Ep 1 (Step 000040): Train loss 6.365, Val loss 6.313\n",
      "Ep 1 (Step 000050): Train loss 6.141, Val loss 6.160\n",
      "Ep 1 (Step 000060): Train loss 5.985, Val loss 6.032\n",
      "Ep 1 (Step 000070): Train loss 5.862, Val loss 5.952\n",
      "Ep 1 (Step 000080): Train loss 5.809, Val loss 5.866\n",
      "Ep 1 (Step 000090): Train loss 5.720, Val loss 5.803\n",
      "Ep 1 (Step 000100): Train loss 5.730, Val loss 5.742\n",
      "Ep 1 (Step 000110): Train loss 5.663, Val loss 5.679\n",
      "Ep 1 (Step 000120): Train loss 5.514, Val loss 5.621\n",
      "Ep 1 (Step 000130): Train loss 5.537, Val loss 5.578\n",
      "Ep 1 (Step 000140): Train loss 5.461, Val loss 5.543\n",
      "Ep 1 (Step 000150): Train loss 5.520, Val loss 5.516\n",
      "Ep 1 (Step 000160): Train loss 5.406, Val loss 5.495\n",
      "Ep 1 (Step 000170): Train loss 5.346, Val loss 5.484\n",
      "Ep 1 (Step 000180): Train loss 5.332, Val loss 5.465\n",
      "Ep 1 (Step 000190): Train loss 5.359, Val loss 5.434\n",
      "Ep 1 (Step 000200): Train loss 5.326, Val loss 5.408\n",
      "Ep 1 (Step 000210): Train loss 5.220, Val loss 5.381\n",
      "Ep 1 (Step 000220): Train loss 5.188, Val loss 5.375\n",
      "Ep 1 (Step 000230): Train loss 5.176, Val loss 5.356\n",
      "Ep 1 (Step 000240): Train loss 5.247, Val loss 5.336\n",
      "Ep 1 (Step 000250): Train loss 5.208, Val loss 5.313\n",
      "Ep 1 (Step 000260): Train loss 5.103, Val loss 5.309\n",
      "Ep 1 (Step 000270): Train loss 5.146, Val loss 5.314\n",
      "Ep 1 (Step 000280): Train loss 5.153, Val loss 5.318\n",
      "Ep 1 (Step 000290): Train loss 5.082, Val loss 5.285\n",
      "Ep 1 (Step 000300): Train loss 5.114, Val loss 5.291\n",
      "Ep 1 (Step 000310): Train loss 5.109, Val loss 5.267\n",
      "Ep 1 (Step 000320): Train loss 5.048, Val loss 5.267\n",
      "Ep 1 (Step 000330): Train loss 5.066, Val loss 5.242\n",
      "Ep 1 (Step 000340): Train loss 5.041, Val loss 5.245\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2451\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.930, Val loss 8.912\n",
      "Ep 1 (Step 000010): Train loss 6.971, Val loss 6.970\n",
      "Ep 1 (Step 000020): Train loss 6.480, Val loss 6.490\n",
      "Ep 1 (Step 000030): Train loss 6.398, Val loss 6.400\n",
      "Ep 1 (Step 000040): Train loss 6.418, Val loss 6.339\n",
      "Ep 1 (Step 000050): Train loss 6.191, Val loss 6.186\n",
      "Ep 1 (Step 000060): Train loss 6.057, Val loss 6.035\n",
      "Ep 1 (Step 000070): Train loss 5.882, Val loss 5.939\n",
      "Ep 1 (Step 000080): Train loss 5.837, Val loss 5.874\n",
      "Ep 1 (Step 000090): Train loss 5.723, Val loss 5.795\n",
      "Ep 1 (Step 000100): Train loss 5.559, Val loss 5.724\n",
      "Ep 1 (Step 000110): Train loss 5.590, Val loss 5.672\n",
      "Ep 1 (Step 000120): Train loss 5.465, Val loss 5.623\n",
      "Ep 1 (Step 000130): Train loss 5.472, Val loss 5.585\n",
      "Ep 1 (Step 000140): Train loss 5.484, Val loss 5.567\n",
      "Ep 1 (Step 000150): Train loss 5.528, Val loss 5.555\n",
      "Ep 1 (Step 000160): Train loss 5.316, Val loss 5.519\n",
      "Ep 1 (Step 000170): Train loss 5.283, Val loss 5.494\n",
      "Ep 1 (Step 000180): Train loss 5.460, Val loss 5.467\n",
      "Ep 1 (Step 000190): Train loss 5.258, Val loss 5.436\n",
      "Ep 1 (Step 000200): Train loss 5.300, Val loss 5.425\n",
      "Ep 1 (Step 000210): Train loss 5.317, Val loss 5.403\n",
      "Ep 1 (Step 000220): Train loss 5.300, Val loss 5.387\n",
      "Ep 1 (Step 000230): Train loss 5.297, Val loss 5.376\n",
      "Ep 1 (Step 000240): Train loss 5.210, Val loss 5.363\n",
      "Ep 1 (Step 000250): Train loss 5.266, Val loss 5.343\n",
      "Ep 1 (Step 000260): Train loss 5.195, Val loss 5.337\n",
      "Ep 1 (Step 000270): Train loss 5.095, Val loss 5.317\n",
      "Ep 1 (Step 000280): Train loss 5.160, Val loss 5.296\n",
      "Ep 1 (Step 000290): Train loss 5.218, Val loss 5.288\n",
      "Ep 1 (Step 000300): Train loss 5.138, Val loss 5.276\n",
      "Ep 1 (Step 000310): Train loss 5.184, Val loss 5.265\n",
      "Ep 1 (Step 000320): Train loss 5.068, Val loss 5.262\n",
      "Ep 1 (Step 000330): Train loss 5.042, Val loss 5.233\n",
      "Ep 1 (Step 000340): Train loss 5.089, Val loss 5.230\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2300\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.973, Val loss 8.950\n",
      "Ep 1 (Step 000010): Train loss 7.043, Val loss 7.015\n",
      "Ep 1 (Step 000020): Train loss 6.517, Val loss 6.502\n",
      "Ep 1 (Step 000030): Train loss 6.449, Val loss 6.417\n",
      "Ep 1 (Step 000040): Train loss 6.358, Val loss 6.379\n",
      "Ep 1 (Step 000050): Train loss 6.188, Val loss 6.197\n",
      "Ep 1 (Step 000060): Train loss 6.051, Val loss 6.048\n",
      "Ep 1 (Step 000070): Train loss 5.930, Val loss 5.940\n",
      "Ep 1 (Step 000080): Train loss 5.918, Val loss 5.878\n",
      "Ep 1 (Step 000090): Train loss 5.787, Val loss 5.784\n",
      "Ep 1 (Step 000100): Train loss 5.611, Val loss 5.712\n",
      "Ep 1 (Step 000110): Train loss 5.628, Val loss 5.661\n",
      "Ep 1 (Step 000120): Train loss 5.572, Val loss 5.624\n",
      "Ep 1 (Step 000130): Train loss 5.508, Val loss 5.589\n",
      "Ep 1 (Step 000140): Train loss 5.468, Val loss 5.540\n",
      "Ep 1 (Step 000150): Train loss 5.441, Val loss 5.522\n",
      "Ep 1 (Step 000160): Train loss 5.434, Val loss 5.494\n",
      "Ep 1 (Step 000170): Train loss 5.363, Val loss 5.466\n",
      "Ep 1 (Step 000180): Train loss 5.347, Val loss 5.433\n",
      "Ep 1 (Step 000190): Train loss 5.359, Val loss 5.422\n",
      "Ep 1 (Step 000200): Train loss 5.291, Val loss 5.396\n",
      "Ep 1 (Step 000210): Train loss 5.167, Val loss 5.386\n",
      "Ep 1 (Step 000220): Train loss 5.250, Val loss 5.363\n",
      "Ep 1 (Step 000230): Train loss 5.164, Val loss 5.339\n",
      "Ep 1 (Step 000240): Train loss 5.134, Val loss 5.308\n",
      "Ep 1 (Step 000250): Train loss 5.202, Val loss 5.294\n",
      "Ep 1 (Step 000260): Train loss 5.152, Val loss 5.293\n",
      "Ep 1 (Step 000270): Train loss 5.158, Val loss 5.288\n",
      "Ep 1 (Step 000280): Train loss 5.134, Val loss 5.276\n",
      "Ep 1 (Step 000290): Train loss 5.072, Val loss 5.247\n",
      "Ep 1 (Step 000300): Train loss 5.085, Val loss 5.234\n",
      "Ep 1 (Step 000310): Train loss 5.089, Val loss 5.234\n",
      "Ep 1 (Step 000320): Train loss 5.043, Val loss 5.223\n",
      "Ep 1 (Step 000330): Train loss 5.031, Val loss 5.209\n",
      "Ep 1 (Step 000340): Train loss 5.131, Val loss 5.197\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1966\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.958, Val loss 8.953\n",
      "Ep 1 (Step 000010): Train loss 7.036, Val loss 7.027\n",
      "Ep 1 (Step 000020): Train loss 6.483, Val loss 6.499\n",
      "Ep 1 (Step 000030): Train loss 6.337, Val loss 6.403\n",
      "Ep 1 (Step 000040): Train loss 6.333, Val loss 6.337\n",
      "Ep 1 (Step 000050): Train loss 6.209, Val loss 6.193\n",
      "Ep 1 (Step 000060): Train loss 6.075, Val loss 6.098\n",
      "Ep 1 (Step 000070): Train loss 5.857, Val loss 5.961\n",
      "Ep 1 (Step 000080): Train loss 5.798, Val loss 5.855\n",
      "Ep 1 (Step 000090): Train loss 5.751, Val loss 5.787\n",
      "Ep 1 (Step 000100): Train loss 5.658, Val loss 5.731\n",
      "Ep 1 (Step 000110): Train loss 5.576, Val loss 5.673\n",
      "Ep 1 (Step 000120): Train loss 5.578, Val loss 5.637\n",
      "Ep 1 (Step 000130): Train loss 5.507, Val loss 5.564\n",
      "Ep 1 (Step 000140): Train loss 5.412, Val loss 5.537\n",
      "Ep 1 (Step 000150): Train loss 5.376, Val loss 5.492\n",
      "Ep 1 (Step 000160): Train loss 5.413, Val loss 5.471\n",
      "Ep 1 (Step 000170): Train loss 5.312, Val loss 5.449\n",
      "Ep 1 (Step 000180): Train loss 5.377, Val loss 5.430\n",
      "Ep 1 (Step 000190): Train loss 5.242, Val loss 5.394\n",
      "Ep 1 (Step 000200): Train loss 5.289, Val loss 5.375\n",
      "Ep 1 (Step 000210): Train loss 5.204, Val loss 5.364\n",
      "Ep 1 (Step 000220): Train loss 5.256, Val loss 5.350\n",
      "Ep 1 (Step 000230): Train loss 5.081, Val loss 5.329\n",
      "Ep 1 (Step 000240): Train loss 5.238, Val loss 5.323\n",
      "Ep 1 (Step 000250): Train loss 5.132, Val loss 5.301\n",
      "Ep 1 (Step 000260): Train loss 5.262, Val loss 5.299\n",
      "Ep 1 (Step 000270): Train loss 5.137, Val loss 5.292\n",
      "Ep 1 (Step 000280): Train loss 5.094, Val loss 5.255\n",
      "Ep 1 (Step 000290): Train loss 5.145, Val loss 5.260\n",
      "Ep 1 (Step 000300): Train loss 5.174, Val loss 5.247\n",
      "Ep 1 (Step 000310): Train loss 5.013, Val loss 5.227\n",
      "Ep 1 (Step 000320): Train loss 5.111, Val loss 5.234\n",
      "Ep 1 (Step 000330): Train loss 5.094, Val loss 5.225\n",
      "Ep 1 (Step 000340): Train loss 5.050, Val loss 5.212\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2116\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.899, Val loss 8.889\n",
      "Ep 1 (Step 000010): Train loss 7.036, Val loss 7.013\n",
      "Ep 1 (Step 000020): Train loss 6.529, Val loss 6.490\n",
      "Ep 1 (Step 000030): Train loss 6.410, Val loss 6.415\n",
      "Ep 1 (Step 000040): Train loss 6.367, Val loss 6.334\n",
      "Ep 1 (Step 000050): Train loss 6.257, Val loss 6.200\n",
      "Ep 1 (Step 000060): Train loss 5.990, Val loss 6.037\n",
      "Ep 1 (Step 000070): Train loss 5.926, Val loss 5.960\n",
      "Ep 1 (Step 000080): Train loss 5.814, Val loss 5.841\n",
      "Ep 1 (Step 000090): Train loss 5.665, Val loss 5.776\n",
      "Ep 1 (Step 000100): Train loss 5.656, Val loss 5.722\n",
      "Ep 1 (Step 000110): Train loss 5.570, Val loss 5.663\n",
      "Ep 1 (Step 000120): Train loss 5.607, Val loss 5.604\n",
      "Ep 1 (Step 000130): Train loss 5.480, Val loss 5.577\n",
      "Ep 1 (Step 000140): Train loss 5.435, Val loss 5.566\n",
      "Ep 1 (Step 000150): Train loss 5.463, Val loss 5.512\n",
      "Ep 1 (Step 000160): Train loss 5.353, Val loss 5.480\n",
      "Ep 1 (Step 000170): Train loss 5.370, Val loss 5.463\n",
      "Ep 1 (Step 000180): Train loss 5.314, Val loss 5.424\n",
      "Ep 1 (Step 000190): Train loss 5.360, Val loss 5.408\n",
      "Ep 1 (Step 000200): Train loss 5.271, Val loss 5.379\n",
      "Ep 1 (Step 000210): Train loss 5.139, Val loss 5.359\n",
      "Ep 1 (Step 000220): Train loss 5.228, Val loss 5.331\n",
      "Ep 1 (Step 000230): Train loss 5.187, Val loss 5.326\n",
      "Ep 1 (Step 000240): Train loss 5.189, Val loss 5.328\n",
      "Ep 1 (Step 000250): Train loss 5.240, Val loss 5.301\n",
      "Ep 1 (Step 000260): Train loss 5.124, Val loss 5.289\n",
      "Ep 1 (Step 000270): Train loss 5.168, Val loss 5.286\n",
      "Ep 1 (Step 000280): Train loss 5.140, Val loss 5.262\n",
      "Ep 1 (Step 000290): Train loss 5.160, Val loss 5.260\n",
      "Ep 1 (Step 000300): Train loss 5.120, Val loss 5.253\n",
      "Ep 1 (Step 000310): Train loss 5.016, Val loss 5.253\n",
      "Ep 1 (Step 000320): Train loss 5.121, Val loss 5.236\n",
      "Ep 1 (Step 000330): Train loss 5.024, Val loss 5.220\n",
      "Ep 1 (Step 000340): Train loss 5.091, Val loss 5.202\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2023\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.656, Val loss 8.637\n",
      "Ep 1 (Step 000010): Train loss 6.575, Val loss 6.603\n",
      "Ep 1 (Step 000020): Train loss 6.547, Val loss 6.542\n",
      "Ep 1 (Step 000030): Train loss 6.270, Val loss 6.292\n",
      "Ep 1 (Step 000040): Train loss 6.120, Val loss 6.105\n",
      "Ep 1 (Step 000050): Train loss 5.938, Val loss 5.964\n",
      "Ep 1 (Step 000060): Train loss 5.808, Val loss 5.807\n",
      "Ep 1 (Step 000070): Train loss 5.767, Val loss 5.732\n",
      "Ep 1 (Step 000080): Train loss 5.597, Val loss 5.641\n",
      "Ep 1 (Step 000090): Train loss 5.609, Val loss 5.618\n",
      "Ep 1 (Step 000100): Train loss 5.484, Val loss 5.577\n",
      "Ep 1 (Step 000110): Train loss 5.419, Val loss 5.535\n",
      "Ep 1 (Step 000120): Train loss 5.507, Val loss 5.515\n",
      "Ep 1 (Step 000130): Train loss 5.325, Val loss 5.477\n",
      "Ep 1 (Step 000140): Train loss 5.478, Val loss 5.474\n",
      "Ep 1 (Step 000150): Train loss 5.377, Val loss 5.439\n",
      "Ep 1 (Step 000160): Train loss 5.241, Val loss 5.405\n",
      "Ep 1 (Step 000170): Train loss 5.342, Val loss 5.382\n",
      "Ep 1 (Step 000180): Train loss 5.260, Val loss 5.360\n",
      "Ep 1 (Step 000190): Train loss 5.209, Val loss 5.315\n",
      "Ep 1 (Step 000200): Train loss 5.129, Val loss 5.347\n",
      "Ep 1 (Step 000210): Train loss 5.118, Val loss 5.307\n",
      "Ep 1 (Step 000220): Train loss 5.167, Val loss 5.295\n",
      "Ep 1 (Step 000230): Train loss 5.166, Val loss 5.265\n",
      "Ep 1 (Step 000240): Train loss 5.194, Val loss 5.265\n",
      "Ep 1 (Step 000250): Train loss 5.065, Val loss 5.241\n",
      "Ep 1 (Step 000260): Train loss 5.151, Val loss 5.252\n",
      "Ep 1 (Step 000270): Train loss 5.162, Val loss 5.228\n",
      "Ep 1 (Step 000280): Train loss 5.055, Val loss 5.213\n",
      "Ep 1 (Step 000290): Train loss 5.037, Val loss 5.208\n",
      "Ep 1 (Step 000300): Train loss 5.000, Val loss 5.197\n",
      "Ep 1 (Step 000310): Train loss 5.035, Val loss 5.195\n",
      "Ep 1 (Step 000320): Train loss 4.946, Val loss 5.173\n",
      "Ep 1 (Step 000330): Train loss 4.951, Val loss 5.187\n",
      "Ep 1 (Step 000340): Train loss 5.100, Val loss 5.155\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1552\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.662, Val loss 8.645\n",
      "Ep 1 (Step 000010): Train loss 6.600, Val loss 6.564\n",
      "Ep 1 (Step 000020): Train loss 6.489, Val loss 6.509\n",
      "Ep 1 (Step 000030): Train loss 6.322, Val loss 6.332\n",
      "Ep 1 (Step 000040): Train loss 6.197, Val loss 6.132\n",
      "Ep 1 (Step 000050): Train loss 5.877, Val loss 5.953\n",
      "Ep 1 (Step 000060): Train loss 5.853, Val loss 5.858\n",
      "Ep 1 (Step 000070): Train loss 5.734, Val loss 5.749\n",
      "Ep 1 (Step 000080): Train loss 5.644, Val loss 5.690\n",
      "Ep 1 (Step 000090): Train loss 5.529, Val loss 5.613\n",
      "Ep 1 (Step 000100): Train loss 5.589, Val loss 5.581\n",
      "Ep 1 (Step 000110): Train loss 5.438, Val loss 5.524\n",
      "Ep 1 (Step 000120): Train loss 5.461, Val loss 5.508\n",
      "Ep 1 (Step 000130): Train loss 5.354, Val loss 5.483\n",
      "Ep 1 (Step 000140): Train loss 5.281, Val loss 5.433\n",
      "Ep 1 (Step 000150): Train loss 5.307, Val loss 5.405\n",
      "Ep 1 (Step 000160): Train loss 5.223, Val loss 5.403\n",
      "Ep 1 (Step 000170): Train loss 5.325, Val loss 5.359\n",
      "Ep 1 (Step 000180): Train loss 5.283, Val loss 5.337\n",
      "Ep 1 (Step 000190): Train loss 5.235, Val loss 5.348\n",
      "Ep 1 (Step 000200): Train loss 5.307, Val loss 5.329\n",
      "Ep 1 (Step 000210): Train loss 5.243, Val loss 5.322\n",
      "Ep 1 (Step 000220): Train loss 5.187, Val loss 5.331\n",
      "Ep 1 (Step 000230): Train loss 5.114, Val loss 5.297\n",
      "Ep 1 (Step 000240): Train loss 5.137, Val loss 5.281\n",
      "Ep 1 (Step 000250): Train loss 5.036, Val loss 5.280\n",
      "Ep 1 (Step 000260): Train loss 5.087, Val loss 5.264\n",
      "Ep 1 (Step 000270): Train loss 5.165, Val loss 5.242\n",
      "Ep 1 (Step 000280): Train loss 5.185, Val loss 5.248\n",
      "Ep 1 (Step 000290): Train loss 4.956, Val loss 5.235\n",
      "Ep 1 (Step 000300): Train loss 4.977, Val loss 5.229\n",
      "Ep 1 (Step 000310): Train loss 4.990, Val loss 5.214\n",
      "Ep 1 (Step 000320): Train loss 5.077, Val loss 5.209\n",
      "Ep 1 (Step 000330): Train loss 4.886, Val loss 5.189\n",
      "Ep 1 (Step 000340): Train loss 4.987, Val loss 5.167\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1674\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.602, Val loss 8.585\n",
      "Ep 1 (Step 000010): Train loss 6.586, Val loss 6.599\n",
      "Ep 1 (Step 000020): Train loss 6.448, Val loss 6.535\n",
      "Ep 1 (Step 000030): Train loss 6.276, Val loss 6.265\n",
      "Ep 1 (Step 000040): Train loss 6.091, Val loss 6.085\n",
      "Ep 1 (Step 000050): Train loss 5.879, Val loss 5.948\n",
      "Ep 1 (Step 000060): Train loss 5.779, Val loss 5.820\n",
      "Ep 1 (Step 000070): Train loss 5.542, Val loss 5.736\n",
      "Ep 1 (Step 000080): Train loss 5.667, Val loss 5.661\n",
      "Ep 1 (Step 000090): Train loss 5.513, Val loss 5.623\n",
      "Ep 1 (Step 000100): Train loss 5.576, Val loss 5.575\n",
      "Ep 1 (Step 000110): Train loss 5.432, Val loss 5.540\n",
      "Ep 1 (Step 000120): Train loss 5.412, Val loss 5.519\n",
      "Ep 1 (Step 000130): Train loss 5.377, Val loss 5.492\n",
      "Ep 1 (Step 000140): Train loss 5.219, Val loss 5.442\n",
      "Ep 1 (Step 000150): Train loss 5.347, Val loss 5.467\n",
      "Ep 1 (Step 000160): Train loss 5.266, Val loss 5.425\n",
      "Ep 1 (Step 000170): Train loss 5.193, Val loss 5.400\n",
      "Ep 1 (Step 000180): Train loss 5.255, Val loss 5.376\n",
      "Ep 1 (Step 000190): Train loss 5.244, Val loss 5.363\n",
      "Ep 1 (Step 000200): Train loss 5.181, Val loss 5.349\n",
      "Ep 1 (Step 000210): Train loss 5.176, Val loss 5.341\n",
      "Ep 1 (Step 000220): Train loss 5.151, Val loss 5.306\n",
      "Ep 1 (Step 000230): Train loss 5.047, Val loss 5.303\n",
      "Ep 1 (Step 000240): Train loss 5.127, Val loss 5.278\n",
      "Ep 1 (Step 000250): Train loss 5.109, Val loss 5.263\n",
      "Ep 1 (Step 000260): Train loss 5.163, Val loss 5.262\n",
      "Ep 1 (Step 000270): Train loss 5.113, Val loss 5.236\n",
      "Ep 1 (Step 000280): Train loss 5.064, Val loss 5.234\n",
      "Ep 1 (Step 000290): Train loss 5.039, Val loss 5.212\n",
      "Ep 1 (Step 000300): Train loss 5.027, Val loss 5.201\n",
      "Ep 1 (Step 000310): Train loss 5.051, Val loss 5.194\n",
      "Ep 1 (Step 000320): Train loss 4.979, Val loss 5.199\n",
      "Ep 1 (Step 000330): Train loss 4.995, Val loss 5.197\n",
      "Ep 1 (Step 000340): Train loss 4.988, Val loss 5.159\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1595\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.615, Val loss 8.581\n",
      "Ep 1 (Step 000010): Train loss 6.610, Val loss 6.581\n",
      "Ep 1 (Step 000020): Train loss 6.501, Val loss 6.480\n",
      "Ep 1 (Step 000030): Train loss 6.311, Val loss 6.331\n",
      "Ep 1 (Step 000040): Train loss 6.187, Val loss 6.114\n",
      "Ep 1 (Step 000050): Train loss 5.881, Val loss 5.958\n",
      "Ep 1 (Step 000060): Train loss 5.790, Val loss 5.880\n",
      "Ep 1 (Step 000070): Train loss 5.760, Val loss 5.739\n",
      "Ep 1 (Step 000080): Train loss 5.610, Val loss 5.663\n",
      "Ep 1 (Step 000090): Train loss 5.521, Val loss 5.595\n",
      "Ep 1 (Step 000100): Train loss 5.486, Val loss 5.536\n",
      "Ep 1 (Step 000110): Train loss 5.482, Val loss 5.489\n",
      "Ep 1 (Step 000120): Train loss 5.408, Val loss 5.468\n",
      "Ep 1 (Step 000130): Train loss 5.441, Val loss 5.453\n",
      "Ep 1 (Step 000140): Train loss 5.334, Val loss 5.457\n",
      "Ep 1 (Step 000150): Train loss 5.353, Val loss 5.397\n",
      "Ep 1 (Step 000160): Train loss 5.329, Val loss 5.404\n",
      "Ep 1 (Step 000170): Train loss 5.264, Val loss 5.350\n",
      "Ep 1 (Step 000180): Train loss 5.204, Val loss 5.314\n",
      "Ep 1 (Step 000190): Train loss 5.197, Val loss 5.320\n",
      "Ep 1 (Step 000200): Train loss 5.027, Val loss 5.286\n",
      "Ep 1 (Step 000210): Train loss 5.205, Val loss 5.278\n",
      "Ep 1 (Step 000220): Train loss 5.058, Val loss 5.259\n",
      "Ep 1 (Step 000230): Train loss 5.153, Val loss 5.247\n",
      "Ep 1 (Step 000240): Train loss 5.056, Val loss 5.243\n",
      "Ep 1 (Step 000250): Train loss 5.083, Val loss 5.238\n",
      "Ep 1 (Step 000260): Train loss 5.052, Val loss 5.228\n",
      "Ep 1 (Step 000270): Train loss 5.089, Val loss 5.208\n",
      "Ep 1 (Step 000280): Train loss 5.009, Val loss 5.208\n",
      "Ep 1 (Step 000290): Train loss 4.990, Val loss 5.196\n",
      "Ep 1 (Step 000300): Train loss 5.070, Val loss 5.175\n",
      "Ep 1 (Step 000310): Train loss 5.057, Val loss 5.193\n",
      "Ep 1 (Step 000320): Train loss 5.053, Val loss 5.170\n",
      "Ep 1 (Step 000330): Train loss 4.985, Val loss 5.174\n",
      "Ep 1 (Step 000340): Train loss 4.838, Val loss 5.140\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1396\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.610, Val loss 8.574\n",
      "Ep 1 (Step 000010): Train loss 6.574, Val loss 6.533\n",
      "Ep 1 (Step 000020): Train loss 6.521, Val loss 6.509\n",
      "Ep 1 (Step 000030): Train loss 6.354, Val loss 6.344\n",
      "Ep 1 (Step 000040): Train loss 6.125, Val loss 6.115\n",
      "Ep 1 (Step 000050): Train loss 5.927, Val loss 5.975\n",
      "Ep 1 (Step 000060): Train loss 5.733, Val loss 5.864\n",
      "Ep 1 (Step 000070): Train loss 5.658, Val loss 5.748\n",
      "Ep 1 (Step 000080): Train loss 5.587, Val loss 5.697\n",
      "Ep 1 (Step 000090): Train loss 5.501, Val loss 5.650\n",
      "Ep 1 (Step 000100): Train loss 5.514, Val loss 5.574\n",
      "Ep 1 (Step 000110): Train loss 5.498, Val loss 5.547\n",
      "Ep 1 (Step 000120): Train loss 5.405, Val loss 5.509\n",
      "Ep 1 (Step 000130): Train loss 5.315, Val loss 5.457\n",
      "Ep 1 (Step 000140): Train loss 5.332, Val loss 5.420\n",
      "Ep 1 (Step 000150): Train loss 5.221, Val loss 5.415\n",
      "Ep 1 (Step 000160): Train loss 5.252, Val loss 5.380\n",
      "Ep 1 (Step 000170): Train loss 5.214, Val loss 5.358\n",
      "Ep 1 (Step 000180): Train loss 5.127, Val loss 5.346\n",
      "Ep 1 (Step 000190): Train loss 5.205, Val loss 5.308\n",
      "Ep 1 (Step 000200): Train loss 5.228, Val loss 5.315\n",
      "Ep 1 (Step 000210): Train loss 5.162, Val loss 5.275\n",
      "Ep 1 (Step 000220): Train loss 5.233, Val loss 5.259\n",
      "Ep 1 (Step 000230): Train loss 5.095, Val loss 5.265\n",
      "Ep 1 (Step 000240): Train loss 5.100, Val loss 5.266\n",
      "Ep 1 (Step 000250): Train loss 5.119, Val loss 5.243\n",
      "Ep 1 (Step 000260): Train loss 5.035, Val loss 5.242\n",
      "Ep 1 (Step 000270): Train loss 5.009, Val loss 5.212\n",
      "Ep 1 (Step 000280): Train loss 5.108, Val loss 5.194\n",
      "Ep 1 (Step 000290): Train loss 4.999, Val loss 5.179\n",
      "Ep 1 (Step 000300): Train loss 4.923, Val loss 5.165\n",
      "Ep 1 (Step 000310): Train loss 4.960, Val loss 5.169\n",
      "Ep 1 (Step 000320): Train loss 5.056, Val loss 5.175\n",
      "Ep 1 (Step 000330): Train loss 4.877, Val loss 5.148\n",
      "Ep 1 (Step 000340): Train loss 5.003, Val loss 5.127\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1267\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.611, Val loss 8.587\n",
      "Ep 1 (Step 000010): Train loss 6.554, Val loss 6.562\n",
      "Ep 1 (Step 000020): Train loss 6.410, Val loss 6.450\n",
      "Ep 1 (Step 000030): Train loss 6.266, Val loss 6.228\n",
      "Ep 1 (Step 000040): Train loss 6.043, Val loss 6.055\n",
      "Ep 1 (Step 000050): Train loss 5.853, Val loss 5.897\n",
      "Ep 1 (Step 000060): Train loss 5.739, Val loss 5.772\n",
      "Ep 1 (Step 000070): Train loss 5.651, Val loss 5.716\n",
      "Ep 1 (Step 000080): Train loss 5.698, Val loss 5.646\n",
      "Ep 1 (Step 000090): Train loss 5.434, Val loss 5.604\n",
      "Ep 1 (Step 000100): Train loss 5.477, Val loss 5.531\n",
      "Ep 1 (Step 000110): Train loss 5.377, Val loss 5.507\n",
      "Ep 1 (Step 000120): Train loss 5.452, Val loss 5.463\n",
      "Ep 1 (Step 000130): Train loss 5.344, Val loss 5.412\n",
      "Ep 1 (Step 000140): Train loss 5.302, Val loss 5.403\n",
      "Ep 1 (Step 000150): Train loss 5.249, Val loss 5.392\n",
      "Ep 1 (Step 000160): Train loss 5.240, Val loss 5.353\n",
      "Ep 1 (Step 000170): Train loss 5.190, Val loss 5.349\n",
      "Ep 1 (Step 000180): Train loss 5.152, Val loss 5.316\n",
      "Ep 1 (Step 000190): Train loss 5.312, Val loss 5.311\n",
      "Ep 1 (Step 000200): Train loss 5.229, Val loss 5.293\n",
      "Ep 1 (Step 000210): Train loss 5.065, Val loss 5.286\n",
      "Ep 1 (Step 000220): Train loss 5.242, Val loss 5.290\n",
      "Ep 1 (Step 000230): Train loss 5.107, Val loss 5.266\n",
      "Ep 1 (Step 000240): Train loss 5.042, Val loss 5.275\n",
      "Ep 1 (Step 000250): Train loss 5.023, Val loss 5.253\n",
      "Ep 1 (Step 000260): Train loss 5.115, Val loss 5.208\n",
      "Ep 1 (Step 000270): Train loss 5.020, Val loss 5.203\n",
      "Ep 1 (Step 000280): Train loss 5.030, Val loss 5.187\n",
      "Ep 1 (Step 000290): Train loss 5.020, Val loss 5.176\n",
      "Ep 1 (Step 000300): Train loss 5.042, Val loss 5.186\n",
      "Ep 1 (Step 000310): Train loss 5.002, Val loss 5.159\n",
      "Ep 1 (Step 000320): Train loss 4.988, Val loss 5.171\n",
      "Ep 1 (Step 000330): Train loss 4.888, Val loss 5.162\n",
      "Ep 1 (Step 000340): Train loss 4.970, Val loss 5.156\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 6, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1558\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.847, Val loss 8.830\n",
      "Ep 1 (Step 000010): Train loss 7.023, Val loss 7.025\n",
      "Ep 1 (Step 000020): Train loss 6.585, Val loss 6.531\n",
      "Ep 1 (Step 000030): Train loss 6.413, Val loss 6.447\n",
      "Ep 1 (Step 000040): Train loss 6.341, Val loss 6.390\n",
      "Ep 1 (Step 000050): Train loss 6.152, Val loss 6.203\n",
      "Ep 1 (Step 000060): Train loss 6.095, Val loss 6.066\n",
      "Ep 1 (Step 000070): Train loss 5.933, Val loss 5.969\n",
      "Ep 1 (Step 000080): Train loss 5.844, Val loss 5.886\n",
      "Ep 1 (Step 000090): Train loss 5.751, Val loss 5.813\n",
      "Ep 1 (Step 000100): Train loss 5.695, Val loss 5.744\n",
      "Ep 1 (Step 000110): Train loss 5.625, Val loss 5.704\n",
      "Ep 1 (Step 000120): Train loss 5.435, Val loss 5.654\n",
      "Ep 1 (Step 000130): Train loss 5.580, Val loss 5.612\n",
      "Ep 1 (Step 000140): Train loss 5.517, Val loss 5.573\n",
      "Ep 1 (Step 000150): Train loss 5.506, Val loss 5.561\n",
      "Ep 1 (Step 000160): Train loss 5.430, Val loss 5.510\n",
      "Ep 1 (Step 000170): Train loss 5.412, Val loss 5.490\n",
      "Ep 1 (Step 000180): Train loss 5.374, Val loss 5.474\n",
      "Ep 1 (Step 000190): Train loss 5.410, Val loss 5.458\n",
      "Ep 1 (Step 000200): Train loss 5.331, Val loss 5.436\n",
      "Ep 1 (Step 000210): Train loss 5.395, Val loss 5.427\n",
      "Ep 1 (Step 000220): Train loss 5.267, Val loss 5.397\n",
      "Ep 1 (Step 000230): Train loss 5.402, Val loss 5.376\n",
      "Ep 1 (Step 000240): Train loss 5.257, Val loss 5.355\n",
      "Ep 1 (Step 000250): Train loss 5.249, Val loss 5.344\n",
      "Ep 1 (Step 000260): Train loss 5.210, Val loss 5.349\n",
      "Ep 1 (Step 000270): Train loss 5.169, Val loss 5.329\n",
      "Ep 1 (Step 000280): Train loss 5.173, Val loss 5.311\n",
      "Ep 1 (Step 000290): Train loss 5.175, Val loss 5.313\n",
      "Ep 1 (Step 000300): Train loss 5.154, Val loss 5.299\n",
      "Ep 1 (Step 000310): Train loss 5.122, Val loss 5.299\n",
      "Ep 1 (Step 000320): Train loss 5.086, Val loss 5.273\n",
      "Ep 1 (Step 000330): Train loss 5.151, Val loss 5.263\n",
      "Ep 1 (Step 000340): Train loss 5.099, Val loss 5.255\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2554\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.862, Val loss 8.831\n",
      "Ep 1 (Step 000010): Train loss 7.069, Val loss 7.017\n",
      "Ep 1 (Step 000020): Train loss 6.510, Val loss 6.488\n",
      "Ep 1 (Step 000030): Train loss 6.435, Val loss 6.409\n",
      "Ep 1 (Step 000040): Train loss 6.334, Val loss 6.369\n",
      "Ep 1 (Step 000050): Train loss 6.207, Val loss 6.228\n",
      "Ep 1 (Step 000060): Train loss 6.010, Val loss 6.066\n",
      "Ep 1 (Step 000070): Train loss 5.961, Val loss 5.978\n",
      "Ep 1 (Step 000080): Train loss 5.789, Val loss 5.884\n",
      "Ep 1 (Step 000090): Train loss 5.851, Val loss 5.811\n",
      "Ep 1 (Step 000100): Train loss 5.773, Val loss 5.767\n",
      "Ep 1 (Step 000110): Train loss 5.715, Val loss 5.677\n",
      "Ep 1 (Step 000120): Train loss 5.600, Val loss 5.634\n",
      "Ep 1 (Step 000130): Train loss 5.513, Val loss 5.602\n",
      "Ep 1 (Step 000140): Train loss 5.516, Val loss 5.587\n",
      "Ep 1 (Step 000150): Train loss 5.422, Val loss 5.551\n",
      "Ep 1 (Step 000160): Train loss 5.611, Val loss 5.521\n",
      "Ep 1 (Step 000170): Train loss 5.260, Val loss 5.507\n",
      "Ep 1 (Step 000180): Train loss 5.399, Val loss 5.474\n",
      "Ep 1 (Step 000190): Train loss 5.333, Val loss 5.455\n",
      "Ep 1 (Step 000200): Train loss 5.301, Val loss 5.429\n",
      "Ep 1 (Step 000210): Train loss 5.358, Val loss 5.408\n",
      "Ep 1 (Step 000220): Train loss 5.342, Val loss 5.402\n",
      "Ep 1 (Step 000230): Train loss 5.262, Val loss 5.382\n",
      "Ep 1 (Step 000240): Train loss 5.235, Val loss 5.390\n",
      "Ep 1 (Step 000250): Train loss 5.241, Val loss 5.383\n",
      "Ep 1 (Step 000260): Train loss 5.223, Val loss 5.358\n",
      "Ep 1 (Step 000270): Train loss 5.305, Val loss 5.357\n",
      "Ep 1 (Step 000280): Train loss 5.188, Val loss 5.328\n",
      "Ep 1 (Step 000290): Train loss 5.203, Val loss 5.322\n",
      "Ep 1 (Step 000300): Train loss 5.138, Val loss 5.300\n",
      "Ep 1 (Step 000310): Train loss 5.188, Val loss 5.294\n",
      "Ep 1 (Step 000320): Train loss 5.075, Val loss 5.289\n",
      "Ep 1 (Step 000330): Train loss 5.239, Val loss 5.283\n",
      "Ep 1 (Step 000340): Train loss 5.113, Val loss 5.267\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2666\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.828, Val loss 8.804\n",
      "Ep 1 (Step 000010): Train loss 7.026, Val loss 7.022\n",
      "Ep 1 (Step 000020): Train loss 6.535, Val loss 6.494\n",
      "Ep 1 (Step 000030): Train loss 6.414, Val loss 6.420\n",
      "Ep 1 (Step 000040): Train loss 6.409, Val loss 6.377\n",
      "Ep 1 (Step 000050): Train loss 6.201, Val loss 6.219\n",
      "Ep 1 (Step 000060): Train loss 6.103, Val loss 6.088\n",
      "Ep 1 (Step 000070): Train loss 5.995, Val loss 5.997\n",
      "Ep 1 (Step 000080): Train loss 5.869, Val loss 5.919\n",
      "Ep 1 (Step 000090): Train loss 5.897, Val loss 5.839\n",
      "Ep 1 (Step 000100): Train loss 5.735, Val loss 5.778\n",
      "Ep 1 (Step 000110): Train loss 5.713, Val loss 5.733\n",
      "Ep 1 (Step 000120): Train loss 5.622, Val loss 5.670\n",
      "Ep 1 (Step 000130): Train loss 5.543, Val loss 5.641\n",
      "Ep 1 (Step 000140): Train loss 5.525, Val loss 5.586\n",
      "Ep 1 (Step 000150): Train loss 5.583, Val loss 5.570\n",
      "Ep 1 (Step 000160): Train loss 5.363, Val loss 5.530\n",
      "Ep 1 (Step 000170): Train loss 5.405, Val loss 5.497\n",
      "Ep 1 (Step 000180): Train loss 5.401, Val loss 5.478\n",
      "Ep 1 (Step 000190): Train loss 5.382, Val loss 5.456\n",
      "Ep 1 (Step 000200): Train loss 5.339, Val loss 5.434\n",
      "Ep 1 (Step 000210): Train loss 5.354, Val loss 5.426\n",
      "Ep 1 (Step 000220): Train loss 5.285, Val loss 5.406\n",
      "Ep 1 (Step 000230): Train loss 5.277, Val loss 5.387\n",
      "Ep 1 (Step 000240): Train loss 5.280, Val loss 5.383\n",
      "Ep 1 (Step 000250): Train loss 5.215, Val loss 5.356\n",
      "Ep 1 (Step 000260): Train loss 5.261, Val loss 5.352\n",
      "Ep 1 (Step 000270): Train loss 5.165, Val loss 5.330\n",
      "Ep 1 (Step 000280): Train loss 5.228, Val loss 5.319\n",
      "Ep 1 (Step 000290): Train loss 5.195, Val loss 5.310\n",
      "Ep 1 (Step 000300): Train loss 5.188, Val loss 5.302\n",
      "Ep 1 (Step 000310): Train loss 5.167, Val loss 5.284\n",
      "Ep 1 (Step 000320): Train loss 5.212, Val loss 5.277\n",
      "Ep 1 (Step 000330): Train loss 5.179, Val loss 5.283\n",
      "Ep 1 (Step 000340): Train loss 5.175, Val loss 5.268\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2681\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.831, Val loss 8.816\n",
      "Ep 1 (Step 000010): Train loss 7.010, Val loss 6.979\n",
      "Ep 1 (Step 000020): Train loss 6.502, Val loss 6.493\n",
      "Ep 1 (Step 000030): Train loss 6.422, Val loss 6.402\n",
      "Ep 1 (Step 000040): Train loss 6.279, Val loss 6.333\n",
      "Ep 1 (Step 000050): Train loss 6.182, Val loss 6.165\n",
      "Ep 1 (Step 000060): Train loss 6.079, Val loss 6.029\n",
      "Ep 1 (Step 000070): Train loss 5.892, Val loss 5.958\n",
      "Ep 1 (Step 000080): Train loss 5.824, Val loss 5.872\n",
      "Ep 1 (Step 000090): Train loss 5.755, Val loss 5.801\n",
      "Ep 1 (Step 000100): Train loss 5.722, Val loss 5.747\n",
      "Ep 1 (Step 000110): Train loss 5.581, Val loss 5.676\n",
      "Ep 1 (Step 000120): Train loss 5.609, Val loss 5.625\n",
      "Ep 1 (Step 000130): Train loss 5.485, Val loss 5.598\n",
      "Ep 1 (Step 000140): Train loss 5.512, Val loss 5.555\n",
      "Ep 1 (Step 000150): Train loss 5.475, Val loss 5.537\n",
      "Ep 1 (Step 000160): Train loss 5.380, Val loss 5.498\n",
      "Ep 1 (Step 000170): Train loss 5.327, Val loss 5.486\n",
      "Ep 1 (Step 000180): Train loss 5.338, Val loss 5.478\n",
      "Ep 1 (Step 000190): Train loss 5.359, Val loss 5.446\n",
      "Ep 1 (Step 000200): Train loss 5.390, Val loss 5.434\n",
      "Ep 1 (Step 000210): Train loss 5.298, Val loss 5.411\n",
      "Ep 1 (Step 000220): Train loss 5.307, Val loss 5.386\n",
      "Ep 1 (Step 000230): Train loss 5.295, Val loss 5.367\n",
      "Ep 1 (Step 000240): Train loss 5.204, Val loss 5.351\n",
      "Ep 1 (Step 000250): Train loss 5.277, Val loss 5.341\n",
      "Ep 1 (Step 000260): Train loss 5.278, Val loss 5.330\n",
      "Ep 1 (Step 000270): Train loss 5.312, Val loss 5.314\n",
      "Ep 1 (Step 000280): Train loss 5.147, Val loss 5.310\n",
      "Ep 1 (Step 000290): Train loss 5.143, Val loss 5.288\n",
      "Ep 1 (Step 000300): Train loss 5.134, Val loss 5.297\n",
      "Ep 1 (Step 000310): Train loss 5.099, Val loss 5.273\n",
      "Ep 1 (Step 000320): Train loss 5.166, Val loss 5.263\n",
      "Ep 1 (Step 000330): Train loss 4.992, Val loss 5.263\n",
      "Ep 1 (Step 000340): Train loss 5.137, Val loss 5.263\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2628\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.853, Val loss 8.814\n",
      "Ep 1 (Step 000010): Train loss 6.999, Val loss 6.997\n",
      "Ep 1 (Step 000020): Train loss 6.534, Val loss 6.507\n",
      "Ep 1 (Step 000030): Train loss 6.459, Val loss 6.424\n",
      "Ep 1 (Step 000040): Train loss 6.400, Val loss 6.365\n",
      "Ep 1 (Step 000050): Train loss 6.220, Val loss 6.257\n",
      "Ep 1 (Step 000060): Train loss 6.130, Val loss 6.135\n",
      "Ep 1 (Step 000070): Train loss 5.898, Val loss 6.023\n",
      "Ep 1 (Step 000080): Train loss 5.865, Val loss 5.892\n",
      "Ep 1 (Step 000090): Train loss 5.738, Val loss 5.827\n",
      "Ep 1 (Step 000100): Train loss 5.686, Val loss 5.775\n",
      "Ep 1 (Step 000110): Train loss 5.671, Val loss 5.717\n",
      "Ep 1 (Step 000120): Train loss 5.550, Val loss 5.665\n",
      "Ep 1 (Step 000130): Train loss 5.514, Val loss 5.611\n",
      "Ep 1 (Step 000140): Train loss 5.516, Val loss 5.560\n",
      "Ep 1 (Step 000150): Train loss 5.409, Val loss 5.545\n",
      "Ep 1 (Step 000160): Train loss 5.442, Val loss 5.507\n",
      "Ep 1 (Step 000170): Train loss 5.317, Val loss 5.485\n",
      "Ep 1 (Step 000180): Train loss 5.363, Val loss 5.457\n",
      "Ep 1 (Step 000190): Train loss 5.305, Val loss 5.428\n",
      "Ep 1 (Step 000200): Train loss 5.318, Val loss 5.407\n",
      "Ep 1 (Step 000210): Train loss 5.250, Val loss 5.396\n",
      "Ep 1 (Step 000220): Train loss 5.330, Val loss 5.385\n",
      "Ep 1 (Step 000230): Train loss 5.228, Val loss 5.358\n",
      "Ep 1 (Step 000240): Train loss 5.259, Val loss 5.365\n",
      "Ep 1 (Step 000250): Train loss 5.242, Val loss 5.338\n",
      "Ep 1 (Step 000260): Train loss 5.221, Val loss 5.324\n",
      "Ep 1 (Step 000270): Train loss 5.185, Val loss 5.317\n",
      "Ep 1 (Step 000280): Train loss 5.136, Val loss 5.313\n",
      "Ep 1 (Step 000290): Train loss 5.203, Val loss 5.296\n",
      "Ep 1 (Step 000300): Train loss 5.182, Val loss 5.279\n",
      "Ep 1 (Step 000310): Train loss 5.257, Val loss 5.281\n",
      "Ep 1 (Step 000320): Train loss 5.200, Val loss 5.253\n",
      "Ep 1 (Step 000330): Train loss 5.093, Val loss 5.239\n",
      "Ep 1 (Step 000340): Train loss 5.017, Val loss 5.249\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2489\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.874, Val loss 8.817\n",
      "Ep 1 (Step 000010): Train loss 6.953, Val loss 6.942\n",
      "Ep 1 (Step 000020): Train loss 6.497, Val loss 6.481\n",
      "Ep 1 (Step 000030): Train loss 6.463, Val loss 6.437\n",
      "Ep 1 (Step 000040): Train loss 6.427, Val loss 6.401\n",
      "Ep 1 (Step 000050): Train loss 6.314, Val loss 6.279\n",
      "Ep 1 (Step 000060): Train loss 6.122, Val loss 6.164\n",
      "Ep 1 (Step 000070): Train loss 5.975, Val loss 6.003\n",
      "Ep 1 (Step 000080): Train loss 5.898, Val loss 5.931\n",
      "Ep 1 (Step 000090): Train loss 5.740, Val loss 5.861\n",
      "Ep 1 (Step 000100): Train loss 5.737, Val loss 5.790\n",
      "Ep 1 (Step 000110): Train loss 5.784, Val loss 5.730\n",
      "Ep 1 (Step 000120): Train loss 5.698, Val loss 5.695\n",
      "Ep 1 (Step 000130): Train loss 5.568, Val loss 5.643\n",
      "Ep 1 (Step 000140): Train loss 5.460, Val loss 5.589\n",
      "Ep 1 (Step 000150): Train loss 5.421, Val loss 5.553\n",
      "Ep 1 (Step 000160): Train loss 5.406, Val loss 5.519\n",
      "Ep 1 (Step 000170): Train loss 5.458, Val loss 5.514\n",
      "Ep 1 (Step 000180): Train loss 5.395, Val loss 5.485\n",
      "Ep 1 (Step 000190): Train loss 5.458, Val loss 5.454\n",
      "Ep 1 (Step 000200): Train loss 5.404, Val loss 5.443\n",
      "Ep 1 (Step 000210): Train loss 5.342, Val loss 5.413\n",
      "Ep 1 (Step 000220): Train loss 5.264, Val loss 5.390\n",
      "Ep 1 (Step 000230): Train loss 5.332, Val loss 5.376\n",
      "Ep 1 (Step 000240): Train loss 5.229, Val loss 5.371\n",
      "Ep 1 (Step 000250): Train loss 5.180, Val loss 5.342\n",
      "Ep 1 (Step 000260): Train loss 5.213, Val loss 5.323\n",
      "Ep 1 (Step 000270): Train loss 5.193, Val loss 5.317\n",
      "Ep 1 (Step 000280): Train loss 5.178, Val loss 5.312\n",
      "Ep 1 (Step 000290): Train loss 5.177, Val loss 5.281\n",
      "Ep 1 (Step 000300): Train loss 5.070, Val loss 5.274\n",
      "Ep 1 (Step 000310): Train loss 5.194, Val loss 5.269\n",
      "Ep 1 (Step 000320): Train loss 5.107, Val loss 5.270\n",
      "Ep 1 (Step 000330): Train loss 5.076, Val loss 5.258\n",
      "Ep 1 (Step 000340): Train loss 5.097, Val loss 5.233\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2328\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.550, Val loss 8.493\n",
      "Ep 1 (Step 000010): Train loss 6.665, Val loss 6.569\n",
      "Ep 1 (Step 000020): Train loss 6.486, Val loss 6.563\n",
      "Ep 1 (Step 000030): Train loss 6.388, Val loss 6.357\n",
      "Ep 1 (Step 000040): Train loss 6.167, Val loss 6.212\n",
      "Ep 1 (Step 000050): Train loss 5.979, Val loss 6.044\n",
      "Ep 1 (Step 000060): Train loss 5.847, Val loss 5.912\n",
      "Ep 1 (Step 000070): Train loss 5.795, Val loss 5.831\n",
      "Ep 1 (Step 000080): Train loss 5.660, Val loss 5.731\n",
      "Ep 1 (Step 000090): Train loss 5.515, Val loss 5.647\n",
      "Ep 1 (Step 000100): Train loss 5.554, Val loss 5.630\n",
      "Ep 1 (Step 000110): Train loss 5.532, Val loss 5.583\n",
      "Ep 1 (Step 000120): Train loss 5.468, Val loss 5.564\n",
      "Ep 1 (Step 000130): Train loss 5.435, Val loss 5.534\n",
      "Ep 1 (Step 000140): Train loss 5.292, Val loss 5.500\n",
      "Ep 1 (Step 000150): Train loss 5.294, Val loss 5.486\n",
      "Ep 1 (Step 000160): Train loss 5.376, Val loss 5.454\n",
      "Ep 1 (Step 000170): Train loss 5.312, Val loss 5.446\n",
      "Ep 1 (Step 000180): Train loss 5.195, Val loss 5.408\n",
      "Ep 1 (Step 000190): Train loss 5.272, Val loss 5.401\n",
      "Ep 1 (Step 000200): Train loss 5.174, Val loss 5.385\n",
      "Ep 1 (Step 000210): Train loss 5.204, Val loss 5.379\n",
      "Ep 1 (Step 000220): Train loss 5.247, Val loss 5.346\n",
      "Ep 1 (Step 000230): Train loss 5.174, Val loss 5.332\n",
      "Ep 1 (Step 000240): Train loss 5.119, Val loss 5.325\n",
      "Ep 1 (Step 000250): Train loss 5.169, Val loss 5.311\n",
      "Ep 1 (Step 000260): Train loss 5.159, Val loss 5.300\n",
      "Ep 1 (Step 000270): Train loss 5.108, Val loss 5.281\n",
      "Ep 1 (Step 000280): Train loss 5.171, Val loss 5.280\n",
      "Ep 1 (Step 000290): Train loss 5.098, Val loss 5.257\n",
      "Ep 1 (Step 000300): Train loss 5.099, Val loss 5.252\n",
      "Ep 1 (Step 000310): Train loss 5.056, Val loss 5.244\n",
      "Ep 1 (Step 000320): Train loss 5.138, Val loss 5.241\n",
      "Ep 1 (Step 000330): Train loss 5.104, Val loss 5.226\n",
      "Ep 1 (Step 000340): Train loss 5.036, Val loss 5.225\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2247\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.498, Val loss 8.429\n",
      "Ep 1 (Step 000010): Train loss 6.531, Val loss 6.558\n",
      "Ep 1 (Step 000020): Train loss 6.461, Val loss 6.527\n",
      "Ep 1 (Step 000030): Train loss 6.377, Val loss 6.387\n",
      "Ep 1 (Step 000040): Train loss 6.169, Val loss 6.172\n",
      "Ep 1 (Step 000050): Train loss 5.985, Val loss 6.034\n",
      "Ep 1 (Step 000060): Train loss 5.901, Val loss 5.940\n",
      "Ep 1 (Step 000070): Train loss 5.773, Val loss 5.841\n",
      "Ep 1 (Step 000080): Train loss 5.725, Val loss 5.749\n",
      "Ep 1 (Step 000090): Train loss 5.611, Val loss 5.686\n",
      "Ep 1 (Step 000100): Train loss 5.517, Val loss 5.625\n",
      "Ep 1 (Step 000110): Train loss 5.531, Val loss 5.612\n",
      "Ep 1 (Step 000120): Train loss 5.561, Val loss 5.588\n",
      "Ep 1 (Step 000130): Train loss 5.418, Val loss 5.537\n",
      "Ep 1 (Step 000140): Train loss 5.462, Val loss 5.509\n",
      "Ep 1 (Step 000150): Train loss 5.401, Val loss 5.466\n",
      "Ep 1 (Step 000160): Train loss 5.350, Val loss 5.430\n",
      "Ep 1 (Step 000170): Train loss 5.358, Val loss 5.451\n",
      "Ep 1 (Step 000180): Train loss 5.327, Val loss 5.406\n",
      "Ep 1 (Step 000190): Train loss 5.222, Val loss 5.385\n",
      "Ep 1 (Step 000200): Train loss 5.296, Val loss 5.370\n",
      "Ep 1 (Step 000210): Train loss 5.241, Val loss 5.363\n",
      "Ep 1 (Step 000220): Train loss 5.206, Val loss 5.339\n",
      "Ep 1 (Step 000230): Train loss 5.177, Val loss 5.304\n",
      "Ep 1 (Step 000240): Train loss 5.185, Val loss 5.295\n",
      "Ep 1 (Step 000250): Train loss 5.091, Val loss 5.318\n",
      "Ep 1 (Step 000260): Train loss 5.172, Val loss 5.276\n",
      "Ep 1 (Step 000270): Train loss 5.209, Val loss 5.268\n",
      "Ep 1 (Step 000280): Train loss 5.128, Val loss 5.265\n",
      "Ep 1 (Step 000290): Train loss 5.160, Val loss 5.279\n",
      "Ep 1 (Step 000300): Train loss 5.130, Val loss 5.259\n",
      "Ep 1 (Step 000310): Train loss 5.182, Val loss 5.259\n",
      "Ep 1 (Step 000320): Train loss 5.023, Val loss 5.249\n",
      "Ep 1 (Step 000330): Train loss 5.026, Val loss 5.228\n",
      "Ep 1 (Step 000340): Train loss 5.110, Val loss 5.229\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2291\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.460, Val loss 8.394\n",
      "Ep 1 (Step 000010): Train loss 6.588, Val loss 6.546\n",
      "Ep 1 (Step 000020): Train loss 6.555, Val loss 6.509\n",
      "Ep 1 (Step 000030): Train loss 6.421, Val loss 6.300\n",
      "Ep 1 (Step 000040): Train loss 6.125, Val loss 6.149\n",
      "Ep 1 (Step 000050): Train loss 6.084, Val loss 5.967\n",
      "Ep 1 (Step 000060): Train loss 5.903, Val loss 5.902\n",
      "Ep 1 (Step 000070): Train loss 5.744, Val loss 5.769\n",
      "Ep 1 (Step 000080): Train loss 5.600, Val loss 5.706\n",
      "Ep 1 (Step 000090): Train loss 5.651, Val loss 5.660\n",
      "Ep 1 (Step 000100): Train loss 5.479, Val loss 5.625\n",
      "Ep 1 (Step 000110): Train loss 5.521, Val loss 5.575\n",
      "Ep 1 (Step 000120): Train loss 5.449, Val loss 5.534\n",
      "Ep 1 (Step 000130): Train loss 5.418, Val loss 5.486\n",
      "Ep 1 (Step 000140): Train loss 5.440, Val loss 5.462\n",
      "Ep 1 (Step 000150): Train loss 5.490, Val loss 5.490\n",
      "Ep 1 (Step 000160): Train loss 5.308, Val loss 5.443\n",
      "Ep 1 (Step 000170): Train loss 5.349, Val loss 5.417\n",
      "Ep 1 (Step 000180): Train loss 5.250, Val loss 5.407\n",
      "Ep 1 (Step 000190): Train loss 5.282, Val loss 5.407\n",
      "Ep 1 (Step 000200): Train loss 5.294, Val loss 5.382\n",
      "Ep 1 (Step 000210): Train loss 5.183, Val loss 5.359\n",
      "Ep 1 (Step 000220): Train loss 5.313, Val loss 5.337\n",
      "Ep 1 (Step 000230): Train loss 5.220, Val loss 5.323\n",
      "Ep 1 (Step 000240): Train loss 5.230, Val loss 5.318\n",
      "Ep 1 (Step 000250): Train loss 5.175, Val loss 5.316\n",
      "Ep 1 (Step 000260): Train loss 5.157, Val loss 5.326\n",
      "Ep 1 (Step 000270): Train loss 5.102, Val loss 5.286\n",
      "Ep 1 (Step 000280): Train loss 5.171, Val loss 5.301\n",
      "Ep 1 (Step 000290): Train loss 5.026, Val loss 5.283\n",
      "Ep 1 (Step 000300): Train loss 5.062, Val loss 5.263\n",
      "Ep 1 (Step 000310): Train loss 5.149, Val loss 5.256\n",
      "Ep 1 (Step 000320): Train loss 5.028, Val loss 5.267\n",
      "Ep 1 (Step 000330): Train loss 5.164, Val loss 5.233\n",
      "Ep 1 (Step 000340): Train loss 5.020, Val loss 5.225\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2247\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.533, Val loss 8.494\n",
      "Ep 1 (Step 000010): Train loss 6.645, Val loss 6.547\n",
      "Ep 1 (Step 000020): Train loss 6.447, Val loss 6.529\n",
      "Ep 1 (Step 000030): Train loss 6.358, Val loss 6.318\n",
      "Ep 1 (Step 000040): Train loss 6.190, Val loss 6.160\n",
      "Ep 1 (Step 000050): Train loss 6.001, Val loss 5.947\n",
      "Ep 1 (Step 000060): Train loss 5.859, Val loss 5.859\n",
      "Ep 1 (Step 000070): Train loss 5.756, Val loss 5.770\n",
      "Ep 1 (Step 000080): Train loss 5.588, Val loss 5.685\n",
      "Ep 1 (Step 000090): Train loss 5.614, Val loss 5.640\n",
      "Ep 1 (Step 000100): Train loss 5.555, Val loss 5.588\n",
      "Ep 1 (Step 000110): Train loss 5.508, Val loss 5.543\n",
      "Ep 1 (Step 000120): Train loss 5.446, Val loss 5.513\n",
      "Ep 1 (Step 000130): Train loss 5.409, Val loss 5.489\n",
      "Ep 1 (Step 000140): Train loss 5.408, Val loss 5.460\n",
      "Ep 1 (Step 000150): Train loss 5.431, Val loss 5.469\n",
      "Ep 1 (Step 000160): Train loss 5.339, Val loss 5.420\n",
      "Ep 1 (Step 000170): Train loss 5.298, Val loss 5.390\n",
      "Ep 1 (Step 000180): Train loss 5.237, Val loss 5.369\n",
      "Ep 1 (Step 000190): Train loss 5.204, Val loss 5.345\n",
      "Ep 1 (Step 000200): Train loss 5.243, Val loss 5.318\n",
      "Ep 1 (Step 000210): Train loss 5.139, Val loss 5.307\n",
      "Ep 1 (Step 000220): Train loss 5.198, Val loss 5.320\n",
      "Ep 1 (Step 000230): Train loss 5.211, Val loss 5.294\n",
      "Ep 1 (Step 000240): Train loss 5.086, Val loss 5.269\n",
      "Ep 1 (Step 000250): Train loss 5.154, Val loss 5.268\n",
      "Ep 1 (Step 000260): Train loss 5.052, Val loss 5.261\n",
      "Ep 1 (Step 000270): Train loss 5.073, Val loss 5.245\n",
      "Ep 1 (Step 000280): Train loss 5.030, Val loss 5.211\n",
      "Ep 1 (Step 000290): Train loss 5.053, Val loss 5.227\n",
      "Ep 1 (Step 000300): Train loss 5.121, Val loss 5.227\n",
      "Ep 1 (Step 000310): Train loss 5.000, Val loss 5.197\n",
      "Ep 1 (Step 000320): Train loss 5.058, Val loss 5.186\n",
      "Ep 1 (Step 000330): Train loss 5.035, Val loss 5.177\n",
      "Ep 1 (Step 000340): Train loss 5.004, Val loss 5.183\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1826\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.490, Val loss 8.461\n",
      "Ep 1 (Step 000010): Train loss 6.620, Val loss 6.582\n",
      "Ep 1 (Step 000020): Train loss 6.484, Val loss 6.501\n",
      "Ep 1 (Step 000030): Train loss 6.355, Val loss 6.367\n",
      "Ep 1 (Step 000040): Train loss 6.112, Val loss 6.177\n",
      "Ep 1 (Step 000050): Train loss 6.047, Val loss 6.029\n",
      "Ep 1 (Step 000060): Train loss 5.805, Val loss 5.901\n",
      "Ep 1 (Step 000070): Train loss 5.711, Val loss 5.816\n",
      "Ep 1 (Step 000080): Train loss 5.775, Val loss 5.741\n",
      "Ep 1 (Step 000090): Train loss 5.534, Val loss 5.650\n",
      "Ep 1 (Step 000100): Train loss 5.670, Val loss 5.588\n",
      "Ep 1 (Step 000110): Train loss 5.479, Val loss 5.539\n",
      "Ep 1 (Step 000120): Train loss 5.495, Val loss 5.499\n",
      "Ep 1 (Step 000130): Train loss 5.343, Val loss 5.475\n",
      "Ep 1 (Step 000140): Train loss 5.359, Val loss 5.437\n",
      "Ep 1 (Step 000150): Train loss 5.303, Val loss 5.448\n",
      "Ep 1 (Step 000160): Train loss 5.414, Val loss 5.412\n",
      "Ep 1 (Step 000170): Train loss 5.403, Val loss 5.397\n",
      "Ep 1 (Step 000180): Train loss 5.258, Val loss 5.368\n",
      "Ep 1 (Step 000190): Train loss 5.279, Val loss 5.357\n",
      "Ep 1 (Step 000200): Train loss 5.187, Val loss 5.339\n",
      "Ep 1 (Step 000210): Train loss 5.313, Val loss 5.349\n",
      "Ep 1 (Step 000220): Train loss 5.188, Val loss 5.330\n",
      "Ep 1 (Step 000230): Train loss 5.167, Val loss 5.298\n",
      "Ep 1 (Step 000240): Train loss 5.191, Val loss 5.282\n",
      "Ep 1 (Step 000250): Train loss 5.203, Val loss 5.280\n",
      "Ep 1 (Step 000260): Train loss 5.144, Val loss 5.283\n",
      "Ep 1 (Step 000270): Train loss 5.179, Val loss 5.274\n",
      "Ep 1 (Step 000280): Train loss 5.133, Val loss 5.244\n",
      "Ep 1 (Step 000290): Train loss 5.132, Val loss 5.250\n",
      "Ep 1 (Step 000300): Train loss 5.093, Val loss 5.246\n",
      "Ep 1 (Step 000310): Train loss 5.181, Val loss 5.228\n",
      "Ep 1 (Step 000320): Train loss 5.100, Val loss 5.214\n",
      "Ep 1 (Step 000330): Train loss 5.041, Val loss 5.178\n",
      "Ep 1 (Step 000340): Train loss 5.051, Val loss 5.166\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1661\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.516, Val loss 8.494\n",
      "Ep 1 (Step 000010): Train loss 6.603, Val loss 6.555\n",
      "Ep 1 (Step 000020): Train loss 6.503, Val loss 6.491\n",
      "Ep 1 (Step 000030): Train loss 6.405, Val loss 6.346\n",
      "Ep 1 (Step 000040): Train loss 6.237, Val loss 6.206\n",
      "Ep 1 (Step 000050): Train loss 5.997, Val loss 6.011\n",
      "Ep 1 (Step 000060): Train loss 5.832, Val loss 5.894\n",
      "Ep 1 (Step 000070): Train loss 5.795, Val loss 5.806\n",
      "Ep 1 (Step 000080): Train loss 5.725, Val loss 5.737\n",
      "Ep 1 (Step 000090): Train loss 5.625, Val loss 5.683\n",
      "Ep 1 (Step 000100): Train loss 5.605, Val loss 5.625\n",
      "Ep 1 (Step 000110): Train loss 5.473, Val loss 5.569\n",
      "Ep 1 (Step 000120): Train loss 5.484, Val loss 5.550\n",
      "Ep 1 (Step 000130): Train loss 5.440, Val loss 5.494\n",
      "Ep 1 (Step 000140): Train loss 5.395, Val loss 5.480\n",
      "Ep 1 (Step 000150): Train loss 5.337, Val loss 5.462\n",
      "Ep 1 (Step 000160): Train loss 5.407, Val loss 5.447\n",
      "Ep 1 (Step 000170): Train loss 5.234, Val loss 5.418\n",
      "Ep 1 (Step 000180): Train loss 5.301, Val loss 5.407\n",
      "Ep 1 (Step 000190): Train loss 5.308, Val loss 5.394\n",
      "Ep 1 (Step 000200): Train loss 5.244, Val loss 5.366\n",
      "Ep 1 (Step 000210): Train loss 5.244, Val loss 5.357\n",
      "Ep 1 (Step 000220): Train loss 5.173, Val loss 5.327\n",
      "Ep 1 (Step 000230): Train loss 5.247, Val loss 5.312\n",
      "Ep 1 (Step 000240): Train loss 5.158, Val loss 5.313\n",
      "Ep 1 (Step 000250): Train loss 5.108, Val loss 5.284\n",
      "Ep 1 (Step 000260): Train loss 5.152, Val loss 5.263\n",
      "Ep 1 (Step 000270): Train loss 5.140, Val loss 5.269\n",
      "Ep 1 (Step 000280): Train loss 5.105, Val loss 5.261\n",
      "Ep 1 (Step 000290): Train loss 5.124, Val loss 5.277\n",
      "Ep 1 (Step 000300): Train loss 5.107, Val loss 5.238\n",
      "Ep 1 (Step 000310): Train loss 5.106, Val loss 5.250\n",
      "Ep 1 (Step 000320): Train loss 5.054, Val loss 5.240\n",
      "Ep 1 (Step 000330): Train loss 4.976, Val loss 5.229\n",
      "Ep 1 (Step 000340): Train loss 5.019, Val loss 5.200\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1996\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.838, Val loss 8.815\n",
      "Ep 1 (Step 000010): Train loss 7.033, Val loss 6.986\n",
      "Ep 1 (Step 000020): Train loss 6.540, Val loss 6.485\n",
      "Ep 1 (Step 000030): Train loss 6.468, Val loss 6.404\n",
      "Ep 1 (Step 000040): Train loss 6.344, Val loss 6.336\n",
      "Ep 1 (Step 000050): Train loss 6.291, Val loss 6.225\n",
      "Ep 1 (Step 000060): Train loss 6.069, Val loss 6.086\n",
      "Ep 1 (Step 000070): Train loss 5.971, Val loss 5.997\n",
      "Ep 1 (Step 000080): Train loss 5.933, Val loss 5.900\n",
      "Ep 1 (Step 000090): Train loss 5.741, Val loss 5.816\n",
      "Ep 1 (Step 000100): Train loss 5.755, Val loss 5.772\n",
      "Ep 1 (Step 000110): Train loss 5.764, Val loss 5.745\n",
      "Ep 1 (Step 000120): Train loss 5.625, Val loss 5.660\n",
      "Ep 1 (Step 000130): Train loss 5.446, Val loss 5.627\n",
      "Ep 1 (Step 000140): Train loss 5.489, Val loss 5.591\n",
      "Ep 1 (Step 000150): Train loss 5.422, Val loss 5.578\n",
      "Ep 1 (Step 000160): Train loss 5.315, Val loss 5.536\n",
      "Ep 1 (Step 000170): Train loss 5.332, Val loss 5.508\n",
      "Ep 1 (Step 000180): Train loss 5.347, Val loss 5.493\n",
      "Ep 1 (Step 000190): Train loss 5.352, Val loss 5.481\n",
      "Ep 1 (Step 000200): Train loss 5.352, Val loss 5.456\n",
      "Ep 1 (Step 000210): Train loss 5.271, Val loss 5.429\n",
      "Ep 1 (Step 000220): Train loss 5.291, Val loss 5.408\n",
      "Ep 1 (Step 000230): Train loss 5.293, Val loss 5.392\n",
      "Ep 1 (Step 000240): Train loss 5.327, Val loss 5.397\n",
      "Ep 1 (Step 000250): Train loss 5.223, Val loss 5.372\n",
      "Ep 1 (Step 000260): Train loss 5.210, Val loss 5.361\n",
      "Ep 1 (Step 000270): Train loss 5.189, Val loss 5.342\n",
      "Ep 1 (Step 000280): Train loss 5.252, Val loss 5.336\n",
      "Ep 1 (Step 000290): Train loss 5.174, Val loss 5.309\n",
      "Ep 1 (Step 000300): Train loss 5.168, Val loss 5.306\n",
      "Ep 1 (Step 000310): Train loss 5.133, Val loss 5.311\n",
      "Ep 1 (Step 000320): Train loss 5.123, Val loss 5.297\n",
      "Ep 1 (Step 000330): Train loss 5.162, Val loss 5.288\n",
      "Ep 1 (Step 000340): Train loss 5.066, Val loss 5.278\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2777\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.816, Val loss 8.796\n",
      "Ep 1 (Step 000010): Train loss 6.999, Val loss 6.928\n",
      "Ep 1 (Step 000020): Train loss 6.529, Val loss 6.472\n",
      "Ep 1 (Step 000030): Train loss 6.428, Val loss 6.441\n",
      "Ep 1 (Step 000040): Train loss 6.396, Val loss 6.388\n",
      "Ep 1 (Step 000050): Train loss 6.256, Val loss 6.224\n",
      "Ep 1 (Step 000060): Train loss 6.138, Val loss 6.111\n",
      "Ep 1 (Step 000070): Train loss 5.972, Val loss 5.983\n",
      "Ep 1 (Step 000080): Train loss 5.931, Val loss 5.912\n",
      "Ep 1 (Step 000090): Train loss 5.836, Val loss 5.848\n",
      "Ep 1 (Step 000100): Train loss 5.709, Val loss 5.784\n",
      "Ep 1 (Step 000110): Train loss 5.734, Val loss 5.729\n",
      "Ep 1 (Step 000120): Train loss 5.530, Val loss 5.671\n",
      "Ep 1 (Step 000130): Train loss 5.594, Val loss 5.640\n",
      "Ep 1 (Step 000140): Train loss 5.601, Val loss 5.619\n",
      "Ep 1 (Step 000150): Train loss 5.550, Val loss 5.567\n",
      "Ep 1 (Step 000160): Train loss 5.500, Val loss 5.566\n",
      "Ep 1 (Step 000170): Train loss 5.472, Val loss 5.536\n",
      "Ep 1 (Step 000180): Train loss 5.409, Val loss 5.493\n",
      "Ep 1 (Step 000190): Train loss 5.296, Val loss 5.473\n",
      "Ep 1 (Step 000200): Train loss 5.325, Val loss 5.456\n",
      "Ep 1 (Step 000210): Train loss 5.298, Val loss 5.444\n",
      "Ep 1 (Step 000220): Train loss 5.187, Val loss 5.431\n",
      "Ep 1 (Step 000230): Train loss 5.272, Val loss 5.417\n",
      "Ep 1 (Step 000240): Train loss 5.276, Val loss 5.395\n",
      "Ep 1 (Step 000250): Train loss 5.180, Val loss 5.388\n",
      "Ep 1 (Step 000260): Train loss 5.230, Val loss 5.371\n",
      "Ep 1 (Step 000270): Train loss 5.230, Val loss 5.353\n",
      "Ep 1 (Step 000280): Train loss 5.225, Val loss 5.351\n",
      "Ep 1 (Step 000290): Train loss 5.209, Val loss 5.337\n",
      "Ep 1 (Step 000300): Train loss 5.178, Val loss 5.313\n",
      "Ep 1 (Step 000310): Train loss 5.179, Val loss 5.309\n",
      "Ep 1 (Step 000320): Train loss 5.190, Val loss 5.281\n",
      "Ep 1 (Step 000330): Train loss 5.073, Val loss 5.274\n",
      "Ep 1 (Step 000340): Train loss 5.139, Val loss 5.254\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2538\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.837, Val loss 8.821\n",
      "Ep 1 (Step 000010): Train loss 7.063, Val loss 7.019\n",
      "Ep 1 (Step 000020): Train loss 6.542, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.342, Val loss 6.386\n",
      "Ep 1 (Step 000040): Train loss 6.291, Val loss 6.300\n",
      "Ep 1 (Step 000050): Train loss 6.085, Val loss 6.143\n",
      "Ep 1 (Step 000060): Train loss 5.993, Val loss 6.042\n",
      "Ep 1 (Step 000070): Train loss 5.991, Val loss 5.978\n",
      "Ep 1 (Step 000080): Train loss 5.835, Val loss 5.882\n",
      "Ep 1 (Step 000090): Train loss 5.798, Val loss 5.820\n",
      "Ep 1 (Step 000100): Train loss 5.655, Val loss 5.764\n",
      "Ep 1 (Step 000110): Train loss 5.659, Val loss 5.695\n",
      "Ep 1 (Step 000120): Train loss 5.548, Val loss 5.648\n",
      "Ep 1 (Step 000130): Train loss 5.569, Val loss 5.615\n",
      "Ep 1 (Step 000140): Train loss 5.460, Val loss 5.585\n",
      "Ep 1 (Step 000150): Train loss 5.483, Val loss 5.561\n",
      "Ep 1 (Step 000160): Train loss 5.440, Val loss 5.534\n",
      "Ep 1 (Step 000170): Train loss 5.360, Val loss 5.503\n",
      "Ep 1 (Step 000180): Train loss 5.352, Val loss 5.477\n",
      "Ep 1 (Step 000190): Train loss 5.339, Val loss 5.457\n",
      "Ep 1 (Step 000200): Train loss 5.377, Val loss 5.447\n",
      "Ep 1 (Step 000210): Train loss 5.333, Val loss 5.416\n",
      "Ep 1 (Step 000220): Train loss 5.337, Val loss 5.398\n",
      "Ep 1 (Step 000230): Train loss 5.288, Val loss 5.381\n",
      "Ep 1 (Step 000240): Train loss 5.271, Val loss 5.357\n",
      "Ep 1 (Step 000250): Train loss 5.222, Val loss 5.344\n",
      "Ep 1 (Step 000260): Train loss 5.284, Val loss 5.346\n",
      "Ep 1 (Step 000270): Train loss 5.189, Val loss 5.319\n",
      "Ep 1 (Step 000280): Train loss 5.189, Val loss 5.296\n",
      "Ep 1 (Step 000290): Train loss 5.234, Val loss 5.303\n",
      "Ep 1 (Step 000300): Train loss 5.212, Val loss 5.273\n",
      "Ep 1 (Step 000310): Train loss 5.140, Val loss 5.269\n",
      "Ep 1 (Step 000320): Train loss 5.202, Val loss 5.262\n",
      "Ep 1 (Step 000330): Train loss 5.079, Val loss 5.283\n",
      "Ep 1 (Step 000340): Train loss 5.099, Val loss 5.261\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2615\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.811, Val loss 8.792\n",
      "Ep 1 (Step 000010): Train loss 6.999, Val loss 6.945\n",
      "Ep 1 (Step 000020): Train loss 6.467, Val loss 6.484\n",
      "Ep 1 (Step 000030): Train loss 6.372, Val loss 6.420\n",
      "Ep 1 (Step 000040): Train loss 6.380, Val loss 6.367\n",
      "Ep 1 (Step 000050): Train loss 6.272, Val loss 6.259\n",
      "Ep 1 (Step 000060): Train loss 6.099, Val loss 6.100\n",
      "Ep 1 (Step 000070): Train loss 6.015, Val loss 5.986\n",
      "Ep 1 (Step 000080): Train loss 5.877, Val loss 5.907\n",
      "Ep 1 (Step 000090): Train loss 5.821, Val loss 5.853\n",
      "Ep 1 (Step 000100): Train loss 5.760, Val loss 5.780\n",
      "Ep 1 (Step 000110): Train loss 5.681, Val loss 5.721\n",
      "Ep 1 (Step 000120): Train loss 5.615, Val loss 5.667\n",
      "Ep 1 (Step 000130): Train loss 5.585, Val loss 5.617\n",
      "Ep 1 (Step 000140): Train loss 5.499, Val loss 5.580\n",
      "Ep 1 (Step 000150): Train loss 5.466, Val loss 5.562\n",
      "Ep 1 (Step 000160): Train loss 5.542, Val loss 5.541\n",
      "Ep 1 (Step 000170): Train loss 5.401, Val loss 5.506\n",
      "Ep 1 (Step 000180): Train loss 5.290, Val loss 5.482\n",
      "Ep 1 (Step 000190): Train loss 5.348, Val loss 5.448\n",
      "Ep 1 (Step 000200): Train loss 5.379, Val loss 5.445\n",
      "Ep 1 (Step 000210): Train loss 5.349, Val loss 5.430\n",
      "Ep 1 (Step 000220): Train loss 5.302, Val loss 5.400\n",
      "Ep 1 (Step 000230): Train loss 5.288, Val loss 5.377\n",
      "Ep 1 (Step 000240): Train loss 5.256, Val loss 5.368\n",
      "Ep 1 (Step 000250): Train loss 5.197, Val loss 5.354\n",
      "Ep 1 (Step 000260): Train loss 5.179, Val loss 5.339\n",
      "Ep 1 (Step 000270): Train loss 5.186, Val loss 5.328\n",
      "Ep 1 (Step 000280): Train loss 5.157, Val loss 5.335\n",
      "Ep 1 (Step 000290): Train loss 5.073, Val loss 5.311\n",
      "Ep 1 (Step 000300): Train loss 5.177, Val loss 5.290\n",
      "Ep 1 (Step 000310): Train loss 5.070, Val loss 5.277\n",
      "Ep 1 (Step 000320): Train loss 5.184, Val loss 5.277\n",
      "Ep 1 (Step 000330): Train loss 5.109, Val loss 5.282\n",
      "Ep 1 (Step 000340): Train loss 5.073, Val loss 5.256\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2561\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.838, Val loss 8.831\n",
      "Ep 1 (Step 000010): Train loss 6.985, Val loss 6.970\n",
      "Ep 1 (Step 000020): Train loss 6.480, Val loss 6.488\n",
      "Ep 1 (Step 000030): Train loss 6.454, Val loss 6.415\n",
      "Ep 1 (Step 000040): Train loss 6.380, Val loss 6.368\n",
      "Ep 1 (Step 000050): Train loss 6.303, Val loss 6.252\n",
      "Ep 1 (Step 000060): Train loss 6.126, Val loss 6.137\n",
      "Ep 1 (Step 000070): Train loss 5.990, Val loss 5.975\n",
      "Ep 1 (Step 000080): Train loss 5.892, Val loss 5.935\n",
      "Ep 1 (Step 000090): Train loss 5.753, Val loss 5.812\n",
      "Ep 1 (Step 000100): Train loss 5.726, Val loss 5.754\n",
      "Ep 1 (Step 000110): Train loss 5.648, Val loss 5.696\n",
      "Ep 1 (Step 000120): Train loss 5.593, Val loss 5.639\n",
      "Ep 1 (Step 000130): Train loss 5.607, Val loss 5.588\n",
      "Ep 1 (Step 000140): Train loss 5.497, Val loss 5.565\n",
      "Ep 1 (Step 000150): Train loss 5.444, Val loss 5.538\n",
      "Ep 1 (Step 000160): Train loss 5.446, Val loss 5.512\n",
      "Ep 1 (Step 000170): Train loss 5.466, Val loss 5.496\n",
      "Ep 1 (Step 000180): Train loss 5.370, Val loss 5.474\n",
      "Ep 1 (Step 000190): Train loss 5.315, Val loss 5.427\n",
      "Ep 1 (Step 000200): Train loss 5.281, Val loss 5.424\n",
      "Ep 1 (Step 000210): Train loss 5.242, Val loss 5.396\n",
      "Ep 1 (Step 000220): Train loss 5.224, Val loss 5.384\n",
      "Ep 1 (Step 000230): Train loss 5.219, Val loss 5.350\n",
      "Ep 1 (Step 000240): Train loss 5.171, Val loss 5.350\n",
      "Ep 1 (Step 000250): Train loss 5.282, Val loss 5.331\n",
      "Ep 1 (Step 000260): Train loss 5.200, Val loss 5.353\n",
      "Ep 1 (Step 000270): Train loss 5.299, Val loss 5.318\n",
      "Ep 1 (Step 000280): Train loss 5.123, Val loss 5.327\n",
      "Ep 1 (Step 000290): Train loss 5.151, Val loss 5.279\n",
      "Ep 1 (Step 000300): Train loss 5.192, Val loss 5.279\n",
      "Ep 1 (Step 000310): Train loss 5.156, Val loss 5.268\n",
      "Ep 1 (Step 000320): Train loss 5.046, Val loss 5.257\n",
      "Ep 1 (Step 000330): Train loss 5.107, Val loss 5.260\n",
      "Ep 1 (Step 000340): Train loss 5.139, Val loss 5.253\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2535\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.831, Val loss 8.823\n",
      "Ep 1 (Step 000010): Train loss 7.092, Val loss 7.040\n",
      "Ep 1 (Step 000020): Train loss 6.493, Val loss 6.492\n",
      "Ep 1 (Step 000030): Train loss 6.410, Val loss 6.410\n",
      "Ep 1 (Step 000040): Train loss 6.365, Val loss 6.379\n",
      "Ep 1 (Step 000050): Train loss 6.205, Val loss 6.240\n",
      "Ep 1 (Step 000060): Train loss 6.127, Val loss 6.088\n",
      "Ep 1 (Step 000070): Train loss 5.832, Val loss 5.994\n",
      "Ep 1 (Step 000080): Train loss 5.859, Val loss 5.912\n",
      "Ep 1 (Step 000090): Train loss 5.791, Val loss 5.833\n",
      "Ep 1 (Step 000100): Train loss 5.665, Val loss 5.779\n",
      "Ep 1 (Step 000110): Train loss 5.652, Val loss 5.726\n",
      "Ep 1 (Step 000120): Train loss 5.591, Val loss 5.681\n",
      "Ep 1 (Step 000130): Train loss 5.500, Val loss 5.628\n",
      "Ep 1 (Step 000140): Train loss 5.497, Val loss 5.585\n",
      "Ep 1 (Step 000150): Train loss 5.513, Val loss 5.561\n",
      "Ep 1 (Step 000160): Train loss 5.394, Val loss 5.529\n",
      "Ep 1 (Step 000170): Train loss 5.330, Val loss 5.494\n",
      "Ep 1 (Step 000180): Train loss 5.388, Val loss 5.490\n",
      "Ep 1 (Step 000190): Train loss 5.346, Val loss 5.469\n",
      "Ep 1 (Step 000200): Train loss 5.304, Val loss 5.427\n",
      "Ep 1 (Step 000210): Train loss 5.266, Val loss 5.404\n",
      "Ep 1 (Step 000220): Train loss 5.267, Val loss 5.386\n",
      "Ep 1 (Step 000230): Train loss 5.310, Val loss 5.361\n",
      "Ep 1 (Step 000240): Train loss 5.216, Val loss 5.360\n",
      "Ep 1 (Step 000250): Train loss 5.214, Val loss 5.344\n",
      "Ep 1 (Step 000260): Train loss 5.222, Val loss 5.351\n",
      "Ep 1 (Step 000270): Train loss 5.190, Val loss 5.330\n",
      "Ep 1 (Step 000280): Train loss 5.151, Val loss 5.329\n",
      "Ep 1 (Step 000290): Train loss 5.219, Val loss 5.304\n",
      "Ep 1 (Step 000300): Train loss 5.156, Val loss 5.286\n",
      "Ep 1 (Step 000310): Train loss 5.139, Val loss 5.269\n",
      "Ep 1 (Step 000320): Train loss 5.164, Val loss 5.253\n",
      "Ep 1 (Step 000330): Train loss 5.103, Val loss 5.251\n",
      "Ep 1 (Step 000340): Train loss 5.112, Val loss 5.251\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2511\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.453, Val loss 8.446\n",
      "Ep 1 (Step 000010): Train loss 6.585, Val loss 6.574\n",
      "Ep 1 (Step 000020): Train loss 6.480, Val loss 6.511\n",
      "Ep 1 (Step 000030): Train loss 6.259, Val loss 6.330\n",
      "Ep 1 (Step 000040): Train loss 6.077, Val loss 6.122\n",
      "Ep 1 (Step 000050): Train loss 6.001, Val loss 5.994\n",
      "Ep 1 (Step 000060): Train loss 5.786, Val loss 5.895\n",
      "Ep 1 (Step 000070): Train loss 5.787, Val loss 5.795\n",
      "Ep 1 (Step 000080): Train loss 5.666, Val loss 5.724\n",
      "Ep 1 (Step 000090): Train loss 5.536, Val loss 5.644\n",
      "Ep 1 (Step 000100): Train loss 5.465, Val loss 5.606\n",
      "Ep 1 (Step 000110): Train loss 5.584, Val loss 5.557\n",
      "Ep 1 (Step 000120): Train loss 5.452, Val loss 5.557\n",
      "Ep 1 (Step 000130): Train loss 5.362, Val loss 5.527\n",
      "Ep 1 (Step 000140): Train loss 5.332, Val loss 5.506\n",
      "Ep 1 (Step 000150): Train loss 5.343, Val loss 5.481\n",
      "Ep 1 (Step 000160): Train loss 5.434, Val loss 5.451\n",
      "Ep 1 (Step 000170): Train loss 5.293, Val loss 5.411\n",
      "Ep 1 (Step 000180): Train loss 5.234, Val loss 5.404\n",
      "Ep 1 (Step 000190): Train loss 5.244, Val loss 5.389\n",
      "Ep 1 (Step 000200): Train loss 5.202, Val loss 5.354\n",
      "Ep 1 (Step 000210): Train loss 5.273, Val loss 5.343\n",
      "Ep 1 (Step 000220): Train loss 5.152, Val loss 5.328\n",
      "Ep 1 (Step 000230): Train loss 5.207, Val loss 5.334\n",
      "Ep 1 (Step 000240): Train loss 5.184, Val loss 5.290\n",
      "Ep 1 (Step 000250): Train loss 5.239, Val loss 5.283\n",
      "Ep 1 (Step 000260): Train loss 5.093, Val loss 5.276\n",
      "Ep 1 (Step 000270): Train loss 5.121, Val loss 5.296\n",
      "Ep 1 (Step 000280): Train loss 5.072, Val loss 5.264\n",
      "Ep 1 (Step 000290): Train loss 5.074, Val loss 5.254\n",
      "Ep 1 (Step 000300): Train loss 5.019, Val loss 5.245\n",
      "Ep 1 (Step 000310): Train loss 5.044, Val loss 5.249\n",
      "Ep 1 (Step 000320): Train loss 5.134, Val loss 5.266\n",
      "Ep 1 (Step 000330): Train loss 5.092, Val loss 5.235\n",
      "Ep 1 (Step 000340): Train loss 5.075, Val loss 5.216\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2162\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.481, Val loss 8.411\n",
      "Ep 1 (Step 000010): Train loss 6.618, Val loss 6.548\n",
      "Ep 1 (Step 000020): Train loss 6.572, Val loss 6.554\n",
      "Ep 1 (Step 000030): Train loss 6.453, Val loss 6.392\n",
      "Ep 1 (Step 000040): Train loss 6.244, Val loss 6.179\n",
      "Ep 1 (Step 000050): Train loss 6.012, Val loss 6.047\n",
      "Ep 1 (Step 000060): Train loss 5.926, Val loss 5.921\n",
      "Ep 1 (Step 000070): Train loss 5.839, Val loss 5.838\n",
      "Ep 1 (Step 000080): Train loss 5.786, Val loss 5.773\n",
      "Ep 1 (Step 000090): Train loss 5.701, Val loss 5.722\n",
      "Ep 1 (Step 000100): Train loss 5.578, Val loss 5.646\n",
      "Ep 1 (Step 000110): Train loss 5.492, Val loss 5.588\n",
      "Ep 1 (Step 000120): Train loss 5.485, Val loss 5.558\n",
      "Ep 1 (Step 000130): Train loss 5.412, Val loss 5.537\n",
      "Ep 1 (Step 000140): Train loss 5.439, Val loss 5.484\n",
      "Ep 1 (Step 000150): Train loss 5.410, Val loss 5.481\n",
      "Ep 1 (Step 000160): Train loss 5.325, Val loss 5.458\n",
      "Ep 1 (Step 000170): Train loss 5.404, Val loss 5.441\n",
      "Ep 1 (Step 000180): Train loss 5.288, Val loss 5.438\n",
      "Ep 1 (Step 000190): Train loss 5.244, Val loss 5.400\n",
      "Ep 1 (Step 000200): Train loss 5.287, Val loss 5.399\n",
      "Ep 1 (Step 000210): Train loss 5.230, Val loss 5.361\n",
      "Ep 1 (Step 000220): Train loss 5.184, Val loss 5.346\n",
      "Ep 1 (Step 000230): Train loss 5.075, Val loss 5.332\n",
      "Ep 1 (Step 000240): Train loss 5.151, Val loss 5.290\n",
      "Ep 1 (Step 000250): Train loss 5.168, Val loss 5.302\n",
      "Ep 1 (Step 000260): Train loss 5.183, Val loss 5.299\n",
      "Ep 1 (Step 000270): Train loss 5.135, Val loss 5.294\n",
      "Ep 1 (Step 000280): Train loss 5.064, Val loss 5.306\n",
      "Ep 1 (Step 000290): Train loss 5.118, Val loss 5.279\n",
      "Ep 1 (Step 000300): Train loss 5.138, Val loss 5.250\n",
      "Ep 1 (Step 000310): Train loss 5.169, Val loss 5.238\n",
      "Ep 1 (Step 000320): Train loss 5.065, Val loss 5.243\n",
      "Ep 1 (Step 000330): Train loss 5.072, Val loss 5.237\n",
      "Ep 1 (Step 000340): Train loss 5.057, Val loss 5.209\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2090\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.527, Val loss 8.496\n",
      "Ep 1 (Step 000010): Train loss 6.628, Val loss 6.555\n",
      "Ep 1 (Step 000020): Train loss 6.624, Val loss 6.515\n",
      "Ep 1 (Step 000030): Train loss 6.420, Val loss 6.417\n",
      "Ep 1 (Step 000040): Train loss 6.247, Val loss 6.250\n",
      "Ep 1 (Step 000050): Train loss 6.037, Val loss 6.061\n",
      "Ep 1 (Step 000060): Train loss 5.918, Val loss 5.940\n",
      "Ep 1 (Step 000070): Train loss 5.791, Val loss 5.805\n",
      "Ep 1 (Step 000080): Train loss 5.674, Val loss 5.745\n",
      "Ep 1 (Step 000090): Train loss 5.588, Val loss 5.672\n",
      "Ep 1 (Step 000100): Train loss 5.580, Val loss 5.646\n",
      "Ep 1 (Step 000110): Train loss 5.546, Val loss 5.605\n",
      "Ep 1 (Step 000120): Train loss 5.462, Val loss 5.554\n",
      "Ep 1 (Step 000130): Train loss 5.409, Val loss 5.512\n",
      "Ep 1 (Step 000140): Train loss 5.397, Val loss 5.498\n",
      "Ep 1 (Step 000150): Train loss 5.379, Val loss 5.485\n",
      "Ep 1 (Step 000160): Train loss 5.297, Val loss 5.450\n",
      "Ep 1 (Step 000170): Train loss 5.330, Val loss 5.410\n",
      "Ep 1 (Step 000180): Train loss 5.350, Val loss 5.425\n",
      "Ep 1 (Step 000190): Train loss 5.261, Val loss 5.422\n",
      "Ep 1 (Step 000200): Train loss 5.242, Val loss 5.370\n",
      "Ep 1 (Step 000210): Train loss 5.283, Val loss 5.346\n",
      "Ep 1 (Step 000220): Train loss 5.192, Val loss 5.334\n",
      "Ep 1 (Step 000230): Train loss 5.201, Val loss 5.325\n",
      "Ep 1 (Step 000240): Train loss 5.198, Val loss 5.320\n",
      "Ep 1 (Step 000250): Train loss 5.118, Val loss 5.305\n",
      "Ep 1 (Step 000260): Train loss 5.193, Val loss 5.286\n",
      "Ep 1 (Step 000270): Train loss 5.162, Val loss 5.269\n",
      "Ep 1 (Step 000280): Train loss 5.099, Val loss 5.272\n",
      "Ep 1 (Step 000290): Train loss 5.152, Val loss 5.282\n",
      "Ep 1 (Step 000300): Train loss 5.050, Val loss 5.244\n",
      "Ep 1 (Step 000310): Train loss 5.142, Val loss 5.248\n",
      "Ep 1 (Step 000320): Train loss 5.033, Val loss 5.245\n",
      "Ep 1 (Step 000330): Train loss 5.116, Val loss 5.257\n",
      "Ep 1 (Step 000340): Train loss 5.126, Val loss 5.244\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2440\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.549, Val loss 8.524\n",
      "Ep 1 (Step 000010): Train loss 6.599, Val loss 6.572\n",
      "Ep 1 (Step 000020): Train loss 6.600, Val loss 6.529\n",
      "Ep 1 (Step 000030): Train loss 6.313, Val loss 6.315\n",
      "Ep 1 (Step 000040): Train loss 6.134, Val loss 6.132\n",
      "Ep 1 (Step 000050): Train loss 5.930, Val loss 5.988\n",
      "Ep 1 (Step 000060): Train loss 5.813, Val loss 5.875\n",
      "Ep 1 (Step 000070): Train loss 5.771, Val loss 5.801\n",
      "Ep 1 (Step 000080): Train loss 5.733, Val loss 5.729\n",
      "Ep 1 (Step 000090): Train loss 5.531, Val loss 5.664\n",
      "Ep 1 (Step 000100): Train loss 5.594, Val loss 5.595\n",
      "Ep 1 (Step 000110): Train loss 5.573, Val loss 5.564\n",
      "Ep 1 (Step 000120): Train loss 5.443, Val loss 5.541\n",
      "Ep 1 (Step 000130): Train loss 5.405, Val loss 5.506\n",
      "Ep 1 (Step 000140): Train loss 5.390, Val loss 5.509\n",
      "Ep 1 (Step 000150): Train loss 5.431, Val loss 5.464\n",
      "Ep 1 (Step 000160): Train loss 5.334, Val loss 5.447\n",
      "Ep 1 (Step 000170): Train loss 5.290, Val loss 5.412\n",
      "Ep 1 (Step 000180): Train loss 5.234, Val loss 5.412\n",
      "Ep 1 (Step 000190): Train loss 5.330, Val loss 5.366\n",
      "Ep 1 (Step 000200): Train loss 5.311, Val loss 5.344\n",
      "Ep 1 (Step 000210): Train loss 5.277, Val loss 5.342\n",
      "Ep 1 (Step 000220): Train loss 5.189, Val loss 5.325\n",
      "Ep 1 (Step 000230): Train loss 5.159, Val loss 5.295\n",
      "Ep 1 (Step 000240): Train loss 5.216, Val loss 5.283\n",
      "Ep 1 (Step 000250): Train loss 5.096, Val loss 5.267\n",
      "Ep 1 (Step 000260): Train loss 5.195, Val loss 5.253\n",
      "Ep 1 (Step 000270): Train loss 5.131, Val loss 5.254\n",
      "Ep 1 (Step 000280): Train loss 5.113, Val loss 5.256\n",
      "Ep 1 (Step 000290): Train loss 5.112, Val loss 5.221\n",
      "Ep 1 (Step 000300): Train loss 5.036, Val loss 5.199\n",
      "Ep 1 (Step 000310): Train loss 5.011, Val loss 5.213\n",
      "Ep 1 (Step 000320): Train loss 5.045, Val loss 5.202\n",
      "Ep 1 (Step 000330): Train loss 5.017, Val loss 5.228\n",
      "Ep 1 (Step 000340): Train loss 5.013, Val loss 5.187\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1866\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.554, Val loss 8.522\n",
      "Ep 1 (Step 000010): Train loss 6.584, Val loss 6.580\n",
      "Ep 1 (Step 000020): Train loss 6.512, Val loss 6.538\n",
      "Ep 1 (Step 000030): Train loss 6.402, Val loss 6.401\n",
      "Ep 1 (Step 000040): Train loss 6.217, Val loss 6.243\n",
      "Ep 1 (Step 000050): Train loss 6.093, Val loss 6.060\n",
      "Ep 1 (Step 000060): Train loss 5.839, Val loss 5.925\n",
      "Ep 1 (Step 000070): Train loss 5.715, Val loss 5.827\n",
      "Ep 1 (Step 000080): Train loss 5.681, Val loss 5.752\n",
      "Ep 1 (Step 000090): Train loss 5.587, Val loss 5.697\n",
      "Ep 1 (Step 000100): Train loss 5.515, Val loss 5.611\n",
      "Ep 1 (Step 000110): Train loss 5.591, Val loss 5.590\n",
      "Ep 1 (Step 000120): Train loss 5.389, Val loss 5.556\n",
      "Ep 1 (Step 000130): Train loss 5.415, Val loss 5.497\n",
      "Ep 1 (Step 000140): Train loss 5.379, Val loss 5.467\n",
      "Ep 1 (Step 000150): Train loss 5.270, Val loss 5.447\n",
      "Ep 1 (Step 000160): Train loss 5.351, Val loss 5.442\n",
      "Ep 1 (Step 000170): Train loss 5.307, Val loss 5.431\n",
      "Ep 1 (Step 000180): Train loss 5.179, Val loss 5.396\n",
      "Ep 1 (Step 000190): Train loss 5.193, Val loss 5.359\n",
      "Ep 1 (Step 000200): Train loss 5.285, Val loss 5.348\n",
      "Ep 1 (Step 000210): Train loss 5.206, Val loss 5.325\n",
      "Ep 1 (Step 000220): Train loss 5.168, Val loss 5.326\n",
      "Ep 1 (Step 000230): Train loss 5.192, Val loss 5.309\n",
      "Ep 1 (Step 000240): Train loss 5.054, Val loss 5.302\n",
      "Ep 1 (Step 000250): Train loss 5.209, Val loss 5.282\n",
      "Ep 1 (Step 000260): Train loss 5.118, Val loss 5.256\n",
      "Ep 1 (Step 000270): Train loss 5.178, Val loss 5.255\n",
      "Ep 1 (Step 000280): Train loss 5.096, Val loss 5.258\n",
      "Ep 1 (Step 000290): Train loss 5.084, Val loss 5.236\n",
      "Ep 1 (Step 000300): Train loss 5.095, Val loss 5.223\n",
      "Ep 1 (Step 000310): Train loss 5.099, Val loss 5.225\n",
      "Ep 1 (Step 000320): Train loss 5.179, Val loss 5.222\n",
      "Ep 1 (Step 000330): Train loss 5.054, Val loss 5.202\n",
      "Ep 1 (Step 000340): Train loss 5.036, Val loss 5.202\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2022\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.496, Val loss 8.477\n",
      "Ep 1 (Step 000010): Train loss 6.616, Val loss 6.554\n",
      "Ep 1 (Step 000020): Train loss 6.481, Val loss 6.513\n",
      "Ep 1 (Step 000030): Train loss 6.335, Val loss 6.383\n",
      "Ep 1 (Step 000040): Train loss 6.161, Val loss 6.175\n",
      "Ep 1 (Step 000050): Train loss 5.988, Val loss 5.995\n",
      "Ep 1 (Step 000060): Train loss 5.907, Val loss 5.899\n",
      "Ep 1 (Step 000070): Train loss 5.798, Val loss 5.837\n",
      "Ep 1 (Step 000080): Train loss 5.639, Val loss 5.737\n",
      "Ep 1 (Step 000090): Train loss 5.592, Val loss 5.677\n",
      "Ep 1 (Step 000100): Train loss 5.571, Val loss 5.610\n",
      "Ep 1 (Step 000110): Train loss 5.546, Val loss 5.558\n",
      "Ep 1 (Step 000120): Train loss 5.383, Val loss 5.570\n",
      "Ep 1 (Step 000130): Train loss 5.422, Val loss 5.518\n",
      "Ep 1 (Step 000140): Train loss 5.331, Val loss 5.507\n",
      "Ep 1 (Step 000150): Train loss 5.394, Val loss 5.456\n",
      "Ep 1 (Step 000160): Train loss 5.310, Val loss 5.434\n",
      "Ep 1 (Step 000170): Train loss 5.260, Val loss 5.405\n",
      "Ep 1 (Step 000180): Train loss 5.247, Val loss 5.393\n",
      "Ep 1 (Step 000190): Train loss 5.294, Val loss 5.377\n",
      "Ep 1 (Step 000200): Train loss 5.219, Val loss 5.363\n",
      "Ep 1 (Step 000210): Train loss 5.304, Val loss 5.349\n",
      "Ep 1 (Step 000220): Train loss 5.292, Val loss 5.351\n",
      "Ep 1 (Step 000230): Train loss 5.144, Val loss 5.318\n",
      "Ep 1 (Step 000240): Train loss 5.115, Val loss 5.316\n",
      "Ep 1 (Step 000250): Train loss 5.156, Val loss 5.300\n",
      "Ep 1 (Step 000260): Train loss 5.083, Val loss 5.299\n",
      "Ep 1 (Step 000270): Train loss 5.157, Val loss 5.256\n",
      "Ep 1 (Step 000280): Train loss 5.110, Val loss 5.243\n",
      "Ep 1 (Step 000290): Train loss 5.046, Val loss 5.248\n",
      "Ep 1 (Step 000300): Train loss 5.109, Val loss 5.249\n",
      "Ep 1 (Step 000310): Train loss 5.057, Val loss 5.224\n",
      "Ep 1 (Step 000320): Train loss 5.077, Val loss 5.220\n",
      "Ep 1 (Step 000330): Train loss 5.064, Val loss 5.195\n",
      "Ep 1 (Step 000340): Train loss 4.987, Val loss 5.198\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1977\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.772, Val loss 8.767\n",
      "Ep 1 (Step 000010): Train loss 7.016, Val loss 6.974\n",
      "Ep 1 (Step 000020): Train loss 6.512, Val loss 6.461\n",
      "Ep 1 (Step 000030): Train loss 6.366, Val loss 6.379\n",
      "Ep 1 (Step 000040): Train loss 6.250, Val loss 6.235\n",
      "Ep 1 (Step 000050): Train loss 6.039, Val loss 6.078\n",
      "Ep 1 (Step 000060): Train loss 5.936, Val loss 5.970\n",
      "Ep 1 (Step 000070): Train loss 5.808, Val loss 5.899\n",
      "Ep 1 (Step 000080): Train loss 5.766, Val loss 5.826\n",
      "Ep 1 (Step 000090): Train loss 5.640, Val loss 5.742\n",
      "Ep 1 (Step 000100): Train loss 5.635, Val loss 5.716\n",
      "Ep 1 (Step 000110): Train loss 5.533, Val loss 5.641\n",
      "Ep 1 (Step 000120): Train loss 5.585, Val loss 5.608\n",
      "Ep 1 (Step 000130): Train loss 5.456, Val loss 5.559\n",
      "Ep 1 (Step 000140): Train loss 5.427, Val loss 5.514\n",
      "Ep 1 (Step 000150): Train loss 5.436, Val loss 5.497\n",
      "Ep 1 (Step 000160): Train loss 5.365, Val loss 5.470\n",
      "Ep 1 (Step 000170): Train loss 5.373, Val loss 5.449\n",
      "Ep 1 (Step 000180): Train loss 5.292, Val loss 5.416\n",
      "Ep 1 (Step 000190): Train loss 5.289, Val loss 5.398\n",
      "Ep 1 (Step 000200): Train loss 5.258, Val loss 5.391\n",
      "Ep 1 (Step 000210): Train loss 5.244, Val loss 5.376\n",
      "Ep 1 (Step 000220): Train loss 5.239, Val loss 5.357\n",
      "Ep 1 (Step 000230): Train loss 5.224, Val loss 5.345\n",
      "Ep 1 (Step 000240): Train loss 5.231, Val loss 5.337\n",
      "Ep 1 (Step 000250): Train loss 5.210, Val loss 5.320\n",
      "Ep 1 (Step 000260): Train loss 5.186, Val loss 5.310\n",
      "Ep 1 (Step 000270): Train loss 5.204, Val loss 5.293\n",
      "Ep 1 (Step 000280): Train loss 5.143, Val loss 5.271\n",
      "Ep 1 (Step 000290): Train loss 5.195, Val loss 5.270\n",
      "Ep 1 (Step 000300): Train loss 5.150, Val loss 5.266\n",
      "Ep 1 (Step 000310): Train loss 5.070, Val loss 5.248\n",
      "Ep 1 (Step 000320): Train loss 5.145, Val loss 5.249\n",
      "Ep 1 (Step 000330): Train loss 5.088, Val loss 5.233\n",
      "Ep 1 (Step 000340): Train loss 5.065, Val loss 5.228\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2277\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.835, Val loss 8.805\n",
      "Ep 1 (Step 000010): Train loss 7.023, Val loss 6.955\n",
      "Ep 1 (Step 000020): Train loss 6.502, Val loss 6.446\n",
      "Ep 1 (Step 000030): Train loss 6.342, Val loss 6.382\n",
      "Ep 1 (Step 000040): Train loss 6.293, Val loss 6.303\n",
      "Ep 1 (Step 000050): Train loss 6.161, Val loss 6.092\n",
      "Ep 1 (Step 000060): Train loss 5.940, Val loss 5.963\n",
      "Ep 1 (Step 000070): Train loss 5.826, Val loss 5.882\n",
      "Ep 1 (Step 000080): Train loss 5.760, Val loss 5.826\n",
      "Ep 1 (Step 000090): Train loss 5.675, Val loss 5.750\n",
      "Ep 1 (Step 000100): Train loss 5.633, Val loss 5.682\n",
      "Ep 1 (Step 000110): Train loss 5.546, Val loss 5.647\n",
      "Ep 1 (Step 000120): Train loss 5.548, Val loss 5.626\n",
      "Ep 1 (Step 000130): Train loss 5.482, Val loss 5.584\n",
      "Ep 1 (Step 000140): Train loss 5.500, Val loss 5.557\n",
      "Ep 1 (Step 000150): Train loss 5.356, Val loss 5.520\n",
      "Ep 1 (Step 000160): Train loss 5.421, Val loss 5.486\n",
      "Ep 1 (Step 000170): Train loss 5.365, Val loss 5.470\n",
      "Ep 1 (Step 000180): Train loss 5.276, Val loss 5.436\n",
      "Ep 1 (Step 000190): Train loss 5.226, Val loss 5.420\n",
      "Ep 1 (Step 000200): Train loss 5.295, Val loss 5.392\n",
      "Ep 1 (Step 000210): Train loss 5.265, Val loss 5.409\n",
      "Ep 1 (Step 000220): Train loss 5.222, Val loss 5.396\n",
      "Ep 1 (Step 000230): Train loss 5.264, Val loss 5.351\n",
      "Ep 1 (Step 000240): Train loss 5.198, Val loss 5.336\n",
      "Ep 1 (Step 000250): Train loss 5.159, Val loss 5.336\n",
      "Ep 1 (Step 000260): Train loss 5.143, Val loss 5.337\n",
      "Ep 1 (Step 000270): Train loss 5.100, Val loss 5.315\n",
      "Ep 1 (Step 000280): Train loss 5.121, Val loss 5.310\n",
      "Ep 1 (Step 000290): Train loss 5.210, Val loss 5.294\n",
      "Ep 1 (Step 000300): Train loss 5.147, Val loss 5.289\n",
      "Ep 1 (Step 000310): Train loss 5.054, Val loss 5.265\n",
      "Ep 1 (Step 000320): Train loss 5.156, Val loss 5.252\n",
      "Ep 1 (Step 000330): Train loss 5.100, Val loss 5.236\n",
      "Ep 1 (Step 000340): Train loss 5.130, Val loss 5.229\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2288\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.826, Val loss 8.808\n",
      "Ep 1 (Step 000010): Train loss 7.042, Val loss 7.017\n",
      "Ep 1 (Step 000020): Train loss 6.519, Val loss 6.464\n",
      "Ep 1 (Step 000030): Train loss 6.404, Val loss 6.368\n",
      "Ep 1 (Step 000040): Train loss 6.308, Val loss 6.282\n",
      "Ep 1 (Step 000050): Train loss 6.163, Val loss 6.103\n",
      "Ep 1 (Step 000060): Train loss 6.008, Val loss 6.001\n",
      "Ep 1 (Step 000070): Train loss 5.919, Val loss 5.884\n",
      "Ep 1 (Step 000080): Train loss 5.780, Val loss 5.812\n",
      "Ep 1 (Step 000090): Train loss 5.643, Val loss 5.716\n",
      "Ep 1 (Step 000100): Train loss 5.674, Val loss 5.673\n",
      "Ep 1 (Step 000110): Train loss 5.651, Val loss 5.621\n",
      "Ep 1 (Step 000120): Train loss 5.566, Val loss 5.583\n",
      "Ep 1 (Step 000130): Train loss 5.500, Val loss 5.542\n",
      "Ep 1 (Step 000140): Train loss 5.456, Val loss 5.515\n",
      "Ep 1 (Step 000150): Train loss 5.408, Val loss 5.489\n",
      "Ep 1 (Step 000160): Train loss 5.482, Val loss 5.470\n",
      "Ep 1 (Step 000170): Train loss 5.334, Val loss 5.443\n",
      "Ep 1 (Step 000180): Train loss 5.321, Val loss 5.422\n",
      "Ep 1 (Step 000190): Train loss 5.190, Val loss 5.408\n",
      "Ep 1 (Step 000200): Train loss 5.265, Val loss 5.401\n",
      "Ep 1 (Step 000210): Train loss 5.284, Val loss 5.389\n",
      "Ep 1 (Step 000220): Train loss 5.256, Val loss 5.352\n",
      "Ep 1 (Step 000230): Train loss 5.194, Val loss 5.335\n",
      "Ep 1 (Step 000240): Train loss 5.160, Val loss 5.325\n",
      "Ep 1 (Step 000250): Train loss 5.149, Val loss 5.315\n",
      "Ep 1 (Step 000260): Train loss 5.118, Val loss 5.294\n",
      "Ep 1 (Step 000270): Train loss 5.165, Val loss 5.285\n",
      "Ep 1 (Step 000280): Train loss 5.107, Val loss 5.276\n",
      "Ep 1 (Step 000290): Train loss 5.157, Val loss 5.284\n",
      "Ep 1 (Step 000300): Train loss 5.071, Val loss 5.264\n",
      "Ep 1 (Step 000310): Train loss 5.102, Val loss 5.261\n",
      "Ep 1 (Step 000320): Train loss 5.000, Val loss 5.242\n",
      "Ep 1 (Step 000330): Train loss 5.078, Val loss 5.241\n",
      "Ep 1 (Step 000340): Train loss 5.124, Val loss 5.245\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2452\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.824, Val loss 8.794\n",
      "Ep 1 (Step 000010): Train loss 7.032, Val loss 7.020\n",
      "Ep 1 (Step 000020): Train loss 6.565, Val loss 6.463\n",
      "Ep 1 (Step 000030): Train loss 6.434, Val loss 6.389\n",
      "Ep 1 (Step 000040): Train loss 6.360, Val loss 6.316\n",
      "Ep 1 (Step 000050): Train loss 6.148, Val loss 6.128\n",
      "Ep 1 (Step 000060): Train loss 5.940, Val loss 5.984\n",
      "Ep 1 (Step 000070): Train loss 5.790, Val loss 5.909\n",
      "Ep 1 (Step 000080): Train loss 5.748, Val loss 5.807\n",
      "Ep 1 (Step 000090): Train loss 5.717, Val loss 5.753\n",
      "Ep 1 (Step 000100): Train loss 5.591, Val loss 5.683\n",
      "Ep 1 (Step 000110): Train loss 5.671, Val loss 5.635\n",
      "Ep 1 (Step 000120): Train loss 5.587, Val loss 5.572\n",
      "Ep 1 (Step 000130): Train loss 5.423, Val loss 5.529\n",
      "Ep 1 (Step 000140): Train loss 5.487, Val loss 5.506\n",
      "Ep 1 (Step 000150): Train loss 5.467, Val loss 5.487\n",
      "Ep 1 (Step 000160): Train loss 5.389, Val loss 5.453\n",
      "Ep 1 (Step 000170): Train loss 5.397, Val loss 5.438\n",
      "Ep 1 (Step 000180): Train loss 5.328, Val loss 5.411\n",
      "Ep 1 (Step 000190): Train loss 5.167, Val loss 5.384\n",
      "Ep 1 (Step 000200): Train loss 5.323, Val loss 5.372\n",
      "Ep 1 (Step 000210): Train loss 5.203, Val loss 5.362\n",
      "Ep 1 (Step 000220): Train loss 5.247, Val loss 5.346\n",
      "Ep 1 (Step 000230): Train loss 5.180, Val loss 5.347\n",
      "Ep 1 (Step 000240): Train loss 5.207, Val loss 5.317\n",
      "Ep 1 (Step 000250): Train loss 5.218, Val loss 5.339\n",
      "Ep 1 (Step 000260): Train loss 5.129, Val loss 5.307\n",
      "Ep 1 (Step 000270): Train loss 5.058, Val loss 5.294\n",
      "Ep 1 (Step 000280): Train loss 5.110, Val loss 5.267\n",
      "Ep 1 (Step 000290): Train loss 5.118, Val loss 5.251\n",
      "Ep 1 (Step 000300): Train loss 5.202, Val loss 5.240\n",
      "Ep 1 (Step 000310): Train loss 5.014, Val loss 5.234\n",
      "Ep 1 (Step 000320): Train loss 5.118, Val loss 5.215\n",
      "Ep 1 (Step 000330): Train loss 5.066, Val loss 5.214\n",
      "Ep 1 (Step 000340): Train loss 4.988, Val loss 5.203\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2033\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.852, Val loss 8.813\n",
      "Ep 1 (Step 000010): Train loss 7.004, Val loss 6.956\n",
      "Ep 1 (Step 000020): Train loss 6.473, Val loss 6.454\n",
      "Ep 1 (Step 000030): Train loss 6.394, Val loss 6.382\n",
      "Ep 1 (Step 000040): Train loss 6.284, Val loss 6.288\n",
      "Ep 1 (Step 000050): Train loss 6.156, Val loss 6.102\n",
      "Ep 1 (Step 000060): Train loss 5.979, Val loss 5.980\n",
      "Ep 1 (Step 000070): Train loss 5.899, Val loss 5.884\n",
      "Ep 1 (Step 000080): Train loss 5.737, Val loss 5.800\n",
      "Ep 1 (Step 000090): Train loss 5.740, Val loss 5.724\n",
      "Ep 1 (Step 000100): Train loss 5.639, Val loss 5.667\n",
      "Ep 1 (Step 000110): Train loss 5.602, Val loss 5.622\n",
      "Ep 1 (Step 000120): Train loss 5.522, Val loss 5.600\n",
      "Ep 1 (Step 000130): Train loss 5.439, Val loss 5.552\n",
      "Ep 1 (Step 000140): Train loss 5.435, Val loss 5.518\n",
      "Ep 1 (Step 000150): Train loss 5.446, Val loss 5.472\n",
      "Ep 1 (Step 000160): Train loss 5.310, Val loss 5.448\n",
      "Ep 1 (Step 000170): Train loss 5.330, Val loss 5.422\n",
      "Ep 1 (Step 000180): Train loss 5.309, Val loss 5.400\n",
      "Ep 1 (Step 000190): Train loss 5.350, Val loss 5.393\n",
      "Ep 1 (Step 000200): Train loss 5.254, Val loss 5.364\n",
      "Ep 1 (Step 000210): Train loss 5.230, Val loss 5.375\n",
      "Ep 1 (Step 000220): Train loss 5.268, Val loss 5.341\n",
      "Ep 1 (Step 000230): Train loss 5.241, Val loss 5.324\n",
      "Ep 1 (Step 000240): Train loss 5.087, Val loss 5.300\n",
      "Ep 1 (Step 000250): Train loss 5.070, Val loss 5.291\n",
      "Ep 1 (Step 000260): Train loss 5.224, Val loss 5.273\n",
      "Ep 1 (Step 000270): Train loss 5.188, Val loss 5.286\n",
      "Ep 1 (Step 000280): Train loss 5.114, Val loss 5.259\n",
      "Ep 1 (Step 000290): Train loss 5.124, Val loss 5.246\n",
      "Ep 1 (Step 000300): Train loss 5.143, Val loss 5.243\n",
      "Ep 1 (Step 000310): Train loss 5.088, Val loss 5.224\n",
      "Ep 1 (Step 000320): Train loss 5.086, Val loss 5.227\n",
      "Ep 1 (Step 000330): Train loss 5.009, Val loss 5.215\n",
      "Ep 1 (Step 000340): Train loss 4.973, Val loss 5.204\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2042\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.874, Val loss 8.847\n",
      "Ep 1 (Step 000010): Train loss 7.016, Val loss 6.971\n",
      "Ep 1 (Step 000020): Train loss 6.505, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.343, Val loss 6.385\n",
      "Ep 1 (Step 000040): Train loss 6.319, Val loss 6.312\n",
      "Ep 1 (Step 000050): Train loss 6.145, Val loss 6.134\n",
      "Ep 1 (Step 000060): Train loss 5.961, Val loss 6.018\n",
      "Ep 1 (Step 000070): Train loss 5.871, Val loss 5.904\n",
      "Ep 1 (Step 000080): Train loss 5.807, Val loss 5.818\n",
      "Ep 1 (Step 000090): Train loss 5.668, Val loss 5.760\n",
      "Ep 1 (Step 000100): Train loss 5.591, Val loss 5.700\n",
      "Ep 1 (Step 000110): Train loss 5.609, Val loss 5.655\n",
      "Ep 1 (Step 000120): Train loss 5.500, Val loss 5.603\n",
      "Ep 1 (Step 000130): Train loss 5.502, Val loss 5.549\n",
      "Ep 1 (Step 000140): Train loss 5.467, Val loss 5.531\n",
      "Ep 1 (Step 000150): Train loss 5.421, Val loss 5.490\n",
      "Ep 1 (Step 000160): Train loss 5.347, Val loss 5.480\n",
      "Ep 1 (Step 000170): Train loss 5.374, Val loss 5.448\n",
      "Ep 1 (Step 000180): Train loss 5.282, Val loss 5.412\n",
      "Ep 1 (Step 000190): Train loss 5.260, Val loss 5.408\n",
      "Ep 1 (Step 000200): Train loss 5.257, Val loss 5.375\n",
      "Ep 1 (Step 000210): Train loss 5.251, Val loss 5.366\n",
      "Ep 1 (Step 000220): Train loss 5.245, Val loss 5.354\n",
      "Ep 1 (Step 000230): Train loss 5.200, Val loss 5.336\n",
      "Ep 1 (Step 000240): Train loss 5.141, Val loss 5.323\n",
      "Ep 1 (Step 000250): Train loss 5.163, Val loss 5.310\n",
      "Ep 1 (Step 000260): Train loss 5.179, Val loss 5.297\n",
      "Ep 1 (Step 000270): Train loss 5.246, Val loss 5.277\n",
      "Ep 1 (Step 000280): Train loss 5.100, Val loss 5.269\n",
      "Ep 1 (Step 000290): Train loss 5.130, Val loss 5.268\n",
      "Ep 1 (Step 000300): Train loss 5.043, Val loss 5.249\n",
      "Ep 1 (Step 000310): Train loss 5.069, Val loss 5.253\n",
      "Ep 1 (Step 000320): Train loss 5.023, Val loss 5.229\n",
      "Ep 1 (Step 000330): Train loss 5.034, Val loss 5.235\n",
      "Ep 1 (Step 000340): Train loss 5.024, Val loss 5.206\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2064\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.441, Val loss 8.433\n",
      "Ep 1 (Step 000010): Train loss 6.595, Val loss 6.559\n",
      "Ep 1 (Step 000020): Train loss 6.517, Val loss 6.497\n",
      "Ep 1 (Step 000030): Train loss 6.326, Val loss 6.299\n",
      "Ep 1 (Step 000040): Train loss 6.087, Val loss 6.113\n",
      "Ep 1 (Step 000050): Train loss 6.057, Val loss 5.985\n",
      "Ep 1 (Step 000060): Train loss 5.761, Val loss 5.883\n",
      "Ep 1 (Step 000070): Train loss 5.701, Val loss 5.740\n",
      "Ep 1 (Step 000080): Train loss 5.688, Val loss 5.678\n",
      "Ep 1 (Step 000090): Train loss 5.633, Val loss 5.689\n",
      "Ep 1 (Step 000100): Train loss 5.587, Val loss 5.628\n",
      "Ep 1 (Step 000110): Train loss 5.477, Val loss 5.569\n",
      "Ep 1 (Step 000120): Train loss 5.423, Val loss 5.524\n",
      "Ep 1 (Step 000130): Train loss 5.375, Val loss 5.499\n",
      "Ep 1 (Step 000140): Train loss 5.413, Val loss 5.472\n",
      "Ep 1 (Step 000150): Train loss 5.321, Val loss 5.424\n",
      "Ep 1 (Step 000160): Train loss 5.249, Val loss 5.403\n",
      "Ep 1 (Step 000170): Train loss 5.337, Val loss 5.390\n",
      "Ep 1 (Step 000180): Train loss 5.283, Val loss 5.372\n",
      "Ep 1 (Step 000190): Train loss 5.263, Val loss 5.377\n",
      "Ep 1 (Step 000200): Train loss 5.237, Val loss 5.350\n",
      "Ep 1 (Step 000210): Train loss 5.108, Val loss 5.308\n",
      "Ep 1 (Step 000220): Train loss 5.184, Val loss 5.311\n",
      "Ep 1 (Step 000230): Train loss 5.061, Val loss 5.274\n",
      "Ep 1 (Step 000240): Train loss 5.062, Val loss 5.295\n",
      "Ep 1 (Step 000250): Train loss 5.080, Val loss 5.279\n",
      "Ep 1 (Step 000260): Train loss 5.061, Val loss 5.276\n",
      "Ep 1 (Step 000270): Train loss 5.061, Val loss 5.265\n",
      "Ep 1 (Step 000280): Train loss 5.079, Val loss 5.259\n",
      "Ep 1 (Step 000290): Train loss 5.123, Val loss 5.241\n",
      "Ep 1 (Step 000300): Train loss 5.081, Val loss 5.237\n",
      "Ep 1 (Step 000310): Train loss 5.088, Val loss 5.224\n",
      "Ep 1 (Step 000320): Train loss 5.086, Val loss 5.216\n",
      "Ep 1 (Step 000330): Train loss 5.061, Val loss 5.214\n",
      "Ep 1 (Step 000340): Train loss 5.028, Val loss 5.219\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2193\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.547, Val loss 8.534\n",
      "Ep 1 (Step 000010): Train loss 6.618, Val loss 6.574\n",
      "Ep 1 (Step 000020): Train loss 6.536, Val loss 6.491\n",
      "Ep 1 (Step 000030): Train loss 6.243, Val loss 6.237\n",
      "Ep 1 (Step 000040): Train loss 6.091, Val loss 6.069\n",
      "Ep 1 (Step 000050): Train loss 5.939, Val loss 5.931\n",
      "Ep 1 (Step 000060): Train loss 5.808, Val loss 5.822\n",
      "Ep 1 (Step 000070): Train loss 5.669, Val loss 5.756\n",
      "Ep 1 (Step 000080): Train loss 5.593, Val loss 5.698\n",
      "Ep 1 (Step 000090): Train loss 5.562, Val loss 5.641\n",
      "Ep 1 (Step 000100): Train loss 5.451, Val loss 5.576\n",
      "Ep 1 (Step 000110): Train loss 5.416, Val loss 5.524\n",
      "Ep 1 (Step 000120): Train loss 5.398, Val loss 5.515\n",
      "Ep 1 (Step 000130): Train loss 5.369, Val loss 5.492\n",
      "Ep 1 (Step 000140): Train loss 5.417, Val loss 5.468\n",
      "Ep 1 (Step 000150): Train loss 5.291, Val loss 5.442\n",
      "Ep 1 (Step 000160): Train loss 5.255, Val loss 5.407\n",
      "Ep 1 (Step 000170): Train loss 5.286, Val loss 5.394\n",
      "Ep 1 (Step 000180): Train loss 5.335, Val loss 5.390\n",
      "Ep 1 (Step 000190): Train loss 5.213, Val loss 5.416\n",
      "Ep 1 (Step 000200): Train loss 5.192, Val loss 5.346\n",
      "Ep 1 (Step 000210): Train loss 5.324, Val loss 5.317\n",
      "Ep 1 (Step 000220): Train loss 5.183, Val loss 5.322\n",
      "Ep 1 (Step 000230): Train loss 5.132, Val loss 5.312\n",
      "Ep 1 (Step 000240): Train loss 5.107, Val loss 5.291\n",
      "Ep 1 (Step 000250): Train loss 5.141, Val loss 5.271\n",
      "Ep 1 (Step 000260): Train loss 5.152, Val loss 5.249\n",
      "Ep 1 (Step 000270): Train loss 5.005, Val loss 5.225\n",
      "Ep 1 (Step 000280): Train loss 5.131, Val loss 5.220\n",
      "Ep 1 (Step 000290): Train loss 5.132, Val loss 5.248\n",
      "Ep 1 (Step 000300): Train loss 4.959, Val loss 5.247\n",
      "Ep 1 (Step 000310): Train loss 5.044, Val loss 5.254\n",
      "Ep 1 (Step 000320): Train loss 5.052, Val loss 5.232\n",
      "Ep 1 (Step 000330): Train loss 4.992, Val loss 5.210\n",
      "Ep 1 (Step 000340): Train loss 4.993, Val loss 5.192\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1923\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.446, Val loss 8.428\n",
      "Ep 1 (Step 000010): Train loss 6.566, Val loss 6.603\n",
      "Ep 1 (Step 000020): Train loss 6.525, Val loss 6.488\n",
      "Ep 1 (Step 000030): Train loss 6.286, Val loss 6.309\n",
      "Ep 1 (Step 000040): Train loss 6.116, Val loss 6.078\n",
      "Ep 1 (Step 000050): Train loss 5.901, Val loss 5.968\n",
      "Ep 1 (Step 000060): Train loss 5.804, Val loss 5.827\n",
      "Ep 1 (Step 000070): Train loss 5.733, Val loss 5.740\n",
      "Ep 1 (Step 000080): Train loss 5.613, Val loss 5.675\n",
      "Ep 1 (Step 000090): Train loss 5.573, Val loss 5.619\n",
      "Ep 1 (Step 000100): Train loss 5.431, Val loss 5.566\n",
      "Ep 1 (Step 000110): Train loss 5.572, Val loss 5.557\n",
      "Ep 1 (Step 000120): Train loss 5.386, Val loss 5.536\n",
      "Ep 1 (Step 000130): Train loss 5.322, Val loss 5.488\n",
      "Ep 1 (Step 000140): Train loss 5.370, Val loss 5.456\n",
      "Ep 1 (Step 000150): Train loss 5.294, Val loss 5.430\n",
      "Ep 1 (Step 000160): Train loss 5.250, Val loss 5.415\n",
      "Ep 1 (Step 000170): Train loss 5.411, Val loss 5.390\n",
      "Ep 1 (Step 000180): Train loss 5.284, Val loss 5.358\n",
      "Ep 1 (Step 000190): Train loss 5.216, Val loss 5.341\n",
      "Ep 1 (Step 000200): Train loss 5.190, Val loss 5.335\n",
      "Ep 1 (Step 000210): Train loss 5.172, Val loss 5.306\n",
      "Ep 1 (Step 000220): Train loss 5.217, Val loss 5.305\n",
      "Ep 1 (Step 000230): Train loss 5.272, Val loss 5.284\n",
      "Ep 1 (Step 000240): Train loss 5.111, Val loss 5.272\n",
      "Ep 1 (Step 000250): Train loss 5.182, Val loss 5.286\n",
      "Ep 1 (Step 000260): Train loss 5.130, Val loss 5.269\n",
      "Ep 1 (Step 000270): Train loss 5.124, Val loss 5.240\n",
      "Ep 1 (Step 000280): Train loss 5.123, Val loss 5.251\n",
      "Ep 1 (Step 000290): Train loss 5.143, Val loss 5.254\n",
      "Ep 1 (Step 000300): Train loss 5.079, Val loss 5.239\n",
      "Ep 1 (Step 000310): Train loss 5.109, Val loss 5.208\n",
      "Ep 1 (Step 000320): Train loss 5.097, Val loss 5.189\n",
      "Ep 1 (Step 000330): Train loss 5.025, Val loss 5.187\n",
      "Ep 1 (Step 000340): Train loss 5.075, Val loss 5.174\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1744\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.446, Val loss 8.407\n",
      "Ep 1 (Step 000010): Train loss 6.510, Val loss 6.554\n",
      "Ep 1 (Step 000020): Train loss 6.526, Val loss 6.505\n",
      "Ep 1 (Step 000030): Train loss 6.347, Val loss 6.317\n",
      "Ep 1 (Step 000040): Train loss 6.179, Val loss 6.187\n",
      "Ep 1 (Step 000050): Train loss 5.976, Val loss 6.008\n",
      "Ep 1 (Step 000060): Train loss 5.781, Val loss 5.889\n",
      "Ep 1 (Step 000070): Train loss 5.682, Val loss 5.795\n",
      "Ep 1 (Step 000080): Train loss 5.545, Val loss 5.704\n",
      "Ep 1 (Step 000090): Train loss 5.550, Val loss 5.628\n",
      "Ep 1 (Step 000100): Train loss 5.620, Val loss 5.603\n",
      "Ep 1 (Step 000110): Train loss 5.470, Val loss 5.540\n",
      "Ep 1 (Step 000120): Train loss 5.512, Val loss 5.531\n",
      "Ep 1 (Step 000130): Train loss 5.478, Val loss 5.488\n",
      "Ep 1 (Step 000140): Train loss 5.301, Val loss 5.448\n",
      "Ep 1 (Step 000150): Train loss 5.358, Val loss 5.409\n",
      "Ep 1 (Step 000160): Train loss 5.262, Val loss 5.418\n",
      "Ep 1 (Step 000170): Train loss 5.242, Val loss 5.376\n",
      "Ep 1 (Step 000180): Train loss 5.225, Val loss 5.384\n",
      "Ep 1 (Step 000190): Train loss 5.251, Val loss 5.356\n",
      "Ep 1 (Step 000200): Train loss 5.188, Val loss 5.330\n",
      "Ep 1 (Step 000210): Train loss 5.206, Val loss 5.307\n",
      "Ep 1 (Step 000220): Train loss 5.203, Val loss 5.290\n",
      "Ep 1 (Step 000230): Train loss 5.157, Val loss 5.300\n",
      "Ep 1 (Step 000240): Train loss 5.188, Val loss 5.282\n",
      "Ep 1 (Step 000250): Train loss 5.075, Val loss 5.250\n",
      "Ep 1 (Step 000260): Train loss 5.041, Val loss 5.243\n",
      "Ep 1 (Step 000270): Train loss 5.134, Val loss 5.227\n",
      "Ep 1 (Step 000280): Train loss 5.006, Val loss 5.208\n",
      "Ep 1 (Step 000290): Train loss 5.101, Val loss 5.194\n",
      "Ep 1 (Step 000300): Train loss 5.018, Val loss 5.182\n",
      "Ep 1 (Step 000310): Train loss 5.013, Val loss 5.188\n",
      "Ep 1 (Step 000320): Train loss 4.961, Val loss 5.191\n",
      "Ep 1 (Step 000330): Train loss 5.007, Val loss 5.155\n",
      "Ep 1 (Step 000340): Train loss 4.989, Val loss 5.169\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1685\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.488, Val loss 8.447\n",
      "Ep 1 (Step 000010): Train loss 6.558, Val loss 6.561\n",
      "Ep 1 (Step 000020): Train loss 6.477, Val loss 6.462\n",
      "Ep 1 (Step 000030): Train loss 6.285, Val loss 6.265\n",
      "Ep 1 (Step 000040): Train loss 6.060, Val loss 6.080\n",
      "Ep 1 (Step 000050): Train loss 5.925, Val loss 5.907\n",
      "Ep 1 (Step 000060): Train loss 5.701, Val loss 5.819\n",
      "Ep 1 (Step 000070): Train loss 5.636, Val loss 5.744\n",
      "Ep 1 (Step 000080): Train loss 5.525, Val loss 5.650\n",
      "Ep 1 (Step 000090): Train loss 5.584, Val loss 5.609\n",
      "Ep 1 (Step 000100): Train loss 5.540, Val loss 5.546\n",
      "Ep 1 (Step 000110): Train loss 5.455, Val loss 5.496\n",
      "Ep 1 (Step 000120): Train loss 5.496, Val loss 5.488\n",
      "Ep 1 (Step 000130): Train loss 5.365, Val loss 5.445\n",
      "Ep 1 (Step 000140): Train loss 5.330, Val loss 5.424\n",
      "Ep 1 (Step 000150): Train loss 5.331, Val loss 5.392\n",
      "Ep 1 (Step 000160): Train loss 5.344, Val loss 5.368\n",
      "Ep 1 (Step 000170): Train loss 5.232, Val loss 5.348\n",
      "Ep 1 (Step 000180): Train loss 5.205, Val loss 5.347\n",
      "Ep 1 (Step 000190): Train loss 5.204, Val loss 5.334\n",
      "Ep 1 (Step 000200): Train loss 5.159, Val loss 5.328\n",
      "Ep 1 (Step 000210): Train loss 5.228, Val loss 5.328\n",
      "Ep 1 (Step 000220): Train loss 5.143, Val loss 5.306\n",
      "Ep 1 (Step 000230): Train loss 5.145, Val loss 5.281\n",
      "Ep 1 (Step 000240): Train loss 5.148, Val loss 5.280\n",
      "Ep 1 (Step 000250): Train loss 5.108, Val loss 5.240\n",
      "Ep 1 (Step 000260): Train loss 4.995, Val loss 5.228\n",
      "Ep 1 (Step 000270): Train loss 5.121, Val loss 5.232\n",
      "Ep 1 (Step 000280): Train loss 5.033, Val loss 5.214\n",
      "Ep 1 (Step 000290): Train loss 5.062, Val loss 5.207\n",
      "Ep 1 (Step 000300): Train loss 5.073, Val loss 5.208\n",
      "Ep 1 (Step 000310): Train loss 5.062, Val loss 5.191\n",
      "Ep 1 (Step 000320): Train loss 5.037, Val loss 5.191\n",
      "Ep 1 (Step 000330): Train loss 5.007, Val loss 5.176\n",
      "Ep 1 (Step 000340): Train loss 4.903, Val loss 5.183\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1827\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.422, Val loss 8.399\n",
      "Ep 1 (Step 000010): Train loss 6.589, Val loss 6.561\n",
      "Ep 1 (Step 000020): Train loss 6.530, Val loss 6.503\n",
      "Ep 1 (Step 000030): Train loss 6.376, Val loss 6.318\n",
      "Ep 1 (Step 000040): Train loss 6.106, Val loss 6.140\n",
      "Ep 1 (Step 000050): Train loss 5.942, Val loss 5.962\n",
      "Ep 1 (Step 000060): Train loss 5.765, Val loss 5.829\n",
      "Ep 1 (Step 000070): Train loss 5.598, Val loss 5.723\n",
      "Ep 1 (Step 000080): Train loss 5.645, Val loss 5.665\n",
      "Ep 1 (Step 000090): Train loss 5.491, Val loss 5.635\n",
      "Ep 1 (Step 000100): Train loss 5.549, Val loss 5.564\n",
      "Ep 1 (Step 000110): Train loss 5.502, Val loss 5.540\n",
      "Ep 1 (Step 000120): Train loss 5.473, Val loss 5.512\n",
      "Ep 1 (Step 000130): Train loss 5.279, Val loss 5.476\n",
      "Ep 1 (Step 000140): Train loss 5.416, Val loss 5.438\n",
      "Ep 1 (Step 000150): Train loss 5.244, Val loss 5.401\n",
      "Ep 1 (Step 000160): Train loss 5.283, Val loss 5.378\n",
      "Ep 1 (Step 000170): Train loss 5.204, Val loss 5.363\n",
      "Ep 1 (Step 000180): Train loss 5.204, Val loss 5.325\n",
      "Ep 1 (Step 000190): Train loss 5.295, Val loss 5.305\n",
      "Ep 1 (Step 000200): Train loss 5.170, Val loss 5.293\n",
      "Ep 1 (Step 000210): Train loss 5.106, Val loss 5.256\n",
      "Ep 1 (Step 000220): Train loss 5.183, Val loss 5.248\n",
      "Ep 1 (Step 000230): Train loss 5.134, Val loss 5.241\n",
      "Ep 1 (Step 000240): Train loss 5.142, Val loss 5.237\n",
      "Ep 1 (Step 000250): Train loss 5.083, Val loss 5.244\n",
      "Ep 1 (Step 000260): Train loss 5.059, Val loss 5.237\n",
      "Ep 1 (Step 000270): Train loss 5.176, Val loss 5.250\n",
      "Ep 1 (Step 000280): Train loss 5.075, Val loss 5.241\n",
      "Ep 1 (Step 000290): Train loss 5.035, Val loss 5.213\n",
      "Ep 1 (Step 000300): Train loss 5.055, Val loss 5.204\n",
      "Ep 1 (Step 000310): Train loss 5.002, Val loss 5.179\n",
      "Ep 1 (Step 000320): Train loss 4.973, Val loss 5.167\n",
      "Ep 1 (Step 000330): Train loss 4.995, Val loss 5.156\n",
      "Ep 1 (Step 000340): Train loss 4.964, Val loss 5.160\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1602\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.782, Val loss 8.780\n",
      "Ep 1 (Step 000010): Train loss 7.001, Val loss 6.972\n",
      "Ep 1 (Step 000020): Train loss 6.491, Val loss 6.464\n",
      "Ep 1 (Step 000030): Train loss 6.410, Val loss 6.396\n",
      "Ep 1 (Step 000040): Train loss 6.278, Val loss 6.267\n",
      "Ep 1 (Step 000050): Train loss 6.123, Val loss 6.121\n",
      "Ep 1 (Step 000060): Train loss 5.989, Val loss 6.005\n",
      "Ep 1 (Step 000070): Train loss 5.834, Val loss 5.918\n",
      "Ep 1 (Step 000080): Train loss 5.758, Val loss 5.826\n",
      "Ep 1 (Step 000090): Train loss 5.674, Val loss 5.761\n",
      "Ep 1 (Step 000100): Train loss 5.690, Val loss 5.698\n",
      "Ep 1 (Step 000110): Train loss 5.618, Val loss 5.639\n",
      "Ep 1 (Step 000120): Train loss 5.478, Val loss 5.600\n",
      "Ep 1 (Step 000130): Train loss 5.496, Val loss 5.570\n",
      "Ep 1 (Step 000140): Train loss 5.415, Val loss 5.529\n",
      "Ep 1 (Step 000150): Train loss 5.345, Val loss 5.482\n",
      "Ep 1 (Step 000160): Train loss 5.310, Val loss 5.465\n",
      "Ep 1 (Step 000170): Train loss 5.336, Val loss 5.437\n",
      "Ep 1 (Step 000180): Train loss 5.242, Val loss 5.407\n",
      "Ep 1 (Step 000190): Train loss 5.267, Val loss 5.369\n",
      "Ep 1 (Step 000200): Train loss 5.281, Val loss 5.367\n",
      "Ep 1 (Step 000210): Train loss 5.260, Val loss 5.350\n",
      "Ep 1 (Step 000220): Train loss 5.157, Val loss 5.326\n",
      "Ep 1 (Step 000230): Train loss 5.240, Val loss 5.332\n",
      "Ep 1 (Step 000240): Train loss 5.220, Val loss 5.318\n",
      "Ep 1 (Step 000250): Train loss 5.221, Val loss 5.319\n",
      "Ep 1 (Step 000260): Train loss 5.161, Val loss 5.281\n",
      "Ep 1 (Step 000270): Train loss 5.211, Val loss 5.288\n",
      "Ep 1 (Step 000280): Train loss 5.080, Val loss 5.262\n",
      "Ep 1 (Step 000290): Train loss 5.151, Val loss 5.282\n",
      "Ep 1 (Step 000300): Train loss 5.072, Val loss 5.257\n",
      "Ep 1 (Step 000310): Train loss 5.103, Val loss 5.242\n",
      "Ep 1 (Step 000320): Train loss 5.046, Val loss 5.245\n",
      "Ep 1 (Step 000330): Train loss 5.011, Val loss 5.237\n",
      "Ep 1 (Step 000340): Train loss 5.022, Val loss 5.239\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2393\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.804, Val loss 8.770\n",
      "Ep 1 (Step 000010): Train loss 7.002, Val loss 6.975\n",
      "Ep 1 (Step 000020): Train loss 6.448, Val loss 6.493\n",
      "Ep 1 (Step 000030): Train loss 6.385, Val loss 6.397\n",
      "Ep 1 (Step 000040): Train loss 6.294, Val loss 6.318\n",
      "Ep 1 (Step 000050): Train loss 6.108, Val loss 6.130\n",
      "Ep 1 (Step 000060): Train loss 5.982, Val loss 6.029\n",
      "Ep 1 (Step 000070): Train loss 5.990, Val loss 5.944\n",
      "Ep 1 (Step 000080): Train loss 5.769, Val loss 5.861\n",
      "Ep 1 (Step 000090): Train loss 5.679, Val loss 5.771\n",
      "Ep 1 (Step 000100): Train loss 5.638, Val loss 5.703\n",
      "Ep 1 (Step 000110): Train loss 5.671, Val loss 5.645\n",
      "Ep 1 (Step 000120): Train loss 5.574, Val loss 5.608\n",
      "Ep 1 (Step 000130): Train loss 5.498, Val loss 5.570\n",
      "Ep 1 (Step 000140): Train loss 5.352, Val loss 5.529\n",
      "Ep 1 (Step 000150): Train loss 5.447, Val loss 5.524\n",
      "Ep 1 (Step 000160): Train loss 5.446, Val loss 5.484\n",
      "Ep 1 (Step 000170): Train loss 5.432, Val loss 5.462\n",
      "Ep 1 (Step 000180): Train loss 5.445, Val loss 5.424\n",
      "Ep 1 (Step 000190): Train loss 5.362, Val loss 5.412\n",
      "Ep 1 (Step 000200): Train loss 5.325, Val loss 5.391\n",
      "Ep 1 (Step 000210): Train loss 5.269, Val loss 5.369\n",
      "Ep 1 (Step 000220): Train loss 5.310, Val loss 5.359\n",
      "Ep 1 (Step 000230): Train loss 5.232, Val loss 5.343\n",
      "Ep 1 (Step 000240): Train loss 5.178, Val loss 5.325\n",
      "Ep 1 (Step 000250): Train loss 5.137, Val loss 5.318\n",
      "Ep 1 (Step 000260): Train loss 5.117, Val loss 5.298\n",
      "Ep 1 (Step 000270): Train loss 5.205, Val loss 5.270\n",
      "Ep 1 (Step 000280): Train loss 5.201, Val loss 5.290\n",
      "Ep 1 (Step 000290): Train loss 5.144, Val loss 5.289\n",
      "Ep 1 (Step 000300): Train loss 5.108, Val loss 5.271\n",
      "Ep 1 (Step 000310): Train loss 5.084, Val loss 5.260\n",
      "Ep 1 (Step 000320): Train loss 5.103, Val loss 5.257\n",
      "Ep 1 (Step 000330): Train loss 5.115, Val loss 5.274\n",
      "Ep 1 (Step 000340): Train loss 5.027, Val loss 5.235\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2350\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.803, Val loss 8.796\n",
      "Ep 1 (Step 000010): Train loss 7.033, Val loss 7.002\n",
      "Ep 1 (Step 000020): Train loss 6.523, Val loss 6.461\n",
      "Ep 1 (Step 000030): Train loss 6.384, Val loss 6.363\n",
      "Ep 1 (Step 000040): Train loss 6.200, Val loss 6.224\n",
      "Ep 1 (Step 000050): Train loss 6.153, Val loss 6.113\n",
      "Ep 1 (Step 000060): Train loss 6.023, Val loss 5.979\n",
      "Ep 1 (Step 000070): Train loss 5.820, Val loss 5.886\n",
      "Ep 1 (Step 000080): Train loss 5.794, Val loss 5.787\n",
      "Ep 1 (Step 000090): Train loss 5.719, Val loss 5.740\n",
      "Ep 1 (Step 000100): Train loss 5.616, Val loss 5.693\n",
      "Ep 1 (Step 000110): Train loss 5.536, Val loss 5.657\n",
      "Ep 1 (Step 000120): Train loss 5.487, Val loss 5.608\n",
      "Ep 1 (Step 000130): Train loss 5.521, Val loss 5.563\n",
      "Ep 1 (Step 000140): Train loss 5.378, Val loss 5.520\n",
      "Ep 1 (Step 000150): Train loss 5.373, Val loss 5.496\n",
      "Ep 1 (Step 000160): Train loss 5.402, Val loss 5.459\n",
      "Ep 1 (Step 000170): Train loss 5.454, Val loss 5.443\n",
      "Ep 1 (Step 000180): Train loss 5.324, Val loss 5.416\n",
      "Ep 1 (Step 000190): Train loss 5.329, Val loss 5.399\n",
      "Ep 1 (Step 000200): Train loss 5.288, Val loss 5.386\n",
      "Ep 1 (Step 000210): Train loss 5.210, Val loss 5.386\n",
      "Ep 1 (Step 000220): Train loss 5.219, Val loss 5.358\n",
      "Ep 1 (Step 000230): Train loss 5.265, Val loss 5.347\n",
      "Ep 1 (Step 000240): Train loss 5.219, Val loss 5.331\n",
      "Ep 1 (Step 000250): Train loss 5.125, Val loss 5.310\n",
      "Ep 1 (Step 000260): Train loss 5.106, Val loss 5.305\n",
      "Ep 1 (Step 000270): Train loss 5.132, Val loss 5.301\n",
      "Ep 1 (Step 000280): Train loss 5.208, Val loss 5.298\n",
      "Ep 1 (Step 000290): Train loss 5.125, Val loss 5.275\n",
      "Ep 1 (Step 000300): Train loss 5.106, Val loss 5.277\n",
      "Ep 1 (Step 000310): Train loss 5.137, Val loss 5.250\n",
      "Ep 1 (Step 000320): Train loss 5.051, Val loss 5.248\n",
      "Ep 1 (Step 000330): Train loss 5.047, Val loss 5.242\n",
      "Ep 1 (Step 000340): Train loss 5.128, Val loss 5.208\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2082\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.800, Val loss 8.777\n",
      "Ep 1 (Step 000010): Train loss 6.984, Val loss 6.910\n",
      "Ep 1 (Step 000020): Train loss 6.503, Val loss 6.444\n",
      "Ep 1 (Step 000030): Train loss 6.380, Val loss 6.370\n",
      "Ep 1 (Step 000040): Train loss 6.272, Val loss 6.285\n",
      "Ep 1 (Step 000050): Train loss 6.195, Val loss 6.114\n",
      "Ep 1 (Step 000060): Train loss 5.996, Val loss 6.019\n",
      "Ep 1 (Step 000070): Train loss 5.982, Val loss 5.912\n",
      "Ep 1 (Step 000080): Train loss 5.746, Val loss 5.831\n",
      "Ep 1 (Step 000090): Train loss 5.690, Val loss 5.762\n",
      "Ep 1 (Step 000100): Train loss 5.618, Val loss 5.708\n",
      "Ep 1 (Step 000110): Train loss 5.515, Val loss 5.652\n",
      "Ep 1 (Step 000120): Train loss 5.480, Val loss 5.601\n",
      "Ep 1 (Step 000130): Train loss 5.447, Val loss 5.549\n",
      "Ep 1 (Step 000140): Train loss 5.370, Val loss 5.506\n",
      "Ep 1 (Step 000150): Train loss 5.419, Val loss 5.478\n",
      "Ep 1 (Step 000160): Train loss 5.306, Val loss 5.445\n",
      "Ep 1 (Step 000170): Train loss 5.249, Val loss 5.437\n",
      "Ep 1 (Step 000180): Train loss 5.229, Val loss 5.409\n",
      "Ep 1 (Step 000190): Train loss 5.198, Val loss 5.394\n",
      "Ep 1 (Step 000200): Train loss 5.224, Val loss 5.378\n",
      "Ep 1 (Step 000210): Train loss 5.252, Val loss 5.368\n",
      "Ep 1 (Step 000220): Train loss 5.346, Val loss 5.354\n",
      "Ep 1 (Step 000230): Train loss 5.232, Val loss 5.328\n",
      "Ep 1 (Step 000240): Train loss 5.232, Val loss 5.320\n",
      "Ep 1 (Step 000250): Train loss 5.255, Val loss 5.301\n",
      "Ep 1 (Step 000260): Train loss 5.160, Val loss 5.300\n",
      "Ep 1 (Step 000270): Train loss 5.107, Val loss 5.269\n",
      "Ep 1 (Step 000280): Train loss 5.122, Val loss 5.274\n",
      "Ep 1 (Step 000290): Train loss 5.033, Val loss 5.271\n",
      "Ep 1 (Step 000300): Train loss 5.060, Val loss 5.249\n",
      "Ep 1 (Step 000310): Train loss 5.054, Val loss 5.231\n",
      "Ep 1 (Step 000320): Train loss 5.015, Val loss 5.225\n",
      "Ep 1 (Step 000330): Train loss 5.046, Val loss 5.215\n",
      "Ep 1 (Step 000340): Train loss 5.035, Val loss 5.202\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2022\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.799, Val loss 8.767\n",
      "Ep 1 (Step 000010): Train loss 6.997, Val loss 6.940\n",
      "Ep 1 (Step 000020): Train loss 6.503, Val loss 6.472\n",
      "Ep 1 (Step 000030): Train loss 6.403, Val loss 6.406\n",
      "Ep 1 (Step 000040): Train loss 6.352, Val loss 6.335\n",
      "Ep 1 (Step 000050): Train loss 6.163, Val loss 6.118\n",
      "Ep 1 (Step 000060): Train loss 5.947, Val loss 6.006\n",
      "Ep 1 (Step 000070): Train loss 5.862, Val loss 5.895\n",
      "Ep 1 (Step 000080): Train loss 5.680, Val loss 5.813\n",
      "Ep 1 (Step 000090): Train loss 5.758, Val loss 5.763\n",
      "Ep 1 (Step 000100): Train loss 5.609, Val loss 5.676\n",
      "Ep 1 (Step 000110): Train loss 5.483, Val loss 5.638\n",
      "Ep 1 (Step 000120): Train loss 5.564, Val loss 5.603\n",
      "Ep 1 (Step 000130): Train loss 5.503, Val loss 5.551\n",
      "Ep 1 (Step 000140): Train loss 5.495, Val loss 5.529\n",
      "Ep 1 (Step 000150): Train loss 5.347, Val loss 5.499\n",
      "Ep 1 (Step 000160): Train loss 5.332, Val loss 5.463\n",
      "Ep 1 (Step 000170): Train loss 5.338, Val loss 5.420\n",
      "Ep 1 (Step 000180): Train loss 5.244, Val loss 5.395\n",
      "Ep 1 (Step 000190): Train loss 5.337, Val loss 5.391\n",
      "Ep 1 (Step 000200): Train loss 5.304, Val loss 5.375\n",
      "Ep 1 (Step 000210): Train loss 5.266, Val loss 5.347\n",
      "Ep 1 (Step 000220): Train loss 5.376, Val loss 5.348\n",
      "Ep 1 (Step 000230): Train loss 5.212, Val loss 5.345\n",
      "Ep 1 (Step 000240): Train loss 5.166, Val loss 5.307\n",
      "Ep 1 (Step 000250): Train loss 5.097, Val loss 5.279\n",
      "Ep 1 (Step 000260): Train loss 5.177, Val loss 5.280\n",
      "Ep 1 (Step 000270): Train loss 5.137, Val loss 5.258\n",
      "Ep 1 (Step 000280): Train loss 5.108, Val loss 5.247\n",
      "Ep 1 (Step 000290): Train loss 5.088, Val loss 5.248\n",
      "Ep 1 (Step 000300): Train loss 5.122, Val loss 5.235\n",
      "Ep 1 (Step 000310): Train loss 5.084, Val loss 5.224\n",
      "Ep 1 (Step 000320): Train loss 5.076, Val loss 5.208\n",
      "Ep 1 (Step 000330): Train loss 4.999, Val loss 5.206\n",
      "Ep 1 (Step 000340): Train loss 4.998, Val loss 5.195\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1951\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.787, Val loss 8.781\n",
      "Ep 1 (Step 000010): Train loss 6.957, Val loss 6.973\n",
      "Ep 1 (Step 000020): Train loss 6.489, Val loss 6.500\n",
      "Ep 1 (Step 000030): Train loss 6.453, Val loss 6.404\n",
      "Ep 1 (Step 000040): Train loss 6.308, Val loss 6.301\n",
      "Ep 1 (Step 000050): Train loss 6.131, Val loss 6.145\n",
      "Ep 1 (Step 000060): Train loss 5.985, Val loss 6.019\n",
      "Ep 1 (Step 000070): Train loss 5.836, Val loss 5.946\n",
      "Ep 1 (Step 000080): Train loss 5.769, Val loss 5.842\n",
      "Ep 1 (Step 000090): Train loss 5.694, Val loss 5.763\n",
      "Ep 1 (Step 000100): Train loss 5.612, Val loss 5.718\n",
      "Ep 1 (Step 000110): Train loss 5.615, Val loss 5.654\n",
      "Ep 1 (Step 000120): Train loss 5.545, Val loss 5.610\n",
      "Ep 1 (Step 000130): Train loss 5.433, Val loss 5.564\n",
      "Ep 1 (Step 000140): Train loss 5.402, Val loss 5.543\n",
      "Ep 1 (Step 000150): Train loss 5.349, Val loss 5.518\n",
      "Ep 1 (Step 000160): Train loss 5.347, Val loss 5.480\n",
      "Ep 1 (Step 000170): Train loss 5.309, Val loss 5.452\n",
      "Ep 1 (Step 000180): Train loss 5.372, Val loss 5.425\n",
      "Ep 1 (Step 000190): Train loss 5.258, Val loss 5.411\n",
      "Ep 1 (Step 000200): Train loss 5.305, Val loss 5.391\n",
      "Ep 1 (Step 000210): Train loss 5.258, Val loss 5.371\n",
      "Ep 1 (Step 000220): Train loss 5.268, Val loss 5.349\n",
      "Ep 1 (Step 000230): Train loss 5.233, Val loss 5.321\n",
      "Ep 1 (Step 000240): Train loss 5.133, Val loss 5.306\n",
      "Ep 1 (Step 000250): Train loss 5.221, Val loss 5.293\n",
      "Ep 1 (Step 000260): Train loss 5.123, Val loss 5.276\n",
      "Ep 1 (Step 000270): Train loss 5.164, Val loss 5.285\n",
      "Ep 1 (Step 000280): Train loss 5.139, Val loss 5.255\n",
      "Ep 1 (Step 000290): Train loss 5.078, Val loss 5.265\n",
      "Ep 1 (Step 000300): Train loss 5.192, Val loss 5.241\n",
      "Ep 1 (Step 000310): Train loss 5.021, Val loss 5.221\n",
      "Ep 1 (Step 000320): Train loss 5.111, Val loss 5.215\n",
      "Ep 1 (Step 000330): Train loss 4.993, Val loss 5.208\n",
      "Ep 1 (Step 000340): Train loss 4.975, Val loss 5.203\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2028\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.508, Val loss 8.516\n",
      "Ep 1 (Step 000010): Train loss 6.560, Val loss 6.544\n",
      "Ep 1 (Step 000020): Train loss 6.455, Val loss 6.464\n",
      "Ep 1 (Step 000030): Train loss 6.284, Val loss 6.263\n",
      "Ep 1 (Step 000040): Train loss 6.160, Val loss 6.129\n",
      "Ep 1 (Step 000050): Train loss 5.986, Val loss 5.942\n",
      "Ep 1 (Step 000060): Train loss 5.787, Val loss 5.847\n",
      "Ep 1 (Step 000070): Train loss 5.650, Val loss 5.750\n",
      "Ep 1 (Step 000080): Train loss 5.619, Val loss 5.666\n",
      "Ep 1 (Step 000090): Train loss 5.575, Val loss 5.593\n",
      "Ep 1 (Step 000100): Train loss 5.497, Val loss 5.535\n",
      "Ep 1 (Step 000110): Train loss 5.458, Val loss 5.534\n",
      "Ep 1 (Step 000120): Train loss 5.408, Val loss 5.516\n",
      "Ep 1 (Step 000130): Train loss 5.268, Val loss 5.466\n",
      "Ep 1 (Step 000140): Train loss 5.285, Val loss 5.441\n",
      "Ep 1 (Step 000150): Train loss 5.301, Val loss 5.402\n",
      "Ep 1 (Step 000160): Train loss 5.436, Val loss 5.404\n",
      "Ep 1 (Step 000170): Train loss 5.258, Val loss 5.365\n",
      "Ep 1 (Step 000180): Train loss 5.296, Val loss 5.346\n",
      "Ep 1 (Step 000190): Train loss 5.321, Val loss 5.336\n",
      "Ep 1 (Step 000200): Train loss 5.216, Val loss 5.322\n",
      "Ep 1 (Step 000210): Train loss 5.143, Val loss 5.314\n",
      "Ep 1 (Step 000220): Train loss 5.168, Val loss 5.282\n",
      "Ep 1 (Step 000230): Train loss 5.121, Val loss 5.300\n",
      "Ep 1 (Step 000240): Train loss 5.131, Val loss 5.273\n",
      "Ep 1 (Step 000250): Train loss 5.187, Val loss 5.285\n",
      "Ep 1 (Step 000260): Train loss 5.134, Val loss 5.256\n",
      "Ep 1 (Step 000270): Train loss 5.072, Val loss 5.271\n",
      "Ep 1 (Step 000280): Train loss 5.063, Val loss 5.247\n",
      "Ep 1 (Step 000290): Train loss 5.071, Val loss 5.231\n",
      "Ep 1 (Step 000300): Train loss 5.058, Val loss 5.243\n",
      "Ep 1 (Step 000310): Train loss 5.043, Val loss 5.236\n",
      "Ep 1 (Step 000320): Train loss 5.060, Val loss 5.210\n",
      "Ep 1 (Step 000330): Train loss 5.079, Val loss 5.193\n",
      "Ep 1 (Step 000340): Train loss 4.925, Val loss 5.200\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1998\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.415, Val loss 8.429\n",
      "Ep 1 (Step 000010): Train loss 6.623, Val loss 6.566\n",
      "Ep 1 (Step 000020): Train loss 6.459, Val loss 6.506\n",
      "Ep 1 (Step 000030): Train loss 6.342, Val loss 6.313\n",
      "Ep 1 (Step 000040): Train loss 6.053, Val loss 6.137\n",
      "Ep 1 (Step 000050): Train loss 5.924, Val loss 5.964\n",
      "Ep 1 (Step 000060): Train loss 5.834, Val loss 5.852\n",
      "Ep 1 (Step 000070): Train loss 5.608, Val loss 5.752\n",
      "Ep 1 (Step 000080): Train loss 5.642, Val loss 5.703\n",
      "Ep 1 (Step 000090): Train loss 5.557, Val loss 5.652\n",
      "Ep 1 (Step 000100): Train loss 5.550, Val loss 5.606\n",
      "Ep 1 (Step 000110): Train loss 5.423, Val loss 5.552\n",
      "Ep 1 (Step 000120): Train loss 5.330, Val loss 5.536\n",
      "Ep 1 (Step 000130): Train loss 5.404, Val loss 5.476\n",
      "Ep 1 (Step 000140): Train loss 5.297, Val loss 5.468\n",
      "Ep 1 (Step 000150): Train loss 5.310, Val loss 5.435\n",
      "Ep 1 (Step 000160): Train loss 5.251, Val loss 5.413\n",
      "Ep 1 (Step 000170): Train loss 5.310, Val loss 5.403\n",
      "Ep 1 (Step 000180): Train loss 5.313, Val loss 5.378\n",
      "Ep 1 (Step 000190): Train loss 5.260, Val loss 5.360\n",
      "Ep 1 (Step 000200): Train loss 5.297, Val loss 5.348\n",
      "Ep 1 (Step 000210): Train loss 5.184, Val loss 5.323\n",
      "Ep 1 (Step 000220): Train loss 5.173, Val loss 5.313\n",
      "Ep 1 (Step 000230): Train loss 5.169, Val loss 5.278\n",
      "Ep 1 (Step 000240): Train loss 5.136, Val loss 5.260\n",
      "Ep 1 (Step 000250): Train loss 5.116, Val loss 5.255\n",
      "Ep 1 (Step 000260): Train loss 5.102, Val loss 5.261\n",
      "Ep 1 (Step 000270): Train loss 5.201, Val loss 5.239\n",
      "Ep 1 (Step 000280): Train loss 5.171, Val loss 5.237\n",
      "Ep 1 (Step 000290): Train loss 5.100, Val loss 5.236\n",
      "Ep 1 (Step 000300): Train loss 5.069, Val loss 5.217\n",
      "Ep 1 (Step 000310): Train loss 5.047, Val loss 5.227\n",
      "Ep 1 (Step 000320): Train loss 5.027, Val loss 5.202\n",
      "Ep 1 (Step 000330): Train loss 4.991, Val loss 5.201\n",
      "Ep 1 (Step 000340): Train loss 5.162, Val loss 5.175\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1752\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.475, Val loss 8.457\n",
      "Ep 1 (Step 000010): Train loss 6.514, Val loss 6.551\n",
      "Ep 1 (Step 000020): Train loss 6.479, Val loss 6.485\n",
      "Ep 1 (Step 000030): Train loss 6.186, Val loss 6.240\n",
      "Ep 1 (Step 000040): Train loss 6.021, Val loss 6.089\n",
      "Ep 1 (Step 000050): Train loss 5.898, Val loss 5.927\n",
      "Ep 1 (Step 000060): Train loss 5.857, Val loss 5.846\n",
      "Ep 1 (Step 000070): Train loss 5.633, Val loss 5.752\n",
      "Ep 1 (Step 000080): Train loss 5.634, Val loss 5.677\n",
      "Ep 1 (Step 000090): Train loss 5.479, Val loss 5.604\n",
      "Ep 1 (Step 000100): Train loss 5.601, Val loss 5.572\n",
      "Ep 1 (Step 000110): Train loss 5.472, Val loss 5.520\n",
      "Ep 1 (Step 000120): Train loss 5.415, Val loss 5.484\n",
      "Ep 1 (Step 000130): Train loss 5.336, Val loss 5.469\n",
      "Ep 1 (Step 000140): Train loss 5.373, Val loss 5.439\n",
      "Ep 1 (Step 000150): Train loss 5.309, Val loss 5.426\n",
      "Ep 1 (Step 000160): Train loss 5.277, Val loss 5.397\n",
      "Ep 1 (Step 000170): Train loss 5.311, Val loss 5.386\n",
      "Ep 1 (Step 000180): Train loss 5.163, Val loss 5.344\n",
      "Ep 1 (Step 000190): Train loss 5.215, Val loss 5.356\n",
      "Ep 1 (Step 000200): Train loss 5.280, Val loss 5.352\n",
      "Ep 1 (Step 000210): Train loss 5.180, Val loss 5.332\n",
      "Ep 1 (Step 000220): Train loss 5.212, Val loss 5.307\n",
      "Ep 1 (Step 000230): Train loss 5.139, Val loss 5.288\n",
      "Ep 1 (Step 000240): Train loss 5.082, Val loss 5.267\n",
      "Ep 1 (Step 000250): Train loss 5.151, Val loss 5.257\n",
      "Ep 1 (Step 000260): Train loss 5.077, Val loss 5.247\n",
      "Ep 1 (Step 000270): Train loss 5.153, Val loss 5.236\n",
      "Ep 1 (Step 000280): Train loss 5.146, Val loss 5.214\n",
      "Ep 1 (Step 000290): Train loss 5.086, Val loss 5.210\n",
      "Ep 1 (Step 000300): Train loss 4.993, Val loss 5.202\n",
      "Ep 1 (Step 000310): Train loss 5.102, Val loss 5.207\n",
      "Ep 1 (Step 000320): Train loss 4.986, Val loss 5.179\n",
      "Ep 1 (Step 000330): Train loss 5.043, Val loss 5.172\n",
      "Ep 1 (Step 000340): Train loss 4.989, Val loss 5.161\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1615\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.418, Val loss 8.374\n",
      "Ep 1 (Step 000010): Train loss 6.554, Val loss 6.550\n",
      "Ep 1 (Step 000020): Train loss 6.497, Val loss 6.475\n",
      "Ep 1 (Step 000030): Train loss 6.280, Val loss 6.266\n",
      "Ep 1 (Step 000040): Train loss 6.086, Val loss 6.078\n",
      "Ep 1 (Step 000050): Train loss 5.946, Val loss 5.956\n",
      "Ep 1 (Step 000060): Train loss 5.789, Val loss 5.861\n",
      "Ep 1 (Step 000070): Train loss 5.713, Val loss 5.767\n",
      "Ep 1 (Step 000080): Train loss 5.623, Val loss 5.676\n",
      "Ep 1 (Step 000090): Train loss 5.598, Val loss 5.639\n",
      "Ep 1 (Step 000100): Train loss 5.477, Val loss 5.562\n",
      "Ep 1 (Step 000110): Train loss 5.465, Val loss 5.552\n",
      "Ep 1 (Step 000120): Train loss 5.388, Val loss 5.477\n",
      "Ep 1 (Step 000130): Train loss 5.304, Val loss 5.459\n",
      "Ep 1 (Step 000140): Train loss 5.293, Val loss 5.453\n",
      "Ep 1 (Step 000150): Train loss 5.324, Val loss 5.423\n",
      "Ep 1 (Step 000160): Train loss 5.224, Val loss 5.390\n",
      "Ep 1 (Step 000170): Train loss 5.249, Val loss 5.361\n",
      "Ep 1 (Step 000180): Train loss 5.197, Val loss 5.351\n",
      "Ep 1 (Step 000190): Train loss 5.241, Val loss 5.318\n",
      "Ep 1 (Step 000200): Train loss 5.242, Val loss 5.297\n",
      "Ep 1 (Step 000210): Train loss 5.147, Val loss 5.285\n",
      "Ep 1 (Step 000220): Train loss 5.123, Val loss 5.255\n",
      "Ep 1 (Step 000230): Train loss 5.222, Val loss 5.237\n",
      "Ep 1 (Step 000240): Train loss 5.059, Val loss 5.234\n",
      "Ep 1 (Step 000250): Train loss 5.063, Val loss 5.217\n",
      "Ep 1 (Step 000260): Train loss 5.098, Val loss 5.207\n",
      "Ep 1 (Step 000270): Train loss 4.940, Val loss 5.198\n",
      "Ep 1 (Step 000280): Train loss 4.996, Val loss 5.203\n",
      "Ep 1 (Step 000290): Train loss 5.114, Val loss 5.193\n",
      "Ep 1 (Step 000300): Train loss 5.051, Val loss 5.173\n",
      "Ep 1 (Step 000310): Train loss 5.028, Val loss 5.171\n",
      "Ep 1 (Step 000320): Train loss 5.005, Val loss 5.168\n",
      "Ep 1 (Step 000330): Train loss 5.050, Val loss 5.159\n",
      "Ep 1 (Step 000340): Train loss 5.054, Val loss 5.144\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1439\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.413, Val loss 8.400\n",
      "Ep 1 (Step 000010): Train loss 6.542, Val loss 6.567\n",
      "Ep 1 (Step 000020): Train loss 6.548, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.317, Val loss 6.241\n",
      "Ep 1 (Step 000040): Train loss 6.087, Val loss 6.073\n",
      "Ep 1 (Step 000050): Train loss 5.852, Val loss 5.919\n",
      "Ep 1 (Step 000060): Train loss 5.798, Val loss 5.823\n",
      "Ep 1 (Step 000070): Train loss 5.745, Val loss 5.702\n",
      "Ep 1 (Step 000080): Train loss 5.589, Val loss 5.683\n",
      "Ep 1 (Step 000090): Train loss 5.509, Val loss 5.624\n",
      "Ep 1 (Step 000100): Train loss 5.530, Val loss 5.613\n",
      "Ep 1 (Step 000110): Train loss 5.488, Val loss 5.522\n",
      "Ep 1 (Step 000120): Train loss 5.376, Val loss 5.478\n",
      "Ep 1 (Step 000130): Train loss 5.451, Val loss 5.443\n",
      "Ep 1 (Step 000140): Train loss 5.336, Val loss 5.425\n",
      "Ep 1 (Step 000150): Train loss 5.291, Val loss 5.418\n",
      "Ep 1 (Step 000160): Train loss 5.245, Val loss 5.393\n",
      "Ep 1 (Step 000170): Train loss 5.298, Val loss 5.394\n",
      "Ep 1 (Step 000180): Train loss 5.276, Val loss 5.351\n",
      "Ep 1 (Step 000190): Train loss 5.238, Val loss 5.319\n",
      "Ep 1 (Step 000200): Train loss 5.179, Val loss 5.309\n",
      "Ep 1 (Step 000210): Train loss 5.126, Val loss 5.290\n",
      "Ep 1 (Step 000220): Train loss 5.141, Val loss 5.253\n",
      "Ep 1 (Step 000230): Train loss 5.182, Val loss 5.247\n",
      "Ep 1 (Step 000240): Train loss 5.077, Val loss 5.264\n",
      "Ep 1 (Step 000250): Train loss 5.143, Val loss 5.250\n",
      "Ep 1 (Step 000260): Train loss 5.130, Val loss 5.230\n",
      "Ep 1 (Step 000270): Train loss 4.984, Val loss 5.204\n",
      "Ep 1 (Step 000280): Train loss 4.968, Val loss 5.194\n",
      "Ep 1 (Step 000290): Train loss 5.092, Val loss 5.202\n",
      "Ep 1 (Step 000300): Train loss 5.099, Val loss 5.181\n",
      "Ep 1 (Step 000310): Train loss 5.100, Val loss 5.183\n",
      "Ep 1 (Step 000320): Train loss 4.967, Val loss 5.170\n",
      "Ep 1 (Step 000330): Train loss 5.008, Val loss 5.156\n",
      "Ep 1 (Step 000340): Train loss 4.961, Val loss 5.141\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1412\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.419, Val loss 8.410\n",
      "Ep 1 (Step 000010): Train loss 6.520, Val loss 6.525\n",
      "Ep 1 (Step 000020): Train loss 6.564, Val loss 6.459\n",
      "Ep 1 (Step 000030): Train loss 6.215, Val loss 6.247\n",
      "Ep 1 (Step 000040): Train loss 6.013, Val loss 6.081\n",
      "Ep 1 (Step 000050): Train loss 5.872, Val loss 5.930\n",
      "Ep 1 (Step 000060): Train loss 5.893, Val loss 5.823\n",
      "Ep 1 (Step 000070): Train loss 5.720, Val loss 5.731\n",
      "Ep 1 (Step 000080): Train loss 5.639, Val loss 5.670\n",
      "Ep 1 (Step 000090): Train loss 5.569, Val loss 5.580\n",
      "Ep 1 (Step 000100): Train loss 5.473, Val loss 5.558\n",
      "Ep 1 (Step 000110): Train loss 5.418, Val loss 5.518\n",
      "Ep 1 (Step 000120): Train loss 5.382, Val loss 5.505\n",
      "Ep 1 (Step 000130): Train loss 5.266, Val loss 5.469\n",
      "Ep 1 (Step 000140): Train loss 5.381, Val loss 5.450\n",
      "Ep 1 (Step 000150): Train loss 5.271, Val loss 5.407\n",
      "Ep 1 (Step 000160): Train loss 5.195, Val loss 5.405\n",
      "Ep 1 (Step 000170): Train loss 5.247, Val loss 5.360\n",
      "Ep 1 (Step 000180): Train loss 5.294, Val loss 5.342\n",
      "Ep 1 (Step 000190): Train loss 5.222, Val loss 5.339\n",
      "Ep 1 (Step 000200): Train loss 5.111, Val loss 5.339\n",
      "Ep 1 (Step 000210): Train loss 5.183, Val loss 5.309\n",
      "Ep 1 (Step 000220): Train loss 5.181, Val loss 5.292\n",
      "Ep 1 (Step 000230): Train loss 5.225, Val loss 5.275\n",
      "Ep 1 (Step 000240): Train loss 5.176, Val loss 5.281\n",
      "Ep 1 (Step 000250): Train loss 5.100, Val loss 5.264\n",
      "Ep 1 (Step 000260): Train loss 5.154, Val loss 5.235\n",
      "Ep 1 (Step 000270): Train loss 5.020, Val loss 5.230\n",
      "Ep 1 (Step 000280): Train loss 5.112, Val loss 5.212\n",
      "Ep 1 (Step 000290): Train loss 4.941, Val loss 5.209\n",
      "Ep 1 (Step 000300): Train loss 5.008, Val loss 5.230\n",
      "Ep 1 (Step 000310): Train loss 4.985, Val loss 5.207\n",
      "Ep 1 (Step 000320): Train loss 4.896, Val loss 5.176\n",
      "Ep 1 (Step 000330): Train loss 5.025, Val loss 5.168\n",
      "Ep 1 (Step 000340): Train loss 5.042, Val loss 5.155\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 8, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1548\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.678, Val loss 8.607\n",
      "Ep 1 (Step 000010): Train loss 7.082, Val loss 7.005\n",
      "Ep 1 (Step 000020): Train loss 6.514, Val loss 6.500\n",
      "Ep 1 (Step 000030): Train loss 6.432, Val loss 6.407\n",
      "Ep 1 (Step 000040): Train loss 6.387, Val loss 6.330\n",
      "Ep 1 (Step 000050): Train loss 6.130, Val loss 6.164\n",
      "Ep 1 (Step 000060): Train loss 6.071, Val loss 6.055\n",
      "Ep 1 (Step 000070): Train loss 5.983, Val loss 5.949\n",
      "Ep 1 (Step 000080): Train loss 5.788, Val loss 5.871\n",
      "Ep 1 (Step 000090): Train loss 5.710, Val loss 5.795\n",
      "Ep 1 (Step 000100): Train loss 5.699, Val loss 5.753\n",
      "Ep 1 (Step 000110): Train loss 5.523, Val loss 5.702\n",
      "Ep 1 (Step 000120): Train loss 5.560, Val loss 5.644\n",
      "Ep 1 (Step 000130): Train loss 5.619, Val loss 5.618\n",
      "Ep 1 (Step 000140): Train loss 5.523, Val loss 5.594\n",
      "Ep 1 (Step 000150): Train loss 5.437, Val loss 5.570\n",
      "Ep 1 (Step 000160): Train loss 5.482, Val loss 5.537\n",
      "Ep 1 (Step 000170): Train loss 5.425, Val loss 5.502\n",
      "Ep 1 (Step 000180): Train loss 5.453, Val loss 5.495\n",
      "Ep 1 (Step 000190): Train loss 5.391, Val loss 5.465\n",
      "Ep 1 (Step 000200): Train loss 5.296, Val loss 5.437\n",
      "Ep 1 (Step 000210): Train loss 5.402, Val loss 5.431\n",
      "Ep 1 (Step 000220): Train loss 5.302, Val loss 5.393\n",
      "Ep 1 (Step 000230): Train loss 5.230, Val loss 5.397\n",
      "Ep 1 (Step 000240): Train loss 5.258, Val loss 5.389\n",
      "Ep 1 (Step 000250): Train loss 5.290, Val loss 5.368\n",
      "Ep 1 (Step 000260): Train loss 5.289, Val loss 5.372\n",
      "Ep 1 (Step 000270): Train loss 5.242, Val loss 5.352\n",
      "Ep 1 (Step 000280): Train loss 5.188, Val loss 5.348\n",
      "Ep 1 (Step 000290): Train loss 5.173, Val loss 5.333\n",
      "Ep 1 (Step 000300): Train loss 5.083, Val loss 5.295\n",
      "Ep 1 (Step 000310): Train loss 5.255, Val loss 5.301\n",
      "Ep 1 (Step 000320): Train loss 5.135, Val loss 5.276\n",
      "Ep 1 (Step 000330): Train loss 5.116, Val loss 5.278\n",
      "Ep 1 (Step 000340): Train loss 5.076, Val loss 5.262\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2624\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.657, Val loss 8.626\n",
      "Ep 1 (Step 000010): Train loss 7.008, Val loss 6.964\n",
      "Ep 1 (Step 000020): Train loss 6.483, Val loss 6.456\n",
      "Ep 1 (Step 000030): Train loss 6.383, Val loss 6.390\n",
      "Ep 1 (Step 000040): Train loss 6.304, Val loss 6.292\n",
      "Ep 1 (Step 000050): Train loss 6.111, Val loss 6.104\n",
      "Ep 1 (Step 000060): Train loss 5.955, Val loss 6.026\n",
      "Ep 1 (Step 000070): Train loss 5.880, Val loss 5.942\n",
      "Ep 1 (Step 000080): Train loss 5.806, Val loss 5.849\n",
      "Ep 1 (Step 000090): Train loss 5.719, Val loss 5.798\n",
      "Ep 1 (Step 000100): Train loss 5.725, Val loss 5.727\n",
      "Ep 1 (Step 000110): Train loss 5.602, Val loss 5.687\n",
      "Ep 1 (Step 000120): Train loss 5.584, Val loss 5.640\n",
      "Ep 1 (Step 000130): Train loss 5.462, Val loss 5.618\n",
      "Ep 1 (Step 000140): Train loss 5.497, Val loss 5.588\n",
      "Ep 1 (Step 000150): Train loss 5.470, Val loss 5.547\n",
      "Ep 1 (Step 000160): Train loss 5.407, Val loss 5.510\n",
      "Ep 1 (Step 000170): Train loss 5.368, Val loss 5.478\n",
      "Ep 1 (Step 000180): Train loss 5.283, Val loss 5.480\n",
      "Ep 1 (Step 000190): Train loss 5.389, Val loss 5.456\n",
      "Ep 1 (Step 000200): Train loss 5.350, Val loss 5.420\n",
      "Ep 1 (Step 000210): Train loss 5.321, Val loss 5.379\n",
      "Ep 1 (Step 000220): Train loss 5.394, Val loss 5.371\n",
      "Ep 1 (Step 000230): Train loss 5.226, Val loss 5.372\n",
      "Ep 1 (Step 000240): Train loss 5.295, Val loss 5.356\n",
      "Ep 1 (Step 000250): Train loss 5.256, Val loss 5.351\n",
      "Ep 1 (Step 000260): Train loss 5.185, Val loss 5.335\n",
      "Ep 1 (Step 000270): Train loss 5.249, Val loss 5.320\n",
      "Ep 1 (Step 000280): Train loss 5.207, Val loss 5.320\n",
      "Ep 1 (Step 000290): Train loss 5.184, Val loss 5.302\n",
      "Ep 1 (Step 000300): Train loss 5.139, Val loss 5.293\n",
      "Ep 1 (Step 000310): Train loss 5.180, Val loss 5.287\n",
      "Ep 1 (Step 000320): Train loss 5.152, Val loss 5.279\n",
      "Ep 1 (Step 000330): Train loss 5.180, Val loss 5.272\n",
      "Ep 1 (Step 000340): Train loss 5.059, Val loss 5.258\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2580\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.629, Val loss 8.642\n",
      "Ep 1 (Step 000010): Train loss 6.997, Val loss 6.971\n",
      "Ep 1 (Step 000020): Train loss 6.539, Val loss 6.481\n",
      "Ep 1 (Step 000030): Train loss 6.392, Val loss 6.420\n",
      "Ep 1 (Step 000040): Train loss 6.358, Val loss 6.369\n",
      "Ep 1 (Step 000050): Train loss 6.210, Val loss 6.197\n",
      "Ep 1 (Step 000060): Train loss 6.083, Val loss 6.057\n",
      "Ep 1 (Step 000070): Train loss 5.943, Val loss 5.963\n",
      "Ep 1 (Step 000080): Train loss 5.818, Val loss 5.876\n",
      "Ep 1 (Step 000090): Train loss 5.794, Val loss 5.823\n",
      "Ep 1 (Step 000100): Train loss 5.682, Val loss 5.760\n",
      "Ep 1 (Step 000110): Train loss 5.561, Val loss 5.692\n",
      "Ep 1 (Step 000120): Train loss 5.540, Val loss 5.659\n",
      "Ep 1 (Step 000130): Train loss 5.481, Val loss 5.635\n",
      "Ep 1 (Step 000140): Train loss 5.492, Val loss 5.583\n",
      "Ep 1 (Step 000150): Train loss 5.407, Val loss 5.561\n",
      "Ep 1 (Step 000160): Train loss 5.389, Val loss 5.538\n",
      "Ep 1 (Step 000170): Train loss 5.462, Val loss 5.505\n",
      "Ep 1 (Step 000180): Train loss 5.477, Val loss 5.473\n",
      "Ep 1 (Step 000190): Train loss 5.393, Val loss 5.455\n",
      "Ep 1 (Step 000200): Train loss 5.352, Val loss 5.425\n",
      "Ep 1 (Step 000210): Train loss 5.221, Val loss 5.408\n",
      "Ep 1 (Step 000220): Train loss 5.259, Val loss 5.396\n",
      "Ep 1 (Step 000230): Train loss 5.362, Val loss 5.409\n",
      "Ep 1 (Step 000240): Train loss 5.205, Val loss 5.361\n",
      "Ep 1 (Step 000250): Train loss 5.291, Val loss 5.348\n",
      "Ep 1 (Step 000260): Train loss 5.301, Val loss 5.332\n",
      "Ep 1 (Step 000270): Train loss 5.179, Val loss 5.313\n",
      "Ep 1 (Step 000280): Train loss 5.165, Val loss 5.294\n",
      "Ep 1 (Step 000290): Train loss 5.123, Val loss 5.306\n",
      "Ep 1 (Step 000300): Train loss 5.175, Val loss 5.283\n",
      "Ep 1 (Step 000310): Train loss 5.228, Val loss 5.297\n",
      "Ep 1 (Step 000320): Train loss 5.139, Val loss 5.266\n",
      "Ep 1 (Step 000330): Train loss 5.112, Val loss 5.257\n",
      "Ep 1 (Step 000340): Train loss 5.102, Val loss 5.249\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2490\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.634, Val loss 8.629\n",
      "Ep 1 (Step 000010): Train loss 6.985, Val loss 6.977\n",
      "Ep 1 (Step 000020): Train loss 6.487, Val loss 6.483\n",
      "Ep 1 (Step 000030): Train loss 6.384, Val loss 6.410\n",
      "Ep 1 (Step 000040): Train loss 6.347, Val loss 6.342\n",
      "Ep 1 (Step 000050): Train loss 6.179, Val loss 6.195\n",
      "Ep 1 (Step 000060): Train loss 6.018, Val loss 6.041\n",
      "Ep 1 (Step 000070): Train loss 5.951, Val loss 5.962\n",
      "Ep 1 (Step 000080): Train loss 5.849, Val loss 5.892\n",
      "Ep 1 (Step 000090): Train loss 5.811, Val loss 5.799\n",
      "Ep 1 (Step 000100): Train loss 5.657, Val loss 5.743\n",
      "Ep 1 (Step 000110): Train loss 5.588, Val loss 5.681\n",
      "Ep 1 (Step 000120): Train loss 5.630, Val loss 5.628\n",
      "Ep 1 (Step 000130): Train loss 5.505, Val loss 5.614\n",
      "Ep 1 (Step 000140): Train loss 5.500, Val loss 5.563\n",
      "Ep 1 (Step 000150): Train loss 5.508, Val loss 5.536\n",
      "Ep 1 (Step 000160): Train loss 5.401, Val loss 5.521\n",
      "Ep 1 (Step 000170): Train loss 5.379, Val loss 5.492\n",
      "Ep 1 (Step 000180): Train loss 5.333, Val loss 5.448\n",
      "Ep 1 (Step 000190): Train loss 5.295, Val loss 5.436\n",
      "Ep 1 (Step 000200): Train loss 5.267, Val loss 5.409\n",
      "Ep 1 (Step 000210): Train loss 5.216, Val loss 5.378\n",
      "Ep 1 (Step 000220): Train loss 5.282, Val loss 5.373\n",
      "Ep 1 (Step 000230): Train loss 5.294, Val loss 5.360\n",
      "Ep 1 (Step 000240): Train loss 5.179, Val loss 5.360\n",
      "Ep 1 (Step 000250): Train loss 5.238, Val loss 5.329\n",
      "Ep 1 (Step 000260): Train loss 5.275, Val loss 5.315\n",
      "Ep 1 (Step 000270): Train loss 5.221, Val loss 5.312\n",
      "Ep 1 (Step 000280): Train loss 5.223, Val loss 5.310\n",
      "Ep 1 (Step 000290): Train loss 5.103, Val loss 5.273\n",
      "Ep 1 (Step 000300): Train loss 5.144, Val loss 5.254\n",
      "Ep 1 (Step 000310): Train loss 5.173, Val loss 5.254\n",
      "Ep 1 (Step 000320): Train loss 5.145, Val loss 5.247\n",
      "Ep 1 (Step 000330): Train loss 5.125, Val loss 5.228\n",
      "Ep 1 (Step 000340): Train loss 5.095, Val loss 5.222\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2224\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.673, Val loss 8.654\n",
      "Ep 1 (Step 000010): Train loss 6.971, Val loss 6.972\n",
      "Ep 1 (Step 000020): Train loss 6.534, Val loss 6.495\n",
      "Ep 1 (Step 000030): Train loss 6.413, Val loss 6.409\n",
      "Ep 1 (Step 000040): Train loss 6.366, Val loss 6.323\n",
      "Ep 1 (Step 000050): Train loss 6.161, Val loss 6.194\n",
      "Ep 1 (Step 000060): Train loss 5.967, Val loss 6.006\n",
      "Ep 1 (Step 000070): Train loss 5.921, Val loss 5.928\n",
      "Ep 1 (Step 000080): Train loss 5.764, Val loss 5.839\n",
      "Ep 1 (Step 000090): Train loss 5.770, Val loss 5.768\n",
      "Ep 1 (Step 000100): Train loss 5.660, Val loss 5.692\n",
      "Ep 1 (Step 000110): Train loss 5.580, Val loss 5.684\n",
      "Ep 1 (Step 000120): Train loss 5.525, Val loss 5.631\n",
      "Ep 1 (Step 000130): Train loss 5.502, Val loss 5.599\n",
      "Ep 1 (Step 000140): Train loss 5.442, Val loss 5.564\n",
      "Ep 1 (Step 000150): Train loss 5.451, Val loss 5.522\n",
      "Ep 1 (Step 000160): Train loss 5.388, Val loss 5.501\n",
      "Ep 1 (Step 000170): Train loss 5.349, Val loss 5.473\n",
      "Ep 1 (Step 000180): Train loss 5.315, Val loss 5.455\n",
      "Ep 1 (Step 000190): Train loss 5.283, Val loss 5.433\n",
      "Ep 1 (Step 000200): Train loss 5.283, Val loss 5.404\n",
      "Ep 1 (Step 000210): Train loss 5.244, Val loss 5.377\n",
      "Ep 1 (Step 000220): Train loss 5.320, Val loss 5.371\n",
      "Ep 1 (Step 000230): Train loss 5.250, Val loss 5.354\n",
      "Ep 1 (Step 000240): Train loss 5.262, Val loss 5.341\n",
      "Ep 1 (Step 000250): Train loss 5.215, Val loss 5.350\n",
      "Ep 1 (Step 000260): Train loss 5.119, Val loss 5.322\n",
      "Ep 1 (Step 000270): Train loss 5.167, Val loss 5.296\n",
      "Ep 1 (Step 000280): Train loss 5.171, Val loss 5.287\n",
      "Ep 1 (Step 000290): Train loss 5.073, Val loss 5.279\n",
      "Ep 1 (Step 000300): Train loss 5.179, Val loss 5.269\n",
      "Ep 1 (Step 000310): Train loss 5.105, Val loss 5.261\n",
      "Ep 1 (Step 000320): Train loss 5.179, Val loss 5.242\n",
      "Ep 1 (Step 000330): Train loss 5.226, Val loss 5.237\n",
      "Ep 1 (Step 000340): Train loss 5.037, Val loss 5.238\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2376\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.654, Val loss 8.658\n",
      "Ep 1 (Step 000010): Train loss 7.025, Val loss 7.003\n",
      "Ep 1 (Step 000020): Train loss 6.500, Val loss 6.486\n",
      "Ep 1 (Step 000030): Train loss 6.406, Val loss 6.396\n",
      "Ep 1 (Step 000040): Train loss 6.237, Val loss 6.262\n",
      "Ep 1 (Step 000050): Train loss 6.057, Val loss 6.092\n",
      "Ep 1 (Step 000060): Train loss 5.996, Val loss 6.013\n",
      "Ep 1 (Step 000070): Train loss 5.858, Val loss 5.896\n",
      "Ep 1 (Step 000080): Train loss 5.816, Val loss 5.822\n",
      "Ep 1 (Step 000090): Train loss 5.797, Val loss 5.795\n",
      "Ep 1 (Step 000100): Train loss 5.574, Val loss 5.698\n",
      "Ep 1 (Step 000110): Train loss 5.557, Val loss 5.651\n",
      "Ep 1 (Step 000120): Train loss 5.593, Val loss 5.647\n",
      "Ep 1 (Step 000130): Train loss 5.445, Val loss 5.583\n",
      "Ep 1 (Step 000140): Train loss 5.473, Val loss 5.565\n",
      "Ep 1 (Step 000150): Train loss 5.547, Val loss 5.523\n",
      "Ep 1 (Step 000160): Train loss 5.422, Val loss 5.490\n",
      "Ep 1 (Step 000170): Train loss 5.296, Val loss 5.459\n",
      "Ep 1 (Step 000180): Train loss 5.324, Val loss 5.430\n",
      "Ep 1 (Step 000190): Train loss 5.335, Val loss 5.415\n",
      "Ep 1 (Step 000200): Train loss 5.342, Val loss 5.395\n",
      "Ep 1 (Step 000210): Train loss 5.301, Val loss 5.380\n",
      "Ep 1 (Step 000220): Train loss 5.238, Val loss 5.359\n",
      "Ep 1 (Step 000230): Train loss 5.256, Val loss 5.333\n",
      "Ep 1 (Step 000240): Train loss 5.270, Val loss 5.316\n",
      "Ep 1 (Step 000250): Train loss 5.224, Val loss 5.313\n",
      "Ep 1 (Step 000260): Train loss 5.240, Val loss 5.295\n",
      "Ep 1 (Step 000270): Train loss 5.175, Val loss 5.308\n",
      "Ep 1 (Step 000280): Train loss 5.154, Val loss 5.303\n",
      "Ep 1 (Step 000290): Train loss 5.257, Val loss 5.297\n",
      "Ep 1 (Step 000300): Train loss 5.196, Val loss 5.277\n",
      "Ep 1 (Step 000310): Train loss 5.125, Val loss 5.261\n",
      "Ep 1 (Step 000320): Train loss 5.145, Val loss 5.236\n",
      "Ep 1 (Step 000330): Train loss 5.072, Val loss 5.227\n",
      "Ep 1 (Step 000340): Train loss 5.084, Val loss 5.223\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2235\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.400, Val loss 8.373\n",
      "Ep 1 (Step 000010): Train loss 6.547, Val loss 6.580\n",
      "Ep 1 (Step 000020): Train loss 6.552, Val loss 6.465\n",
      "Ep 1 (Step 000030): Train loss 6.356, Val loss 6.328\n",
      "Ep 1 (Step 000040): Train loss 6.203, Val loss 6.203\n",
      "Ep 1 (Step 000050): Train loss 5.949, Val loss 6.040\n",
      "Ep 1 (Step 000060): Train loss 5.882, Val loss 5.914\n",
      "Ep 1 (Step 000070): Train loss 5.832, Val loss 5.813\n",
      "Ep 1 (Step 000080): Train loss 5.798, Val loss 5.781\n",
      "Ep 1 (Step 000090): Train loss 5.640, Val loss 5.713\n",
      "Ep 1 (Step 000100): Train loss 5.579, Val loss 5.633\n",
      "Ep 1 (Step 000110): Train loss 5.562, Val loss 5.590\n",
      "Ep 1 (Step 000120): Train loss 5.501, Val loss 5.566\n",
      "Ep 1 (Step 000130): Train loss 5.572, Val loss 5.539\n",
      "Ep 1 (Step 000140): Train loss 5.397, Val loss 5.508\n",
      "Ep 1 (Step 000150): Train loss 5.438, Val loss 5.501\n",
      "Ep 1 (Step 000160): Train loss 5.321, Val loss 5.480\n",
      "Ep 1 (Step 000170): Train loss 5.359, Val loss 5.463\n",
      "Ep 1 (Step 000180): Train loss 5.294, Val loss 5.437\n",
      "Ep 1 (Step 000190): Train loss 5.333, Val loss 5.426\n",
      "Ep 1 (Step 000200): Train loss 5.312, Val loss 5.416\n",
      "Ep 1 (Step 000210): Train loss 5.296, Val loss 5.419\n",
      "Ep 1 (Step 000220): Train loss 5.282, Val loss 5.394\n",
      "Ep 1 (Step 000230): Train loss 5.239, Val loss 5.407\n",
      "Ep 1 (Step 000240): Train loss 5.198, Val loss 5.385\n",
      "Ep 1 (Step 000250): Train loss 5.140, Val loss 5.355\n",
      "Ep 1 (Step 000260): Train loss 5.301, Val loss 5.356\n",
      "Ep 1 (Step 000270): Train loss 5.162, Val loss 5.333\n",
      "Ep 1 (Step 000280): Train loss 5.227, Val loss 5.320\n",
      "Ep 1 (Step 000290): Train loss 5.157, Val loss 5.319\n",
      "Ep 1 (Step 000300): Train loss 5.114, Val loss 5.285\n",
      "Ep 1 (Step 000310): Train loss 5.130, Val loss 5.287\n",
      "Ep 1 (Step 000320): Train loss 5.183, Val loss 5.288\n",
      "Ep 1 (Step 000330): Train loss 5.174, Val loss 5.267\n",
      "Ep 1 (Step 000340): Train loss 5.107, Val loss 5.254\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2540\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.310, Val loss 8.275\n",
      "Ep 1 (Step 000010): Train loss 6.670, Val loss 6.610\n",
      "Ep 1 (Step 000020): Train loss 6.562, Val loss 6.541\n",
      "Ep 1 (Step 000030): Train loss 6.354, Val loss 6.367\n",
      "Ep 1 (Step 000040): Train loss 6.157, Val loss 6.213\n",
      "Ep 1 (Step 000050): Train loss 6.037, Val loss 6.039\n",
      "Ep 1 (Step 000060): Train loss 5.939, Val loss 5.911\n",
      "Ep 1 (Step 000070): Train loss 5.796, Val loss 5.835\n",
      "Ep 1 (Step 000080): Train loss 5.737, Val loss 5.787\n",
      "Ep 1 (Step 000090): Train loss 5.613, Val loss 5.715\n",
      "Ep 1 (Step 000100): Train loss 5.621, Val loss 5.680\n",
      "Ep 1 (Step 000110): Train loss 5.554, Val loss 5.667\n",
      "Ep 1 (Step 000120): Train loss 5.503, Val loss 5.611\n",
      "Ep 1 (Step 000130): Train loss 5.582, Val loss 5.572\n",
      "Ep 1 (Step 000140): Train loss 5.500, Val loss 5.571\n",
      "Ep 1 (Step 000150): Train loss 5.386, Val loss 5.526\n",
      "Ep 1 (Step 000160): Train loss 5.310, Val loss 5.483\n",
      "Ep 1 (Step 000170): Train loss 5.391, Val loss 5.501\n",
      "Ep 1 (Step 000180): Train loss 5.319, Val loss 5.460\n",
      "Ep 1 (Step 000190): Train loss 5.327, Val loss 5.416\n",
      "Ep 1 (Step 000200): Train loss 5.343, Val loss 5.442\n",
      "Ep 1 (Step 000210): Train loss 5.206, Val loss 5.408\n",
      "Ep 1 (Step 000220): Train loss 5.309, Val loss 5.380\n",
      "Ep 1 (Step 000230): Train loss 5.268, Val loss 5.367\n",
      "Ep 1 (Step 000240): Train loss 5.254, Val loss 5.363\n",
      "Ep 1 (Step 000250): Train loss 5.228, Val loss 5.348\n",
      "Ep 1 (Step 000260): Train loss 5.244, Val loss 5.320\n",
      "Ep 1 (Step 000270): Train loss 5.165, Val loss 5.304\n",
      "Ep 1 (Step 000280): Train loss 5.163, Val loss 5.338\n",
      "Ep 1 (Step 000290): Train loss 5.151, Val loss 5.290\n",
      "Ep 1 (Step 000300): Train loss 5.152, Val loss 5.296\n",
      "Ep 1 (Step 000310): Train loss 5.218, Val loss 5.283\n",
      "Ep 1 (Step 000320): Train loss 5.157, Val loss 5.282\n",
      "Ep 1 (Step 000330): Train loss 5.136, Val loss 5.230\n",
      "Ep 1 (Step 000340): Train loss 5.086, Val loss 5.220\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2205\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.394, Val loss 8.324\n",
      "Ep 1 (Step 000010): Train loss 6.632, Val loss 6.619\n",
      "Ep 1 (Step 000020): Train loss 6.551, Val loss 6.528\n",
      "Ep 1 (Step 000030): Train loss 6.399, Val loss 6.374\n",
      "Ep 1 (Step 000040): Train loss 6.142, Val loss 6.266\n",
      "Ep 1 (Step 000050): Train loss 6.029, Val loss 6.060\n",
      "Ep 1 (Step 000060): Train loss 5.964, Val loss 5.976\n",
      "Ep 1 (Step 000070): Train loss 5.876, Val loss 5.858\n",
      "Ep 1 (Step 000080): Train loss 5.697, Val loss 5.796\n",
      "Ep 1 (Step 000090): Train loss 5.652, Val loss 5.703\n",
      "Ep 1 (Step 000100): Train loss 5.713, Val loss 5.668\n",
      "Ep 1 (Step 000110): Train loss 5.666, Val loss 5.635\n",
      "Ep 1 (Step 000120): Train loss 5.491, Val loss 5.572\n",
      "Ep 1 (Step 000130): Train loss 5.451, Val loss 5.538\n",
      "Ep 1 (Step 000140): Train loss 5.269, Val loss 5.501\n",
      "Ep 1 (Step 000150): Train loss 5.350, Val loss 5.468\n",
      "Ep 1 (Step 000160): Train loss 5.397, Val loss 5.472\n",
      "Ep 1 (Step 000170): Train loss 5.377, Val loss 5.456\n",
      "Ep 1 (Step 000180): Train loss 5.346, Val loss 5.422\n",
      "Ep 1 (Step 000190): Train loss 5.399, Val loss 5.431\n",
      "Ep 1 (Step 000200): Train loss 5.251, Val loss 5.412\n",
      "Ep 1 (Step 000210): Train loss 5.347, Val loss 5.399\n",
      "Ep 1 (Step 000220): Train loss 5.281, Val loss 5.376\n",
      "Ep 1 (Step 000230): Train loss 5.308, Val loss 5.380\n",
      "Ep 1 (Step 000240): Train loss 5.219, Val loss 5.366\n",
      "Ep 1 (Step 000250): Train loss 5.202, Val loss 5.359\n",
      "Ep 1 (Step 000260): Train loss 5.185, Val loss 5.334\n",
      "Ep 1 (Step 000270): Train loss 5.162, Val loss 5.343\n",
      "Ep 1 (Step 000280): Train loss 5.216, Val loss 5.315\n",
      "Ep 1 (Step 000290): Train loss 5.137, Val loss 5.297\n",
      "Ep 1 (Step 000300): Train loss 5.062, Val loss 5.290\n",
      "Ep 1 (Step 000310): Train loss 5.104, Val loss 5.283\n",
      "Ep 1 (Step 000320): Train loss 5.188, Val loss 5.252\n",
      "Ep 1 (Step 000330): Train loss 5.119, Val loss 5.252\n",
      "Ep 1 (Step 000340): Train loss 5.165, Val loss 5.302\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.3024\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.325, Val loss 8.327\n",
      "Ep 1 (Step 000010): Train loss 6.588, Val loss 6.553\n",
      "Ep 1 (Step 000020): Train loss 6.484, Val loss 6.466\n",
      "Ep 1 (Step 000030): Train loss 6.347, Val loss 6.360\n",
      "Ep 1 (Step 000040): Train loss 6.280, Val loss 6.265\n",
      "Ep 1 (Step 000050): Train loss 6.030, Val loss 6.024\n",
      "Ep 1 (Step 000060): Train loss 5.908, Val loss 5.965\n",
      "Ep 1 (Step 000070): Train loss 5.797, Val loss 5.843\n",
      "Ep 1 (Step 000080): Train loss 5.683, Val loss 5.765\n",
      "Ep 1 (Step 000090): Train loss 5.641, Val loss 5.698\n",
      "Ep 1 (Step 000100): Train loss 5.595, Val loss 5.662\n",
      "Ep 1 (Step 000110): Train loss 5.546, Val loss 5.599\n",
      "Ep 1 (Step 000120): Train loss 5.589, Val loss 5.561\n",
      "Ep 1 (Step 000130): Train loss 5.512, Val loss 5.541\n",
      "Ep 1 (Step 000140): Train loss 5.461, Val loss 5.527\n",
      "Ep 1 (Step 000150): Train loss 5.409, Val loss 5.500\n",
      "Ep 1 (Step 000160): Train loss 5.325, Val loss 5.457\n",
      "Ep 1 (Step 000170): Train loss 5.287, Val loss 5.423\n",
      "Ep 1 (Step 000180): Train loss 5.254, Val loss 5.412\n",
      "Ep 1 (Step 000190): Train loss 5.231, Val loss 5.399\n",
      "Ep 1 (Step 000200): Train loss 5.238, Val loss 5.384\n",
      "Ep 1 (Step 000210): Train loss 5.152, Val loss 5.381\n",
      "Ep 1 (Step 000220): Train loss 5.178, Val loss 5.341\n",
      "Ep 1 (Step 000230): Train loss 5.240, Val loss 5.349\n",
      "Ep 1 (Step 000240): Train loss 5.203, Val loss 5.326\n",
      "Ep 1 (Step 000250): Train loss 5.207, Val loss 5.314\n",
      "Ep 1 (Step 000260): Train loss 5.147, Val loss 5.303\n",
      "Ep 1 (Step 000270): Train loss 5.088, Val loss 5.280\n",
      "Ep 1 (Step 000280): Train loss 5.098, Val loss 5.278\n",
      "Ep 1 (Step 000290): Train loss 5.119, Val loss 5.289\n",
      "Ep 1 (Step 000300): Train loss 5.180, Val loss 5.271\n",
      "Ep 1 (Step 000310): Train loss 5.122, Val loss 5.237\n",
      "Ep 1 (Step 000320): Train loss 5.088, Val loss 5.249\n",
      "Ep 1 (Step 000330): Train loss 5.106, Val loss 5.235\n",
      "Ep 1 (Step 000340): Train loss 5.058, Val loss 5.204\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2038\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.365, Val loss 8.331\n",
      "Ep 1 (Step 000010): Train loss 6.686, Val loss 6.584\n",
      "Ep 1 (Step 000020): Train loss 6.441, Val loss 6.509\n",
      "Ep 1 (Step 000030): Train loss 6.345, Val loss 6.380\n",
      "Ep 1 (Step 000040): Train loss 6.248, Val loss 6.170\n",
      "Ep 1 (Step 000050): Train loss 6.095, Val loss 6.030\n",
      "Ep 1 (Step 000060): Train loss 5.779, Val loss 5.887\n",
      "Ep 1 (Step 000070): Train loss 5.821, Val loss 5.824\n",
      "Ep 1 (Step 000080): Train loss 5.676, Val loss 5.732\n",
      "Ep 1 (Step 000090): Train loss 5.577, Val loss 5.683\n",
      "Ep 1 (Step 000100): Train loss 5.577, Val loss 5.627\n",
      "Ep 1 (Step 000110): Train loss 5.546, Val loss 5.573\n",
      "Ep 1 (Step 000120): Train loss 5.400, Val loss 5.537\n",
      "Ep 1 (Step 000130): Train loss 5.368, Val loss 5.528\n",
      "Ep 1 (Step 000140): Train loss 5.416, Val loss 5.487\n",
      "Ep 1 (Step 000150): Train loss 5.413, Val loss 5.508\n",
      "Ep 1 (Step 000160): Train loss 5.336, Val loss 5.453\n",
      "Ep 1 (Step 000170): Train loss 5.313, Val loss 5.426\n",
      "Ep 1 (Step 000180): Train loss 5.308, Val loss 5.415\n",
      "Ep 1 (Step 000190): Train loss 5.235, Val loss 5.409\n",
      "Ep 1 (Step 000200): Train loss 5.269, Val loss 5.368\n",
      "Ep 1 (Step 000210): Train loss 5.226, Val loss 5.379\n",
      "Ep 1 (Step 000220): Train loss 5.253, Val loss 5.348\n",
      "Ep 1 (Step 000230): Train loss 5.176, Val loss 5.329\n",
      "Ep 1 (Step 000240): Train loss 5.185, Val loss 5.312\n",
      "Ep 1 (Step 000250): Train loss 5.147, Val loss 5.293\n",
      "Ep 1 (Step 000260): Train loss 5.127, Val loss 5.299\n",
      "Ep 1 (Step 000270): Train loss 5.200, Val loss 5.303\n",
      "Ep 1 (Step 000280): Train loss 5.136, Val loss 5.294\n",
      "Ep 1 (Step 000290): Train loss 5.095, Val loss 5.275\n",
      "Ep 1 (Step 000300): Train loss 5.079, Val loss 5.252\n",
      "Ep 1 (Step 000310): Train loss 5.107, Val loss 5.289\n",
      "Ep 1 (Step 000320): Train loss 5.080, Val loss 5.252\n",
      "Ep 1 (Step 000330): Train loss 5.003, Val loss 5.227\n",
      "Ep 1 (Step 000340): Train loss 5.097, Val loss 5.225\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2254\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.360, Val loss 8.331\n",
      "Ep 1 (Step 000010): Train loss 6.579, Val loss 6.539\n",
      "Ep 1 (Step 000020): Train loss 6.494, Val loss 6.492\n",
      "Ep 1 (Step 000030): Train loss 6.440, Val loss 6.348\n",
      "Ep 1 (Step 000040): Train loss 6.257, Val loss 6.148\n",
      "Ep 1 (Step 000050): Train loss 6.067, Val loss 6.039\n",
      "Ep 1 (Step 000060): Train loss 5.903, Val loss 5.936\n",
      "Ep 1 (Step 000070): Train loss 5.868, Val loss 5.831\n",
      "Ep 1 (Step 000080): Train loss 5.727, Val loss 5.770\n",
      "Ep 1 (Step 000090): Train loss 5.609, Val loss 5.675\n",
      "Ep 1 (Step 000100): Train loss 5.591, Val loss 5.619\n",
      "Ep 1 (Step 000110): Train loss 5.553, Val loss 5.606\n",
      "Ep 1 (Step 000120): Train loss 5.547, Val loss 5.565\n",
      "Ep 1 (Step 000130): Train loss 5.480, Val loss 5.567\n",
      "Ep 1 (Step 000140): Train loss 5.474, Val loss 5.515\n",
      "Ep 1 (Step 000150): Train loss 5.393, Val loss 5.530\n",
      "Ep 1 (Step 000160): Train loss 5.371, Val loss 5.460\n",
      "Ep 1 (Step 000170): Train loss 5.371, Val loss 5.455\n",
      "Ep 1 (Step 000180): Train loss 5.286, Val loss 5.421\n",
      "Ep 1 (Step 000190): Train loss 5.318, Val loss 5.377\n",
      "Ep 1 (Step 000200): Train loss 5.309, Val loss 5.392\n",
      "Ep 1 (Step 000210): Train loss 5.225, Val loss 5.374\n",
      "Ep 1 (Step 000220): Train loss 5.199, Val loss 5.374\n",
      "Ep 1 (Step 000230): Train loss 5.247, Val loss 5.371\n",
      "Ep 1 (Step 000240): Train loss 5.264, Val loss 5.362\n",
      "Ep 1 (Step 000250): Train loss 5.146, Val loss 5.332\n",
      "Ep 1 (Step 000260): Train loss 5.225, Val loss 5.320\n",
      "Ep 1 (Step 000270): Train loss 5.090, Val loss 5.288\n",
      "Ep 1 (Step 000280): Train loss 5.122, Val loss 5.282\n",
      "Ep 1 (Step 000290): Train loss 5.191, Val loss 5.274\n",
      "Ep 1 (Step 000300): Train loss 5.228, Val loss 5.297\n",
      "Ep 1 (Step 000310): Train loss 5.078, Val loss 5.282\n",
      "Ep 1 (Step 000320): Train loss 5.069, Val loss 5.255\n",
      "Ep 1 (Step 000330): Train loss 5.163, Val loss 5.246\n",
      "Ep 1 (Step 000340): Train loss 5.082, Val loss 5.242\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2417\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.662, Val loss 8.660\n",
      "Ep 1 (Step 000010): Train loss 7.025, Val loss 6.969\n",
      "Ep 1 (Step 000020): Train loss 6.527, Val loss 6.483\n",
      "Ep 1 (Step 000030): Train loss 6.435, Val loss 6.378\n",
      "Ep 1 (Step 000040): Train loss 6.335, Val loss 6.283\n",
      "Ep 1 (Step 000050): Train loss 6.169, Val loss 6.171\n",
      "Ep 1 (Step 000060): Train loss 5.952, Val loss 6.025\n",
      "Ep 1 (Step 000070): Train loss 5.820, Val loss 5.895\n",
      "Ep 1 (Step 000080): Train loss 5.778, Val loss 5.831\n",
      "Ep 1 (Step 000090): Train loss 5.640, Val loss 5.786\n",
      "Ep 1 (Step 000100): Train loss 5.681, Val loss 5.735\n",
      "Ep 1 (Step 000110): Train loss 5.619, Val loss 5.672\n",
      "Ep 1 (Step 000120): Train loss 5.523, Val loss 5.632\n",
      "Ep 1 (Step 000130): Train loss 5.465, Val loss 5.579\n",
      "Ep 1 (Step 000140): Train loss 5.544, Val loss 5.548\n",
      "Ep 1 (Step 000150): Train loss 5.418, Val loss 5.526\n",
      "Ep 1 (Step 000160): Train loss 5.410, Val loss 5.508\n",
      "Ep 1 (Step 000170): Train loss 5.410, Val loss 5.472\n",
      "Ep 1 (Step 000180): Train loss 5.375, Val loss 5.463\n",
      "Ep 1 (Step 000190): Train loss 5.376, Val loss 5.439\n",
      "Ep 1 (Step 000200): Train loss 5.368, Val loss 5.426\n",
      "Ep 1 (Step 000210): Train loss 5.313, Val loss 5.397\n",
      "Ep 1 (Step 000220): Train loss 5.360, Val loss 5.378\n",
      "Ep 1 (Step 000230): Train loss 5.213, Val loss 5.358\n",
      "Ep 1 (Step 000240): Train loss 5.209, Val loss 5.352\n",
      "Ep 1 (Step 000250): Train loss 5.203, Val loss 5.332\n",
      "Ep 1 (Step 000260): Train loss 5.248, Val loss 5.330\n",
      "Ep 1 (Step 000270): Train loss 5.213, Val loss 5.328\n",
      "Ep 1 (Step 000280): Train loss 5.214, Val loss 5.328\n",
      "Ep 1 (Step 000290): Train loss 5.216, Val loss 5.308\n",
      "Ep 1 (Step 000300): Train loss 5.126, Val loss 5.296\n",
      "Ep 1 (Step 000310): Train loss 5.132, Val loss 5.298\n",
      "Ep 1 (Step 000320): Train loss 5.065, Val loss 5.278\n",
      "Ep 1 (Step 000330): Train loss 5.193, Val loss 5.268\n",
      "Ep 1 (Step 000340): Train loss 5.086, Val loss 5.259\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2588\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.724, Val loss 8.721\n",
      "Ep 1 (Step 000010): Train loss 7.017, Val loss 6.986\n",
      "Ep 1 (Step 000020): Train loss 6.541, Val loss 6.494\n",
      "Ep 1 (Step 000030): Train loss 6.429, Val loss 6.416\n",
      "Ep 1 (Step 000040): Train loss 6.404, Val loss 6.356\n",
      "Ep 1 (Step 000050): Train loss 6.154, Val loss 6.187\n",
      "Ep 1 (Step 000060): Train loss 6.027, Val loss 6.098\n",
      "Ep 1 (Step 000070): Train loss 5.869, Val loss 5.950\n",
      "Ep 1 (Step 000080): Train loss 5.896, Val loss 5.860\n",
      "Ep 1 (Step 000090): Train loss 5.778, Val loss 5.796\n",
      "Ep 1 (Step 000100): Train loss 5.604, Val loss 5.737\n",
      "Ep 1 (Step 000110): Train loss 5.601, Val loss 5.672\n",
      "Ep 1 (Step 000120): Train loss 5.553, Val loss 5.655\n",
      "Ep 1 (Step 000130): Train loss 5.548, Val loss 5.604\n",
      "Ep 1 (Step 000140): Train loss 5.540, Val loss 5.582\n",
      "Ep 1 (Step 000150): Train loss 5.541, Val loss 5.540\n",
      "Ep 1 (Step 000160): Train loss 5.420, Val loss 5.515\n",
      "Ep 1 (Step 000170): Train loss 5.390, Val loss 5.491\n",
      "Ep 1 (Step 000180): Train loss 5.475, Val loss 5.473\n",
      "Ep 1 (Step 000190): Train loss 5.365, Val loss 5.457\n",
      "Ep 1 (Step 000200): Train loss 5.330, Val loss 5.450\n",
      "Ep 1 (Step 000210): Train loss 5.384, Val loss 5.409\n",
      "Ep 1 (Step 000220): Train loss 5.275, Val loss 5.395\n",
      "Ep 1 (Step 000230): Train loss 5.252, Val loss 5.382\n",
      "Ep 1 (Step 000240): Train loss 5.264, Val loss 5.370\n",
      "Ep 1 (Step 000250): Train loss 5.274, Val loss 5.351\n",
      "Ep 1 (Step 000260): Train loss 5.232, Val loss 5.350\n",
      "Ep 1 (Step 000270): Train loss 5.184, Val loss 5.340\n",
      "Ep 1 (Step 000280): Train loss 5.173, Val loss 5.329\n",
      "Ep 1 (Step 000290): Train loss 5.178, Val loss 5.301\n",
      "Ep 1 (Step 000300): Train loss 5.165, Val loss 5.317\n",
      "Ep 1 (Step 000310): Train loss 5.131, Val loss 5.291\n",
      "Ep 1 (Step 000320): Train loss 5.123, Val loss 5.276\n",
      "Ep 1 (Step 000330): Train loss 5.149, Val loss 5.271\n",
      "Ep 1 (Step 000340): Train loss 5.085, Val loss 5.273\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2734\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.670, Val loss 8.625\n",
      "Ep 1 (Step 000010): Train loss 6.994, Val loss 7.001\n",
      "Ep 1 (Step 000020): Train loss 6.519, Val loss 6.474\n",
      "Ep 1 (Step 000030): Train loss 6.404, Val loss 6.375\n",
      "Ep 1 (Step 000040): Train loss 6.235, Val loss 6.231\n",
      "Ep 1 (Step 000050): Train loss 6.124, Val loss 6.141\n",
      "Ep 1 (Step 000060): Train loss 6.032, Val loss 5.999\n",
      "Ep 1 (Step 000070): Train loss 5.979, Val loss 5.924\n",
      "Ep 1 (Step 000080): Train loss 5.766, Val loss 5.833\n",
      "Ep 1 (Step 000090): Train loss 5.791, Val loss 5.761\n",
      "Ep 1 (Step 000100): Train loss 5.707, Val loss 5.711\n",
      "Ep 1 (Step 000110): Train loss 5.625, Val loss 5.660\n",
      "Ep 1 (Step 000120): Train loss 5.574, Val loss 5.622\n",
      "Ep 1 (Step 000130): Train loss 5.489, Val loss 5.574\n",
      "Ep 1 (Step 000140): Train loss 5.464, Val loss 5.544\n",
      "Ep 1 (Step 000150): Train loss 5.393, Val loss 5.524\n",
      "Ep 1 (Step 000160): Train loss 5.380, Val loss 5.489\n",
      "Ep 1 (Step 000170): Train loss 5.339, Val loss 5.489\n",
      "Ep 1 (Step 000180): Train loss 5.346, Val loss 5.460\n",
      "Ep 1 (Step 000190): Train loss 5.350, Val loss 5.417\n",
      "Ep 1 (Step 000200): Train loss 5.346, Val loss 5.392\n",
      "Ep 1 (Step 000210): Train loss 5.321, Val loss 5.386\n",
      "Ep 1 (Step 000220): Train loss 5.223, Val loss 5.379\n",
      "Ep 1 (Step 000230): Train loss 5.263, Val loss 5.360\n",
      "Ep 1 (Step 000240): Train loss 5.267, Val loss 5.365\n",
      "Ep 1 (Step 000250): Train loss 5.259, Val loss 5.335\n",
      "Ep 1 (Step 000260): Train loss 5.292, Val loss 5.326\n",
      "Ep 1 (Step 000270): Train loss 5.183, Val loss 5.304\n",
      "Ep 1 (Step 000280): Train loss 5.154, Val loss 5.293\n",
      "Ep 1 (Step 000290): Train loss 5.247, Val loss 5.285\n",
      "Ep 1 (Step 000300): Train loss 5.192, Val loss 5.271\n",
      "Ep 1 (Step 000310): Train loss 5.186, Val loss 5.289\n",
      "Ep 1 (Step 000320): Train loss 5.142, Val loss 5.271\n",
      "Ep 1 (Step 000330): Train loss 5.219, Val loss 5.263\n",
      "Ep 1 (Step 000340): Train loss 5.158, Val loss 5.263\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2626\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.667, Val loss 8.627\n",
      "Ep 1 (Step 000010): Train loss 7.051, Val loss 6.956\n",
      "Ep 1 (Step 000020): Train loss 6.446, Val loss 6.447\n",
      "Ep 1 (Step 000030): Train loss 6.448, Val loss 6.382\n",
      "Ep 1 (Step 000040): Train loss 6.297, Val loss 6.317\n",
      "Ep 1 (Step 000050): Train loss 6.198, Val loss 6.181\n",
      "Ep 1 (Step 000060): Train loss 6.063, Val loss 6.042\n",
      "Ep 1 (Step 000070): Train loss 5.918, Val loss 5.953\n",
      "Ep 1 (Step 000080): Train loss 5.794, Val loss 5.842\n",
      "Ep 1 (Step 000090): Train loss 5.720, Val loss 5.791\n",
      "Ep 1 (Step 000100): Train loss 5.654, Val loss 5.737\n",
      "Ep 1 (Step 000110): Train loss 5.622, Val loss 5.665\n",
      "Ep 1 (Step 000120): Train loss 5.595, Val loss 5.629\n",
      "Ep 1 (Step 000130): Train loss 5.504, Val loss 5.589\n",
      "Ep 1 (Step 000140): Train loss 5.477, Val loss 5.540\n",
      "Ep 1 (Step 000150): Train loss 5.441, Val loss 5.515\n",
      "Ep 1 (Step 000160): Train loss 5.441, Val loss 5.487\n",
      "Ep 1 (Step 000170): Train loss 5.309, Val loss 5.461\n",
      "Ep 1 (Step 000180): Train loss 5.365, Val loss 5.442\n",
      "Ep 1 (Step 000190): Train loss 5.381, Val loss 5.431\n",
      "Ep 1 (Step 000200): Train loss 5.293, Val loss 5.413\n",
      "Ep 1 (Step 000210): Train loss 5.363, Val loss 5.394\n",
      "Ep 1 (Step 000220): Train loss 5.326, Val loss 5.382\n",
      "Ep 1 (Step 000230): Train loss 5.256, Val loss 5.364\n",
      "Ep 1 (Step 000240): Train loss 5.271, Val loss 5.335\n",
      "Ep 1 (Step 000250): Train loss 5.229, Val loss 5.342\n",
      "Ep 1 (Step 000260): Train loss 5.189, Val loss 5.311\n",
      "Ep 1 (Step 000270): Train loss 5.098, Val loss 5.317\n",
      "Ep 1 (Step 000280): Train loss 5.164, Val loss 5.284\n",
      "Ep 1 (Step 000290): Train loss 5.200, Val loss 5.278\n",
      "Ep 1 (Step 000300): Train loss 5.089, Val loss 5.263\n",
      "Ep 1 (Step 000310): Train loss 5.144, Val loss 5.252\n",
      "Ep 1 (Step 000320): Train loss 5.184, Val loss 5.244\n",
      "Ep 1 (Step 000330): Train loss 5.123, Val loss 5.231\n",
      "Ep 1 (Step 000340): Train loss 5.075, Val loss 5.233\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2330\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.686, Val loss 8.687\n",
      "Ep 1 (Step 000010): Train loss 7.087, Val loss 7.011\n",
      "Ep 1 (Step 000020): Train loss 6.490, Val loss 6.487\n",
      "Ep 1 (Step 000030): Train loss 6.395, Val loss 6.416\n",
      "Ep 1 (Step 000040): Train loss 6.289, Val loss 6.335\n",
      "Ep 1 (Step 000050): Train loss 6.154, Val loss 6.237\n",
      "Ep 1 (Step 000060): Train loss 6.134, Val loss 6.047\n",
      "Ep 1 (Step 000070): Train loss 5.919, Val loss 5.953\n",
      "Ep 1 (Step 000080): Train loss 5.807, Val loss 5.892\n",
      "Ep 1 (Step 000090): Train loss 5.694, Val loss 5.790\n",
      "Ep 1 (Step 000100): Train loss 5.706, Val loss 5.742\n",
      "Ep 1 (Step 000110): Train loss 5.690, Val loss 5.675\n",
      "Ep 1 (Step 000120): Train loss 5.597, Val loss 5.630\n",
      "Ep 1 (Step 000130): Train loss 5.577, Val loss 5.590\n",
      "Ep 1 (Step 000140): Train loss 5.479, Val loss 5.557\n",
      "Ep 1 (Step 000150): Train loss 5.378, Val loss 5.531\n",
      "Ep 1 (Step 000160): Train loss 5.511, Val loss 5.533\n",
      "Ep 1 (Step 000170): Train loss 5.426, Val loss 5.502\n",
      "Ep 1 (Step 000180): Train loss 5.362, Val loss 5.467\n",
      "Ep 1 (Step 000190): Train loss 5.403, Val loss 5.448\n",
      "Ep 1 (Step 000200): Train loss 5.317, Val loss 5.440\n",
      "Ep 1 (Step 000210): Train loss 5.294, Val loss 5.408\n",
      "Ep 1 (Step 000220): Train loss 5.234, Val loss 5.399\n",
      "Ep 1 (Step 000230): Train loss 5.223, Val loss 5.369\n",
      "Ep 1 (Step 000240): Train loss 5.240, Val loss 5.389\n",
      "Ep 1 (Step 000250): Train loss 5.267, Val loss 5.334\n",
      "Ep 1 (Step 000260): Train loss 5.165, Val loss 5.334\n",
      "Ep 1 (Step 000270): Train loss 5.288, Val loss 5.323\n",
      "Ep 1 (Step 000280): Train loss 5.219, Val loss 5.302\n",
      "Ep 1 (Step 000290): Train loss 5.175, Val loss 5.282\n",
      "Ep 1 (Step 000300): Train loss 5.147, Val loss 5.280\n",
      "Ep 1 (Step 000310): Train loss 5.190, Val loss 5.253\n",
      "Ep 1 (Step 000320): Train loss 5.160, Val loss 5.257\n",
      "Ep 1 (Step 000330): Train loss 5.014, Val loss 5.248\n",
      "Ep 1 (Step 000340): Train loss 5.090, Val loss 5.226\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2255\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.747, Val loss 8.718\n",
      "Ep 1 (Step 000010): Train loss 7.010, Val loss 7.001\n",
      "Ep 1 (Step 000020): Train loss 6.525, Val loss 6.480\n",
      "Ep 1 (Step 000030): Train loss 6.320, Val loss 6.393\n",
      "Ep 1 (Step 000040): Train loss 6.339, Val loss 6.321\n",
      "Ep 1 (Step 000050): Train loss 6.134, Val loss 6.190\n",
      "Ep 1 (Step 000060): Train loss 6.049, Val loss 6.004\n",
      "Ep 1 (Step 000070): Train loss 5.924, Val loss 5.900\n",
      "Ep 1 (Step 000080): Train loss 5.894, Val loss 5.849\n",
      "Ep 1 (Step 000090): Train loss 5.739, Val loss 5.777\n",
      "Ep 1 (Step 000100): Train loss 5.622, Val loss 5.730\n",
      "Ep 1 (Step 000110): Train loss 5.528, Val loss 5.685\n",
      "Ep 1 (Step 000120): Train loss 5.496, Val loss 5.624\n",
      "Ep 1 (Step 000130): Train loss 5.593, Val loss 5.587\n",
      "Ep 1 (Step 000140): Train loss 5.408, Val loss 5.536\n",
      "Ep 1 (Step 000150): Train loss 5.470, Val loss 5.542\n",
      "Ep 1 (Step 000160): Train loss 5.378, Val loss 5.484\n",
      "Ep 1 (Step 000170): Train loss 5.404, Val loss 5.469\n",
      "Ep 1 (Step 000180): Train loss 5.364, Val loss 5.452\n",
      "Ep 1 (Step 000190): Train loss 5.350, Val loss 5.419\n",
      "Ep 1 (Step 000200): Train loss 5.368, Val loss 5.401\n",
      "Ep 1 (Step 000210): Train loss 5.320, Val loss 5.402\n",
      "Ep 1 (Step 000220): Train loss 5.230, Val loss 5.372\n",
      "Ep 1 (Step 000230): Train loss 5.245, Val loss 5.360\n",
      "Ep 1 (Step 000240): Train loss 5.263, Val loss 5.338\n",
      "Ep 1 (Step 000250): Train loss 5.254, Val loss 5.325\n",
      "Ep 1 (Step 000260): Train loss 5.242, Val loss 5.310\n",
      "Ep 1 (Step 000270): Train loss 5.181, Val loss 5.314\n",
      "Ep 1 (Step 000280): Train loss 5.263, Val loss 5.286\n",
      "Ep 1 (Step 000290): Train loss 5.209, Val loss 5.270\n",
      "Ep 1 (Step 000300): Train loss 5.188, Val loss 5.273\n",
      "Ep 1 (Step 000310): Train loss 5.183, Val loss 5.262\n",
      "Ep 1 (Step 000320): Train loss 5.107, Val loss 5.246\n",
      "Ep 1 (Step 000330): Train loss 5.128, Val loss 5.231\n",
      "Ep 1 (Step 000340): Train loss 5.112, Val loss 5.220\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2196\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.328, Val loss 8.304\n",
      "Ep 1 (Step 000010): Train loss 6.577, Val loss 6.582\n",
      "Ep 1 (Step 000020): Train loss 6.566, Val loss 6.503\n",
      "Ep 1 (Step 000030): Train loss 6.393, Val loss 6.299\n",
      "Ep 1 (Step 000040): Train loss 6.135, Val loss 6.177\n",
      "Ep 1 (Step 000050): Train loss 6.051, Val loss 6.055\n",
      "Ep 1 (Step 000060): Train loss 5.843, Val loss 5.936\n",
      "Ep 1 (Step 000070): Train loss 5.787, Val loss 5.874\n",
      "Ep 1 (Step 000080): Train loss 5.698, Val loss 5.762\n",
      "Ep 1 (Step 000090): Train loss 5.681, Val loss 5.736\n",
      "Ep 1 (Step 000100): Train loss 5.534, Val loss 5.667\n",
      "Ep 1 (Step 000110): Train loss 5.591, Val loss 5.594\n",
      "Ep 1 (Step 000120): Train loss 5.531, Val loss 5.565\n",
      "Ep 1 (Step 000130): Train loss 5.660, Val loss 5.531\n",
      "Ep 1 (Step 000140): Train loss 5.454, Val loss 5.525\n",
      "Ep 1 (Step 000150): Train loss 5.397, Val loss 5.508\n",
      "Ep 1 (Step 000160): Train loss 5.411, Val loss 5.485\n",
      "Ep 1 (Step 000170): Train loss 5.334, Val loss 5.453\n",
      "Ep 1 (Step 000180): Train loss 5.331, Val loss 5.442\n",
      "Ep 1 (Step 000190): Train loss 5.295, Val loss 5.421\n",
      "Ep 1 (Step 000200): Train loss 5.318, Val loss 5.425\n",
      "Ep 1 (Step 000210): Train loss 5.294, Val loss 5.399\n",
      "Ep 1 (Step 000220): Train loss 5.212, Val loss 5.383\n",
      "Ep 1 (Step 000230): Train loss 5.251, Val loss 5.385\n",
      "Ep 1 (Step 000240): Train loss 5.216, Val loss 5.365\n",
      "Ep 1 (Step 000250): Train loss 5.187, Val loss 5.359\n",
      "Ep 1 (Step 000260): Train loss 5.266, Val loss 5.349\n",
      "Ep 1 (Step 000270): Train loss 5.150, Val loss 5.327\n",
      "Ep 1 (Step 000280): Train loss 5.180, Val loss 5.326\n",
      "Ep 1 (Step 000290): Train loss 5.212, Val loss 5.311\n",
      "Ep 1 (Step 000300): Train loss 5.163, Val loss 5.294\n",
      "Ep 1 (Step 000310): Train loss 5.151, Val loss 5.317\n",
      "Ep 1 (Step 000320): Train loss 5.120, Val loss 5.301\n",
      "Ep 1 (Step 000330): Train loss 5.075, Val loss 5.275\n",
      "Ep 1 (Step 000340): Train loss 5.075, Val loss 5.254\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2536\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.361, Val loss 8.316\n",
      "Ep 1 (Step 000010): Train loss 6.556, Val loss 6.580\n",
      "Ep 1 (Step 000020): Train loss 6.479, Val loss 6.502\n",
      "Ep 1 (Step 000030): Train loss 6.340, Val loss 6.333\n",
      "Ep 1 (Step 000040): Train loss 6.090, Val loss 6.204\n",
      "Ep 1 (Step 000050): Train loss 5.993, Val loss 6.003\n",
      "Ep 1 (Step 000060): Train loss 5.869, Val loss 5.895\n",
      "Ep 1 (Step 000070): Train loss 5.785, Val loss 5.792\n",
      "Ep 1 (Step 000080): Train loss 5.649, Val loss 5.709\n",
      "Ep 1 (Step 000090): Train loss 5.647, Val loss 5.664\n",
      "Ep 1 (Step 000100): Train loss 5.615, Val loss 5.596\n",
      "Ep 1 (Step 000110): Train loss 5.512, Val loss 5.573\n",
      "Ep 1 (Step 000120): Train loss 5.528, Val loss 5.540\n",
      "Ep 1 (Step 000130): Train loss 5.465, Val loss 5.521\n",
      "Ep 1 (Step 000140): Train loss 5.401, Val loss 5.478\n",
      "Ep 1 (Step 000150): Train loss 5.407, Val loss 5.459\n",
      "Ep 1 (Step 000160): Train loss 5.372, Val loss 5.434\n",
      "Ep 1 (Step 000170): Train loss 5.330, Val loss 5.416\n",
      "Ep 1 (Step 000180): Train loss 5.411, Val loss 5.405\n",
      "Ep 1 (Step 000190): Train loss 5.230, Val loss 5.390\n",
      "Ep 1 (Step 000200): Train loss 5.243, Val loss 5.390\n",
      "Ep 1 (Step 000210): Train loss 5.226, Val loss 5.369\n",
      "Ep 1 (Step 000220): Train loss 5.129, Val loss 5.356\n",
      "Ep 1 (Step 000230): Train loss 5.215, Val loss 5.341\n",
      "Ep 1 (Step 000240): Train loss 5.286, Val loss 5.331\n",
      "Ep 1 (Step 000250): Train loss 5.146, Val loss 5.325\n",
      "Ep 1 (Step 000260): Train loss 5.188, Val loss 5.303\n",
      "Ep 1 (Step 000270): Train loss 5.208, Val loss 5.279\n",
      "Ep 1 (Step 000280): Train loss 5.197, Val loss 5.287\n",
      "Ep 1 (Step 000290): Train loss 5.194, Val loss 5.285\n",
      "Ep 1 (Step 000300): Train loss 5.071, Val loss 5.268\n",
      "Ep 1 (Step 000310): Train loss 5.158, Val loss 5.267\n",
      "Ep 1 (Step 000320): Train loss 5.126, Val loss 5.272\n",
      "Ep 1 (Step 000330): Train loss 5.045, Val loss 5.267\n",
      "Ep 1 (Step 000340): Train loss 5.094, Val loss 5.238\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2377\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.367, Val loss 8.343\n",
      "Ep 1 (Step 000010): Train loss 6.572, Val loss 6.584\n",
      "Ep 1 (Step 000020): Train loss 6.525, Val loss 6.518\n",
      "Ep 1 (Step 000030): Train loss 6.381, Val loss 6.370\n",
      "Ep 1 (Step 000040): Train loss 6.137, Val loss 6.229\n",
      "Ep 1 (Step 000050): Train loss 6.066, Val loss 6.064\n",
      "Ep 1 (Step 000060): Train loss 5.904, Val loss 5.942\n",
      "Ep 1 (Step 000070): Train loss 5.796, Val loss 5.832\n",
      "Ep 1 (Step 000080): Train loss 5.711, Val loss 5.761\n",
      "Ep 1 (Step 000090): Train loss 5.676, Val loss 5.725\n",
      "Ep 1 (Step 000100): Train loss 5.586, Val loss 5.649\n",
      "Ep 1 (Step 000110): Train loss 5.529, Val loss 5.590\n",
      "Ep 1 (Step 000120): Train loss 5.565, Val loss 5.565\n",
      "Ep 1 (Step 000130): Train loss 5.512, Val loss 5.561\n",
      "Ep 1 (Step 000140): Train loss 5.481, Val loss 5.531\n",
      "Ep 1 (Step 000150): Train loss 5.449, Val loss 5.500\n",
      "Ep 1 (Step 000160): Train loss 5.351, Val loss 5.476\n",
      "Ep 1 (Step 000170): Train loss 5.417, Val loss 5.476\n",
      "Ep 1 (Step 000180): Train loss 5.282, Val loss 5.453\n",
      "Ep 1 (Step 000190): Train loss 5.375, Val loss 5.417\n",
      "Ep 1 (Step 000200): Train loss 5.392, Val loss 5.405\n",
      "Ep 1 (Step 000210): Train loss 5.279, Val loss 5.413\n",
      "Ep 1 (Step 000220): Train loss 5.160, Val loss 5.374\n",
      "Ep 1 (Step 000230): Train loss 5.327, Val loss 5.375\n",
      "Ep 1 (Step 000240): Train loss 5.235, Val loss 5.348\n",
      "Ep 1 (Step 000250): Train loss 5.209, Val loss 5.351\n",
      "Ep 1 (Step 000260): Train loss 5.281, Val loss 5.318\n",
      "Ep 1 (Step 000270): Train loss 5.149, Val loss 5.305\n",
      "Ep 1 (Step 000280): Train loss 5.190, Val loss 5.332\n",
      "Ep 1 (Step 000290): Train loss 5.230, Val loss 5.312\n",
      "Ep 1 (Step 000300): Train loss 5.163, Val loss 5.297\n",
      "Ep 1 (Step 000310): Train loss 5.172, Val loss 5.278\n",
      "Ep 1 (Step 000320): Train loss 5.154, Val loss 5.270\n",
      "Ep 1 (Step 000330): Train loss 5.195, Val loss 5.255\n",
      "Ep 1 (Step 000340): Train loss 5.139, Val loss 5.243\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2425\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.315, Val loss 8.259\n",
      "Ep 1 (Step 000010): Train loss 6.550, Val loss 6.548\n",
      "Ep 1 (Step 000020): Train loss 6.519, Val loss 6.504\n",
      "Ep 1 (Step 000030): Train loss 6.406, Val loss 6.381\n",
      "Ep 1 (Step 000040): Train loss 6.154, Val loss 6.193\n",
      "Ep 1 (Step 000050): Train loss 6.086, Val loss 6.043\n",
      "Ep 1 (Step 000060): Train loss 5.908, Val loss 5.908\n",
      "Ep 1 (Step 000070): Train loss 5.798, Val loss 5.809\n",
      "Ep 1 (Step 000080): Train loss 5.643, Val loss 5.749\n",
      "Ep 1 (Step 000090): Train loss 5.659, Val loss 5.694\n",
      "Ep 1 (Step 000100): Train loss 5.635, Val loss 5.631\n",
      "Ep 1 (Step 000110): Train loss 5.578, Val loss 5.580\n",
      "Ep 1 (Step 000120): Train loss 5.528, Val loss 5.560\n",
      "Ep 1 (Step 000130): Train loss 5.510, Val loss 5.529\n",
      "Ep 1 (Step 000140): Train loss 5.453, Val loss 5.499\n",
      "Ep 1 (Step 000150): Train loss 5.446, Val loss 5.476\n",
      "Ep 1 (Step 000160): Train loss 5.463, Val loss 5.479\n",
      "Ep 1 (Step 000170): Train loss 5.386, Val loss 5.451\n",
      "Ep 1 (Step 000180): Train loss 5.230, Val loss 5.408\n",
      "Ep 1 (Step 000190): Train loss 5.369, Val loss 5.408\n",
      "Ep 1 (Step 000200): Train loss 5.194, Val loss 5.400\n",
      "Ep 1 (Step 000210): Train loss 5.316, Val loss 5.396\n",
      "Ep 1 (Step 000220): Train loss 5.240, Val loss 5.368\n",
      "Ep 1 (Step 000230): Train loss 5.117, Val loss 5.345\n",
      "Ep 1 (Step 000240): Train loss 5.142, Val loss 5.313\n",
      "Ep 1 (Step 000250): Train loss 5.245, Val loss 5.309\n",
      "Ep 1 (Step 000260): Train loss 5.167, Val loss 5.283\n",
      "Ep 1 (Step 000270): Train loss 5.162, Val loss 5.286\n",
      "Ep 1 (Step 000280): Train loss 5.163, Val loss 5.293\n",
      "Ep 1 (Step 000290): Train loss 5.040, Val loss 5.278\n",
      "Ep 1 (Step 000300): Train loss 5.071, Val loss 5.273\n",
      "Ep 1 (Step 000310): Train loss 5.108, Val loss 5.257\n",
      "Ep 1 (Step 000320): Train loss 5.056, Val loss 5.244\n",
      "Ep 1 (Step 000330): Train loss 4.997, Val loss 5.239\n",
      "Ep 1 (Step 000340): Train loss 5.024, Val loss 5.230\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2304\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.323, Val loss 8.311\n",
      "Ep 1 (Step 000010): Train loss 6.594, Val loss 6.574\n",
      "Ep 1 (Step 000020): Train loss 6.470, Val loss 6.520\n",
      "Ep 1 (Step 000030): Train loss 6.515, Val loss 6.411\n",
      "Ep 1 (Step 000040): Train loss 6.153, Val loss 6.222\n",
      "Ep 1 (Step 000050): Train loss 6.133, Val loss 6.074\n",
      "Ep 1 (Step 000060): Train loss 5.848, Val loss 5.933\n",
      "Ep 1 (Step 000070): Train loss 5.763, Val loss 5.833\n",
      "Ep 1 (Step 000080): Train loss 5.708, Val loss 5.756\n",
      "Ep 1 (Step 000090): Train loss 5.651, Val loss 5.703\n",
      "Ep 1 (Step 000100): Train loss 5.640, Val loss 5.653\n",
      "Ep 1 (Step 000110): Train loss 5.537, Val loss 5.646\n",
      "Ep 1 (Step 000120): Train loss 5.510, Val loss 5.584\n",
      "Ep 1 (Step 000130): Train loss 5.542, Val loss 5.547\n",
      "Ep 1 (Step 000140): Train loss 5.494, Val loss 5.502\n",
      "Ep 1 (Step 000150): Train loss 5.320, Val loss 5.495\n",
      "Ep 1 (Step 000160): Train loss 5.349, Val loss 5.481\n",
      "Ep 1 (Step 000170): Train loss 5.381, Val loss 5.444\n",
      "Ep 1 (Step 000180): Train loss 5.297, Val loss 5.442\n",
      "Ep 1 (Step 000190): Train loss 5.311, Val loss 5.406\n",
      "Ep 1 (Step 000200): Train loss 5.321, Val loss 5.403\n",
      "Ep 1 (Step 000210): Train loss 5.225, Val loss 5.393\n",
      "Ep 1 (Step 000220): Train loss 5.265, Val loss 5.387\n",
      "Ep 1 (Step 000230): Train loss 5.215, Val loss 5.354\n",
      "Ep 1 (Step 000240): Train loss 5.273, Val loss 5.350\n",
      "Ep 1 (Step 000250): Train loss 5.196, Val loss 5.325\n",
      "Ep 1 (Step 000260): Train loss 5.169, Val loss 5.301\n",
      "Ep 1 (Step 000270): Train loss 5.159, Val loss 5.289\n",
      "Ep 1 (Step 000280): Train loss 5.131, Val loss 5.302\n",
      "Ep 1 (Step 000290): Train loss 5.096, Val loss 5.276\n",
      "Ep 1 (Step 000300): Train loss 5.072, Val loss 5.273\n",
      "Ep 1 (Step 000310): Train loss 5.166, Val loss 5.247\n",
      "Ep 1 (Step 000320): Train loss 5.027, Val loss 5.241\n",
      "Ep 1 (Step 000330): Train loss 5.087, Val loss 5.263\n",
      "Ep 1 (Step 000340): Train loss 5.151, Val loss 5.231\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2310\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.288, Val loss 8.262\n",
      "Ep 1 (Step 000010): Train loss 6.641, Val loss 6.580\n",
      "Ep 1 (Step 000020): Train loss 6.495, Val loss 6.512\n",
      "Ep 1 (Step 000030): Train loss 6.351, Val loss 6.367\n",
      "Ep 1 (Step 000040): Train loss 6.163, Val loss 6.186\n",
      "Ep 1 (Step 000050): Train loss 5.976, Val loss 6.022\n",
      "Ep 1 (Step 000060): Train loss 5.868, Val loss 5.918\n",
      "Ep 1 (Step 000070): Train loss 5.765, Val loss 5.813\n",
      "Ep 1 (Step 000080): Train loss 5.690, Val loss 5.766\n",
      "Ep 1 (Step 000090): Train loss 5.625, Val loss 5.695\n",
      "Ep 1 (Step 000100): Train loss 5.571, Val loss 5.622\n",
      "Ep 1 (Step 000110): Train loss 5.529, Val loss 5.587\n",
      "Ep 1 (Step 000120): Train loss 5.538, Val loss 5.535\n",
      "Ep 1 (Step 000130): Train loss 5.389, Val loss 5.521\n",
      "Ep 1 (Step 000140): Train loss 5.319, Val loss 5.488\n",
      "Ep 1 (Step 000150): Train loss 5.427, Val loss 5.485\n",
      "Ep 1 (Step 000160): Train loss 5.307, Val loss 5.461\n",
      "Ep 1 (Step 000170): Train loss 5.283, Val loss 5.432\n",
      "Ep 1 (Step 000180): Train loss 5.279, Val loss 5.432\n",
      "Ep 1 (Step 000190): Train loss 5.288, Val loss 5.394\n",
      "Ep 1 (Step 000200): Train loss 5.206, Val loss 5.378\n",
      "Ep 1 (Step 000210): Train loss 5.209, Val loss 5.354\n",
      "Ep 1 (Step 000220): Train loss 5.162, Val loss 5.338\n",
      "Ep 1 (Step 000230): Train loss 5.262, Val loss 5.346\n",
      "Ep 1 (Step 000240): Train loss 5.126, Val loss 5.317\n",
      "Ep 1 (Step 000250): Train loss 5.197, Val loss 5.316\n",
      "Ep 1 (Step 000260): Train loss 5.093, Val loss 5.328\n",
      "Ep 1 (Step 000270): Train loss 5.191, Val loss 5.285\n",
      "Ep 1 (Step 000280): Train loss 5.230, Val loss 5.264\n",
      "Ep 1 (Step 000290): Train loss 5.169, Val loss 5.263\n",
      "Ep 1 (Step 000300): Train loss 5.210, Val loss 5.255\n",
      "Ep 1 (Step 000310): Train loss 5.157, Val loss 5.248\n",
      "Ep 1 (Step 000320): Train loss 5.073, Val loss 5.232\n",
      "Ep 1 (Step 000330): Train loss 5.025, Val loss 5.228\n",
      "Ep 1 (Step 000340): Train loss 5.074, Val loss 5.220\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2202\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.632, Val loss 8.580\n",
      "Ep 1 (Step 000010): Train loss 7.035, Val loss 6.999\n",
      "Ep 1 (Step 000020): Train loss 6.478, Val loss 6.455\n",
      "Ep 1 (Step 000030): Train loss 6.364, Val loss 6.351\n",
      "Ep 1 (Step 000040): Train loss 6.242, Val loss 6.215\n",
      "Ep 1 (Step 000050): Train loss 6.082, Val loss 6.073\n",
      "Ep 1 (Step 000060): Train loss 5.935, Val loss 5.980\n",
      "Ep 1 (Step 000070): Train loss 5.853, Val loss 5.860\n",
      "Ep 1 (Step 000080): Train loss 5.727, Val loss 5.792\n",
      "Ep 1 (Step 000090): Train loss 5.640, Val loss 5.744\n",
      "Ep 1 (Step 000100): Train loss 5.604, Val loss 5.670\n",
      "Ep 1 (Step 000110): Train loss 5.489, Val loss 5.637\n",
      "Ep 1 (Step 000120): Train loss 5.506, Val loss 5.591\n",
      "Ep 1 (Step 000130): Train loss 5.401, Val loss 5.541\n",
      "Ep 1 (Step 000140): Train loss 5.427, Val loss 5.520\n",
      "Ep 1 (Step 000150): Train loss 5.400, Val loss 5.492\n",
      "Ep 1 (Step 000160): Train loss 5.395, Val loss 5.486\n",
      "Ep 1 (Step 000170): Train loss 5.348, Val loss 5.451\n",
      "Ep 1 (Step 000180): Train loss 5.246, Val loss 5.439\n",
      "Ep 1 (Step 000190): Train loss 5.284, Val loss 5.406\n",
      "Ep 1 (Step 000200): Train loss 5.285, Val loss 5.387\n",
      "Ep 1 (Step 000210): Train loss 5.278, Val loss 5.383\n",
      "Ep 1 (Step 000220): Train loss 5.223, Val loss 5.358\n",
      "Ep 1 (Step 000230): Train loss 5.285, Val loss 5.366\n",
      "Ep 1 (Step 000240): Train loss 5.270, Val loss 5.361\n",
      "Ep 1 (Step 000250): Train loss 5.249, Val loss 5.326\n",
      "Ep 1 (Step 000260): Train loss 5.090, Val loss 5.331\n",
      "Ep 1 (Step 000270): Train loss 5.111, Val loss 5.298\n",
      "Ep 1 (Step 000280): Train loss 5.077, Val loss 5.299\n",
      "Ep 1 (Step 000290): Train loss 5.179, Val loss 5.275\n",
      "Ep 1 (Step 000300): Train loss 5.103, Val loss 5.275\n",
      "Ep 1 (Step 000310): Train loss 5.014, Val loss 5.246\n",
      "Ep 1 (Step 000320): Train loss 5.089, Val loss 5.235\n",
      "Ep 1 (Step 000330): Train loss 5.085, Val loss 5.232\n",
      "Ep 1 (Step 000340): Train loss 5.157, Val loss 5.210\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2096\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.588, Val loss 8.557\n",
      "Ep 1 (Step 000010): Train loss 6.983, Val loss 6.978\n",
      "Ep 1 (Step 000020): Train loss 6.455, Val loss 6.473\n",
      "Ep 1 (Step 000030): Train loss 6.340, Val loss 6.359\n",
      "Ep 1 (Step 000040): Train loss 6.239, Val loss 6.221\n",
      "Ep 1 (Step 000050): Train loss 5.992, Val loss 6.024\n",
      "Ep 1 (Step 000060): Train loss 5.889, Val loss 5.936\n",
      "Ep 1 (Step 000070): Train loss 5.875, Val loss 5.849\n",
      "Ep 1 (Step 000080): Train loss 5.775, Val loss 5.780\n",
      "Ep 1 (Step 000090): Train loss 5.624, Val loss 5.710\n",
      "Ep 1 (Step 000100): Train loss 5.556, Val loss 5.661\n",
      "Ep 1 (Step 000110): Train loss 5.523, Val loss 5.609\n",
      "Ep 1 (Step 000120): Train loss 5.449, Val loss 5.580\n",
      "Ep 1 (Step 000130): Train loss 5.547, Val loss 5.547\n",
      "Ep 1 (Step 000140): Train loss 5.492, Val loss 5.531\n",
      "Ep 1 (Step 000150): Train loss 5.377, Val loss 5.487\n",
      "Ep 1 (Step 000160): Train loss 5.401, Val loss 5.458\n",
      "Ep 1 (Step 000170): Train loss 5.320, Val loss 5.441\n",
      "Ep 1 (Step 000180): Train loss 5.293, Val loss 5.412\n",
      "Ep 1 (Step 000190): Train loss 5.334, Val loss 5.405\n",
      "Ep 1 (Step 000200): Train loss 5.288, Val loss 5.389\n",
      "Ep 1 (Step 000210): Train loss 5.297, Val loss 5.377\n",
      "Ep 1 (Step 000220): Train loss 5.232, Val loss 5.373\n",
      "Ep 1 (Step 000230): Train loss 5.121, Val loss 5.358\n",
      "Ep 1 (Step 000240): Train loss 5.144, Val loss 5.323\n",
      "Ep 1 (Step 000250): Train loss 5.076, Val loss 5.316\n",
      "Ep 1 (Step 000260): Train loss 5.122, Val loss 5.300\n",
      "Ep 1 (Step 000270): Train loss 5.209, Val loss 5.290\n",
      "Ep 1 (Step 000280): Train loss 5.094, Val loss 5.275\n",
      "Ep 1 (Step 000290): Train loss 5.129, Val loss 5.262\n",
      "Ep 1 (Step 000300): Train loss 5.169, Val loss 5.256\n",
      "Ep 1 (Step 000310): Train loss 5.084, Val loss 5.240\n",
      "Ep 1 (Step 000320): Train loss 5.086, Val loss 5.231\n",
      "Ep 1 (Step 000330): Train loss 5.020, Val loss 5.229\n",
      "Ep 1 (Step 000340): Train loss 5.106, Val loss 5.212\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2121\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.638, Val loss 8.605\n",
      "Ep 1 (Step 000010): Train loss 6.950, Val loss 6.956\n",
      "Ep 1 (Step 000020): Train loss 6.517, Val loss 6.449\n",
      "Ep 1 (Step 000030): Train loss 6.347, Val loss 6.316\n",
      "Ep 1 (Step 000040): Train loss 6.184, Val loss 6.173\n",
      "Ep 1 (Step 000050): Train loss 6.060, Val loss 6.020\n",
      "Ep 1 (Step 000060): Train loss 5.946, Val loss 5.930\n",
      "Ep 1 (Step 000070): Train loss 5.711, Val loss 5.841\n",
      "Ep 1 (Step 000080): Train loss 5.736, Val loss 5.750\n",
      "Ep 1 (Step 000090): Train loss 5.665, Val loss 5.688\n",
      "Ep 1 (Step 000100): Train loss 5.588, Val loss 5.651\n",
      "Ep 1 (Step 000110): Train loss 5.532, Val loss 5.591\n",
      "Ep 1 (Step 000120): Train loss 5.544, Val loss 5.561\n",
      "Ep 1 (Step 000130): Train loss 5.473, Val loss 5.510\n",
      "Ep 1 (Step 000140): Train loss 5.451, Val loss 5.491\n",
      "Ep 1 (Step 000150): Train loss 5.361, Val loss 5.475\n",
      "Ep 1 (Step 000160): Train loss 5.358, Val loss 5.453\n",
      "Ep 1 (Step 000170): Train loss 5.383, Val loss 5.426\n",
      "Ep 1 (Step 000180): Train loss 5.232, Val loss 5.409\n",
      "Ep 1 (Step 000190): Train loss 5.344, Val loss 5.374\n",
      "Ep 1 (Step 000200): Train loss 5.284, Val loss 5.370\n",
      "Ep 1 (Step 000210): Train loss 5.271, Val loss 5.368\n",
      "Ep 1 (Step 000220): Train loss 5.206, Val loss 5.353\n",
      "Ep 1 (Step 000230): Train loss 5.251, Val loss 5.361\n",
      "Ep 1 (Step 000240): Train loss 5.183, Val loss 5.319\n",
      "Ep 1 (Step 000250): Train loss 5.236, Val loss 5.296\n",
      "Ep 1 (Step 000260): Train loss 5.079, Val loss 5.304\n",
      "Ep 1 (Step 000270): Train loss 5.193, Val loss 5.290\n",
      "Ep 1 (Step 000280): Train loss 5.065, Val loss 5.279\n",
      "Ep 1 (Step 000290): Train loss 5.162, Val loss 5.258\n",
      "Ep 1 (Step 000300): Train loss 5.102, Val loss 5.249\n",
      "Ep 1 (Step 000310): Train loss 5.086, Val loss 5.225\n",
      "Ep 1 (Step 000320): Train loss 5.072, Val loss 5.210\n",
      "Ep 1 (Step 000330): Train loss 5.179, Val loss 5.218\n",
      "Ep 1 (Step 000340): Train loss 5.123, Val loss 5.219\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2189\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.650, Val loss 8.636\n",
      "Ep 1 (Step 000010): Train loss 6.989, Val loss 6.963\n",
      "Ep 1 (Step 000020): Train loss 6.469, Val loss 6.461\n",
      "Ep 1 (Step 000030): Train loss 6.342, Val loss 6.358\n",
      "Ep 1 (Step 000040): Train loss 6.247, Val loss 6.292\n",
      "Ep 1 (Step 000050): Train loss 5.950, Val loss 6.062\n",
      "Ep 1 (Step 000060): Train loss 5.913, Val loss 5.950\n",
      "Ep 1 (Step 000070): Train loss 5.928, Val loss 5.887\n",
      "Ep 1 (Step 000080): Train loss 5.746, Val loss 5.796\n",
      "Ep 1 (Step 000090): Train loss 5.601, Val loss 5.711\n",
      "Ep 1 (Step 000100): Train loss 5.584, Val loss 5.653\n",
      "Ep 1 (Step 000110): Train loss 5.482, Val loss 5.626\n",
      "Ep 1 (Step 000120): Train loss 5.532, Val loss 5.573\n",
      "Ep 1 (Step 000130): Train loss 5.359, Val loss 5.530\n",
      "Ep 1 (Step 000140): Train loss 5.330, Val loss 5.512\n",
      "Ep 1 (Step 000150): Train loss 5.389, Val loss 5.472\n",
      "Ep 1 (Step 000160): Train loss 5.272, Val loss 5.453\n",
      "Ep 1 (Step 000170): Train loss 5.336, Val loss 5.433\n",
      "Ep 1 (Step 000180): Train loss 5.308, Val loss 5.407\n",
      "Ep 1 (Step 000190): Train loss 5.215, Val loss 5.396\n",
      "Ep 1 (Step 000200): Train loss 5.390, Val loss 5.362\n",
      "Ep 1 (Step 000210): Train loss 5.249, Val loss 5.345\n",
      "Ep 1 (Step 000220): Train loss 5.136, Val loss 5.328\n",
      "Ep 1 (Step 000230): Train loss 5.252, Val loss 5.321\n",
      "Ep 1 (Step 000240): Train loss 5.232, Val loss 5.317\n",
      "Ep 1 (Step 000250): Train loss 5.194, Val loss 5.284\n",
      "Ep 1 (Step 000260): Train loss 5.183, Val loss 5.283\n",
      "Ep 1 (Step 000270): Train loss 5.086, Val loss 5.271\n",
      "Ep 1 (Step 000280): Train loss 5.041, Val loss 5.244\n",
      "Ep 1 (Step 000290): Train loss 5.079, Val loss 5.239\n",
      "Ep 1 (Step 000300): Train loss 5.152, Val loss 5.239\n",
      "Ep 1 (Step 000310): Train loss 5.048, Val loss 5.234\n",
      "Ep 1 (Step 000320): Train loss 5.040, Val loss 5.233\n",
      "Ep 1 (Step 000330): Train loss 5.035, Val loss 5.216\n",
      "Ep 1 (Step 000340): Train loss 5.069, Val loss 5.205\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2048\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.637, Val loss 8.648\n",
      "Ep 1 (Step 000010): Train loss 7.030, Val loss 6.986\n",
      "Ep 1 (Step 000020): Train loss 6.433, Val loss 6.441\n",
      "Ep 1 (Step 000030): Train loss 6.416, Val loss 6.322\n",
      "Ep 1 (Step 000040): Train loss 6.158, Val loss 6.189\n",
      "Ep 1 (Step 000050): Train loss 6.020, Val loss 6.071\n",
      "Ep 1 (Step 000060): Train loss 5.911, Val loss 5.944\n",
      "Ep 1 (Step 000070): Train loss 5.877, Val loss 5.863\n",
      "Ep 1 (Step 000080): Train loss 5.752, Val loss 5.772\n",
      "Ep 1 (Step 000090): Train loss 5.661, Val loss 5.700\n",
      "Ep 1 (Step 000100): Train loss 5.520, Val loss 5.637\n",
      "Ep 1 (Step 000110): Train loss 5.525, Val loss 5.612\n",
      "Ep 1 (Step 000120): Train loss 5.453, Val loss 5.559\n",
      "Ep 1 (Step 000130): Train loss 5.441, Val loss 5.526\n",
      "Ep 1 (Step 000140): Train loss 5.452, Val loss 5.487\n",
      "Ep 1 (Step 000150): Train loss 5.469, Val loss 5.430\n",
      "Ep 1 (Step 000160): Train loss 5.309, Val loss 5.421\n",
      "Ep 1 (Step 000170): Train loss 5.288, Val loss 5.388\n",
      "Ep 1 (Step 000180): Train loss 5.328, Val loss 5.379\n",
      "Ep 1 (Step 000190): Train loss 5.285, Val loss 5.340\n",
      "Ep 1 (Step 000200): Train loss 5.339, Val loss 5.325\n",
      "Ep 1 (Step 000210): Train loss 5.135, Val loss 5.313\n",
      "Ep 1 (Step 000220): Train loss 5.304, Val loss 5.319\n",
      "Ep 1 (Step 000230): Train loss 5.185, Val loss 5.295\n",
      "Ep 1 (Step 000240): Train loss 5.161, Val loss 5.287\n",
      "Ep 1 (Step 000250): Train loss 5.160, Val loss 5.279\n",
      "Ep 1 (Step 000260): Train loss 5.158, Val loss 5.271\n",
      "Ep 1 (Step 000270): Train loss 5.114, Val loss 5.241\n",
      "Ep 1 (Step 000280): Train loss 5.086, Val loss 5.234\n",
      "Ep 1 (Step 000290): Train loss 5.037, Val loss 5.221\n",
      "Ep 1 (Step 000300): Train loss 5.075, Val loss 5.221\n",
      "Ep 1 (Step 000310): Train loss 5.081, Val loss 5.223\n",
      "Ep 1 (Step 000320): Train loss 5.002, Val loss 5.201\n",
      "Ep 1 (Step 000330): Train loss 5.095, Val loss 5.193\n",
      "Ep 1 (Step 000340): Train loss 5.036, Val loss 5.189\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1892\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.589, Val loss 8.557\n",
      "Ep 1 (Step 000010): Train loss 6.946, Val loss 6.952\n",
      "Ep 1 (Step 000020): Train loss 6.516, Val loss 6.461\n",
      "Ep 1 (Step 000030): Train loss 6.363, Val loss 6.357\n",
      "Ep 1 (Step 000040): Train loss 6.156, Val loss 6.215\n",
      "Ep 1 (Step 000050): Train loss 5.918, Val loss 6.024\n",
      "Ep 1 (Step 000060): Train loss 5.878, Val loss 5.942\n",
      "Ep 1 (Step 000070): Train loss 5.799, Val loss 5.825\n",
      "Ep 1 (Step 000080): Train loss 5.743, Val loss 5.758\n",
      "Ep 1 (Step 000090): Train loss 5.637, Val loss 5.699\n",
      "Ep 1 (Step 000100): Train loss 5.562, Val loss 5.645\n",
      "Ep 1 (Step 000110): Train loss 5.596, Val loss 5.611\n",
      "Ep 1 (Step 000120): Train loss 5.528, Val loss 5.576\n",
      "Ep 1 (Step 000130): Train loss 5.460, Val loss 5.539\n",
      "Ep 1 (Step 000140): Train loss 5.456, Val loss 5.510\n",
      "Ep 1 (Step 000150): Train loss 5.340, Val loss 5.470\n",
      "Ep 1 (Step 000160): Train loss 5.370, Val loss 5.440\n",
      "Ep 1 (Step 000170): Train loss 5.292, Val loss 5.410\n",
      "Ep 1 (Step 000180): Train loss 5.240, Val loss 5.401\n",
      "Ep 1 (Step 000190): Train loss 5.244, Val loss 5.381\n",
      "Ep 1 (Step 000200): Train loss 5.255, Val loss 5.355\n",
      "Ep 1 (Step 000210): Train loss 5.240, Val loss 5.347\n",
      "Ep 1 (Step 000220): Train loss 5.199, Val loss 5.339\n",
      "Ep 1 (Step 000230): Train loss 5.173, Val loss 5.303\n",
      "Ep 1 (Step 000240): Train loss 5.174, Val loss 5.310\n",
      "Ep 1 (Step 000250): Train loss 5.206, Val loss 5.278\n",
      "Ep 1 (Step 000260): Train loss 5.114, Val loss 5.281\n",
      "Ep 1 (Step 000270): Train loss 5.022, Val loss 5.278\n",
      "Ep 1 (Step 000280): Train loss 5.078, Val loss 5.244\n",
      "Ep 1 (Step 000290): Train loss 5.095, Val loss 5.228\n",
      "Ep 1 (Step 000300): Train loss 5.072, Val loss 5.228\n",
      "Ep 1 (Step 000310): Train loss 5.091, Val loss 5.221\n",
      "Ep 1 (Step 000320): Train loss 5.059, Val loss 5.213\n",
      "Ep 1 (Step 000330): Train loss 5.073, Val loss 5.202\n",
      "Ep 1 (Step 000340): Train loss 5.031, Val loss 5.187\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1874\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.275, Val loss 8.228\n",
      "Ep 1 (Step 000010): Train loss 6.637, Val loss 6.542\n",
      "Ep 1 (Step 000020): Train loss 6.481, Val loss 6.485\n",
      "Ep 1 (Step 000030): Train loss 6.298, Val loss 6.330\n",
      "Ep 1 (Step 000040): Train loss 6.200, Val loss 6.229\n",
      "Ep 1 (Step 000050): Train loss 5.928, Val loss 6.002\n",
      "Ep 1 (Step 000060): Train loss 5.846, Val loss 5.906\n",
      "Ep 1 (Step 000070): Train loss 5.771, Val loss 5.820\n",
      "Ep 1 (Step 000080): Train loss 5.652, Val loss 5.740\n",
      "Ep 1 (Step 000090): Train loss 5.629, Val loss 5.692\n",
      "Ep 1 (Step 000100): Train loss 5.513, Val loss 5.619\n",
      "Ep 1 (Step 000110): Train loss 5.533, Val loss 5.564\n",
      "Ep 1 (Step 000120): Train loss 5.450, Val loss 5.565\n",
      "Ep 1 (Step 000130): Train loss 5.484, Val loss 5.540\n",
      "Ep 1 (Step 000140): Train loss 5.445, Val loss 5.517\n",
      "Ep 1 (Step 000150): Train loss 5.393, Val loss 5.482\n",
      "Ep 1 (Step 000160): Train loss 5.338, Val loss 5.436\n",
      "Ep 1 (Step 000170): Train loss 5.363, Val loss 5.436\n",
      "Ep 1 (Step 000180): Train loss 5.366, Val loss 5.418\n",
      "Ep 1 (Step 000190): Train loss 5.317, Val loss 5.406\n",
      "Ep 1 (Step 000200): Train loss 5.274, Val loss 5.393\n",
      "Ep 1 (Step 000210): Train loss 5.258, Val loss 5.360\n",
      "Ep 1 (Step 000220): Train loss 5.262, Val loss 5.344\n",
      "Ep 1 (Step 000230): Train loss 5.235, Val loss 5.325\n",
      "Ep 1 (Step 000240): Train loss 5.286, Val loss 5.326\n",
      "Ep 1 (Step 000250): Train loss 5.250, Val loss 5.294\n",
      "Ep 1 (Step 000260): Train loss 5.126, Val loss 5.299\n",
      "Ep 1 (Step 000270): Train loss 5.151, Val loss 5.295\n",
      "Ep 1 (Step 000280): Train loss 5.165, Val loss 5.279\n",
      "Ep 1 (Step 000290): Train loss 5.079, Val loss 5.268\n",
      "Ep 1 (Step 000300): Train loss 5.238, Val loss 5.255\n",
      "Ep 1 (Step 000310): Train loss 5.082, Val loss 5.263\n",
      "Ep 1 (Step 000320): Train loss 5.177, Val loss 5.241\n",
      "Ep 1 (Step 000330): Train loss 5.086, Val loss 5.217\n",
      "Ep 1 (Step 000340): Train loss 5.094, Val loss 5.227\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2269\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.410, Val loss 8.333\n",
      "Ep 1 (Step 000010): Train loss 6.626, Val loss 6.532\n",
      "Ep 1 (Step 000020): Train loss 6.508, Val loss 6.436\n",
      "Ep 1 (Step 000030): Train loss 6.272, Val loss 6.269\n",
      "Ep 1 (Step 000040): Train loss 6.149, Val loss 6.079\n",
      "Ep 1 (Step 000050): Train loss 5.985, Val loss 5.983\n",
      "Ep 1 (Step 000060): Train loss 5.837, Val loss 5.864\n",
      "Ep 1 (Step 000070): Train loss 5.727, Val loss 5.783\n",
      "Ep 1 (Step 000080): Train loss 5.668, Val loss 5.729\n",
      "Ep 1 (Step 000090): Train loss 5.627, Val loss 5.673\n",
      "Ep 1 (Step 000100): Train loss 5.600, Val loss 5.632\n",
      "Ep 1 (Step 000110): Train loss 5.542, Val loss 5.563\n",
      "Ep 1 (Step 000120): Train loss 5.505, Val loss 5.546\n",
      "Ep 1 (Step 000130): Train loss 5.432, Val loss 5.523\n",
      "Ep 1 (Step 000140): Train loss 5.461, Val loss 5.501\n",
      "Ep 1 (Step 000150): Train loss 5.441, Val loss 5.507\n",
      "Ep 1 (Step 000160): Train loss 5.392, Val loss 5.495\n",
      "Ep 1 (Step 000170): Train loss 5.348, Val loss 5.454\n",
      "Ep 1 (Step 000180): Train loss 5.380, Val loss 5.409\n",
      "Ep 1 (Step 000190): Train loss 5.199, Val loss 5.398\n",
      "Ep 1 (Step 000200): Train loss 5.284, Val loss 5.394\n",
      "Ep 1 (Step 000210): Train loss 5.224, Val loss 5.375\n",
      "Ep 1 (Step 000220): Train loss 5.327, Val loss 5.391\n",
      "Ep 1 (Step 000230): Train loss 5.215, Val loss 5.362\n",
      "Ep 1 (Step 000240): Train loss 5.172, Val loss 5.333\n",
      "Ep 1 (Step 000250): Train loss 5.226, Val loss 5.305\n",
      "Ep 1 (Step 000260): Train loss 5.189, Val loss 5.308\n",
      "Ep 1 (Step 000270): Train loss 5.221, Val loss 5.298\n",
      "Ep 1 (Step 000280): Train loss 5.166, Val loss 5.314\n",
      "Ep 1 (Step 000290): Train loss 5.223, Val loss 5.269\n",
      "Ep 1 (Step 000300): Train loss 5.095, Val loss 5.274\n",
      "Ep 1 (Step 000310): Train loss 5.144, Val loss 5.243\n",
      "Ep 1 (Step 000320): Train loss 5.163, Val loss 5.250\n",
      "Ep 1 (Step 000330): Train loss 5.057, Val loss 5.236\n",
      "Ep 1 (Step 000340): Train loss 5.079, Val loss 5.232\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2315\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.272, Val loss 8.235\n",
      "Ep 1 (Step 000010): Train loss 6.571, Val loss 6.550\n",
      "Ep 1 (Step 000020): Train loss 6.517, Val loss 6.460\n",
      "Ep 1 (Step 000030): Train loss 6.322, Val loss 6.252\n",
      "Ep 1 (Step 000040): Train loss 6.067, Val loss 6.084\n",
      "Ep 1 (Step 000050): Train loss 5.930, Val loss 5.970\n",
      "Ep 1 (Step 000060): Train loss 5.776, Val loss 5.889\n",
      "Ep 1 (Step 000070): Train loss 5.769, Val loss 5.801\n",
      "Ep 1 (Step 000080): Train loss 5.653, Val loss 5.725\n",
      "Ep 1 (Step 000090): Train loss 5.654, Val loss 5.677\n",
      "Ep 1 (Step 000100): Train loss 5.536, Val loss 5.642\n",
      "Ep 1 (Step 000110): Train loss 5.463, Val loss 5.582\n",
      "Ep 1 (Step 000120): Train loss 5.396, Val loss 5.544\n",
      "Ep 1 (Step 000130): Train loss 5.396, Val loss 5.537\n",
      "Ep 1 (Step 000140): Train loss 5.362, Val loss 5.495\n",
      "Ep 1 (Step 000150): Train loss 5.332, Val loss 5.462\n",
      "Ep 1 (Step 000160): Train loss 5.329, Val loss 5.454\n",
      "Ep 1 (Step 000170): Train loss 5.395, Val loss 5.428\n",
      "Ep 1 (Step 000180): Train loss 5.425, Val loss 5.419\n",
      "Ep 1 (Step 000190): Train loss 5.329, Val loss 5.395\n",
      "Ep 1 (Step 000200): Train loss 5.320, Val loss 5.360\n",
      "Ep 1 (Step 000210): Train loss 5.251, Val loss 5.340\n",
      "Ep 1 (Step 000220): Train loss 5.232, Val loss 5.353\n",
      "Ep 1 (Step 000230): Train loss 5.148, Val loss 5.315\n",
      "Ep 1 (Step 000240): Train loss 5.244, Val loss 5.303\n",
      "Ep 1 (Step 000250): Train loss 5.169, Val loss 5.297\n",
      "Ep 1 (Step 000260): Train loss 5.194, Val loss 5.293\n",
      "Ep 1 (Step 000270): Train loss 5.159, Val loss 5.274\n",
      "Ep 1 (Step 000280): Train loss 5.166, Val loss 5.294\n",
      "Ep 1 (Step 000290): Train loss 5.108, Val loss 5.276\n",
      "Ep 1 (Step 000300): Train loss 5.176, Val loss 5.252\n",
      "Ep 1 (Step 000310): Train loss 5.057, Val loss 5.242\n",
      "Ep 1 (Step 000320): Train loss 5.046, Val loss 5.223\n",
      "Ep 1 (Step 000330): Train loss 5.095, Val loss 5.210\n",
      "Ep 1 (Step 000340): Train loss 5.112, Val loss 5.211\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2108\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.311, Val loss 8.287\n",
      "Ep 1 (Step 000010): Train loss 6.560, Val loss 6.566\n",
      "Ep 1 (Step 000020): Train loss 6.551, Val loss 6.488\n",
      "Ep 1 (Step 000030): Train loss 6.204, Val loss 6.272\n",
      "Ep 1 (Step 000040): Train loss 6.094, Val loss 6.059\n",
      "Ep 1 (Step 000050): Train loss 5.898, Val loss 5.949\n",
      "Ep 1 (Step 000060): Train loss 5.853, Val loss 5.860\n",
      "Ep 1 (Step 000070): Train loss 5.863, Val loss 5.752\n",
      "Ep 1 (Step 000080): Train loss 5.613, Val loss 5.687\n",
      "Ep 1 (Step 000090): Train loss 5.523, Val loss 5.634\n",
      "Ep 1 (Step 000100): Train loss 5.498, Val loss 5.613\n",
      "Ep 1 (Step 000110): Train loss 5.538, Val loss 5.554\n",
      "Ep 1 (Step 000120): Train loss 5.515, Val loss 5.520\n",
      "Ep 1 (Step 000130): Train loss 5.413, Val loss 5.504\n",
      "Ep 1 (Step 000140): Train loss 5.361, Val loss 5.453\n",
      "Ep 1 (Step 000150): Train loss 5.289, Val loss 5.441\n",
      "Ep 1 (Step 000160): Train loss 5.270, Val loss 5.404\n",
      "Ep 1 (Step 000170): Train loss 5.264, Val loss 5.394\n",
      "Ep 1 (Step 000180): Train loss 5.263, Val loss 5.397\n",
      "Ep 1 (Step 000190): Train loss 5.307, Val loss 5.359\n",
      "Ep 1 (Step 000200): Train loss 5.085, Val loss 5.341\n",
      "Ep 1 (Step 000210): Train loss 5.209, Val loss 5.341\n",
      "Ep 1 (Step 000220): Train loss 5.239, Val loss 5.300\n",
      "Ep 1 (Step 000230): Train loss 5.115, Val loss 5.291\n",
      "Ep 1 (Step 000240): Train loss 5.115, Val loss 5.297\n",
      "Ep 1 (Step 000250): Train loss 5.123, Val loss 5.287\n",
      "Ep 1 (Step 000260): Train loss 5.040, Val loss 5.266\n",
      "Ep 1 (Step 000270): Train loss 5.151, Val loss 5.241\n",
      "Ep 1 (Step 000280): Train loss 5.061, Val loss 5.221\n",
      "Ep 1 (Step 000290): Train loss 4.999, Val loss 5.213\n",
      "Ep 1 (Step 000300): Train loss 5.006, Val loss 5.230\n",
      "Ep 1 (Step 000310): Train loss 4.982, Val loss 5.191\n",
      "Ep 1 (Step 000320): Train loss 5.024, Val loss 5.186\n",
      "Ep 1 (Step 000330): Train loss 4.967, Val loss 5.175\n",
      "Ep 1 (Step 000340): Train loss 4.988, Val loss 5.160\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1600\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.253, Val loss 8.311\n",
      "Ep 1 (Step 000010): Train loss 6.616, Val loss 6.619\n",
      "Ep 1 (Step 000020): Train loss 6.422, Val loss 6.475\n",
      "Ep 1 (Step 000030): Train loss 6.317, Val loss 6.311\n",
      "Ep 1 (Step 000040): Train loss 6.027, Val loss 6.118\n",
      "Ep 1 (Step 000050): Train loss 5.929, Val loss 5.952\n",
      "Ep 1 (Step 000060): Train loss 5.787, Val loss 5.842\n",
      "Ep 1 (Step 000070): Train loss 5.766, Val loss 5.749\n",
      "Ep 1 (Step 000080): Train loss 5.561, Val loss 5.677\n",
      "Ep 1 (Step 000090): Train loss 5.682, Val loss 5.624\n",
      "Ep 1 (Step 000100): Train loss 5.582, Val loss 5.576\n",
      "Ep 1 (Step 000110): Train loss 5.486, Val loss 5.536\n",
      "Ep 1 (Step 000120): Train loss 5.457, Val loss 5.509\n",
      "Ep 1 (Step 000130): Train loss 5.449, Val loss 5.486\n",
      "Ep 1 (Step 000140): Train loss 5.397, Val loss 5.452\n",
      "Ep 1 (Step 000150): Train loss 5.282, Val loss 5.432\n",
      "Ep 1 (Step 000160): Train loss 5.321, Val loss 5.448\n",
      "Ep 1 (Step 000170): Train loss 5.301, Val loss 5.412\n",
      "Ep 1 (Step 000180): Train loss 5.327, Val loss 5.383\n",
      "Ep 1 (Step 000190): Train loss 5.203, Val loss 5.371\n",
      "Ep 1 (Step 000200): Train loss 5.246, Val loss 5.347\n",
      "Ep 1 (Step 000210): Train loss 5.229, Val loss 5.355\n",
      "Ep 1 (Step 000220): Train loss 5.121, Val loss 5.322\n",
      "Ep 1 (Step 000230): Train loss 5.232, Val loss 5.319\n",
      "Ep 1 (Step 000240): Train loss 5.221, Val loss 5.292\n",
      "Ep 1 (Step 000250): Train loss 5.040, Val loss 5.268\n",
      "Ep 1 (Step 000260): Train loss 5.123, Val loss 5.268\n",
      "Ep 1 (Step 000270): Train loss 5.225, Val loss 5.229\n",
      "Ep 1 (Step 000280): Train loss 5.100, Val loss 5.224\n",
      "Ep 1 (Step 000290): Train loss 5.205, Val loss 5.237\n",
      "Ep 1 (Step 000300): Train loss 5.023, Val loss 5.224\n",
      "Ep 1 (Step 000310): Train loss 5.044, Val loss 5.196\n",
      "Ep 1 (Step 000320): Train loss 5.055, Val loss 5.198\n",
      "Ep 1 (Step 000330): Train loss 5.034, Val loss 5.166\n",
      "Ep 1 (Step 000340): Train loss 5.178, Val loss 5.184\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1840\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.305, Val loss 8.244\n",
      "Ep 1 (Step 000010): Train loss 6.588, Val loss 6.568\n",
      "Ep 1 (Step 000020): Train loss 6.516, Val loss 6.499\n",
      "Ep 1 (Step 000030): Train loss 6.205, Val loss 6.222\n",
      "Ep 1 (Step 000040): Train loss 6.014, Val loss 6.089\n",
      "Ep 1 (Step 000050): Train loss 5.889, Val loss 5.932\n",
      "Ep 1 (Step 000060): Train loss 5.788, Val loss 5.817\n",
      "Ep 1 (Step 000070): Train loss 5.703, Val loss 5.716\n",
      "Ep 1 (Step 000080): Train loss 5.684, Val loss 5.681\n",
      "Ep 1 (Step 000090): Train loss 5.534, Val loss 5.631\n",
      "Ep 1 (Step 000100): Train loss 5.493, Val loss 5.587\n",
      "Ep 1 (Step 000110): Train loss 5.473, Val loss 5.532\n",
      "Ep 1 (Step 000120): Train loss 5.401, Val loss 5.508\n",
      "Ep 1 (Step 000130): Train loss 5.439, Val loss 5.468\n",
      "Ep 1 (Step 000140): Train loss 5.391, Val loss 5.466\n",
      "Ep 1 (Step 000150): Train loss 5.356, Val loss 5.423\n",
      "Ep 1 (Step 000160): Train loss 5.367, Val loss 5.423\n",
      "Ep 1 (Step 000170): Train loss 5.316, Val loss 5.377\n",
      "Ep 1 (Step 000180): Train loss 5.254, Val loss 5.356\n",
      "Ep 1 (Step 000190): Train loss 5.293, Val loss 5.358\n",
      "Ep 1 (Step 000200): Train loss 5.404, Val loss 5.362\n",
      "Ep 1 (Step 000210): Train loss 5.157, Val loss 5.321\n",
      "Ep 1 (Step 000220): Train loss 5.131, Val loss 5.298\n",
      "Ep 1 (Step 000230): Train loss 5.089, Val loss 5.306\n",
      "Ep 1 (Step 000240): Train loss 5.177, Val loss 5.281\n",
      "Ep 1 (Step 000250): Train loss 5.135, Val loss 5.279\n",
      "Ep 1 (Step 000260): Train loss 5.133, Val loss 5.253\n",
      "Ep 1 (Step 000270): Train loss 5.135, Val loss 5.274\n",
      "Ep 1 (Step 000280): Train loss 5.146, Val loss 5.230\n",
      "Ep 1 (Step 000290): Train loss 5.096, Val loss 5.235\n",
      "Ep 1 (Step 000300): Train loss 5.081, Val loss 5.201\n",
      "Ep 1 (Step 000310): Train loss 5.147, Val loss 5.195\n",
      "Ep 1 (Step 000320): Train loss 5.037, Val loss 5.198\n",
      "Ep 1 (Step 000330): Train loss 4.996, Val loss 5.200\n",
      "Ep 1 (Step 000340): Train loss 4.969, Val loss 5.188\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1880\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.544, Val loss 8.535\n",
      "Ep 1 (Step 000010): Train loss 7.004, Val loss 6.952\n",
      "Ep 1 (Step 000020): Train loss 6.555, Val loss 6.477\n",
      "Ep 1 (Step 000030): Train loss 6.389, Val loss 6.382\n",
      "Ep 1 (Step 000040): Train loss 6.290, Val loss 6.254\n",
      "Ep 1 (Step 000050): Train loss 6.009, Val loss 6.072\n",
      "Ep 1 (Step 000060): Train loss 6.011, Val loss 5.997\n",
      "Ep 1 (Step 000070): Train loss 5.795, Val loss 5.870\n",
      "Ep 1 (Step 000080): Train loss 5.760, Val loss 5.792\n",
      "Ep 1 (Step 000090): Train loss 5.661, Val loss 5.728\n",
      "Ep 1 (Step 000100): Train loss 5.613, Val loss 5.670\n",
      "Ep 1 (Step 000110): Train loss 5.568, Val loss 5.631\n",
      "Ep 1 (Step 000120): Train loss 5.509, Val loss 5.580\n",
      "Ep 1 (Step 000130): Train loss 5.505, Val loss 5.559\n",
      "Ep 1 (Step 000140): Train loss 5.397, Val loss 5.530\n",
      "Ep 1 (Step 000150): Train loss 5.475, Val loss 5.510\n",
      "Ep 1 (Step 000160): Train loss 5.329, Val loss 5.470\n",
      "Ep 1 (Step 000170): Train loss 5.407, Val loss 5.445\n",
      "Ep 1 (Step 000180): Train loss 5.277, Val loss 5.422\n",
      "Ep 1 (Step 000190): Train loss 5.234, Val loss 5.402\n",
      "Ep 1 (Step 000200): Train loss 5.301, Val loss 5.380\n",
      "Ep 1 (Step 000210): Train loss 5.244, Val loss 5.383\n",
      "Ep 1 (Step 000220): Train loss 5.277, Val loss 5.353\n",
      "Ep 1 (Step 000230): Train loss 5.257, Val loss 5.357\n",
      "Ep 1 (Step 000240): Train loss 5.291, Val loss 5.313\n",
      "Ep 1 (Step 000250): Train loss 5.198, Val loss 5.292\n",
      "Ep 1 (Step 000260): Train loss 5.099, Val loss 5.298\n",
      "Ep 1 (Step 000270): Train loss 5.179, Val loss 5.276\n",
      "Ep 1 (Step 000280): Train loss 5.072, Val loss 5.264\n",
      "Ep 1 (Step 000290): Train loss 5.202, Val loss 5.253\n",
      "Ep 1 (Step 000300): Train loss 5.052, Val loss 5.233\n",
      "Ep 1 (Step 000310): Train loss 5.100, Val loss 5.239\n",
      "Ep 1 (Step 000320): Train loss 5.111, Val loss 5.242\n",
      "Ep 1 (Step 000330): Train loss 5.103, Val loss 5.224\n",
      "Ep 1 (Step 000340): Train loss 5.070, Val loss 5.217\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2168\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.603, Val loss 8.578\n",
      "Ep 1 (Step 000010): Train loss 7.010, Val loss 6.982\n",
      "Ep 1 (Step 000020): Train loss 6.430, Val loss 6.462\n",
      "Ep 1 (Step 000030): Train loss 6.363, Val loss 6.383\n",
      "Ep 1 (Step 000040): Train loss 6.242, Val loss 6.270\n",
      "Ep 1 (Step 000050): Train loss 6.123, Val loss 6.121\n",
      "Ep 1 (Step 000060): Train loss 5.995, Val loss 6.010\n",
      "Ep 1 (Step 000070): Train loss 5.881, Val loss 5.914\n",
      "Ep 1 (Step 000080): Train loss 5.765, Val loss 5.828\n",
      "Ep 1 (Step 000090): Train loss 5.619, Val loss 5.749\n",
      "Ep 1 (Step 000100): Train loss 5.564, Val loss 5.702\n",
      "Ep 1 (Step 000110): Train loss 5.518, Val loss 5.671\n",
      "Ep 1 (Step 000120): Train loss 5.509, Val loss 5.628\n",
      "Ep 1 (Step 000130): Train loss 5.472, Val loss 5.577\n",
      "Ep 1 (Step 000140): Train loss 5.364, Val loss 5.532\n",
      "Ep 1 (Step 000150): Train loss 5.379, Val loss 5.494\n",
      "Ep 1 (Step 000160): Train loss 5.413, Val loss 5.451\n",
      "Ep 1 (Step 000170): Train loss 5.372, Val loss 5.434\n",
      "Ep 1 (Step 000180): Train loss 5.413, Val loss 5.426\n",
      "Ep 1 (Step 000190): Train loss 5.252, Val loss 5.413\n",
      "Ep 1 (Step 000200): Train loss 5.242, Val loss 5.388\n",
      "Ep 1 (Step 000210): Train loss 5.294, Val loss 5.371\n",
      "Ep 1 (Step 000220): Train loss 5.259, Val loss 5.352\n",
      "Ep 1 (Step 000230): Train loss 5.230, Val loss 5.318\n",
      "Ep 1 (Step 000240): Train loss 5.219, Val loss 5.315\n",
      "Ep 1 (Step 000250): Train loss 5.177, Val loss 5.314\n",
      "Ep 1 (Step 000260): Train loss 5.160, Val loss 5.302\n",
      "Ep 1 (Step 000270): Train loss 5.141, Val loss 5.293\n",
      "Ep 1 (Step 000280): Train loss 5.155, Val loss 5.296\n",
      "Ep 1 (Step 000290): Train loss 5.153, Val loss 5.282\n",
      "Ep 1 (Step 000300): Train loss 5.159, Val loss 5.266\n",
      "Ep 1 (Step 000310): Train loss 5.136, Val loss 5.237\n",
      "Ep 1 (Step 000320): Train loss 5.026, Val loss 5.229\n",
      "Ep 1 (Step 000330): Train loss 5.143, Val loss 5.246\n",
      "Ep 1 (Step 000340): Train loss 5.023, Val loss 5.230\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2299\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.602, Val loss 8.545\n",
      "Ep 1 (Step 000010): Train loss 6.989, Val loss 6.958\n",
      "Ep 1 (Step 000020): Train loss 6.479, Val loss 6.447\n",
      "Ep 1 (Step 000030): Train loss 6.362, Val loss 6.314\n",
      "Ep 1 (Step 000040): Train loss 6.167, Val loss 6.158\n",
      "Ep 1 (Step 000050): Train loss 6.002, Val loss 6.030\n",
      "Ep 1 (Step 000060): Train loss 5.896, Val loss 5.939\n",
      "Ep 1 (Step 000070): Train loss 5.870, Val loss 5.839\n",
      "Ep 1 (Step 000080): Train loss 5.677, Val loss 5.760\n",
      "Ep 1 (Step 000090): Train loss 5.678, Val loss 5.688\n",
      "Ep 1 (Step 000100): Train loss 5.556, Val loss 5.647\n",
      "Ep 1 (Step 000110): Train loss 5.711, Val loss 5.609\n",
      "Ep 1 (Step 000120): Train loss 5.500, Val loss 5.560\n",
      "Ep 1 (Step 000130): Train loss 5.402, Val loss 5.534\n",
      "Ep 1 (Step 000140): Train loss 5.396, Val loss 5.519\n",
      "Ep 1 (Step 000150): Train loss 5.403, Val loss 5.478\n",
      "Ep 1 (Step 000160): Train loss 5.309, Val loss 5.444\n",
      "Ep 1 (Step 000170): Train loss 5.347, Val loss 5.445\n",
      "Ep 1 (Step 000180): Train loss 5.306, Val loss 5.417\n",
      "Ep 1 (Step 000190): Train loss 5.308, Val loss 5.377\n",
      "Ep 1 (Step 000200): Train loss 5.235, Val loss 5.358\n",
      "Ep 1 (Step 000210): Train loss 5.234, Val loss 5.353\n",
      "Ep 1 (Step 000220): Train loss 5.214, Val loss 5.352\n",
      "Ep 1 (Step 000230): Train loss 5.225, Val loss 5.353\n",
      "Ep 1 (Step 000240): Train loss 5.133, Val loss 5.314\n",
      "Ep 1 (Step 000250): Train loss 5.196, Val loss 5.311\n",
      "Ep 1 (Step 000260): Train loss 5.211, Val loss 5.282\n",
      "Ep 1 (Step 000270): Train loss 5.131, Val loss 5.283\n",
      "Ep 1 (Step 000280): Train loss 5.227, Val loss 5.265\n",
      "Ep 1 (Step 000290): Train loss 5.095, Val loss 5.266\n",
      "Ep 1 (Step 000300): Train loss 5.092, Val loss 5.260\n",
      "Ep 1 (Step 000310): Train loss 5.105, Val loss 5.248\n",
      "Ep 1 (Step 000320): Train loss 5.203, Val loss 5.245\n",
      "Ep 1 (Step 000330): Train loss 5.110, Val loss 5.233\n",
      "Ep 1 (Step 000340): Train loss 5.053, Val loss 5.202\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2018\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.673, Val loss 8.675\n",
      "Ep 1 (Step 000010): Train loss 6.996, Val loss 6.988\n",
      "Ep 1 (Step 000020): Train loss 6.528, Val loss 6.481\n",
      "Ep 1 (Step 000030): Train loss 6.375, Val loss 6.377\n",
      "Ep 1 (Step 000040): Train loss 6.271, Val loss 6.250\n",
      "Ep 1 (Step 000050): Train loss 6.042, Val loss 6.074\n",
      "Ep 1 (Step 000060): Train loss 6.001, Val loss 5.946\n",
      "Ep 1 (Step 000070): Train loss 5.850, Val loss 5.874\n",
      "Ep 1 (Step 000080): Train loss 5.665, Val loss 5.775\n",
      "Ep 1 (Step 000090): Train loss 5.737, Val loss 5.717\n",
      "Ep 1 (Step 000100): Train loss 5.599, Val loss 5.679\n",
      "Ep 1 (Step 000110): Train loss 5.480, Val loss 5.617\n",
      "Ep 1 (Step 000120): Train loss 5.486, Val loss 5.582\n",
      "Ep 1 (Step 000130): Train loss 5.404, Val loss 5.558\n",
      "Ep 1 (Step 000140): Train loss 5.432, Val loss 5.517\n",
      "Ep 1 (Step 000150): Train loss 5.337, Val loss 5.490\n",
      "Ep 1 (Step 000160): Train loss 5.280, Val loss 5.458\n",
      "Ep 1 (Step 000170): Train loss 5.260, Val loss 5.431\n",
      "Ep 1 (Step 000180): Train loss 5.253, Val loss 5.409\n",
      "Ep 1 (Step 000190): Train loss 5.238, Val loss 5.378\n",
      "Ep 1 (Step 000200): Train loss 5.179, Val loss 5.361\n",
      "Ep 1 (Step 000210): Train loss 5.303, Val loss 5.346\n",
      "Ep 1 (Step 000220): Train loss 5.203, Val loss 5.329\n",
      "Ep 1 (Step 000230): Train loss 5.264, Val loss 5.321\n",
      "Ep 1 (Step 000240): Train loss 5.067, Val loss 5.285\n",
      "Ep 1 (Step 000250): Train loss 5.140, Val loss 5.262\n",
      "Ep 1 (Step 000260): Train loss 5.112, Val loss 5.268\n",
      "Ep 1 (Step 000270): Train loss 5.136, Val loss 5.276\n",
      "Ep 1 (Step 000280): Train loss 5.146, Val loss 5.267\n",
      "Ep 1 (Step 000290): Train loss 5.145, Val loss 5.236\n",
      "Ep 1 (Step 000300): Train loss 5.125, Val loss 5.225\n",
      "Ep 1 (Step 000310): Train loss 5.058, Val loss 5.218\n",
      "Ep 1 (Step 000320): Train loss 5.088, Val loss 5.204\n",
      "Ep 1 (Step 000330): Train loss 4.941, Val loss 5.198\n",
      "Ep 1 (Step 000340): Train loss 5.050, Val loss 5.175\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1753\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.612, Val loss 8.606\n",
      "Ep 1 (Step 000010): Train loss 6.980, Val loss 6.967\n",
      "Ep 1 (Step 000020): Train loss 6.404, Val loss 6.474\n",
      "Ep 1 (Step 000030): Train loss 6.377, Val loss 6.357\n",
      "Ep 1 (Step 000040): Train loss 6.292, Val loss 6.275\n",
      "Ep 1 (Step 000050): Train loss 5.974, Val loss 6.021\n",
      "Ep 1 (Step 000060): Train loss 5.934, Val loss 5.949\n",
      "Ep 1 (Step 000070): Train loss 5.764, Val loss 5.832\n",
      "Ep 1 (Step 000080): Train loss 5.716, Val loss 5.768\n",
      "Ep 1 (Step 000090): Train loss 5.715, Val loss 5.733\n",
      "Ep 1 (Step 000100): Train loss 5.640, Val loss 5.646\n",
      "Ep 1 (Step 000110): Train loss 5.565, Val loss 5.626\n",
      "Ep 1 (Step 000120): Train loss 5.489, Val loss 5.569\n",
      "Ep 1 (Step 000130): Train loss 5.451, Val loss 5.550\n",
      "Ep 1 (Step 000140): Train loss 5.366, Val loss 5.510\n",
      "Ep 1 (Step 000150): Train loss 5.386, Val loss 5.473\n",
      "Ep 1 (Step 000160): Train loss 5.392, Val loss 5.446\n",
      "Ep 1 (Step 000170): Train loss 5.345, Val loss 5.440\n",
      "Ep 1 (Step 000180): Train loss 5.213, Val loss 5.421\n",
      "Ep 1 (Step 000190): Train loss 5.254, Val loss 5.396\n",
      "Ep 1 (Step 000200): Train loss 5.274, Val loss 5.382\n",
      "Ep 1 (Step 000210): Train loss 5.173, Val loss 5.335\n",
      "Ep 1 (Step 000220): Train loss 5.173, Val loss 5.316\n",
      "Ep 1 (Step 000230): Train loss 5.186, Val loss 5.304\n",
      "Ep 1 (Step 000240): Train loss 5.160, Val loss 5.295\n",
      "Ep 1 (Step 000250): Train loss 5.145, Val loss 5.281\n",
      "Ep 1 (Step 000260): Train loss 5.165, Val loss 5.255\n",
      "Ep 1 (Step 000270): Train loss 5.133, Val loss 5.253\n",
      "Ep 1 (Step 000280): Train loss 5.087, Val loss 5.235\n",
      "Ep 1 (Step 000290): Train loss 5.130, Val loss 5.225\n",
      "Ep 1 (Step 000300): Train loss 5.042, Val loss 5.218\n",
      "Ep 1 (Step 000310): Train loss 5.104, Val loss 5.208\n",
      "Ep 1 (Step 000320): Train loss 5.003, Val loss 5.202\n",
      "Ep 1 (Step 000330): Train loss 5.098, Val loss 5.185\n",
      "Ep 1 (Step 000340): Train loss 4.937, Val loss 5.179\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1794\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.568, Val loss 8.585\n",
      "Ep 1 (Step 000010): Train loss 6.941, Val loss 6.950\n",
      "Ep 1 (Step 000020): Train loss 6.464, Val loss 6.462\n",
      "Ep 1 (Step 000030): Train loss 6.348, Val loss 6.374\n",
      "Ep 1 (Step 000040): Train loss 6.228, Val loss 6.263\n",
      "Ep 1 (Step 000050): Train loss 6.064, Val loss 6.070\n",
      "Ep 1 (Step 000060): Train loss 5.914, Val loss 5.986\n",
      "Ep 1 (Step 000070): Train loss 5.836, Val loss 5.883\n",
      "Ep 1 (Step 000080): Train loss 5.734, Val loss 5.806\n",
      "Ep 1 (Step 000090): Train loss 5.679, Val loss 5.728\n",
      "Ep 1 (Step 000100): Train loss 5.579, Val loss 5.675\n",
      "Ep 1 (Step 000110): Train loss 5.563, Val loss 5.625\n",
      "Ep 1 (Step 000120): Train loss 5.530, Val loss 5.584\n",
      "Ep 1 (Step 000130): Train loss 5.485, Val loss 5.556\n",
      "Ep 1 (Step 000140): Train loss 5.417, Val loss 5.529\n",
      "Ep 1 (Step 000150): Train loss 5.432, Val loss 5.503\n",
      "Ep 1 (Step 000160): Train loss 5.348, Val loss 5.471\n",
      "Ep 1 (Step 000170): Train loss 5.396, Val loss 5.429\n",
      "Ep 1 (Step 000180): Train loss 5.307, Val loss 5.426\n",
      "Ep 1 (Step 000190): Train loss 5.301, Val loss 5.425\n",
      "Ep 1 (Step 000200): Train loss 5.162, Val loss 5.396\n",
      "Ep 1 (Step 000210): Train loss 5.235, Val loss 5.386\n",
      "Ep 1 (Step 000220): Train loss 5.196, Val loss 5.351\n",
      "Ep 1 (Step 000230): Train loss 5.283, Val loss 5.349\n",
      "Ep 1 (Step 000240): Train loss 5.203, Val loss 5.306\n",
      "Ep 1 (Step 000250): Train loss 5.228, Val loss 5.289\n",
      "Ep 1 (Step 000260): Train loss 5.161, Val loss 5.300\n",
      "Ep 1 (Step 000270): Train loss 5.110, Val loss 5.285\n",
      "Ep 1 (Step 000280): Train loss 5.120, Val loss 5.268\n",
      "Ep 1 (Step 000290): Train loss 5.099, Val loss 5.242\n",
      "Ep 1 (Step 000300): Train loss 5.014, Val loss 5.230\n",
      "Ep 1 (Step 000310): Train loss 5.090, Val loss 5.227\n",
      "Ep 1 (Step 000320): Train loss 5.038, Val loss 5.207\n",
      "Ep 1 (Step 000330): Train loss 5.034, Val loss 5.200\n",
      "Ep 1 (Step 000340): Train loss 5.023, Val loss 5.192\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.1918\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.398, Val loss 8.342\n",
      "Ep 1 (Step 000010): Train loss 6.580, Val loss 6.580\n",
      "Ep 1 (Step 000020): Train loss 6.511, Val loss 6.502\n",
      "Ep 1 (Step 000030): Train loss 6.252, Val loss 6.287\n",
      "Ep 1 (Step 000040): Train loss 6.049, Val loss 6.135\n",
      "Ep 1 (Step 000050): Train loss 5.945, Val loss 5.992\n",
      "Ep 1 (Step 000060): Train loss 5.806, Val loss 5.885\n",
      "Ep 1 (Step 000070): Train loss 5.774, Val loss 5.792\n",
      "Ep 1 (Step 000080): Train loss 5.707, Val loss 5.747\n",
      "Ep 1 (Step 000090): Train loss 5.652, Val loss 5.701\n",
      "Ep 1 (Step 000100): Train loss 5.569, Val loss 5.656\n",
      "Ep 1 (Step 000110): Train loss 5.472, Val loss 5.612\n",
      "Ep 1 (Step 000120): Train loss 5.396, Val loss 5.558\n",
      "Ep 1 (Step 000130): Train loss 5.414, Val loss 5.515\n",
      "Ep 1 (Step 000140): Train loss 5.387, Val loss 5.478\n",
      "Ep 1 (Step 000150): Train loss 5.351, Val loss 5.467\n",
      "Ep 1 (Step 000160): Train loss 5.332, Val loss 5.439\n",
      "Ep 1 (Step 000170): Train loss 5.331, Val loss 5.421\n",
      "Ep 1 (Step 000180): Train loss 5.344, Val loss 5.430\n",
      "Ep 1 (Step 000190): Train loss 5.297, Val loss 5.407\n",
      "Ep 1 (Step 000200): Train loss 5.226, Val loss 5.421\n",
      "Ep 1 (Step 000210): Train loss 5.269, Val loss 5.379\n",
      "Ep 1 (Step 000220): Train loss 5.249, Val loss 5.376\n",
      "Ep 1 (Step 000230): Train loss 5.239, Val loss 5.350\n",
      "Ep 1 (Step 000240): Train loss 5.234, Val loss 5.327\n",
      "Ep 1 (Step 000250): Train loss 5.213, Val loss 5.300\n",
      "Ep 1 (Step 000260): Train loss 5.139, Val loss 5.281\n",
      "Ep 1 (Step 000270): Train loss 5.129, Val loss 5.283\n",
      "Ep 1 (Step 000280): Train loss 5.166, Val loss 5.237\n",
      "Ep 1 (Step 000290): Train loss 5.047, Val loss 5.246\n",
      "Ep 1 (Step 000300): Train loss 5.140, Val loss 5.265\n",
      "Ep 1 (Step 000310): Train loss 5.148, Val loss 5.227\n",
      "Ep 1 (Step 000320): Train loss 5.043, Val loss 5.240\n",
      "Ep 1 (Step 000330): Train loss 5.083, Val loss 5.205\n",
      "Ep 1 (Step 000340): Train loss 5.070, Val loss 5.211\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2106\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.348, Val loss 8.308\n",
      "Ep 1 (Step 000010): Train loss 6.560, Val loss 6.570\n",
      "Ep 1 (Step 000020): Train loss 6.406, Val loss 6.458\n",
      "Ep 1 (Step 000030): Train loss 6.351, Val loss 6.264\n",
      "Ep 1 (Step 000040): Train loss 6.097, Val loss 6.113\n",
      "Ep 1 (Step 000050): Train loss 5.918, Val loss 5.949\n",
      "Ep 1 (Step 000060): Train loss 5.878, Val loss 5.877\n",
      "Ep 1 (Step 000070): Train loss 5.728, Val loss 5.796\n",
      "Ep 1 (Step 000080): Train loss 5.641, Val loss 5.703\n",
      "Ep 1 (Step 000090): Train loss 5.586, Val loss 5.671\n",
      "Ep 1 (Step 000100): Train loss 5.506, Val loss 5.615\n",
      "Ep 1 (Step 000110): Train loss 5.513, Val loss 5.578\n",
      "Ep 1 (Step 000120): Train loss 5.396, Val loss 5.538\n",
      "Ep 1 (Step 000130): Train loss 5.447, Val loss 5.536\n",
      "Ep 1 (Step 000140): Train loss 5.458, Val loss 5.520\n",
      "Ep 1 (Step 000150): Train loss 5.372, Val loss 5.481\n",
      "Ep 1 (Step 000160): Train loss 5.315, Val loss 5.442\n",
      "Ep 1 (Step 000170): Train loss 5.299, Val loss 5.434\n",
      "Ep 1 (Step 000180): Train loss 5.311, Val loss 5.399\n",
      "Ep 1 (Step 000190): Train loss 5.354, Val loss 5.402\n",
      "Ep 1 (Step 000200): Train loss 5.324, Val loss 5.376\n",
      "Ep 1 (Step 000210): Train loss 5.203, Val loss 5.368\n",
      "Ep 1 (Step 000220): Train loss 5.311, Val loss 5.345\n",
      "Ep 1 (Step 000230): Train loss 5.291, Val loss 5.320\n",
      "Ep 1 (Step 000240): Train loss 5.295, Val loss 5.317\n",
      "Ep 1 (Step 000250): Train loss 5.138, Val loss 5.306\n",
      "Ep 1 (Step 000260): Train loss 5.224, Val loss 5.288\n",
      "Ep 1 (Step 000270): Train loss 5.108, Val loss 5.277\n",
      "Ep 1 (Step 000280): Train loss 5.096, Val loss 5.274\n",
      "Ep 1 (Step 000290): Train loss 5.065, Val loss 5.261\n",
      "Ep 1 (Step 000300): Train loss 5.003, Val loss 5.248\n",
      "Ep 1 (Step 000310): Train loss 5.147, Val loss 5.249\n",
      "Ep 1 (Step 000320): Train loss 5.004, Val loss 5.219\n",
      "Ep 1 (Step 000330): Train loss 5.064, Val loss 5.237\n",
      "Ep 1 (Step 000340): Train loss 5.079, Val loss 5.250\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2500\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.328, Val loss 8.301\n",
      "Ep 1 (Step 000010): Train loss 6.600, Val loss 6.553\n",
      "Ep 1 (Step 000020): Train loss 6.489, Val loss 6.486\n",
      "Ep 1 (Step 000030): Train loss 6.385, Val loss 6.337\n",
      "Ep 1 (Step 000040): Train loss 6.177, Val loss 6.162\n",
      "Ep 1 (Step 000050): Train loss 6.074, Val loss 6.054\n",
      "Ep 1 (Step 000060): Train loss 5.902, Val loss 5.907\n",
      "Ep 1 (Step 000070): Train loss 5.791, Val loss 5.819\n",
      "Ep 1 (Step 000080): Train loss 5.672, Val loss 5.775\n",
      "Ep 1 (Step 000090): Train loss 5.652, Val loss 5.701\n",
      "Ep 1 (Step 000100): Train loss 5.566, Val loss 5.662\n",
      "Ep 1 (Step 000110): Train loss 5.444, Val loss 5.578\n",
      "Ep 1 (Step 000120): Train loss 5.433, Val loss 5.557\n",
      "Ep 1 (Step 000130): Train loss 5.468, Val loss 5.538\n",
      "Ep 1 (Step 000140): Train loss 5.442, Val loss 5.512\n",
      "Ep 1 (Step 000150): Train loss 5.496, Val loss 5.470\n",
      "Ep 1 (Step 000160): Train loss 5.398, Val loss 5.447\n",
      "Ep 1 (Step 000170): Train loss 5.322, Val loss 5.423\n",
      "Ep 1 (Step 000180): Train loss 5.230, Val loss 5.405\n",
      "Ep 1 (Step 000190): Train loss 5.293, Val loss 5.400\n",
      "Ep 1 (Step 000200): Train loss 5.292, Val loss 5.397\n",
      "Ep 1 (Step 000210): Train loss 5.198, Val loss 5.362\n",
      "Ep 1 (Step 000220): Train loss 5.174, Val loss 5.364\n",
      "Ep 1 (Step 000230): Train loss 5.111, Val loss 5.340\n",
      "Ep 1 (Step 000240): Train loss 5.153, Val loss 5.309\n",
      "Ep 1 (Step 000250): Train loss 5.203, Val loss 5.313\n",
      "Ep 1 (Step 000260): Train loss 5.270, Val loss 5.287\n",
      "Ep 1 (Step 000270): Train loss 5.168, Val loss 5.303\n",
      "Ep 1 (Step 000280): Train loss 5.167, Val loss 5.285\n",
      "Ep 1 (Step 000290): Train loss 5.133, Val loss 5.267\n",
      "Ep 1 (Step 000300): Train loss 5.164, Val loss 5.251\n",
      "Ep 1 (Step 000310): Train loss 5.162, Val loss 5.232\n",
      "Ep 1 (Step 000320): Train loss 5.026, Val loss 5.246\n",
      "Ep 1 (Step 000330): Train loss 5.152, Val loss 5.219\n",
      "Ep 1 (Step 000340): Train loss 5.190, Val loss 5.238\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2381\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.384, Val loss 8.327\n",
      "Ep 1 (Step 000010): Train loss 6.582, Val loss 6.568\n",
      "Ep 1 (Step 000020): Train loss 6.451, Val loss 6.486\n",
      "Ep 1 (Step 000030): Train loss 6.369, Val loss 6.294\n",
      "Ep 1 (Step 000040): Train loss 6.134, Val loss 6.121\n",
      "Ep 1 (Step 000050): Train loss 5.955, Val loss 5.989\n",
      "Ep 1 (Step 000060): Train loss 5.863, Val loss 5.887\n",
      "Ep 1 (Step 000070): Train loss 5.756, Val loss 5.788\n",
      "Ep 1 (Step 000080): Train loss 5.651, Val loss 5.716\n",
      "Ep 1 (Step 000090): Train loss 5.633, Val loss 5.658\n",
      "Ep 1 (Step 000100): Train loss 5.528, Val loss 5.593\n",
      "Ep 1 (Step 000110): Train loss 5.564, Val loss 5.577\n",
      "Ep 1 (Step 000120): Train loss 5.455, Val loss 5.524\n",
      "Ep 1 (Step 000130): Train loss 5.488, Val loss 5.528\n",
      "Ep 1 (Step 000140): Train loss 5.424, Val loss 5.509\n",
      "Ep 1 (Step 000150): Train loss 5.325, Val loss 5.477\n",
      "Ep 1 (Step 000160): Train loss 5.310, Val loss 5.443\n",
      "Ep 1 (Step 000170): Train loss 5.333, Val loss 5.422\n",
      "Ep 1 (Step 000180): Train loss 5.318, Val loss 5.406\n",
      "Ep 1 (Step 000190): Train loss 5.240, Val loss 5.372\n",
      "Ep 1 (Step 000200): Train loss 5.240, Val loss 5.362\n",
      "Ep 1 (Step 000210): Train loss 5.183, Val loss 5.371\n",
      "Ep 1 (Step 000220): Train loss 5.191, Val loss 5.356\n",
      "Ep 1 (Step 000230): Train loss 5.178, Val loss 5.307\n",
      "Ep 1 (Step 000240): Train loss 5.181, Val loss 5.284\n",
      "Ep 1 (Step 000250): Train loss 5.226, Val loss 5.292\n",
      "Ep 1 (Step 000260): Train loss 5.150, Val loss 5.282\n",
      "Ep 1 (Step 000270): Train loss 5.094, Val loss 5.264\n",
      "Ep 1 (Step 000280): Train loss 5.096, Val loss 5.231\n",
      "Ep 1 (Step 000290): Train loss 5.105, Val loss 5.228\n",
      "Ep 1 (Step 000300): Train loss 5.113, Val loss 5.239\n",
      "Ep 1 (Step 000310): Train loss 5.121, Val loss 5.240\n",
      "Ep 1 (Step 000320): Train loss 5.123, Val loss 5.222\n",
      "Ep 1 (Step 000330): Train loss 5.052, Val loss 5.200\n",
      "Ep 1 (Step 000340): Train loss 5.144, Val loss 5.175\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.1752\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.287, Val loss 8.291\n",
      "Ep 1 (Step 000010): Train loss 6.582, Val loss 6.577\n",
      "Ep 1 (Step 000020): Train loss 6.509, Val loss 6.491\n",
      "Ep 1 (Step 000030): Train loss 6.356, Val loss 6.344\n",
      "Ep 1 (Step 000040): Train loss 6.115, Val loss 6.156\n",
      "Ep 1 (Step 000050): Train loss 5.947, Val loss 6.004\n",
      "Ep 1 (Step 000060): Train loss 5.834, Val loss 5.916\n",
      "Ep 1 (Step 000070): Train loss 5.812, Val loss 5.816\n",
      "Ep 1 (Step 000080): Train loss 5.679, Val loss 5.742\n",
      "Ep 1 (Step 000090): Train loss 5.613, Val loss 5.668\n",
      "Ep 1 (Step 000100): Train loss 5.641, Val loss 5.624\n",
      "Ep 1 (Step 000110): Train loss 5.575, Val loss 5.588\n",
      "Ep 1 (Step 000120): Train loss 5.538, Val loss 5.540\n",
      "Ep 1 (Step 000130): Train loss 5.444, Val loss 5.526\n",
      "Ep 1 (Step 000140): Train loss 5.420, Val loss 5.480\n",
      "Ep 1 (Step 000150): Train loss 5.394, Val loss 5.454\n",
      "Ep 1 (Step 000160): Train loss 5.402, Val loss 5.425\n",
      "Ep 1 (Step 000170): Train loss 5.350, Val loss 5.411\n",
      "Ep 1 (Step 000180): Train loss 5.285, Val loss 5.403\n",
      "Ep 1 (Step 000190): Train loss 5.262, Val loss 5.383\n",
      "Ep 1 (Step 000200): Train loss 5.252, Val loss 5.355\n",
      "Ep 1 (Step 000210): Train loss 5.224, Val loss 5.363\n",
      "Ep 1 (Step 000220): Train loss 5.102, Val loss 5.336\n",
      "Ep 1 (Step 000230): Train loss 5.110, Val loss 5.331\n",
      "Ep 1 (Step 000240): Train loss 5.139, Val loss 5.347\n",
      "Ep 1 (Step 000250): Train loss 5.074, Val loss 5.282\n",
      "Ep 1 (Step 000260): Train loss 5.265, Val loss 5.280\n",
      "Ep 1 (Step 000270): Train loss 5.178, Val loss 5.252\n",
      "Ep 1 (Step 000280): Train loss 5.082, Val loss 5.235\n",
      "Ep 1 (Step 000290): Train loss 5.064, Val loss 5.224\n",
      "Ep 1 (Step 000300): Train loss 5.090, Val loss 5.236\n",
      "Ep 1 (Step 000310): Train loss 5.023, Val loss 5.236\n",
      "Ep 1 (Step 000320): Train loss 5.003, Val loss 5.194\n",
      "Ep 1 (Step 000330): Train loss 5.098, Val loss 5.208\n",
      "Ep 1 (Step 000340): Train loss 5.048, Val loss 5.170\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.1698\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.474, Val loss 8.435\n",
      "Ep 1 (Step 000010): Train loss 6.608, Val loss 6.531\n",
      "Ep 1 (Step 000020): Train loss 6.508, Val loss 6.444\n",
      "Ep 1 (Step 000030): Train loss 6.598, Val loss 6.529\n",
      "Ep 1 (Step 000040): Train loss 6.095, Val loss 6.143\n",
      "Ep 1 (Step 000050): Train loss 5.987, Val loss 5.961\n",
      "Ep 1 (Step 000060): Train loss 5.827, Val loss 5.846\n",
      "Ep 1 (Step 000070): Train loss 5.783, Val loss 5.777\n",
      "Ep 1 (Step 000080): Train loss 5.757, Val loss 5.704\n",
      "Ep 1 (Step 000090): Train loss 5.616, Val loss 5.634\n",
      "Ep 1 (Step 000100): Train loss 5.515, Val loss 5.597\n",
      "Ep 1 (Step 000110): Train loss 5.465, Val loss 5.559\n",
      "Ep 1 (Step 000120): Train loss 5.414, Val loss 5.546\n",
      "Ep 1 (Step 000130): Train loss 5.414, Val loss 5.508\n",
      "Ep 1 (Step 000140): Train loss 5.410, Val loss 5.477\n",
      "Ep 1 (Step 000150): Train loss 5.404, Val loss 5.426\n",
      "Ep 1 (Step 000160): Train loss 5.280, Val loss 5.420\n",
      "Ep 1 (Step 000170): Train loss 5.246, Val loss 5.404\n",
      "Ep 1 (Step 000180): Train loss 5.351, Val loss 5.403\n",
      "Ep 1 (Step 000190): Train loss 5.310, Val loss 5.368\n",
      "Ep 1 (Step 000200): Train loss 5.188, Val loss 5.341\n",
      "Ep 1 (Step 000210): Train loss 5.267, Val loss 5.328\n",
      "Ep 1 (Step 000220): Train loss 5.141, Val loss 5.323\n",
      "Ep 1 (Step 000230): Train loss 5.198, Val loss 5.318\n",
      "Ep 1 (Step 000240): Train loss 5.175, Val loss 5.303\n",
      "Ep 1 (Step 000250): Train loss 5.156, Val loss 5.277\n",
      "Ep 1 (Step 000260): Train loss 5.109, Val loss 5.277\n",
      "Ep 1 (Step 000270): Train loss 5.054, Val loss 5.262\n",
      "Ep 1 (Step 000280): Train loss 5.067, Val loss 5.249\n",
      "Ep 1 (Step 000290): Train loss 5.088, Val loss 5.248\n",
      "Ep 1 (Step 000300): Train loss 5.081, Val loss 5.269\n",
      "Ep 1 (Step 000310): Train loss 5.114, Val loss 5.234\n",
      "Ep 1 (Step 000320): Train loss 5.218, Val loss 5.237\n",
      "Ep 1 (Step 000330): Train loss 5.063, Val loss 5.220\n",
      "Ep 1 (Step 000340): Train loss 5.075, Val loss 5.221\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 12, 'drop_rate': 0.2, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2213\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.578, Val loss 8.534\n",
      "Ep 1 (Step 000010): Train loss 7.033, Val loss 6.991\n",
      "Ep 1 (Step 000020): Train loss 6.436, Val loss 6.453\n",
      "Ep 1 (Step 000030): Train loss 6.352, Val loss 6.362\n",
      "Ep 1 (Step 000040): Train loss 6.209, Val loss 6.207\n",
      "Ep 1 (Step 000050): Train loss 6.106, Val loss 6.104\n",
      "Ep 1 (Step 000060): Train loss 5.966, Val loss 5.992\n",
      "Ep 1 (Step 000070): Train loss 5.877, Val loss 5.862\n",
      "Ep 1 (Step 000080): Train loss 5.782, Val loss 5.814\n",
      "Ep 1 (Step 000090): Train loss 5.717, Val loss 5.748\n",
      "Ep 1 (Step 000100): Train loss 5.554, Val loss 5.703\n",
      "Ep 1 (Step 000110): Train loss 5.499, Val loss 5.647\n",
      "Ep 1 (Step 000120): Train loss 5.509, Val loss 5.625\n",
      "Ep 1 (Step 000130): Train loss 5.493, Val loss 5.595\n",
      "Ep 1 (Step 000140): Train loss 5.427, Val loss 5.546\n",
      "Ep 1 (Step 000150): Train loss 5.499, Val loss 5.490\n",
      "Ep 1 (Step 000160): Train loss 5.349, Val loss 5.482\n",
      "Ep 1 (Step 000170): Train loss 5.380, Val loss 5.444\n",
      "Ep 1 (Step 000180): Train loss 5.362, Val loss 5.428\n",
      "Ep 1 (Step 000190): Train loss 5.333, Val loss 5.418\n",
      "Ep 1 (Step 000200): Train loss 5.303, Val loss 5.403\n",
      "Ep 1 (Step 000210): Train loss 5.331, Val loss 5.376\n",
      "Ep 1 (Step 000220): Train loss 5.256, Val loss 5.378\n",
      "Ep 1 (Step 000230): Train loss 5.305, Val loss 5.375\n",
      "Ep 1 (Step 000240): Train loss 5.218, Val loss 5.342\n",
      "Ep 1 (Step 000250): Train loss 5.247, Val loss 5.365\n",
      "Ep 1 (Step 000260): Train loss 5.197, Val loss 5.353\n",
      "Ep 1 (Step 000270): Train loss 5.191, Val loss 5.312\n",
      "Ep 1 (Step 000280): Train loss 5.215, Val loss 5.323\n",
      "Ep 1 (Step 000290): Train loss 5.204, Val loss 5.300\n",
      "Ep 1 (Step 000300): Train loss 5.207, Val loss 5.300\n",
      "Ep 1 (Step 000310): Train loss 5.207, Val loss 5.299\n",
      "Ep 1 (Step 000320): Train loss 5.154, Val loss 5.266\n",
      "Ep 1 (Step 000330): Train loss 5.152, Val loss 5.263\n",
      "Ep 1 (Step 000340): Train loss 5.190, Val loss 5.263\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2633\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.515, Val loss 8.489\n",
      "Ep 1 (Step 000010): Train loss 6.979, Val loss 6.979\n",
      "Ep 1 (Step 000020): Train loss 6.533, Val loss 6.477\n",
      "Ep 1 (Step 000030): Train loss 6.396, Val loss 6.376\n",
      "Ep 1 (Step 000040): Train loss 6.261, Val loss 6.237\n",
      "Ep 1 (Step 000050): Train loss 6.088, Val loss 6.087\n",
      "Ep 1 (Step 000060): Train loss 5.927, Val loss 5.974\n",
      "Ep 1 (Step 000070): Train loss 5.865, Val loss 5.873\n",
      "Ep 1 (Step 000080): Train loss 5.792, Val loss 5.824\n",
      "Ep 1 (Step 000090): Train loss 5.679, Val loss 5.755\n",
      "Ep 1 (Step 000100): Train loss 5.606, Val loss 5.691\n",
      "Ep 1 (Step 000110): Train loss 5.624, Val loss 5.667\n",
      "Ep 1 (Step 000120): Train loss 5.556, Val loss 5.600\n",
      "Ep 1 (Step 000130): Train loss 5.543, Val loss 5.580\n",
      "Ep 1 (Step 000140): Train loss 5.497, Val loss 5.540\n",
      "Ep 1 (Step 000150): Train loss 5.387, Val loss 5.528\n",
      "Ep 1 (Step 000160): Train loss 5.348, Val loss 5.504\n",
      "Ep 1 (Step 000170): Train loss 5.317, Val loss 5.478\n",
      "Ep 1 (Step 000180): Train loss 5.362, Val loss 5.465\n",
      "Ep 1 (Step 000190): Train loss 5.409, Val loss 5.451\n",
      "Ep 1 (Step 000200): Train loss 5.337, Val loss 5.440\n",
      "Ep 1 (Step 000210): Train loss 5.351, Val loss 5.416\n",
      "Ep 1 (Step 000220): Train loss 5.305, Val loss 5.402\n",
      "Ep 1 (Step 000230): Train loss 5.321, Val loss 5.400\n",
      "Ep 1 (Step 000240): Train loss 5.198, Val loss 5.358\n",
      "Ep 1 (Step 000250): Train loss 5.284, Val loss 5.347\n",
      "Ep 1 (Step 000260): Train loss 5.260, Val loss 5.343\n",
      "Ep 1 (Step 000270): Train loss 5.144, Val loss 5.331\n",
      "Ep 1 (Step 000280): Train loss 5.253, Val loss 5.321\n",
      "Ep 1 (Step 000290): Train loss 5.183, Val loss 5.307\n",
      "Ep 1 (Step 000300): Train loss 5.253, Val loss 5.302\n",
      "Ep 1 (Step 000310): Train loss 5.163, Val loss 5.293\n",
      "Ep 1 (Step 000320): Train loss 5.189, Val loss 5.268\n",
      "Ep 1 (Step 000330): Train loss 5.177, Val loss 5.261\n",
      "Ep 1 (Step 000340): Train loss 5.147, Val loss 5.271\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2711\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.541, Val loss 8.499\n",
      "Ep 1 (Step 000010): Train loss 6.952, Val loss 6.949\n",
      "Ep 1 (Step 000020): Train loss 6.475, Val loss 6.462\n",
      "Ep 1 (Step 000030): Train loss 6.292, Val loss 6.367\n",
      "Ep 1 (Step 000040): Train loss 6.275, Val loss 6.251\n",
      "Ep 1 (Step 000050): Train loss 6.117, Val loss 6.076\n",
      "Ep 1 (Step 000060): Train loss 6.040, Val loss 5.997\n",
      "Ep 1 (Step 000070): Train loss 5.908, Val loss 5.899\n",
      "Ep 1 (Step 000080): Train loss 5.824, Val loss 5.825\n",
      "Ep 1 (Step 000090): Train loss 5.708, Val loss 5.775\n",
      "Ep 1 (Step 000100): Train loss 5.627, Val loss 5.710\n",
      "Ep 1 (Step 000110): Train loss 5.596, Val loss 5.663\n",
      "Ep 1 (Step 000120): Train loss 5.662, Val loss 5.629\n",
      "Ep 1 (Step 000130): Train loss 5.591, Val loss 5.585\n",
      "Ep 1 (Step 000140): Train loss 5.451, Val loss 5.562\n",
      "Ep 1 (Step 000150): Train loss 5.333, Val loss 5.518\n",
      "Ep 1 (Step 000160): Train loss 5.445, Val loss 5.508\n",
      "Ep 1 (Step 000170): Train loss 5.401, Val loss 5.478\n",
      "Ep 1 (Step 000180): Train loss 5.355, Val loss 5.452\n",
      "Ep 1 (Step 000190): Train loss 5.378, Val loss 5.444\n",
      "Ep 1 (Step 000200): Train loss 5.404, Val loss 5.446\n",
      "Ep 1 (Step 000210): Train loss 5.331, Val loss 5.421\n",
      "Ep 1 (Step 000220): Train loss 5.379, Val loss 5.395\n",
      "Ep 1 (Step 000230): Train loss 5.264, Val loss 5.385\n",
      "Ep 1 (Step 000240): Train loss 5.249, Val loss 5.366\n",
      "Ep 1 (Step 000250): Train loss 5.274, Val loss 5.363\n",
      "Ep 1 (Step 000260): Train loss 5.152, Val loss 5.347\n",
      "Ep 1 (Step 000270): Train loss 5.183, Val loss 5.332\n",
      "Ep 1 (Step 000280): Train loss 5.190, Val loss 5.318\n",
      "Ep 1 (Step 000290): Train loss 5.200, Val loss 5.311\n",
      "Ep 1 (Step 000300): Train loss 5.138, Val loss 5.298\n",
      "Ep 1 (Step 000310): Train loss 5.179, Val loss 5.294\n",
      "Ep 1 (Step 000320): Train loss 5.157, Val loss 5.278\n",
      "Ep 1 (Step 000330): Train loss 5.195, Val loss 5.259\n",
      "Ep 1 (Step 000340): Train loss 5.167, Val loss 5.255\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2547\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.515, Val loss 8.492\n",
      "Ep 1 (Step 000010): Train loss 6.999, Val loss 6.976\n",
      "Ep 1 (Step 000020): Train loss 6.548, Val loss 6.478\n",
      "Ep 1 (Step 000030): Train loss 6.443, Val loss 6.387\n",
      "Ep 1 (Step 000040): Train loss 6.325, Val loss 6.284\n",
      "Ep 1 (Step 000050): Train loss 6.119, Val loss 6.154\n",
      "Ep 1 (Step 000060): Train loss 5.991, Val loss 6.043\n",
      "Ep 1 (Step 000070): Train loss 5.880, Val loss 5.926\n",
      "Ep 1 (Step 000080): Train loss 5.858, Val loss 5.851\n",
      "Ep 1 (Step 000090): Train loss 5.775, Val loss 5.783\n",
      "Ep 1 (Step 000100): Train loss 5.596, Val loss 5.709\n",
      "Ep 1 (Step 000110): Train loss 5.578, Val loss 5.660\n",
      "Ep 1 (Step 000120): Train loss 5.603, Val loss 5.636\n",
      "Ep 1 (Step 000130): Train loss 5.549, Val loss 5.571\n",
      "Ep 1 (Step 000140): Train loss 5.430, Val loss 5.544\n",
      "Ep 1 (Step 000150): Train loss 5.473, Val loss 5.516\n",
      "Ep 1 (Step 000160): Train loss 5.452, Val loss 5.512\n",
      "Ep 1 (Step 000170): Train loss 5.341, Val loss 5.476\n",
      "Ep 1 (Step 000180): Train loss 5.495, Val loss 5.470\n",
      "Ep 1 (Step 000190): Train loss 5.435, Val loss 5.436\n",
      "Ep 1 (Step 000200): Train loss 5.358, Val loss 5.417\n",
      "Ep 1 (Step 000210): Train loss 5.277, Val loss 5.401\n",
      "Ep 1 (Step 000220): Train loss 5.321, Val loss 5.389\n",
      "Ep 1 (Step 000230): Train loss 5.237, Val loss 5.353\n",
      "Ep 1 (Step 000240): Train loss 5.240, Val loss 5.348\n",
      "Ep 1 (Step 000250): Train loss 5.207, Val loss 5.339\n",
      "Ep 1 (Step 000260): Train loss 5.353, Val loss 5.310\n",
      "Ep 1 (Step 000270): Train loss 5.263, Val loss 5.318\n",
      "Ep 1 (Step 000280): Train loss 5.174, Val loss 5.310\n",
      "Ep 1 (Step 000290): Train loss 5.188, Val loss 5.298\n",
      "Ep 1 (Step 000300): Train loss 5.228, Val loss 5.277\n",
      "Ep 1 (Step 000310): Train loss 5.149, Val loss 5.268\n",
      "Ep 1 (Step 000320): Train loss 5.112, Val loss 5.248\n",
      "Ep 1 (Step 000330): Train loss 5.167, Val loss 5.262\n",
      "Ep 1 (Step 000340): Train loss 5.139, Val loss 5.238\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2384\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.492, Val loss 8.446\n",
      "Ep 1 (Step 000010): Train loss 6.970, Val loss 6.952\n",
      "Ep 1 (Step 000020): Train loss 6.494, Val loss 6.474\n",
      "Ep 1 (Step 000030): Train loss 6.351, Val loss 6.395\n",
      "Ep 1 (Step 000040): Train loss 6.290, Val loss 6.309\n",
      "Ep 1 (Step 000050): Train loss 6.098, Val loss 6.157\n",
      "Ep 1 (Step 000060): Train loss 5.971, Val loss 6.043\n",
      "Ep 1 (Step 000070): Train loss 5.951, Val loss 5.932\n",
      "Ep 1 (Step 000080): Train loss 5.857, Val loss 5.856\n",
      "Ep 1 (Step 000090): Train loss 5.718, Val loss 5.801\n",
      "Ep 1 (Step 000100): Train loss 5.657, Val loss 5.733\n",
      "Ep 1 (Step 000110): Train loss 5.574, Val loss 5.706\n",
      "Ep 1 (Step 000120): Train loss 5.509, Val loss 5.650\n",
      "Ep 1 (Step 000130): Train loss 5.546, Val loss 5.598\n",
      "Ep 1 (Step 000140): Train loss 5.458, Val loss 5.561\n",
      "Ep 1 (Step 000150): Train loss 5.529, Val loss 5.544\n",
      "Ep 1 (Step 000160): Train loss 5.425, Val loss 5.519\n",
      "Ep 1 (Step 000170): Train loss 5.441, Val loss 5.507\n",
      "Ep 1 (Step 000180): Train loss 5.288, Val loss 5.471\n",
      "Ep 1 (Step 000190): Train loss 5.412, Val loss 5.450\n",
      "Ep 1 (Step 000200): Train loss 5.366, Val loss 5.445\n",
      "Ep 1 (Step 000210): Train loss 5.236, Val loss 5.412\n",
      "Ep 1 (Step 000220): Train loss 5.178, Val loss 5.385\n",
      "Ep 1 (Step 000230): Train loss 5.215, Val loss 5.354\n",
      "Ep 1 (Step 000240): Train loss 5.211, Val loss 5.364\n",
      "Ep 1 (Step 000250): Train loss 5.212, Val loss 5.333\n",
      "Ep 1 (Step 000260): Train loss 5.186, Val loss 5.339\n",
      "Ep 1 (Step 000270): Train loss 5.289, Val loss 5.320\n",
      "Ep 1 (Step 000280): Train loss 5.174, Val loss 5.289\n",
      "Ep 1 (Step 000290): Train loss 5.168, Val loss 5.293\n",
      "Ep 1 (Step 000300): Train loss 5.188, Val loss 5.269\n",
      "Ep 1 (Step 000310): Train loss 5.187, Val loss 5.276\n",
      "Ep 1 (Step 000320): Train loss 5.193, Val loss 5.246\n",
      "Ep 1 (Step 000330): Train loss 5.180, Val loss 5.234\n",
      "Ep 1 (Step 000340): Train loss 5.126, Val loss 5.228\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2284\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.510, Val loss 8.473\n",
      "Ep 1 (Step 000010): Train loss 6.960, Val loss 6.941\n",
      "Ep 1 (Step 000020): Train loss 6.552, Val loss 6.471\n",
      "Ep 1 (Step 000030): Train loss 6.397, Val loss 6.387\n",
      "Ep 1 (Step 000040): Train loss 6.221, Val loss 6.267\n",
      "Ep 1 (Step 000050): Train loss 6.166, Val loss 6.142\n",
      "Ep 1 (Step 000060): Train loss 6.023, Val loss 6.054\n",
      "Ep 1 (Step 000070): Train loss 5.869, Val loss 5.940\n",
      "Ep 1 (Step 000080): Train loss 5.728, Val loss 5.856\n",
      "Ep 1 (Step 000090): Train loss 5.823, Val loss 5.792\n",
      "Ep 1 (Step 000100): Train loss 5.680, Val loss 5.724\n",
      "Ep 1 (Step 000110): Train loss 5.607, Val loss 5.678\n",
      "Ep 1 (Step 000120): Train loss 5.604, Val loss 5.623\n",
      "Ep 1 (Step 000130): Train loss 5.529, Val loss 5.585\n",
      "Ep 1 (Step 000140): Train loss 5.389, Val loss 5.563\n",
      "Ep 1 (Step 000150): Train loss 5.488, Val loss 5.527\n",
      "Ep 1 (Step 000160): Train loss 5.356, Val loss 5.514\n",
      "Ep 1 (Step 000170): Train loss 5.531, Val loss 5.471\n",
      "Ep 1 (Step 000180): Train loss 5.332, Val loss 5.445\n",
      "Ep 1 (Step 000190): Train loss 5.385, Val loss 5.427\n",
      "Ep 1 (Step 000200): Train loss 5.321, Val loss 5.410\n",
      "Ep 1 (Step 000210): Train loss 5.296, Val loss 5.394\n",
      "Ep 1 (Step 000220): Train loss 5.289, Val loss 5.390\n",
      "Ep 1 (Step 000230): Train loss 5.221, Val loss 5.361\n",
      "Ep 1 (Step 000240): Train loss 5.341, Val loss 5.357\n",
      "Ep 1 (Step 000250): Train loss 5.167, Val loss 5.337\n",
      "Ep 1 (Step 000260): Train loss 5.215, Val loss 5.330\n",
      "Ep 1 (Step 000270): Train loss 5.201, Val loss 5.317\n",
      "Ep 1 (Step 000280): Train loss 5.201, Val loss 5.295\n",
      "Ep 1 (Step 000290): Train loss 5.142, Val loss 5.310\n",
      "Ep 1 (Step 000300): Train loss 5.172, Val loss 5.268\n",
      "Ep 1 (Step 000310): Train loss 5.156, Val loss 5.252\n",
      "Ep 1 (Step 000320): Train loss 5.185, Val loss 5.237\n",
      "Ep 1 (Step 000330): Train loss 5.064, Val loss 5.230\n",
      "Ep 1 (Step 000340): Train loss 5.064, Val loss 5.222\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2218\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.292, Val loss 8.289\n",
      "Ep 1 (Step 000010): Train loss 6.566, Val loss 6.556\n",
      "Ep 1 (Step 000020): Train loss 6.562, Val loss 6.497\n",
      "Ep 1 (Step 000030): Train loss 6.390, Val loss 6.369\n",
      "Ep 1 (Step 000040): Train loss 6.187, Val loss 6.169\n",
      "Ep 1 (Step 000050): Train loss 6.082, Val loss 6.039\n",
      "Ep 1 (Step 000060): Train loss 5.917, Val loss 5.933\n",
      "Ep 1 (Step 000070): Train loss 5.767, Val loss 5.858\n",
      "Ep 1 (Step 000080): Train loss 5.737, Val loss 5.803\n",
      "Ep 1 (Step 000090): Train loss 5.651, Val loss 5.732\n",
      "Ep 1 (Step 000100): Train loss 5.556, Val loss 5.678\n",
      "Ep 1 (Step 000110): Train loss 5.585, Val loss 5.636\n",
      "Ep 1 (Step 000120): Train loss 5.647, Val loss 5.628\n",
      "Ep 1 (Step 000130): Train loss 5.556, Val loss 5.605\n",
      "Ep 1 (Step 000140): Train loss 5.451, Val loss 5.565\n",
      "Ep 1 (Step 000150): Train loss 5.489, Val loss 5.532\n",
      "Ep 1 (Step 000160): Train loss 5.466, Val loss 5.511\n",
      "Ep 1 (Step 000170): Train loss 5.361, Val loss 5.449\n",
      "Ep 1 (Step 000180): Train loss 5.259, Val loss 5.458\n",
      "Ep 1 (Step 000190): Train loss 5.311, Val loss 5.438\n",
      "Ep 1 (Step 000200): Train loss 5.295, Val loss 5.427\n",
      "Ep 1 (Step 000210): Train loss 5.332, Val loss 5.415\n",
      "Ep 1 (Step 000220): Train loss 5.283, Val loss 5.432\n",
      "Ep 1 (Step 000230): Train loss 5.303, Val loss 5.384\n",
      "Ep 1 (Step 000240): Train loss 5.227, Val loss 5.368\n",
      "Ep 1 (Step 000250): Train loss 5.229, Val loss 5.359\n",
      "Ep 1 (Step 000260): Train loss 5.260, Val loss 5.355\n",
      "Ep 1 (Step 000270): Train loss 5.225, Val loss 5.344\n",
      "Ep 1 (Step 000280): Train loss 5.185, Val loss 5.314\n",
      "Ep 1 (Step 000290): Train loss 5.165, Val loss 5.320\n",
      "Ep 1 (Step 000300): Train loss 5.358, Val loss 5.297\n",
      "Ep 1 (Step 000310): Train loss 5.152, Val loss 5.314\n",
      "Ep 1 (Step 000320): Train loss 5.216, Val loss 5.300\n",
      "Ep 1 (Step 000330): Train loss 5.250, Val loss 5.300\n",
      "Ep 1 (Step 000340): Train loss 5.095, Val loss 5.272\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2720\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.226, Val loss 8.256\n",
      "Ep 1 (Step 000010): Train loss 6.531, Val loss 6.546\n",
      "Ep 1 (Step 000020): Train loss 6.512, Val loss 6.539\n",
      "Ep 1 (Step 000030): Train loss 6.233, Val loss 6.325\n",
      "Ep 1 (Step 000040): Train loss 6.298, Val loss 6.190\n",
      "Ep 1 (Step 000050): Train loss 6.034, Val loss 6.085\n",
      "Ep 1 (Step 000060): Train loss 5.931, Val loss 5.933\n",
      "Ep 1 (Step 000070): Train loss 5.881, Val loss 5.852\n",
      "Ep 1 (Step 000080): Train loss 5.698, Val loss 5.805\n",
      "Ep 1 (Step 000090): Train loss 5.712, Val loss 5.781\n",
      "Ep 1 (Step 000100): Train loss 5.578, Val loss 5.702\n",
      "Ep 1 (Step 000110): Train loss 5.589, Val loss 5.687\n",
      "Ep 1 (Step 000120): Train loss 5.628, Val loss 5.631\n",
      "Ep 1 (Step 000130): Train loss 5.528, Val loss 5.593\n",
      "Ep 1 (Step 000140): Train loss 5.474, Val loss 5.570\n",
      "Ep 1 (Step 000150): Train loss 5.382, Val loss 5.559\n",
      "Ep 1 (Step 000160): Train loss 5.466, Val loss 5.520\n",
      "Ep 1 (Step 000170): Train loss 5.428, Val loss 5.513\n",
      "Ep 1 (Step 000180): Train loss 5.349, Val loss 5.488\n",
      "Ep 1 (Step 000190): Train loss 5.331, Val loss 5.472\n",
      "Ep 1 (Step 000200): Train loss 5.346, Val loss 5.471\n",
      "Ep 1 (Step 000210): Train loss 5.309, Val loss 5.465\n",
      "Ep 1 (Step 000220): Train loss 5.276, Val loss 5.439\n",
      "Ep 1 (Step 000230): Train loss 5.229, Val loss 5.412\n",
      "Ep 1 (Step 000240): Train loss 5.308, Val loss 5.386\n",
      "Ep 1 (Step 000250): Train loss 5.250, Val loss 5.401\n",
      "Ep 1 (Step 000260): Train loss 5.256, Val loss 5.379\n",
      "Ep 1 (Step 000270): Train loss 5.224, Val loss 5.373\n",
      "Ep 1 (Step 000280): Train loss 5.232, Val loss 5.334\n",
      "Ep 1 (Step 000290): Train loss 5.235, Val loss 5.337\n",
      "Ep 1 (Step 000300): Train loss 5.233, Val loss 5.316\n",
      "Ep 1 (Step 000310): Train loss 5.199, Val loss 5.352\n",
      "Ep 1 (Step 000320): Train loss 5.088, Val loss 5.337\n",
      "Ep 1 (Step 000330): Train loss 5.249, Val loss 5.309\n",
      "Ep 1 (Step 000340): Train loss 5.227, Val loss 5.291\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2907\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.269, Val loss 8.200\n",
      "Ep 1 (Step 000010): Train loss 6.639, Val loss 6.594\n",
      "Ep 1 (Step 000020): Train loss 6.538, Val loss 6.503\n",
      "Ep 1 (Step 000030): Train loss 6.348, Val loss 6.330\n",
      "Ep 1 (Step 000040): Train loss 6.198, Val loss 6.176\n",
      "Ep 1 (Step 000050): Train loss 5.994, Val loss 6.045\n",
      "Ep 1 (Step 000060): Train loss 5.892, Val loss 5.901\n",
      "Ep 1 (Step 000070): Train loss 5.763, Val loss 5.822\n",
      "Ep 1 (Step 000080): Train loss 5.747, Val loss 5.768\n",
      "Ep 1 (Step 000090): Train loss 5.732, Val loss 5.720\n",
      "Ep 1 (Step 000100): Train loss 5.569, Val loss 5.656\n",
      "Ep 1 (Step 000110): Train loss 5.602, Val loss 5.621\n",
      "Ep 1 (Step 000120): Train loss 5.553, Val loss 5.574\n",
      "Ep 1 (Step 000130): Train loss 5.493, Val loss 5.567\n",
      "Ep 1 (Step 000140): Train loss 5.446, Val loss 5.549\n",
      "Ep 1 (Step 000150): Train loss 5.469, Val loss 5.501\n",
      "Ep 1 (Step 000160): Train loss 5.386, Val loss 5.504\n",
      "Ep 1 (Step 000170): Train loss 5.356, Val loss 5.478\n",
      "Ep 1 (Step 000180): Train loss 5.306, Val loss 5.471\n",
      "Ep 1 (Step 000190): Train loss 5.380, Val loss 5.454\n",
      "Ep 1 (Step 000200): Train loss 5.248, Val loss 5.432\n",
      "Ep 1 (Step 000210): Train loss 5.277, Val loss 5.432\n",
      "Ep 1 (Step 000220): Train loss 5.316, Val loss 5.417\n",
      "Ep 1 (Step 000230): Train loss 5.290, Val loss 5.421\n",
      "Ep 1 (Step 000240): Train loss 5.252, Val loss 5.375\n",
      "Ep 1 (Step 000250): Train loss 5.224, Val loss 5.369\n",
      "Ep 1 (Step 000260): Train loss 5.248, Val loss 5.370\n",
      "Ep 1 (Step 000270): Train loss 5.177, Val loss 5.348\n",
      "Ep 1 (Step 000280): Train loss 5.214, Val loss 5.345\n",
      "Ep 1 (Step 000290): Train loss 5.231, Val loss 5.340\n",
      "Ep 1 (Step 000300): Train loss 5.249, Val loss 5.331\n",
      "Ep 1 (Step 000310): Train loss 5.151, Val loss 5.310\n",
      "Ep 1 (Step 000320): Train loss 5.127, Val loss 5.307\n",
      "Ep 1 (Step 000330): Train loss 5.177, Val loss 5.291\n",
      "Ep 1 (Step 000340): Train loss 5.238, Val loss 5.280\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2804\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.344, Val loss 8.335\n",
      "Ep 1 (Step 000010): Train loss 6.591, Val loss 6.576\n",
      "Ep 1 (Step 000020): Train loss 6.454, Val loss 6.501\n",
      "Ep 1 (Step 000030): Train loss 6.312, Val loss 6.361\n",
      "Ep 1 (Step 000040): Train loss 6.152, Val loss 6.186\n",
      "Ep 1 (Step 000050): Train loss 5.971, Val loss 6.042\n",
      "Ep 1 (Step 000060): Train loss 5.881, Val loss 5.946\n",
      "Ep 1 (Step 000070): Train loss 5.842, Val loss 5.855\n",
      "Ep 1 (Step 000080): Train loss 5.692, Val loss 5.793\n",
      "Ep 1 (Step 000090): Train loss 5.657, Val loss 5.714\n",
      "Ep 1 (Step 000100): Train loss 5.626, Val loss 5.664\n",
      "Ep 1 (Step 000110): Train loss 5.569, Val loss 5.610\n",
      "Ep 1 (Step 000120): Train loss 5.509, Val loss 5.582\n",
      "Ep 1 (Step 000130): Train loss 5.538, Val loss 5.557\n",
      "Ep 1 (Step 000140): Train loss 5.423, Val loss 5.502\n",
      "Ep 1 (Step 000150): Train loss 5.491, Val loss 5.482\n",
      "Ep 1 (Step 000160): Train loss 5.402, Val loss 5.470\n",
      "Ep 1 (Step 000170): Train loss 5.354, Val loss 5.446\n",
      "Ep 1 (Step 000180): Train loss 5.344, Val loss 5.420\n",
      "Ep 1 (Step 000190): Train loss 5.375, Val loss 5.452\n",
      "Ep 1 (Step 000200): Train loss 5.294, Val loss 5.405\n",
      "Ep 1 (Step 000210): Train loss 5.336, Val loss 5.399\n",
      "Ep 1 (Step 000220): Train loss 5.243, Val loss 5.365\n",
      "Ep 1 (Step 000230): Train loss 5.231, Val loss 5.359\n",
      "Ep 1 (Step 000240): Train loss 5.330, Val loss 5.347\n",
      "Ep 1 (Step 000250): Train loss 5.288, Val loss 5.352\n",
      "Ep 1 (Step 000260): Train loss 5.293, Val loss 5.339\n",
      "Ep 1 (Step 000270): Train loss 5.266, Val loss 5.326\n",
      "Ep 1 (Step 000280): Train loss 5.382, Val loss 5.315\n",
      "Ep 1 (Step 000290): Train loss 5.265, Val loss 5.315\n",
      "Ep 1 (Step 000300): Train loss 5.219, Val loss 5.311\n",
      "Ep 1 (Step 000310): Train loss 5.168, Val loss 5.280\n",
      "Ep 1 (Step 000320): Train loss 5.183, Val loss 5.259\n",
      "Ep 1 (Step 000330): Train loss 5.129, Val loss 5.262\n",
      "Ep 1 (Step 000340): Train loss 5.103, Val loss 5.264\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2640\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.399, Val loss 8.347\n",
      "Ep 1 (Step 000010): Train loss 6.612, Val loss 6.550\n",
      "Ep 1 (Step 000020): Train loss 6.462, Val loss 6.445\n",
      "Ep 1 (Step 000030): Train loss 6.388, Val loss 6.309\n",
      "Ep 1 (Step 000040): Train loss 6.144, Val loss 6.163\n",
      "Ep 1 (Step 000050): Train loss 5.987, Val loss 6.027\n",
      "Ep 1 (Step 000060): Train loss 5.893, Val loss 5.953\n",
      "Ep 1 (Step 000070): Train loss 5.783, Val loss 5.846\n",
      "Ep 1 (Step 000080): Train loss 5.638, Val loss 5.778\n",
      "Ep 1 (Step 000090): Train loss 5.641, Val loss 5.709\n",
      "Ep 1 (Step 000100): Train loss 5.637, Val loss 5.673\n",
      "Ep 1 (Step 000110): Train loss 5.611, Val loss 5.621\n",
      "Ep 1 (Step 000120): Train loss 5.519, Val loss 5.586\n",
      "Ep 1 (Step 000130): Train loss 5.486, Val loss 5.553\n",
      "Ep 1 (Step 000140): Train loss 5.482, Val loss 5.532\n",
      "Ep 1 (Step 000150): Train loss 5.478, Val loss 5.512\n",
      "Ep 1 (Step 000160): Train loss 5.392, Val loss 5.482\n",
      "Ep 1 (Step 000170): Train loss 5.350, Val loss 5.474\n",
      "Ep 1 (Step 000180): Train loss 5.466, Val loss 5.461\n",
      "Ep 1 (Step 000190): Train loss 5.348, Val loss 5.441\n",
      "Ep 1 (Step 000200): Train loss 5.303, Val loss 5.416\n",
      "Ep 1 (Step 000210): Train loss 5.371, Val loss 5.401\n",
      "Ep 1 (Step 000220): Train loss 5.173, Val loss 5.388\n",
      "Ep 1 (Step 000230): Train loss 5.284, Val loss 5.382\n",
      "Ep 1 (Step 000240): Train loss 5.286, Val loss 5.335\n",
      "Ep 1 (Step 000250): Train loss 5.178, Val loss 5.355\n",
      "Ep 1 (Step 000260): Train loss 5.101, Val loss 5.333\n",
      "Ep 1 (Step 000270): Train loss 5.218, Val loss 5.309\n",
      "Ep 1 (Step 000280): Train loss 5.163, Val loss 5.320\n",
      "Ep 1 (Step 000290): Train loss 5.171, Val loss 5.312\n",
      "Ep 1 (Step 000300): Train loss 5.107, Val loss 5.282\n",
      "Ep 1 (Step 000310): Train loss 5.199, Val loss 5.261\n",
      "Ep 1 (Step 000320): Train loss 5.170, Val loss 5.253\n",
      "Ep 1 (Step 000330): Train loss 5.277, Val loss 5.261\n",
      "Ep 1 (Step 000340): Train loss 5.041, Val loss 5.254\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2536\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.273, Val loss 8.218\n",
      "Ep 1 (Step 000010): Train loss 6.630, Val loss 6.555\n",
      "Ep 1 (Step 000020): Train loss 6.538, Val loss 6.485\n",
      "Ep 1 (Step 000030): Train loss 6.376, Val loss 6.338\n",
      "Ep 1 (Step 000040): Train loss 6.264, Val loss 6.267\n",
      "Ep 1 (Step 000050): Train loss 6.079, Val loss 6.080\n",
      "Ep 1 (Step 000060): Train loss 5.858, Val loss 5.934\n",
      "Ep 1 (Step 000070): Train loss 5.747, Val loss 5.877\n",
      "Ep 1 (Step 000080): Train loss 5.766, Val loss 5.815\n",
      "Ep 1 (Step 000090): Train loss 5.721, Val loss 5.756\n",
      "Ep 1 (Step 000100): Train loss 5.629, Val loss 5.666\n",
      "Ep 1 (Step 000110): Train loss 5.535, Val loss 5.611\n",
      "Ep 1 (Step 000120): Train loss 5.469, Val loss 5.574\n",
      "Ep 1 (Step 000130): Train loss 5.447, Val loss 5.535\n",
      "Ep 1 (Step 000140): Train loss 5.464, Val loss 5.511\n",
      "Ep 1 (Step 000150): Train loss 5.455, Val loss 5.513\n",
      "Ep 1 (Step 000160): Train loss 5.400, Val loss 5.459\n",
      "Ep 1 (Step 000170): Train loss 5.382, Val loss 5.433\n",
      "Ep 1 (Step 000180): Train loss 5.306, Val loss 5.420\n",
      "Ep 1 (Step 000190): Train loss 5.315, Val loss 5.422\n",
      "Ep 1 (Step 000200): Train loss 5.286, Val loss 5.402\n",
      "Ep 1 (Step 000210): Train loss 5.219, Val loss 5.376\n",
      "Ep 1 (Step 000220): Train loss 5.215, Val loss 5.360\n",
      "Ep 1 (Step 000230): Train loss 5.278, Val loss 5.358\n",
      "Ep 1 (Step 000240): Train loss 5.272, Val loss 5.337\n",
      "Ep 1 (Step 000250): Train loss 5.228, Val loss 5.329\n",
      "Ep 1 (Step 000260): Train loss 5.130, Val loss 5.319\n",
      "Ep 1 (Step 000270): Train loss 5.130, Val loss 5.314\n",
      "Ep 1 (Step 000280): Train loss 5.246, Val loss 5.312\n",
      "Ep 1 (Step 000290): Train loss 5.206, Val loss 5.309\n",
      "Ep 1 (Step 000300): Train loss 5.127, Val loss 5.308\n",
      "Ep 1 (Step 000310): Train loss 5.111, Val loss 5.301\n",
      "Ep 1 (Step 000320): Train loss 5.124, Val loss 5.268\n",
      "Ep 1 (Step 000330): Train loss 5.139, Val loss 5.258\n",
      "Ep 1 (Step 000340): Train loss 5.142, Val loss 5.257\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': False, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2568\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.475, Val loss 8.483\n",
      "Ep 1 (Step 000010): Train loss 6.937, Val loss 6.909\n",
      "Ep 1 (Step 000020): Train loss 6.452, Val loss 6.448\n",
      "Ep 1 (Step 000030): Train loss 6.411, Val loss 6.364\n",
      "Ep 1 (Step 000040): Train loss 6.248, Val loss 6.235\n",
      "Ep 1 (Step 000050): Train loss 6.044, Val loss 6.103\n",
      "Ep 1 (Step 000060): Train loss 6.039, Val loss 6.026\n",
      "Ep 1 (Step 000070): Train loss 5.855, Val loss 5.913\n",
      "Ep 1 (Step 000080): Train loss 5.749, Val loss 5.812\n",
      "Ep 1 (Step 000090): Train loss 5.723, Val loss 5.752\n",
      "Ep 1 (Step 000100): Train loss 5.631, Val loss 5.715\n",
      "Ep 1 (Step 000110): Train loss 5.665, Val loss 5.686\n",
      "Ep 1 (Step 000120): Train loss 5.594, Val loss 5.612\n",
      "Ep 1 (Step 000130): Train loss 5.443, Val loss 5.607\n",
      "Ep 1 (Step 000140): Train loss 5.485, Val loss 5.565\n",
      "Ep 1 (Step 000150): Train loss 5.489, Val loss 5.532\n",
      "Ep 1 (Step 000160): Train loss 5.371, Val loss 5.498\n",
      "Ep 1 (Step 000170): Train loss 5.439, Val loss 5.479\n",
      "Ep 1 (Step 000180): Train loss 5.299, Val loss 5.455\n",
      "Ep 1 (Step 000190): Train loss 5.437, Val loss 5.432\n",
      "Ep 1 (Step 000200): Train loss 5.305, Val loss 5.429\n",
      "Ep 1 (Step 000210): Train loss 5.317, Val loss 5.408\n",
      "Ep 1 (Step 000220): Train loss 5.306, Val loss 5.403\n",
      "Ep 1 (Step 000230): Train loss 5.292, Val loss 5.393\n",
      "Ep 1 (Step 000240): Train loss 5.223, Val loss 5.390\n",
      "Ep 1 (Step 000250): Train loss 5.234, Val loss 5.358\n",
      "Ep 1 (Step 000260): Train loss 5.263, Val loss 5.329\n",
      "Ep 1 (Step 000270): Train loss 5.246, Val loss 5.314\n",
      "Ep 1 (Step 000280): Train loss 5.273, Val loss 5.312\n",
      "Ep 1 (Step 000290): Train loss 5.265, Val loss 5.309\n",
      "Ep 1 (Step 000300): Train loss 5.159, Val loss 5.306\n",
      "Ep 1 (Step 000310): Train loss 5.168, Val loss 5.278\n",
      "Ep 1 (Step 000320): Train loss 5.093, Val loss 5.272\n",
      "Ep 1 (Step 000330): Train loss 5.065, Val loss 5.265\n",
      "Ep 1 (Step 000340): Train loss 5.171, Val loss 5.266\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2656\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.526, Val loss 8.465\n",
      "Ep 1 (Step 000010): Train loss 7.011, Val loss 6.967\n",
      "Ep 1 (Step 000020): Train loss 6.509, Val loss 6.459\n",
      "Ep 1 (Step 000030): Train loss 6.397, Val loss 6.367\n",
      "Ep 1 (Step 000040): Train loss 6.293, Val loss 6.265\n",
      "Ep 1 (Step 000050): Train loss 6.121, Val loss 6.103\n",
      "Ep 1 (Step 000060): Train loss 5.938, Val loss 6.014\n",
      "Ep 1 (Step 000070): Train loss 5.942, Val loss 5.910\n",
      "Ep 1 (Step 000080): Train loss 5.788, Val loss 5.834\n",
      "Ep 1 (Step 000090): Train loss 5.751, Val loss 5.766\n",
      "Ep 1 (Step 000100): Train loss 5.720, Val loss 5.714\n",
      "Ep 1 (Step 000110): Train loss 5.702, Val loss 5.678\n",
      "Ep 1 (Step 000120): Train loss 5.553, Val loss 5.628\n",
      "Ep 1 (Step 000130): Train loss 5.532, Val loss 5.588\n",
      "Ep 1 (Step 000140): Train loss 5.444, Val loss 5.543\n",
      "Ep 1 (Step 000150): Train loss 5.474, Val loss 5.546\n",
      "Ep 1 (Step 000160): Train loss 5.429, Val loss 5.494\n",
      "Ep 1 (Step 000170): Train loss 5.389, Val loss 5.475\n",
      "Ep 1 (Step 000180): Train loss 5.383, Val loss 5.461\n",
      "Ep 1 (Step 000190): Train loss 5.414, Val loss 5.437\n",
      "Ep 1 (Step 000200): Train loss 5.302, Val loss 5.403\n",
      "Ep 1 (Step 000210): Train loss 5.302, Val loss 5.404\n",
      "Ep 1 (Step 000220): Train loss 5.310, Val loss 5.386\n",
      "Ep 1 (Step 000230): Train loss 5.348, Val loss 5.348\n",
      "Ep 1 (Step 000240): Train loss 5.295, Val loss 5.356\n",
      "Ep 1 (Step 000250): Train loss 5.257, Val loss 5.354\n",
      "Ep 1 (Step 000260): Train loss 5.261, Val loss 5.330\n",
      "Ep 1 (Step 000270): Train loss 5.252, Val loss 5.331\n",
      "Ep 1 (Step 000280): Train loss 5.149, Val loss 5.310\n",
      "Ep 1 (Step 000290): Train loss 5.238, Val loss 5.294\n",
      "Ep 1 (Step 000300): Train loss 5.131, Val loss 5.302\n",
      "Ep 1 (Step 000310): Train loss 5.163, Val loss 5.294\n",
      "Ep 1 (Step 000320): Train loss 5.046, Val loss 5.278\n",
      "Ep 1 (Step 000330): Train loss 5.222, Val loss 5.251\n",
      "Ep 1 (Step 000340): Train loss 5.167, Val loss 5.246\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2464\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.444, Val loss 8.450\n",
      "Ep 1 (Step 000010): Train loss 6.993, Val loss 6.949\n",
      "Ep 1 (Step 000020): Train loss 6.511, Val loss 6.471\n",
      "Ep 1 (Step 000030): Train loss 6.362, Val loss 6.399\n",
      "Ep 1 (Step 000040): Train loss 6.228, Val loss 6.253\n",
      "Ep 1 (Step 000050): Train loss 6.158, Val loss 6.115\n",
      "Ep 1 (Step 000060): Train loss 5.928, Val loss 6.001\n",
      "Ep 1 (Step 000070): Train loss 5.782, Val loss 5.901\n",
      "Ep 1 (Step 000080): Train loss 5.782, Val loss 5.842\n",
      "Ep 1 (Step 000090): Train loss 5.724, Val loss 5.772\n",
      "Ep 1 (Step 000100): Train loss 5.624, Val loss 5.710\n",
      "Ep 1 (Step 000110): Train loss 5.596, Val loss 5.672\n",
      "Ep 1 (Step 000120): Train loss 5.635, Val loss 5.643\n",
      "Ep 1 (Step 000130): Train loss 5.620, Val loss 5.583\n",
      "Ep 1 (Step 000140): Train loss 5.537, Val loss 5.549\n",
      "Ep 1 (Step 000150): Train loss 5.448, Val loss 5.534\n",
      "Ep 1 (Step 000160): Train loss 5.315, Val loss 5.525\n",
      "Ep 1 (Step 000170): Train loss 5.388, Val loss 5.473\n",
      "Ep 1 (Step 000180): Train loss 5.378, Val loss 5.456\n",
      "Ep 1 (Step 000190): Train loss 5.312, Val loss 5.461\n",
      "Ep 1 (Step 000200): Train loss 5.373, Val loss 5.425\n",
      "Ep 1 (Step 000210): Train loss 5.370, Val loss 5.408\n",
      "Ep 1 (Step 000220): Train loss 5.309, Val loss 5.379\n",
      "Ep 1 (Step 000230): Train loss 5.335, Val loss 5.376\n",
      "Ep 1 (Step 000240): Train loss 5.244, Val loss 5.351\n",
      "Ep 1 (Step 000250): Train loss 5.223, Val loss 5.345\n",
      "Ep 1 (Step 000260): Train loss 5.277, Val loss 5.334\n",
      "Ep 1 (Step 000270): Train loss 5.254, Val loss 5.314\n",
      "Ep 1 (Step 000280): Train loss 5.256, Val loss 5.299\n",
      "Ep 1 (Step 000290): Train loss 5.204, Val loss 5.294\n",
      "Ep 1 (Step 000300): Train loss 5.152, Val loss 5.284\n",
      "Ep 1 (Step 000310): Train loss 5.124, Val loss 5.286\n",
      "Ep 1 (Step 000320): Train loss 5.136, Val loss 5.281\n",
      "Ep 1 (Step 000330): Train loss 5.280, Val loss 5.261\n",
      "Ep 1 (Step 000340): Train loss 5.165, Val loss 5.253\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2530\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.506, Val loss 8.479\n",
      "Ep 1 (Step 000010): Train loss 6.988, Val loss 6.973\n",
      "Ep 1 (Step 000020): Train loss 6.502, Val loss 6.467\n",
      "Ep 1 (Step 000030): Train loss 6.295, Val loss 6.367\n",
      "Ep 1 (Step 000040): Train loss 6.179, Val loss 6.257\n",
      "Ep 1 (Step 000050): Train loss 6.120, Val loss 6.166\n",
      "Ep 1 (Step 000060): Train loss 5.978, Val loss 6.016\n",
      "Ep 1 (Step 000070): Train loss 5.842, Val loss 5.921\n",
      "Ep 1 (Step 000080): Train loss 5.782, Val loss 5.835\n",
      "Ep 1 (Step 000090): Train loss 5.690, Val loss 5.772\n",
      "Ep 1 (Step 000100): Train loss 5.617, Val loss 5.720\n",
      "Ep 1 (Step 000110): Train loss 5.596, Val loss 5.648\n",
      "Ep 1 (Step 000120): Train loss 5.592, Val loss 5.614\n",
      "Ep 1 (Step 000130): Train loss 5.443, Val loss 5.567\n",
      "Ep 1 (Step 000140): Train loss 5.488, Val loss 5.524\n",
      "Ep 1 (Step 000150): Train loss 5.437, Val loss 5.497\n",
      "Ep 1 (Step 000160): Train loss 5.362, Val loss 5.481\n",
      "Ep 1 (Step 000170): Train loss 5.332, Val loss 5.456\n",
      "Ep 1 (Step 000180): Train loss 5.366, Val loss 5.458\n",
      "Ep 1 (Step 000190): Train loss 5.393, Val loss 5.442\n",
      "Ep 1 (Step 000200): Train loss 5.346, Val loss 5.409\n",
      "Ep 1 (Step 000210): Train loss 5.299, Val loss 5.386\n",
      "Ep 1 (Step 000220): Train loss 5.316, Val loss 5.388\n",
      "Ep 1 (Step 000230): Train loss 5.405, Val loss 5.364\n",
      "Ep 1 (Step 000240): Train loss 5.289, Val loss 5.331\n",
      "Ep 1 (Step 000250): Train loss 5.237, Val loss 5.319\n",
      "Ep 1 (Step 000260): Train loss 5.182, Val loss 5.311\n",
      "Ep 1 (Step 000270): Train loss 5.234, Val loss 5.286\n",
      "Ep 1 (Step 000280): Train loss 5.221, Val loss 5.280\n",
      "Ep 1 (Step 000290): Train loss 5.187, Val loss 5.270\n",
      "Ep 1 (Step 000300): Train loss 5.183, Val loss 5.261\n",
      "Ep 1 (Step 000310): Train loss 5.115, Val loss 5.242\n",
      "Ep 1 (Step 000320): Train loss 5.157, Val loss 5.235\n",
      "Ep 1 (Step 000330): Train loss 5.043, Val loss 5.244\n",
      "Ep 1 (Step 000340): Train loss 5.133, Val loss 5.232\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2325\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.485, Val loss 8.461\n",
      "Ep 1 (Step 000010): Train loss 7.013, Val loss 6.964\n",
      "Ep 1 (Step 000020): Train loss 6.504, Val loss 6.463\n",
      "Ep 1 (Step 000030): Train loss 6.414, Val loss 6.389\n",
      "Ep 1 (Step 000040): Train loss 6.363, Val loss 6.274\n",
      "Ep 1 (Step 000050): Train loss 6.091, Val loss 6.107\n",
      "Ep 1 (Step 000060): Train loss 6.005, Val loss 5.973\n",
      "Ep 1 (Step 000070): Train loss 5.863, Val loss 5.899\n",
      "Ep 1 (Step 000080): Train loss 5.714, Val loss 5.844\n",
      "Ep 1 (Step 000090): Train loss 5.665, Val loss 5.765\n",
      "Ep 1 (Step 000100): Train loss 5.636, Val loss 5.732\n",
      "Ep 1 (Step 000110): Train loss 5.622, Val loss 5.661\n",
      "Ep 1 (Step 000120): Train loss 5.535, Val loss 5.606\n",
      "Ep 1 (Step 000130): Train loss 5.526, Val loss 5.564\n",
      "Ep 1 (Step 000140): Train loss 5.483, Val loss 5.547\n",
      "Ep 1 (Step 000150): Train loss 5.346, Val loss 5.522\n",
      "Ep 1 (Step 000160): Train loss 5.337, Val loss 5.505\n",
      "Ep 1 (Step 000170): Train loss 5.388, Val loss 5.479\n",
      "Ep 1 (Step 000180): Train loss 5.458, Val loss 5.435\n",
      "Ep 1 (Step 000190): Train loss 5.323, Val loss 5.428\n",
      "Ep 1 (Step 000200): Train loss 5.246, Val loss 5.410\n",
      "Ep 1 (Step 000210): Train loss 5.253, Val loss 5.378\n",
      "Ep 1 (Step 000220): Train loss 5.231, Val loss 5.366\n",
      "Ep 1 (Step 000230): Train loss 5.243, Val loss 5.348\n",
      "Ep 1 (Step 000240): Train loss 5.230, Val loss 5.340\n",
      "Ep 1 (Step 000250): Train loss 5.258, Val loss 5.328\n",
      "Ep 1 (Step 000260): Train loss 5.229, Val loss 5.315\n",
      "Ep 1 (Step 000270): Train loss 5.176, Val loss 5.292\n",
      "Ep 1 (Step 000280): Train loss 5.179, Val loss 5.285\n",
      "Ep 1 (Step 000290): Train loss 5.154, Val loss 5.284\n",
      "Ep 1 (Step 000300): Train loss 5.135, Val loss 5.268\n",
      "Ep 1 (Step 000310): Train loss 5.121, Val loss 5.262\n",
      "Ep 1 (Step 000320): Train loss 5.076, Val loss 5.246\n",
      "Ep 1 (Step 000330): Train loss 5.063, Val loss 5.228\n",
      "Ep 1 (Step 000340): Train loss 4.989, Val loss 5.217\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2174\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.524, Val loss 8.458\n",
      "Ep 1 (Step 000010): Train loss 6.956, Val loss 6.912\n",
      "Ep 1 (Step 000020): Train loss 6.499, Val loss 6.457\n",
      "Ep 1 (Step 000030): Train loss 6.480, Val loss 6.397\n",
      "Ep 1 (Step 000040): Train loss 6.269, Val loss 6.310\n",
      "Ep 1 (Step 000050): Train loss 6.200, Val loss 6.193\n",
      "Ep 1 (Step 000060): Train loss 6.001, Val loss 6.040\n",
      "Ep 1 (Step 000070): Train loss 5.934, Val loss 5.950\n",
      "Ep 1 (Step 000080): Train loss 5.771, Val loss 5.833\n",
      "Ep 1 (Step 000090): Train loss 5.739, Val loss 5.783\n",
      "Ep 1 (Step 000100): Train loss 5.609, Val loss 5.718\n",
      "Ep 1 (Step 000110): Train loss 5.571, Val loss 5.666\n",
      "Ep 1 (Step 000120): Train loss 5.602, Val loss 5.623\n",
      "Ep 1 (Step 000130): Train loss 5.598, Val loss 5.609\n",
      "Ep 1 (Step 000140): Train loss 5.548, Val loss 5.565\n",
      "Ep 1 (Step 000150): Train loss 5.492, Val loss 5.551\n",
      "Ep 1 (Step 000160): Train loss 5.415, Val loss 5.514\n",
      "Ep 1 (Step 000170): Train loss 5.345, Val loss 5.480\n",
      "Ep 1 (Step 000180): Train loss 5.328, Val loss 5.452\n",
      "Ep 1 (Step 000190): Train loss 5.386, Val loss 5.437\n",
      "Ep 1 (Step 000200): Train loss 5.438, Val loss 5.442\n",
      "Ep 1 (Step 000210): Train loss 5.278, Val loss 5.404\n",
      "Ep 1 (Step 000220): Train loss 5.301, Val loss 5.376\n",
      "Ep 1 (Step 000230): Train loss 5.237, Val loss 5.358\n",
      "Ep 1 (Step 000240): Train loss 5.218, Val loss 5.342\n",
      "Ep 1 (Step 000250): Train loss 5.131, Val loss 5.334\n",
      "Ep 1 (Step 000260): Train loss 5.293, Val loss 5.321\n",
      "Ep 1 (Step 000270): Train loss 5.209, Val loss 5.305\n",
      "Ep 1 (Step 000280): Train loss 5.217, Val loss 5.286\n",
      "Ep 1 (Step 000290): Train loss 5.179, Val loss 5.261\n",
      "Ep 1 (Step 000300): Train loss 5.135, Val loss 5.284\n",
      "Ep 1 (Step 000310): Train loss 5.153, Val loss 5.256\n",
      "Ep 1 (Step 000320): Train loss 5.097, Val loss 5.251\n",
      "Ep 1 (Step 000330): Train loss 5.106, Val loss 5.238\n",
      "Ep 1 (Step 000340): Train loss 5.108, Val loss 5.238\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0002, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2380\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.230, Val loss 8.215\n",
      "Ep 1 (Step 000010): Train loss 6.567, Val loss 6.548\n",
      "Ep 1 (Step 000020): Train loss 6.505, Val loss 6.492\n",
      "Ep 1 (Step 000030): Train loss 6.289, Val loss 6.340\n",
      "Ep 1 (Step 000040): Train loss 6.128, Val loss 6.191\n",
      "Ep 1 (Step 000050): Train loss 5.928, Val loss 6.032\n",
      "Ep 1 (Step 000060): Train loss 5.904, Val loss 5.962\n",
      "Ep 1 (Step 000070): Train loss 5.805, Val loss 5.862\n",
      "Ep 1 (Step 000080): Train loss 5.792, Val loss 5.815\n",
      "Ep 1 (Step 000090): Train loss 5.659, Val loss 5.740\n",
      "Ep 1 (Step 000100): Train loss 5.581, Val loss 5.719\n",
      "Ep 1 (Step 000110): Train loss 5.580, Val loss 5.662\n",
      "Ep 1 (Step 000120): Train loss 5.530, Val loss 5.614\n",
      "Ep 1 (Step 000130): Train loss 5.619, Val loss 5.608\n",
      "Ep 1 (Step 000140): Train loss 5.481, Val loss 5.568\n",
      "Ep 1 (Step 000150): Train loss 5.428, Val loss 5.560\n",
      "Ep 1 (Step 000160): Train loss 5.375, Val loss 5.524\n",
      "Ep 1 (Step 000170): Train loss 5.454, Val loss 5.534\n",
      "Ep 1 (Step 000180): Train loss 5.374, Val loss 5.485\n",
      "Ep 1 (Step 000190): Train loss 5.383, Val loss 5.488\n",
      "Ep 1 (Step 000200): Train loss 5.356, Val loss 5.469\n",
      "Ep 1 (Step 000210): Train loss 5.347, Val loss 5.428\n",
      "Ep 1 (Step 000220): Train loss 5.315, Val loss 5.390\n",
      "Ep 1 (Step 000230): Train loss 5.398, Val loss 5.402\n",
      "Ep 1 (Step 000240): Train loss 5.308, Val loss 5.400\n",
      "Ep 1 (Step 000250): Train loss 5.198, Val loss 5.375\n",
      "Ep 1 (Step 000260): Train loss 5.294, Val loss 5.372\n",
      "Ep 1 (Step 000270): Train loss 5.290, Val loss 5.346\n",
      "Ep 1 (Step 000280): Train loss 5.353, Val loss 5.338\n",
      "Ep 1 (Step 000290): Train loss 5.196, Val loss 5.314\n",
      "Ep 1 (Step 000300): Train loss 5.239, Val loss 5.303\n",
      "Ep 1 (Step 000310): Train loss 5.158, Val loss 5.310\n",
      "Ep 1 (Step 000320): Train loss 5.239, Val loss 5.284\n",
      "Ep 1 (Step 000330): Train loss 5.215, Val loss 5.274\n",
      "Ep 1 (Step 000340): Train loss 5.156, Val loss 5.296\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2956\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.355, Val loss 8.280\n",
      "Ep 1 (Step 000010): Train loss 6.568, Val loss 6.531\n",
      "Ep 1 (Step 000020): Train loss 6.453, Val loss 6.472\n",
      "Ep 1 (Step 000030): Train loss 6.304, Val loss 6.285\n",
      "Ep 1 (Step 000040): Train loss 6.214, Val loss 6.191\n",
      "Ep 1 (Step 000050): Train loss 6.069, Val loss 6.031\n",
      "Ep 1 (Step 000060): Train loss 5.920, Val loss 5.962\n",
      "Ep 1 (Step 000070): Train loss 5.834, Val loss 5.857\n",
      "Ep 1 (Step 000080): Train loss 5.782, Val loss 5.770\n",
      "Ep 1 (Step 000090): Train loss 5.634, Val loss 5.688\n",
      "Ep 1 (Step 000100): Train loss 5.614, Val loss 5.645\n",
      "Ep 1 (Step 000110): Train loss 5.593, Val loss 5.628\n",
      "Ep 1 (Step 000120): Train loss 5.515, Val loss 5.560\n",
      "Ep 1 (Step 000130): Train loss 5.421, Val loss 5.537\n",
      "Ep 1 (Step 000140): Train loss 5.418, Val loss 5.546\n",
      "Ep 1 (Step 000150): Train loss 5.512, Val loss 5.529\n",
      "Ep 1 (Step 000160): Train loss 5.367, Val loss 5.480\n",
      "Ep 1 (Step 000170): Train loss 5.425, Val loss 5.501\n",
      "Ep 1 (Step 000180): Train loss 5.416, Val loss 5.473\n",
      "Ep 1 (Step 000190): Train loss 5.364, Val loss 5.442\n",
      "Ep 1 (Step 000200): Train loss 5.314, Val loss 5.425\n",
      "Ep 1 (Step 000210): Train loss 5.324, Val loss 5.404\n",
      "Ep 1 (Step 000220): Train loss 5.334, Val loss 5.388\n",
      "Ep 1 (Step 000230): Train loss 5.161, Val loss 5.394\n",
      "Ep 1 (Step 000240): Train loss 5.234, Val loss 5.362\n",
      "Ep 1 (Step 000250): Train loss 5.208, Val loss 5.370\n",
      "Ep 1 (Step 000260): Train loss 5.162, Val loss 5.344\n",
      "Ep 1 (Step 000270): Train loss 5.196, Val loss 5.335\n",
      "Ep 1 (Step 000280): Train loss 5.266, Val loss 5.327\n",
      "Ep 1 (Step 000290): Train loss 5.230, Val loss 5.343\n",
      "Ep 1 (Step 000300): Train loss 5.203, Val loss 5.330\n",
      "Ep 1 (Step 000310): Train loss 5.178, Val loss 5.308\n",
      "Ep 1 (Step 000320): Train loss 5.171, Val loss 5.291\n",
      "Ep 1 (Step 000330): Train loss 5.094, Val loss 5.302\n",
      "Ep 1 (Step 000340): Train loss 5.183, Val loss 5.268\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2684\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.259, Val loss 8.217\n",
      "Ep 1 (Step 000010): Train loss 6.632, Val loss 6.560\n",
      "Ep 1 (Step 000020): Train loss 6.552, Val loss 6.488\n",
      "Ep 1 (Step 000030): Train loss 6.354, Val loss 6.354\n",
      "Ep 1 (Step 000040): Train loss 6.174, Val loss 6.163\n",
      "Ep 1 (Step 000050): Train loss 6.006, Val loss 6.014\n",
      "Ep 1 (Step 000060): Train loss 5.916, Val loss 5.878\n",
      "Ep 1 (Step 000070): Train loss 5.853, Val loss 5.805\n",
      "Ep 1 (Step 000080): Train loss 5.749, Val loss 5.753\n",
      "Ep 1 (Step 000090): Train loss 5.640, Val loss 5.672\n",
      "Ep 1 (Step 000100): Train loss 5.605, Val loss 5.654\n",
      "Ep 1 (Step 000110): Train loss 5.577, Val loss 5.600\n",
      "Ep 1 (Step 000120): Train loss 5.534, Val loss 5.576\n",
      "Ep 1 (Step 000130): Train loss 5.490, Val loss 5.556\n",
      "Ep 1 (Step 000140): Train loss 5.452, Val loss 5.530\n",
      "Ep 1 (Step 000150): Train loss 5.423, Val loss 5.524\n",
      "Ep 1 (Step 000160): Train loss 5.517, Val loss 5.476\n",
      "Ep 1 (Step 000170): Train loss 5.340, Val loss 5.469\n",
      "Ep 1 (Step 000180): Train loss 5.361, Val loss 5.453\n",
      "Ep 1 (Step 000190): Train loss 5.373, Val loss 5.419\n",
      "Ep 1 (Step 000200): Train loss 5.290, Val loss 5.417\n",
      "Ep 1 (Step 000210): Train loss 5.275, Val loss 5.399\n",
      "Ep 1 (Step 000220): Train loss 5.315, Val loss 5.399\n",
      "Ep 1 (Step 000230): Train loss 5.238, Val loss 5.381\n",
      "Ep 1 (Step 000240): Train loss 5.227, Val loss 5.392\n",
      "Ep 1 (Step 000250): Train loss 5.333, Val loss 5.381\n",
      "Ep 1 (Step 000260): Train loss 5.135, Val loss 5.351\n",
      "Ep 1 (Step 000270): Train loss 5.142, Val loss 5.343\n",
      "Ep 1 (Step 000280): Train loss 5.233, Val loss 5.316\n",
      "Ep 1 (Step 000290): Train loss 5.210, Val loss 5.334\n",
      "Ep 1 (Step 000300): Train loss 5.134, Val loss 5.335\n",
      "Ep 1 (Step 000310): Train loss 5.162, Val loss 5.297\n",
      "Ep 1 (Step 000320): Train loss 5.090, Val loss 5.284\n",
      "Ep 1 (Step 000330): Train loss 5.116, Val loss 5.271\n",
      "Ep 1 (Step 000340): Train loss 5.107, Val loss 5.256\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.95), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2561\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.311, Val loss 8.263\n",
      "Ep 1 (Step 000010): Train loss 6.633, Val loss 6.539\n",
      "Ep 1 (Step 000020): Train loss 6.465, Val loss 6.503\n",
      "Ep 1 (Step 000030): Train loss 6.368, Val loss 6.394\n",
      "Ep 1 (Step 000040): Train loss 6.289, Val loss 6.254\n",
      "Ep 1 (Step 000050): Train loss 6.002, Val loss 6.081\n",
      "Ep 1 (Step 000060): Train loss 5.933, Val loss 5.974\n",
      "Ep 1 (Step 000070): Train loss 5.828, Val loss 5.870\n",
      "Ep 1 (Step 000080): Train loss 5.719, Val loss 5.822\n",
      "Ep 1 (Step 000090): Train loss 5.747, Val loss 5.762\n",
      "Ep 1 (Step 000100): Train loss 5.572, Val loss 5.699\n",
      "Ep 1 (Step 000110): Train loss 5.560, Val loss 5.668\n",
      "Ep 1 (Step 000120): Train loss 5.588, Val loss 5.603\n",
      "Ep 1 (Step 000130): Train loss 5.473, Val loss 5.556\n",
      "Ep 1 (Step 000140): Train loss 5.448, Val loss 5.529\n",
      "Ep 1 (Step 000150): Train loss 5.471, Val loss 5.508\n",
      "Ep 1 (Step 000160): Train loss 5.414, Val loss 5.499\n",
      "Ep 1 (Step 000170): Train loss 5.348, Val loss 5.465\n",
      "Ep 1 (Step 000180): Train loss 5.374, Val loss 5.448\n",
      "Ep 1 (Step 000190): Train loss 5.410, Val loss 5.446\n",
      "Ep 1 (Step 000200): Train loss 5.275, Val loss 5.411\n",
      "Ep 1 (Step 000210): Train loss 5.292, Val loss 5.406\n",
      "Ep 1 (Step 000220): Train loss 5.288, Val loss 5.382\n",
      "Ep 1 (Step 000230): Train loss 5.201, Val loss 5.370\n",
      "Ep 1 (Step 000240): Train loss 5.267, Val loss 5.355\n",
      "Ep 1 (Step 000250): Train loss 5.301, Val loss 5.337\n",
      "Ep 1 (Step 000260): Train loss 5.231, Val loss 5.333\n",
      "Ep 1 (Step 000270): Train loss 5.263, Val loss 5.306\n",
      "Ep 1 (Step 000280): Train loss 5.130, Val loss 5.289\n",
      "Ep 1 (Step 000290): Train loss 5.145, Val loss 5.289\n",
      "Ep 1 (Step 000300): Train loss 5.173, Val loss 5.289\n",
      "Ep 1 (Step 000310): Train loss 5.163, Val loss 5.267\n",
      "Ep 1 (Step 000320): Train loss 5.125, Val loss 5.268\n",
      "Ep 1 (Step 000330): Train loss 5.084, Val loss 5.257\n",
      "Ep 1 (Step 000340): Train loss 5.093, Val loss 5.247\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.01}, Val Loss: 5.2470\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.287, Val loss 8.263\n",
      "Ep 1 (Step 000010): Train loss 6.640, Val loss 6.595\n",
      "Ep 1 (Step 000020): Train loss 6.478, Val loss 6.508\n",
      "Ep 1 (Step 000030): Train loss 9.806, Val loss 9.709\n",
      "Ep 1 (Step 000040): Train loss 6.267, Val loss 6.303\n",
      "Ep 1 (Step 000050): Train loss 6.067, Val loss 6.085\n",
      "Ep 1 (Step 000060): Train loss 5.906, Val loss 5.938\n",
      "Ep 1 (Step 000070): Train loss 5.914, Val loss 5.853\n",
      "Ep 1 (Step 000080): Train loss 5.721, Val loss 5.773\n",
      "Ep 1 (Step 000090): Train loss 5.657, Val loss 5.727\n",
      "Ep 1 (Step 000100): Train loss 5.723, Val loss 5.687\n",
      "Ep 1 (Step 000110): Train loss 5.578, Val loss 5.632\n",
      "Ep 1 (Step 000120): Train loss 5.530, Val loss 5.596\n",
      "Ep 1 (Step 000130): Train loss 5.496, Val loss 5.583\n",
      "Ep 1 (Step 000140): Train loss 5.376, Val loss 5.550\n",
      "Ep 1 (Step 000150): Train loss 5.438, Val loss 5.525\n",
      "Ep 1 (Step 000160): Train loss 5.407, Val loss 5.491\n",
      "Ep 1 (Step 000170): Train loss 5.339, Val loss 5.472\n",
      "Ep 1 (Step 000180): Train loss 5.338, Val loss 5.457\n",
      "Ep 1 (Step 000190): Train loss 5.372, Val loss 5.444\n",
      "Ep 1 (Step 000200): Train loss 5.343, Val loss 5.432\n",
      "Ep 1 (Step 000210): Train loss 5.290, Val loss 5.416\n",
      "Ep 1 (Step 000220): Train loss 5.212, Val loss 5.389\n",
      "Ep 1 (Step 000230): Train loss 5.331, Val loss 5.401\n",
      "Ep 1 (Step 000240): Train loss 5.165, Val loss 5.373\n",
      "Ep 1 (Step 000250): Train loss 5.241, Val loss 5.358\n",
      "Ep 1 (Step 000260): Train loss 5.267, Val loss 5.340\n",
      "Ep 1 (Step 000270): Train loss 5.148, Val loss 5.324\n",
      "Ep 1 (Step 000280): Train loss 5.183, Val loss 5.307\n",
      "Ep 1 (Step 000290): Train loss 5.209, Val loss 5.280\n",
      "Ep 1 (Step 000300): Train loss 5.256, Val loss 5.284\n",
      "Ep 1 (Step 000310): Train loss 5.246, Val loss 5.281\n",
      "Ep 1 (Step 000320): Train loss 5.183, Val loss 5.269\n",
      "Ep 1 (Step 000330): Train loss 5.057, Val loss 5.256\n",
      "Ep 1 (Step 000340): Train loss 5.117, Val loss 5.270\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.05}, Val Loss: 5.2698\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.289, Val loss 8.262\n",
      "Ep 1 (Step 000010): Train loss 6.572, Val loss 6.550\n",
      "Ep 1 (Step 000020): Train loss 6.524, Val loss 6.475\n",
      "Ep 1 (Step 000030): Train loss 6.247, Val loss 6.298\n",
      "Ep 1 (Step 000040): Train loss 6.117, Val loss 6.168\n",
      "Ep 1 (Step 000050): Train loss 5.991, Val loss 6.053\n",
      "Ep 1 (Step 000060): Train loss 5.884, Val loss 5.964\n",
      "Ep 1 (Step 000070): Train loss 5.757, Val loss 5.843\n",
      "Ep 1 (Step 000080): Train loss 5.728, Val loss 5.776\n",
      "Ep 1 (Step 000090): Train loss 5.595, Val loss 5.753\n",
      "Ep 1 (Step 000100): Train loss 5.670, Val loss 5.703\n",
      "Ep 1 (Step 000110): Train loss 5.540, Val loss 5.650\n",
      "Ep 1 (Step 000120): Train loss 5.546, Val loss 5.623\n",
      "Ep 1 (Step 000130): Train loss 5.459, Val loss 5.582\n",
      "Ep 1 (Step 000140): Train loss 5.408, Val loss 5.534\n",
      "Ep 1 (Step 000150): Train loss 5.434, Val loss 5.534\n",
      "Ep 1 (Step 000160): Train loss 5.406, Val loss 5.506\n",
      "Ep 1 (Step 000170): Train loss 5.375, Val loss 5.466\n",
      "Ep 1 (Step 000180): Train loss 5.331, Val loss 5.433\n",
      "Ep 1 (Step 000190): Train loss 5.348, Val loss 5.432\n",
      "Ep 1 (Step 000200): Train loss 5.316, Val loss 5.437\n",
      "Ep 1 (Step 000210): Train loss 5.320, Val loss 5.417\n",
      "Ep 1 (Step 000220): Train loss 5.253, Val loss 5.407\n",
      "Ep 1 (Step 000230): Train loss 5.231, Val loss 5.381\n",
      "Ep 1 (Step 000240): Train loss 5.248, Val loss 5.357\n",
      "Ep 1 (Step 000250): Train loss 5.274, Val loss 5.339\n",
      "Ep 1 (Step 000260): Train loss 5.208, Val loss 5.342\n",
      "Ep 1 (Step 000270): Train loss 5.181, Val loss 5.341\n",
      "Ep 1 (Step 000280): Train loss 5.214, Val loss 5.317\n",
      "Ep 1 (Step 000290): Train loss 5.194, Val loss 5.325\n",
      "Ep 1 (Step 000300): Train loss 5.156, Val loss 5.305\n",
      "Ep 1 (Step 000310): Train loss 5.126, Val loss 5.290\n",
      "Ep 1 (Step 000320): Train loss 5.131, Val loss 5.296\n",
      "Ep 1 (Step 000330): Train loss 5.212, Val loss 5.280\n",
      "Ep 1 (Step 000340): Train loss 5.122, Val loss 5.252\n",
      "==================================================\n",
      "Config: {'vocab_size': 10000, 'context_length': 256, 'emb_dim': 768, 'n_heads': 4, 'n_layers': 16, 'drop_rate': 0.3, 'qkv_bias': True, 'device': device(type='cuda')}, Optimizer: {'lr': 0.0004, 'betas': (0.9, 0.98), 'eps': 1e-08, 'weight_decay': 0.001}, Val Loss: 5.2518\n",
      "==================================================\n",
      "Ep 1 (Step 000000): Train loss 8.455, Val loss 8.462\n",
      "Ep 1 (Step 000010): Train loss 6.968, Val loss 6.935\n"
     ]
    }
   ],
   "source": [
    "grid_search_gpt(config_grid, optimizer_grid, train_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1cdca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07283b0e-7486-4478-8eea-55a156e6964b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
