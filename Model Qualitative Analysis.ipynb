{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da754f1d",
   "metadata": {},
   "source": [
    "# GPT & Prejudice — Qualitative Output Analysis\n",
    "\n",
    "**Goal (TL;DR):** Systematically systematically collect and annotate generations, looking for **patterns and potential biases** in how the model represents themes such as **gender, marriage, wealth, class, family, and society**. The main focus is to see **what stereotypes, prejudices, or social assumptions** the model has absorbed from its contained training corpus.\n",
    "\n",
    "**Model (quick overview)**\n",
    "- Decoder-only GPT (custom PyTorch).\n",
    "- Vocab: 50,257 (GPT-2 tokenizer)\n",
    "- Hidden size: 896 • Layers: 8 • Heads: 14 • Dropout: 0.2\n",
    "- Trained on: 19th-century authors corpus (40 novels from 10 female writers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1482ac",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load the model\n",
    "For this analysis you can use the **Hugging Face Hub** version of our model.  \n",
    "\n",
    "Load the model using the `from_pretrained` API with `trust_remote_code=True`, since our GPT implementation is custom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00380c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 23:08:08.477805: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "repo_id = \"HTW-KI-Werkstatt/gpt_and_prejudice\"\n",
    "\n",
    "# Load tokenizer (GPT-2 tokenizer is used)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "\n",
    "# Load model (custom GPT implementation with trust_remote_code)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.to(\"cpu\").eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f84fa",
   "metadata": {},
   "source": [
    "### [Optional]: download a snapshot of the model locally\n",
    "to avoid downloading a copy each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293971aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "REPO = \"HTW-KI-Werkstatt/gpt_and_prejudice\"\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=REPO,\n",
    "    revision=\"main\",\n",
    "    local_dir=\"./gpt_and_prejudice_snapshot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81237701",
   "metadata": {},
   "source": [
    "Then load from disk; nothing is downloaded/updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(local_dir=\"./gpt_and_prejudice_snapshot\", trust_remote_code=True, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir=\"./gpt_and_prejudice_snapshot\", trust_remote_code=True, local_files_only=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24678b00",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09125a",
   "metadata": {},
   "source": [
    "## 2. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7845d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b271c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She is very fond of her own family, and I dare say she is very fond of her, and I should like to see her. I don't want her to be a good girl, for she is a very pretty girl, and I think I shall\n"
     ]
    }
   ],
   "source": [
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"She is\",\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.4,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f6d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a good fellow, and he has a great deal of good sense. I am sure he is a very good man, and I am sure he is very good-natured, and I am sure he is very good-natured, and very\n"
     ]
    }
   ],
   "source": [
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"He is\",\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.4,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6def714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
