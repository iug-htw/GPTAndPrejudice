{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da754f1d",
   "metadata": {},
   "source": [
    "# GPT & Prejudice — Qualitative Output Analysis\n",
    "\n",
    "**Goal (TL;DR):** Systematically systematically collect and annotate generations, looking for **patterns and potential biases** in how the model represents themes such as **gender, marriage, wealth, class, family, and society**. The main focus is to see **what stereotypes, prejudices, or social assumptions** the model has absorbed from its contained training corpus.\n",
    "\n",
    "**Model (quick overview)**\n",
    "- Decoder-only GPT (custom PyTorch).\n",
    "- Vocab: 50,257 (GPT-2 tokenizer) • Context length: 256\n",
    "- Hidden size: 896 • Layers: 8 • Heads: 14 • Dropout: 0.2\n",
    "- Trained on: 19th-century authors corpus (40 novels from 10 female writers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1482ac",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load the model\n",
    "For this analysis you can use the **Hugging Face Hub** version of our model. \n",
    "The model files and data can be found at [https://huggingface.co/HTW-KI-Werkstatt/gpt_and_prejudice](https://huggingface.co/HTW-KI-Werkstatt/gpt_and_prejudice)\n",
    "\n",
    "There are many ways you can access it.\n",
    "\n",
    "\n",
    "##### A. You can load the remote model using the `from_pretrained` API with `trust_remote_code=True`, since our GPT implementation is custom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00380c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 23:08:08.477805: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "repo_id = \"HTW-KI-Werkstatt/gpt_and_prejudice\"\n",
    "\n",
    "# Load tokenizer (GPT-2 tokenizer is used)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "\n",
    "# Load model (custom GPT implementation with trust_remote_code)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.to(\"cpu\").eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f84fa",
   "metadata": {},
   "source": [
    "##### B. [Optional]: download a snapshot of the model locally to avoid downloading a copy each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293971aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "REPO = \"HTW-KI-Werkstatt/gpt_and_prejudice\"\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=REPO,\n",
    "    revision=\"main\",\n",
    "    local_dir=\"./gpt_and_prejudice_snapshot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81237701",
   "metadata": {},
   "source": [
    "Then load from disk; nothing is downloaded/updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(local_dir=\"./gpt_and_prejudice_snapshot\", trust_remote_code=True, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir=\"./gpt_and_prejudice_snapshot\", trust_remote_code=True, local_files_only=True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5b519-75ff-48e9-8cf1-0895ff60a9d2",
   "metadata": {},
   "source": [
    "##### C. [Optional]: manually download the `.pth` checkpoint of the model\n",
    "\n",
    "1. From the hugingface repository, download the `model_896_14_8_256.pth` checkpoint and add it to the root of the current direcory.\n",
    "2. Load the model as follows:\n",
    "   \n",
    "   ```\n",
    "   from utils.model import load_GPT_model\n",
    "\n",
    "   model = load_GPT_model(path=\"model_896_14_8_256.pth\", device=\"cpu\")\n",
    "   model.eval()\n",
    "   ```\n",
    "\n",
    "The `utils.model` is a custom script of helper funcion located at the `./utils/model.py` in the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24678b00",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09125a",
   "metadata": {},
   "source": [
    "## 2. Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76236d95-f556-4834-8b54-d9fc8ba27758",
   "metadata": {},
   "source": [
    "We use our custom `generate()` function to produce text continuations from the model. (located at `./generate_text.py`)  \n",
    "This function runs the model step by step, sampling new tokens according to a probability distribution, until the desired length is reached or an end-of-sequence token appears.\n",
    "\n",
    "**Parameters used:**\n",
    "\n",
    "- **`model`** — the trained GPT model.\n",
    "- **`prompt`** — the starting text given to the model. \n",
    "- **`max_new_tokens`** — the maximum number of tokens to generate beyond the prompt.  \n",
    "  Setting this to `50` means the output will continue for at most 50 tokens.\n",
    "  <br/> A token is a word or a part of a word.\n",
    "  <br/> _For example: 'hardly' can be counted as a token, or segmented into two tokens: 'hard' and 'ly'._\n",
    "- **`temperature`** — controls **randomness** in sampling.  \n",
    "  Lower values (<1.0) make the model more deterministic, higher values make it more creative.  \n",
    "  Altering this can allow for some variation in the text.\n",
    "- **`top_k`** — restricts sampling to the top-K most likely tokens at each step.  \n",
    "  With `top_k=50`, only the 50 most probable tokens are considered at each step, helping to avoid very unlikely words.\n",
    "\n",
    "Together, `temperature` and `top_k` balance **creativity vs. reliability** in the generated text.  \n",
    "Playing arund with those two parameters can yield interesting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7845d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b271c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She is very fond of her own family, and I dare say she is very fond of her, and I should like to see her. I don't want her to be a good girl, for she is a very pretty girl, and I think I shall\n"
     ]
    }
   ],
   "source": [
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"She is\",\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.4,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f6d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a good fellow, and he has a great deal of good sense. I am sure he is a very good man, and I am sure he is very good-natured, and I am sure he is very good-natured, and very\n"
     ]
    }
   ],
   "source": [
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"He is\",\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.4,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6def714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
