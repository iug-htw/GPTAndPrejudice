{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b225ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from clean_gutenberg_collected_work import clean_gutenberg_collected_work\n",
    "from sparse_auto_encoder import SparseAutoencoder\n",
    "from train_sae import train_sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b0d461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3191ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.2,\n",
    "    \"qkv_bias\": True,\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd8e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "checkpoint = torch.load(\"model_768_12_12_old_tok.pth\", weights_only=True)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f4fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb48a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_and_clean_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads a text file and splits it into sentences while cleaning the text.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the text file.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of cleaned sentences from the book.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = clean_gutenberg_collected_work(file_path)\n",
    "\n",
    "    # Split text into sentences (simple heuristic using punctuation)\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "\n",
    "    # Remove very short or long sentences\n",
    "    sentences = [s.strip() for s in sentences if 5 < len(s.split()) < 60]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f25032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"original_texts/\"\n",
    "dataset = []\n",
    "\n",
    "sentences = load_and_clean_text(os.path.join(directory, 'complete_jane_austen.txt'))\n",
    "dataset += sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f23472ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def get_token_embeddings(text, model, tokenizer, layers=[6, 12]):\n",
    "    \"\"\"\n",
    "    Extracts token embeddings from specified transformer layers.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "    - model: Custom GPT model.\n",
    "    - tokenizer: tiktoken encoding object.\n",
    "    - layers (list): Transformer layers to extract embeddings from.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Layer-wise token embeddings {layer_number: embeddings}\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = text_to_token_ids(text, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, hidden_states = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    embeddings = {}\n",
    "    for layer in layers:\n",
    "        if layer - 1 < len(hidden_states):\n",
    "            embeddings[layer] = hidden_states[layer - 1].squeeze(0).cpu().numpy()\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: Layer {layer} is out of range (max index {len(hidden_states) - 1})\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28bb562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token embeddings:\n",
      "Layer 6: (718536, 768)\n",
      "Layer 12: (718536, 768)\n"
     ]
    }
   ],
   "source": [
    "layer6_embeddings = []\n",
    "layer12_embeddings = []\n",
    "\n",
    "for sentence in dataset:\n",
    "    embeddings = get_token_embeddings(sentence, model, tokenizer)\n",
    "    layer6_embeddings.append(embeddings[6])\n",
    "    layer12_embeddings.append(embeddings[12])\n",
    "\n",
    "# Convert to NumPy and flatten tokens into dataset\n",
    "layer6_embeddings = np.vstack(layer6_embeddings)\n",
    "layer12_embeddings = np.vstack(layer12_embeddings)\n",
    "\n",
    "os.makedirs(\"sae_data\", exist_ok=True)\n",
    "np.save(\"sae_data/layer6_embeddings.npy\", layer6_embeddings)\n",
    "np.save(\"sae_data/layer12_embeddings.npy\", layer12_embeddings)\n",
    "\n",
    "print(\"Saved token embeddings:\")\n",
    "print(f\"Layer 6: {layer6_embeddings.shape}\")\n",
    "print(f\"Layer 12: {layer12_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c72b1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer6_embeddings = np.load(\"sae_data/layer6_embeddings.npy\")\n",
    "layer12_embeddings = np.load(\"sae_data/layer12_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62df3de3-c92d-4399-9906-5675803c20f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sparse_auto_encoder import SparseAutoencoder\n",
    "\n",
    "def objective(trial, device=\"cpu\", embeddings_path=\"training_embeddings.npy\"):\n",
    "    # Hyperparameter search space\n",
    "    print(50*\"=\")\n",
    "    print(f\"Trial number {trial.number + 1}\")\n",
    "    print(50*\"=\")\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 512, step=64)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    # Load your embeddings\n",
    "    embeddings = np.load(embeddings_path)  # Replace with actual file\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "    input_dim = embeddings.shape[1]\n",
    "    sae = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(sae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    dataset = TensorDataset(embeddings)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience, early_stop_counter = 10, 0\n",
    "\n",
    "    for epoch in range(30):\n",
    "        sae.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, encoded = sae(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            sparsity_loss = torch.norm(encoded, p=1) * 1e-4\n",
    "            total_loss = loss + sparsity_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += total_loss.item()\n",
    "\n",
    "        sae.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch[0].to(device)\n",
    "                outputs, encoded = sae(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                sparsity_loss = torch.norm(encoded, p=1) * 1e-4\n",
    "                total_loss = loss + sparsity_loss\n",
    "                val_loss += total_loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch}: Train loss {train_loss:.4f}, Val loss {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            break\n",
    "\n",
    "    return best_val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ad1e0d-2ec6-4e76-b20a-85949d7930d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 05:49:50,242] A new study created in memory with name: no-name-debab08a-1ae8-4354-b1d7-2e8a93fcaf53\n",
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Trial number 1\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.5013, Val loss 2.7664\n",
      "Epoch 1: Train loss 2.6397, Val loss 2.5404\n",
      "Epoch 2: Train loss 2.5101, Val loss 2.4382\n",
      "Epoch 3: Train loss 2.4494, Val loss 2.3934\n",
      "Epoch 4: Train loss 2.4061, Val loss 2.3609\n",
      "Epoch 5: Train loss 2.3730, Val loss 2.3305\n",
      "Epoch 6: Train loss 2.3432, Val loss 2.3005\n",
      "Epoch 7: Train loss 2.3171, Val loss 2.2783\n",
      "Epoch 8: Train loss 2.2942, Val loss 2.2555\n",
      "Epoch 9: Train loss 2.2737, Val loss 2.2374\n",
      "Epoch 10: Train loss 2.2572, Val loss 2.2252\n",
      "Epoch 11: Train loss 2.2438, Val loss 2.2149\n",
      "Epoch 12: Train loss 2.2321, Val loss 2.2054\n",
      "Epoch 13: Train loss 2.2236, Val loss 2.2009\n",
      "Epoch 14: Train loss 2.2164, Val loss 2.1995\n",
      "Epoch 15: Train loss 2.2110, Val loss 2.1948\n",
      "Epoch 16: Train loss 2.2063, Val loss 2.1908\n",
      "Epoch 17: Train loss 2.2020, Val loss 2.1906\n",
      "Epoch 18: Train loss 2.1982, Val loss 2.1864\n",
      "Epoch 19: Train loss 2.1944, Val loss 2.1859\n",
      "Epoch 20: Train loss 2.1912, Val loss 2.1838\n",
      "Epoch 21: Train loss 2.1882, Val loss 2.1842\n",
      "Epoch 22: Train loss 2.1858, Val loss 2.1784\n",
      "Epoch 23: Train loss 2.1831, Val loss 2.1818\n",
      "Epoch 24: Train loss 2.1809, Val loss 2.1789\n",
      "Epoch 25: Train loss 2.1786, Val loss 2.1784\n",
      "Epoch 26: Train loss 2.1770, Val loss 2.1826\n",
      "Epoch 27: Train loss 2.1748, Val loss 2.1746\n",
      "Epoch 28: Train loss 2.1729, Val loss 2.1777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:02:12,080] Trial 0 finished with value: 2.170551270788366 and parameters: {'batch_size': 128, 'lr': 0.0002688498549528964, 'hidden_dim': 320, 'weight_decay': 0.00024172216807324387}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1711, Val loss 2.1706\n",
      "==================================================\n",
      "Trial number 2\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss 2.4140, Val loss 2.3188\n",
      "Epoch 1: Train loss 2.3013, Val loss 2.3061\n",
      "Epoch 2: Train loss 2.2921, Val loss 2.2908\n",
      "Epoch 3: Train loss 2.2870, Val loss 2.3128\n",
      "Epoch 4: Train loss 2.2838, Val loss 2.3026\n",
      "Epoch 5: Train loss 2.2817, Val loss 2.3122\n",
      "Epoch 6: Train loss 2.2804, Val loss 2.2955\n",
      "Epoch 7: Train loss 2.2793, Val loss 2.2935\n",
      "Epoch 8: Train loss 2.2782, Val loss 2.2808\n",
      "Epoch 9: Train loss 2.2774, Val loss 2.3122\n",
      "Epoch 10: Train loss 2.2773, Val loss 2.2876\n",
      "Epoch 11: Train loss 2.2768, Val loss 2.2953\n",
      "Epoch 12: Train loss 2.2759, Val loss 2.2960\n",
      "Epoch 13: Train loss 2.2757, Val loss 2.2897\n",
      "Epoch 14: Train loss 2.2754, Val loss 2.2881\n",
      "Epoch 15: Train loss 2.2601, Val loss 2.2521\n",
      "Epoch 16: Train loss 2.2585, Val loss 2.2471\n",
      "Epoch 17: Train loss 2.2585, Val loss 2.2550\n",
      "Epoch 18: Train loss 2.2587, Val loss 2.2596\n",
      "Epoch 19: Train loss 2.2585, Val loss 2.2635\n",
      "Epoch 20: Train loss 2.2582, Val loss 2.2584\n",
      "Epoch 21: Train loss 2.2581, Val loss 2.2611\n",
      "Epoch 22: Train loss 2.2582, Val loss 2.2607\n",
      "Epoch 23: Train loss 2.2479, Val loss 2.2250\n",
      "Epoch 24: Train loss 2.2469, Val loss 2.2332\n",
      "Epoch 25: Train loss 2.2468, Val loss 2.2317\n",
      "Epoch 26: Train loss 2.2467, Val loss 2.2267\n",
      "Epoch 27: Train loss 2.2465, Val loss 2.2312\n",
      "Epoch 28: Train loss 2.2465, Val loss 2.2292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:22:54,658] Trial 1 finished with value: 2.224995661976669 and parameters: {'batch_size': 64, 'lr': 0.004508739037375544, 'hidden_dim': 192, 'weight_decay': 0.0001467563404464736}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.2463, Val loss 2.2263\n",
      "==================================================\n",
      "Trial number 3\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.1771, Val loss 2.8172\n",
      "Epoch 1: Train loss 2.7800, Val loss 2.6882\n",
      "Epoch 2: Train loss 2.6947, Val loss 2.6175\n",
      "Epoch 3: Train loss 2.6386, Val loss 2.5764\n",
      "Epoch 4: Train loss 2.6049, Val loss 2.5540\n",
      "Epoch 5: Train loss 2.5865, Val loss 2.5463\n",
      "Epoch 6: Train loss 2.5802, Val loss 2.5443\n",
      "Epoch 7: Train loss 2.5765, Val loss 2.5453\n",
      "Epoch 8: Train loss 2.5741, Val loss 2.5435\n",
      "Epoch 9: Train loss 2.5720, Val loss 2.5434\n",
      "Epoch 10: Train loss 2.5707, Val loss 2.5442\n",
      "Epoch 11: Train loss 2.5698, Val loss 2.5418\n",
      "Epoch 12: Train loss 2.5687, Val loss 2.5428\n",
      "Epoch 13: Train loss 2.5678, Val loss 2.5433\n",
      "Epoch 14: Train loss 2.5671, Val loss 2.5422\n",
      "Epoch 15: Train loss 2.5665, Val loss 2.5457\n",
      "Epoch 16: Train loss 2.5662, Val loss 2.5436\n",
      "Epoch 17: Train loss 2.5657, Val loss 2.5425\n",
      "Epoch 18: Train loss 2.5633, Val loss 2.5331\n",
      "Epoch 19: Train loss 2.5630, Val loss 2.5343\n",
      "Epoch 20: Train loss 2.5628, Val loss 2.5353\n",
      "Epoch 21: Train loss 2.5624, Val loss 2.5335\n",
      "Epoch 22: Train loss 2.5624, Val loss 2.5336\n",
      "Epoch 23: Train loss 2.5620, Val loss 2.5333\n",
      "Epoch 24: Train loss 2.5618, Val loss 2.5328\n",
      "Epoch 25: Train loss 2.5608, Val loss 2.5299\n",
      "Epoch 26: Train loss 2.5606, Val loss 2.5301\n",
      "Epoch 27: Train loss 2.5606, Val loss 2.5311\n",
      "Epoch 28: Train loss 2.5605, Val loss 2.5301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:43:08,899] Trial 2 finished with value: 2.5284160660719817 and parameters: {'batch_size': 64, 'lr': 0.00020725071060021414, 'hidden_dim': 64, 'weight_decay': 1.9243970614633706e-06}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.5604, Val loss 2.5284\n",
      "==================================================\n",
      "Trial number 4\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.8168, Val loss 2.6210\n",
      "Epoch 1: Train loss 2.5877, Val loss 2.6165\n",
      "Epoch 2: Train loss 2.5764, Val loss 2.5997\n",
      "Epoch 3: Train loss 2.5724, Val loss 2.5933\n",
      "Epoch 4: Train loss 2.5704, Val loss 2.6060\n",
      "Epoch 5: Train loss 2.5694, Val loss 2.6090\n",
      "Epoch 6: Train loss 2.5685, Val loss 2.5808\n",
      "Epoch 7: Train loss 2.5677, Val loss 2.5884\n",
      "Epoch 8: Train loss 2.5670, Val loss 2.5909\n",
      "Epoch 9: Train loss 2.5670, Val loss 2.5738\n",
      "Epoch 10: Train loss 2.5666, Val loss 2.5881\n",
      "Epoch 11: Train loss 2.5661, Val loss 2.5779\n",
      "Epoch 12: Train loss 2.5656, Val loss 2.5883\n",
      "Epoch 13: Train loss 2.5655, Val loss 2.5677\n",
      "Epoch 14: Train loss 2.5651, Val loss 2.5784\n",
      "Epoch 15: Train loss 2.5650, Val loss 2.5797\n",
      "Epoch 16: Train loss 2.5648, Val loss 2.5900\n",
      "Epoch 17: Train loss 2.5645, Val loss 2.5777\n",
      "Epoch 18: Train loss 2.5645, Val loss 2.5801\n",
      "Epoch 19: Train loss 2.5645, Val loss 2.5810\n",
      "Epoch 20: Train loss 2.5585, Val loss 2.5615\n",
      "Epoch 21: Train loss 2.5579, Val loss 2.5576\n",
      "Epoch 22: Train loss 2.5575, Val loss 2.5573\n",
      "Epoch 23: Train loss 2.5575, Val loss 2.5587\n",
      "Epoch 24: Train loss 2.5575, Val loss 2.5647\n",
      "Epoch 25: Train loss 2.5574, Val loss 2.5576\n",
      "Epoch 26: Train loss 2.5575, Val loss 2.5544\n",
      "Epoch 27: Train loss 2.5573, Val loss 2.5627\n",
      "Epoch 28: Train loss 2.5575, Val loss 2.5561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:55:02,432] Trial 3 finished with value: 2.5543929240920327 and parameters: {'batch_size': 128, 'lr': 0.0017955378450738243, 'hidden_dim': 64, 'weight_decay': 1.980034613217017e-05}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.5572, Val loss 2.5640\n",
      "==================================================\n",
      "Trial number 5\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.7470, Val loss 2.3950\n",
      "Epoch 1: Train loss 2.3955, Val loss 2.2924\n",
      "Epoch 2: Train loss 2.3048, Val loss 2.2097\n",
      "Epoch 3: Train loss 2.2311, Val loss 2.1628\n",
      "Epoch 4: Train loss 2.1922, Val loss 2.1437\n",
      "Epoch 5: Train loss 2.1740, Val loss 2.1422\n",
      "Epoch 6: Train loss 2.1642, Val loss 2.1383\n",
      "Epoch 7: Train loss 2.1578, Val loss 2.1427\n",
      "Epoch 8: Train loss 2.1526, Val loss 2.1437\n",
      "Epoch 9: Train loss 2.1479, Val loss 2.1372\n",
      "Epoch 10: Train loss 2.1445, Val loss 2.1387\n",
      "Epoch 11: Train loss 2.1415, Val loss 2.1432\n",
      "Epoch 12: Train loss 2.1381, Val loss 2.1517\n",
      "Epoch 13: Train loss 2.1356, Val loss 2.1578\n",
      "Epoch 14: Train loss 2.1332, Val loss 2.1525\n",
      "Epoch 15: Train loss 2.1310, Val loss 2.1545\n",
      "Epoch 16: Train loss 2.1234, Val loss 2.1080\n",
      "Epoch 17: Train loss 2.1219, Val loss 2.1100\n",
      "Epoch 18: Train loss 2.1206, Val loss 2.1085\n",
      "Epoch 19: Train loss 2.1194, Val loss 2.1072\n",
      "Epoch 20: Train loss 2.1182, Val loss 2.1102\n",
      "Epoch 21: Train loss 2.1174, Val loss 2.1108\n",
      "Epoch 22: Train loss 2.1164, Val loss 2.1132\n",
      "Epoch 23: Train loss 2.1155, Val loss 2.1107\n",
      "Epoch 24: Train loss 2.1145, Val loss 2.1094\n",
      "Epoch 25: Train loss 2.1136, Val loss 2.1119\n",
      "Epoch 26: Train loss 2.1095, Val loss 2.0839\n",
      "Epoch 27: Train loss 2.1087, Val loss 2.0843\n",
      "Epoch 28: Train loss 2.1081, Val loss 2.0863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 07:16:17,436] Trial 4 finished with value: 2.083881515848338 and parameters: {'batch_size': 64, 'lr': 0.0003881008304162215, 'hidden_dim': 384, 'weight_decay': 2.3923055206292162e-05}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1077, Val loss 2.0868\n",
      "==================================================\n",
      "Trial number 6\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.3573, Val loss 2.2031\n",
      "Epoch 1: Train loss 2.2544, Val loss 2.1934\n",
      "Epoch 2: Train loss 2.2470, Val loss 2.1944\n",
      "Epoch 3: Train loss 2.2429, Val loss 2.1903\n",
      "Epoch 4: Train loss 2.2405, Val loss 2.1887\n",
      "Epoch 5: Train loss 2.2391, Val loss 2.1896\n",
      "Epoch 6: Train loss 2.2378, Val loss 2.1853\n",
      "Epoch 7: Train loss 2.2372, Val loss 2.1904\n",
      "Epoch 8: Train loss 2.2364, Val loss 2.1852\n",
      "Epoch 9: Train loss 2.2354, Val loss 2.1833\n",
      "Epoch 10: Train loss 2.2351, Val loss 2.1861\n",
      "Epoch 11: Train loss 2.2346, Val loss 2.1805\n",
      "Epoch 12: Train loss 2.2338, Val loss 2.1858\n",
      "Epoch 13: Train loss 2.2336, Val loss 2.1823\n",
      "Epoch 14: Train loss 2.2334, Val loss 2.1866\n",
      "Epoch 15: Train loss 2.2330, Val loss 2.1816\n",
      "Epoch 16: Train loss 2.2328, Val loss 2.1794\n",
      "Epoch 17: Train loss 2.2324, Val loss 2.1818\n",
      "Epoch 18: Train loss 2.2323, Val loss 2.1867\n",
      "Epoch 19: Train loss 2.2321, Val loss 2.1766\n",
      "Epoch 20: Train loss 2.2320, Val loss 2.1891\n",
      "Epoch 21: Train loss 2.2317, Val loss 2.1924\n",
      "Epoch 22: Train loss 2.2316, Val loss 2.1749\n",
      "Epoch 23: Train loss 2.2314, Val loss 2.1798\n",
      "Epoch 24: Train loss 2.2308, Val loss 2.1811\n",
      "Epoch 25: Train loss 2.2312, Val loss 2.1879\n",
      "Epoch 26: Train loss 2.2308, Val loss 2.1752\n",
      "Epoch 27: Train loss 2.2309, Val loss 2.1794\n",
      "Epoch 28: Train loss 2.2306, Val loss 2.1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 07:55:47,399] Trial 5 finished with value: 2.1656301614224165 and parameters: {'batch_size': 32, 'lr': 0.0021729419335939316, 'hidden_dim': 256, 'weight_decay': 0.00048608168804415455}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.2222, Val loss 2.1656\n",
      "==================================================\n",
      "Trial number 7\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.5723, Val loss 2.2847\n",
      "Epoch 1: Train loss 2.2485, Val loss 2.2205\n",
      "Epoch 2: Train loss 2.1990, Val loss 2.2098\n",
      "Epoch 3: Train loss 2.1830, Val loss 2.1955\n",
      "Epoch 4: Train loss 2.1738, Val loss 2.2090\n",
      "Epoch 5: Train loss 2.1684, Val loss 2.2012\n",
      "Epoch 6: Train loss 2.1642, Val loss 2.2004\n",
      "Epoch 7: Train loss 2.1604, Val loss 2.2010\n",
      "Epoch 8: Train loss 2.1579, Val loss 2.1832\n",
      "Epoch 9: Train loss 2.1553, Val loss 2.1774\n",
      "Epoch 10: Train loss 2.1524, Val loss 2.2113\n",
      "Epoch 11: Train loss 2.1509, Val loss 2.2076\n",
      "Epoch 12: Train loss 2.1490, Val loss 2.2049\n",
      "Epoch 13: Train loss 2.1477, Val loss 2.2049\n",
      "Epoch 14: Train loss 2.1454, Val loss 2.1735\n",
      "Epoch 15: Train loss 2.1450, Val loss 2.1901\n",
      "Epoch 16: Train loss 2.1440, Val loss 2.1863\n",
      "Epoch 17: Train loss 2.1427, Val loss 2.2090\n",
      "Epoch 18: Train loss 2.1412, Val loss 2.1864\n",
      "Epoch 19: Train loss 2.1403, Val loss 2.2033\n",
      "Epoch 20: Train loss 2.1400, Val loss 2.5091\n",
      "Epoch 21: Train loss 2.1314, Val loss 2.1538\n",
      "Epoch 22: Train loss 2.1290, Val loss 2.1983\n",
      "Epoch 23: Train loss 2.1280, Val loss 2.1678\n",
      "Epoch 24: Train loss 2.1274, Val loss 2.1459\n",
      "Epoch 25: Train loss 2.1267, Val loss 2.1677\n",
      "Epoch 26: Train loss 2.1267, Val loss 2.1792\n",
      "Epoch 27: Train loss 2.1258, Val loss 2.1469\n",
      "Epoch 28: Train loss 2.1254, Val loss 2.1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 08:08:39,194] Trial 6 finished with value: 2.1458648746663873 and parameters: {'batch_size': 128, 'lr': 0.002262804895465889, 'hidden_dim': 512, 'weight_decay': 1.2074671078795074e-05}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1242, Val loss 2.1671\n",
      "==================================================\n",
      "Trial number 8\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.6830, Val loss 2.9287\n",
      "Epoch 1: Train loss 2.6973, Val loss 2.5486\n",
      "Epoch 2: Train loss 2.5121, Val loss 2.4390\n",
      "Epoch 3: Train loss 2.4410, Val loss 2.3816\n",
      "Epoch 4: Train loss 2.3982, Val loss 2.3592\n",
      "Epoch 5: Train loss 2.3705, Val loss 2.3356\n",
      "Epoch 6: Train loss 2.3475, Val loss 2.3143\n",
      "Epoch 7: Train loss 2.3279, Val loss 2.2913\n",
      "Epoch 8: Train loss 2.3100, Val loss 2.2707\n",
      "Epoch 9: Train loss 2.2940, Val loss 2.2571\n",
      "Epoch 10: Train loss 2.2788, Val loss 2.2455\n",
      "Epoch 11: Train loss 2.2643, Val loss 2.2292\n",
      "Epoch 12: Train loss 2.2505, Val loss 2.2204\n",
      "Epoch 13: Train loss 2.2369, Val loss 2.2035\n",
      "Epoch 14: Train loss 2.2259, Val loss 2.1959\n",
      "Epoch 15: Train loss 2.2149, Val loss 2.1837\n",
      "Epoch 16: Train loss 2.2055, Val loss 2.1777\n",
      "Epoch 17: Train loss 2.1969, Val loss 2.1710\n",
      "Epoch 18: Train loss 2.1900, Val loss 2.1665\n",
      "Epoch 19: Train loss 2.1841, Val loss 2.1618\n",
      "Epoch 20: Train loss 2.1790, Val loss 2.1631\n",
      "Epoch 21: Train loss 2.1749, Val loss 2.1544\n",
      "Epoch 22: Train loss 2.1704, Val loss 2.1520\n",
      "Epoch 23: Train loss 2.1667, Val loss 2.1490\n",
      "Epoch 24: Train loss 2.1631, Val loss 2.1474\n",
      "Epoch 25: Train loss 2.1603, Val loss 2.1428\n",
      "Epoch 26: Train loss 2.1573, Val loss 2.1437\n",
      "Epoch 27: Train loss 2.1547, Val loss 2.1403\n",
      "Epoch 28: Train loss 2.1524, Val loss 2.1394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 08:21:38,398] Trial 7 finished with value: 2.1394470236518166 and parameters: {'batch_size': 128, 'lr': 0.00019293742668148652, 'hidden_dim': 448, 'weight_decay': 0.00022579186946369568}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1504, Val loss 2.1417\n",
      "==================================================\n",
      "Trial number 9\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4441, Val loss 2.1521\n",
      "Epoch 1: Train loss 2.1927, Val loss 2.1148\n",
      "Epoch 2: Train loss 2.1680, Val loss 2.1177\n",
      "Epoch 3: Train loss 2.1613, Val loss 2.1051\n",
      "Epoch 4: Train loss 2.1571, Val loss 2.1005\n",
      "Epoch 5: Train loss 2.1540, Val loss 2.1026\n",
      "Epoch 6: Train loss 2.1515, Val loss 2.0959\n",
      "Epoch 7: Train loss 2.1492, Val loss 2.0967\n",
      "Epoch 8: Train loss 2.1474, Val loss 2.0888\n",
      "Epoch 9: Train loss 2.1458, Val loss 2.0864\n",
      "Epoch 10: Train loss 2.1449, Val loss 2.0961\n",
      "Epoch 11: Train loss 2.1438, Val loss 2.0909\n",
      "Epoch 12: Train loss 2.1426, Val loss 2.0885\n",
      "Epoch 13: Train loss 2.1418, Val loss 2.0868\n",
      "Epoch 14: Train loss 2.1407, Val loss 2.0861\n",
      "Epoch 15: Train loss 2.1403, Val loss 2.0882\n",
      "Epoch 16: Train loss 2.1397, Val loss 2.0880\n",
      "Epoch 17: Train loss 2.1392, Val loss 2.0777\n",
      "Epoch 18: Train loss 2.1386, Val loss 2.0790\n",
      "Epoch 19: Train loss 2.1383, Val loss 2.0814\n",
      "Epoch 20: Train loss 2.1380, Val loss 2.0770\n",
      "Epoch 21: Train loss 2.1374, Val loss 2.0794\n",
      "Epoch 22: Train loss 2.1374, Val loss 2.0781\n",
      "Epoch 23: Train loss 2.1368, Val loss 2.0749\n",
      "Epoch 24: Train loss 2.1363, Val loss 2.0786\n",
      "Epoch 25: Train loss 2.1359, Val loss 2.0803\n",
      "Epoch 26: Train loss 2.1354, Val loss 2.0753\n",
      "Epoch 27: Train loss 2.1348, Val loss 2.0771\n",
      "Epoch 28: Train loss 2.1344, Val loss 2.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 09:00:44,986] Trial 8 finished with value: 2.07491034575172 and parameters: {'batch_size': 32, 'lr': 0.000816547713285386, 'hidden_dim': 512, 'weight_decay': 2.9090240560596477e-06}. Best is trial 8 with value: 2.07491034575172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1341, Val loss 2.0782\n",
      "==================================================\n",
      "Trial number 10\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.6428, Val loss 2.9041\n",
      "Epoch 1: Train loss 2.6809, Val loss 2.5343\n",
      "Epoch 2: Train loss 2.5082, Val loss 2.4249\n",
      "Epoch 3: Train loss 2.4365, Val loss 2.3805\n",
      "Epoch 4: Train loss 2.3950, Val loss 2.3383\n",
      "Epoch 5: Train loss 2.3643, Val loss 2.3227\n",
      "Epoch 6: Train loss 2.3415, Val loss 2.3036\n",
      "Epoch 7: Train loss 2.3214, Val loss 2.2759\n",
      "Epoch 8: Train loss 2.3022, Val loss 2.2607\n",
      "Epoch 9: Train loss 2.2851, Val loss 2.2427\n",
      "Epoch 10: Train loss 2.2683, Val loss 2.2313\n",
      "Epoch 11: Train loss 2.2530, Val loss 2.2129\n",
      "Epoch 12: Train loss 2.2384, Val loss 2.2007\n",
      "Epoch 13: Train loss 2.2255, Val loss 2.1905\n",
      "Epoch 14: Train loss 2.2141, Val loss 2.1795\n",
      "Epoch 15: Train loss 2.2038, Val loss 2.1727\n",
      "Epoch 16: Train loss 2.1950, Val loss 2.1652\n",
      "Epoch 17: Train loss 2.1873, Val loss 2.1620\n",
      "Epoch 18: Train loss 2.1811, Val loss 2.1559\n",
      "Epoch 19: Train loss 2.1750, Val loss 2.1516\n",
      "Epoch 20: Train loss 2.1699, Val loss 2.1459\n",
      "Epoch 21: Train loss 2.1655, Val loss 2.1439\n",
      "Epoch 22: Train loss 2.1620, Val loss 2.1426\n",
      "Epoch 23: Train loss 2.1590, Val loss 2.1400\n",
      "Epoch 24: Train loss 2.1558, Val loss 2.1384\n",
      "Epoch 25: Train loss 2.1535, Val loss 2.1373\n",
      "Epoch 26: Train loss 2.1515, Val loss 2.1362\n",
      "Epoch 27: Train loss 2.1494, Val loss 2.1331\n",
      "Epoch 28: Train loss 2.1475, Val loss 2.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 09:13:30,328] Trial 9 finished with value: 2.133140113136985 and parameters: {'batch_size': 128, 'lr': 0.00020385491002032069, 'hidden_dim': 448, 'weight_decay': 0.0009293870195375112}. Best is trial 8 with value: 2.07491034575172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1461, Val loss 2.1341\n",
      "==================================================\n",
      "Trial number 11\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4641, Val loss 2.1610\n",
      "Epoch 1: Train loss 2.1981, Val loss 2.1177\n",
      "Epoch 2: Train loss 2.1680, Val loss 2.1113\n",
      "Epoch 3: Train loss 2.1603, Val loss 2.1073\n",
      "Epoch 4: Train loss 2.1552, Val loss 2.1021\n",
      "Epoch 5: Train loss 2.1526, Val loss 2.1009\n",
      "Epoch 6: Train loss 2.1501, Val loss 2.1042\n",
      "Epoch 7: Train loss 2.1482, Val loss 2.0891\n",
      "Epoch 8: Train loss 2.1468, Val loss 2.0926\n",
      "Epoch 9: Train loss 2.1453, Val loss 2.0885\n",
      "Epoch 10: Train loss 2.1440, Val loss 2.0865\n",
      "Epoch 11: Train loss 2.1432, Val loss 2.0876\n",
      "Epoch 12: Train loss 2.1423, Val loss 2.0805\n",
      "Epoch 13: Train loss 2.1417, Val loss 2.0836\n",
      "Epoch 14: Train loss 2.1412, Val loss 2.0824\n",
      "Epoch 15: Train loss 2.1406, Val loss 2.0840\n",
      "Epoch 16: Train loss 2.1400, Val loss 2.0843\n",
      "Epoch 17: Train loss 2.1391, Val loss 2.0755\n",
      "Epoch 18: Train loss 2.1386, Val loss 2.0769\n",
      "Epoch 19: Train loss 2.1380, Val loss 2.0733\n",
      "Epoch 20: Train loss 2.1372, Val loss 2.0794\n",
      "Epoch 21: Train loss 2.1368, Val loss 2.0805\n",
      "Epoch 22: Train loss 2.1367, Val loss 2.0795\n",
      "Epoch 23: Train loss 2.1365, Val loss 2.0813\n",
      "Epoch 24: Train loss 2.1361, Val loss 2.0719\n",
      "Epoch 25: Train loss 2.1360, Val loss 2.0764\n",
      "Epoch 26: Train loss 2.1357, Val loss 2.0776\n",
      "Epoch 27: Train loss 2.1354, Val loss 2.0761\n",
      "Epoch 28: Train loss 2.1354, Val loss 2.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 09:52:38,080] Trial 10 finished with value: 2.070989271797911 and parameters: {'batch_size': 32, 'lr': 0.0007246691209707045, 'hidden_dim': 512, 'weight_decay': 1.0552397856143292e-06}. Best is trial 10 with value: 2.070989271797911.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1352, Val loss 2.0710\n",
      "==================================================\n",
      "Trial number 12\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4702, Val loss 2.1798\n",
      "Epoch 1: Train loss 2.2019, Val loss 2.1129\n",
      "Epoch 2: Train loss 2.1672, Val loss 2.1201\n",
      "Epoch 3: Train loss 2.1594, Val loss 2.1079\n",
      "Epoch 4: Train loss 2.1543, Val loss 2.1025\n",
      "Epoch 5: Train loss 2.1510, Val loss 2.1049\n",
      "Epoch 6: Train loss 2.1492, Val loss 2.0979\n",
      "Epoch 7: Train loss 2.1475, Val loss 2.0987\n",
      "Epoch 8: Train loss 2.1465, Val loss 2.0943\n",
      "Epoch 9: Train loss 2.1456, Val loss 2.0965\n",
      "Epoch 10: Train loss 2.1447, Val loss 2.0882\n",
      "Epoch 11: Train loss 2.1438, Val loss 2.0918\n",
      "Epoch 12: Train loss 2.1429, Val loss 2.0872\n",
      "Epoch 13: Train loss 2.1420, Val loss 2.0878\n",
      "Epoch 14: Train loss 2.1413, Val loss 2.0828\n",
      "Epoch 15: Train loss 2.1404, Val loss 2.0836\n",
      "Epoch 16: Train loss 2.1395, Val loss 2.0862\n",
      "Epoch 17: Train loss 2.1389, Val loss 2.0900\n",
      "Epoch 18: Train loss 2.1385, Val loss 2.0852\n",
      "Epoch 19: Train loss 2.1379, Val loss 2.0827\n",
      "Epoch 20: Train loss 2.1371, Val loss 2.0838\n",
      "Epoch 21: Train loss 2.1289, Val loss 2.0680\n",
      "Epoch 22: Train loss 2.1273, Val loss 2.0689\n",
      "Epoch 23: Train loss 2.1269, Val loss 2.0667\n",
      "Epoch 24: Train loss 2.1266, Val loss 2.0674\n",
      "Epoch 25: Train loss 2.1263, Val loss 2.0709\n",
      "Epoch 26: Train loss 2.1263, Val loss 2.0661\n",
      "Epoch 27: Train loss 2.1258, Val loss 2.0678\n",
      "Epoch 28: Train loss 2.1256, Val loss 2.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 10:31:43,896] Trial 11 finished with value: 2.0660949138034574 and parameters: {'batch_size': 32, 'lr': 0.0007016224567866245, 'hidden_dim': 512, 'weight_decay': 1.1004487564277892e-06}. Best is trial 11 with value: 2.0660949138034574.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1256, Val loss 2.0701\n",
      "==================================================\n",
      "Trial number 13\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4893, Val loss 2.1768\n",
      "Epoch 1: Train loss 2.2109, Val loss 2.1531\n",
      "Epoch 2: Train loss 2.1821, Val loss 2.1659\n",
      "Epoch 3: Train loss 2.1730, Val loss 2.1861\n",
      "Epoch 4: Train loss 2.1683, Val loss 2.1886\n",
      "Epoch 5: Train loss 2.1654, Val loss 2.2205\n",
      "Epoch 6: Train loss 2.1636, Val loss 2.2249\n",
      "Epoch 7: Train loss 2.1614, Val loss 2.2632\n",
      "Epoch 8: Train loss 2.1478, Val loss 2.1724\n",
      "Epoch 9: Train loss 2.1455, Val loss 2.1609\n",
      "Epoch 10: Train loss 2.1442, Val loss 2.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 10:47:19,730] Trial 12 finished with value: 2.153145467405852 and parameters: {'batch_size': 32, 'lr': 0.0006288969424953504, 'hidden_dim': 384, 'weight_decay': 1.0034603972640884e-06}. Best is trial 11 with value: 2.0660949138034574.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train loss 2.1431, Val loss 2.2010\n",
      "==================================================\n",
      "Trial number 14\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.5095, Val loss 2.2279\n",
      "Epoch 1: Train loss 2.2277, Val loss 2.1206\n",
      "Epoch 2: Train loss 2.1678, Val loss 2.1205\n",
      "Epoch 3: Train loss 2.1543, Val loss 2.1307\n",
      "Epoch 4: Train loss 2.1478, Val loss 2.1580\n",
      "Epoch 5: Train loss 2.1434, Val loss 2.1555\n",
      "Epoch 6: Train loss 2.1400, Val loss 2.1728\n",
      "Epoch 7: Train loss 2.1374, Val loss 2.1752\n",
      "Epoch 8: Train loss 2.1244, Val loss 2.0986\n",
      "Epoch 9: Train loss 2.1213, Val loss 2.1100\n",
      "Epoch 10: Train loss 2.1196, Val loss 2.1081\n",
      "Epoch 11: Train loss 2.1185, Val loss 2.1205\n",
      "Epoch 12: Train loss 2.1173, Val loss 2.1234\n",
      "Epoch 13: Train loss 2.1164, Val loss 2.1237\n",
      "Epoch 14: Train loss 2.1156, Val loss 2.1255\n",
      "Epoch 15: Train loss 2.1085, Val loss 2.0756\n",
      "Epoch 16: Train loss 2.1080, Val loss 2.0798\n",
      "Epoch 17: Train loss 2.1071, Val loss 2.0785\n",
      "Epoch 18: Train loss 2.1066, Val loss 2.0757\n",
      "Epoch 19: Train loss 2.1058, Val loss 2.0778\n",
      "Epoch 20: Train loss 2.1053, Val loss 2.0760\n",
      "Epoch 21: Train loss 2.1048, Val loss 2.0761\n",
      "Epoch 22: Train loss 2.1011, Val loss 2.0572\n",
      "Epoch 23: Train loss 2.1008, Val loss 2.0589\n",
      "Epoch 24: Train loss 2.1005, Val loss 2.0608\n",
      "Epoch 25: Train loss 2.1003, Val loss 2.0577\n",
      "Epoch 26: Train loss 2.1000, Val loss 2.0595\n",
      "Epoch 27: Train loss 2.0998, Val loss 2.0610\n",
      "Epoch 28: Train loss 2.0993, Val loss 2.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 11:26:55,833] Trial 13 finished with value: 2.05181546670681 and parameters: {'batch_size': 32, 'lr': 0.0005587614855267997, 'hidden_dim': 512, 'weight_decay': 6.056437420027853e-06}. Best is trial 13 with value: 2.05181546670681.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.0975, Val loss 2.0518\n",
      "==================================================\n",
      "Trial number 15\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.8744, Val loss 2.4504\n",
      "Epoch 1: Train loss 2.4838, Val loss 2.3448\n",
      "Epoch 2: Train loss 2.4184, Val loss 2.2957\n",
      "Epoch 3: Train loss 2.3694, Val loss 2.2535\n",
      "Epoch 4: Train loss 2.3175, Val loss 2.1969\n",
      "Epoch 5: Train loss 2.2606, Val loss 2.1474\n",
      "Epoch 6: Train loss 2.2106, Val loss 2.1180\n",
      "Epoch 7: Train loss 2.1856, Val loss 2.1069\n",
      "Epoch 8: Train loss 2.1742, Val loss 2.1009\n",
      "Epoch 9: Train loss 2.1669, Val loss 2.0953\n",
      "Epoch 10: Train loss 2.1617, Val loss 2.0941\n",
      "Epoch 11: Train loss 2.1573, Val loss 2.0918\n",
      "Epoch 12: Train loss 2.1538, Val loss 2.0895\n",
      "Epoch 13: Train loss 2.1505, Val loss 2.0892\n",
      "Epoch 14: Train loss 2.1486, Val loss 2.0874\n",
      "Epoch 15: Train loss 2.1460, Val loss 2.0858\n",
      "Epoch 16: Train loss 2.1435, Val loss 2.0855\n",
      "Epoch 17: Train loss 2.1417, Val loss 2.0863\n",
      "Epoch 18: Train loss 2.1397, Val loss 2.0826\n",
      "Epoch 19: Train loss 2.1383, Val loss 2.0815\n",
      "Epoch 20: Train loss 2.1371, Val loss 2.0823\n",
      "Epoch 21: Train loss 2.1358, Val loss 2.0834\n",
      "Epoch 22: Train loss 2.1352, Val loss 2.0832\n",
      "Epoch 23: Train loss 2.1346, Val loss 2.0843\n",
      "Epoch 24: Train loss 2.1339, Val loss 2.0844\n",
      "Epoch 25: Train loss 2.1330, Val loss 2.0864\n",
      "Epoch 26: Train loss 2.1294, Val loss 2.0745\n",
      "Epoch 27: Train loss 2.1293, Val loss 2.0760\n",
      "Epoch 28: Train loss 2.1289, Val loss 2.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 12:07:36,156] Trial 14 finished with value: 2.0745204591968585 and parameters: {'batch_size': 32, 'lr': 0.00010999744095048953, 'hidden_dim': 384, 'weight_decay': 5.383174846292139e-06}. Best is trial 13 with value: 2.05181546670681.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1285, Val loss 2.0749\n",
      "Best hyperparameters: {'batch_size': 32, 'lr': 0.0005587614855267997, 'hidden_dim': 512, 'weight_decay': 6.056437420027853e-06}\n",
      "Search completed in 377.77 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda trial: objective(trial, device, embeddings_path=\"sae_data/layer6_embeddings.npy\"), n_trials=15)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Search completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c637e474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] | Train Loss: 0.564636 | Val Loss: 0.443621\n",
      "Epoch [2/500] | Train Loss: 0.429605 | Val Loss: 0.415526\n",
      "Epoch [3/500] | Train Loss: 0.404336 | Val Loss: 0.391972\n",
      "Epoch [4/500] | Train Loss: 0.380943 | Val Loss: 0.371695\n",
      "Epoch [5/500] | Train Loss: 0.363363 | Val Loss: 0.355106\n",
      "Epoch [6/500] | Train Loss: 0.348986 | Val Loss: 0.341697\n",
      "Epoch [7/500] | Train Loss: 0.337461 | Val Loss: 0.333137\n",
      "Epoch [8/500] | Train Loss: 0.328246 | Val Loss: 0.325167\n",
      "Epoch [9/500] | Train Loss: 0.321236 | Val Loss: 0.319153\n",
      "Epoch [10/500] | Train Loss: 0.315880 | Val Loss: 0.314109\n",
      "Epoch [11/500] | Train Loss: 0.311361 | Val Loss: 0.310885\n",
      "Epoch [12/500] | Train Loss: 0.307849 | Val Loss: 0.307511\n",
      "Epoch [13/500] | Train Loss: 0.305241 | Val Loss: 0.305991\n",
      "Epoch [14/500] | Train Loss: 0.303034 | Val Loss: 0.303367\n",
      "Epoch [15/500] | Train Loss: 0.300390 | Val Loss: 0.301121\n",
      "Epoch [16/500] | Train Loss: 0.298111 | Val Loss: 0.298208\n",
      "Epoch [17/500] | Train Loss: 0.296032 | Val Loss: 0.296372\n",
      "Epoch [18/500] | Train Loss: 0.293674 | Val Loss: 0.293474\n",
      "Epoch [19/500] | Train Loss: 0.291256 | Val Loss: 0.291606\n",
      "Epoch [20/500] | Train Loss: 0.289391 | Val Loss: 0.292636\n",
      "Epoch [21/500] | Train Loss: 0.287566 | Val Loss: 0.289255\n",
      "Epoch [22/500] | Train Loss: 0.286223 | Val Loss: 0.288303\n",
      "Epoch [23/500] | Train Loss: 0.284864 | Val Loss: 0.288210\n",
      "Epoch [24/500] | Train Loss: 0.283861 | Val Loss: 0.285019\n",
      "Epoch [25/500] | Train Loss: 0.282756 | Val Loss: 0.285151\n",
      "Epoch [26/500] | Train Loss: 0.282141 | Val Loss: 0.283566\n",
      "Epoch [27/500] | Train Loss: 0.281662 | Val Loss: 0.282636\n",
      "Epoch [28/500] | Train Loss: 0.281117 | Val Loss: 0.282136\n",
      "Epoch [29/500] | Train Loss: 0.280267 | Val Loss: 0.281693\n",
      "Epoch [30/500] | Train Loss: 0.279668 | Val Loss: 0.280591\n",
      "Epoch [31/500] | Train Loss: 0.279192 | Val Loss: 0.280592\n",
      "Epoch [32/500] | Train Loss: 0.278558 | Val Loss: 0.281683\n",
      "Epoch [33/500] | Train Loss: 0.278182 | Val Loss: 0.280373\n",
      "Epoch [34/500] | Train Loss: 0.277748 | Val Loss: 0.280598\n",
      "Epoch [35/500] | Train Loss: 0.277313 | Val Loss: 0.279215\n",
      "Epoch [36/500] | Train Loss: 0.276788 | Val Loss: 0.278286\n",
      "Epoch [37/500] | Train Loss: 0.275929 | Val Loss: 0.278453\n",
      "Epoch [38/500] | Train Loss: 0.275110 | Val Loss: 0.277104\n",
      "Epoch [39/500] | Train Loss: 0.274491 | Val Loss: 0.275802\n",
      "Epoch [40/500] | Train Loss: 0.273776 | Val Loss: 0.275813\n",
      "Epoch [41/500] | Train Loss: 0.273303 | Val Loss: 0.275305\n",
      "Epoch [42/500] | Train Loss: 0.272736 | Val Loss: 0.274760\n",
      "Epoch [43/500] | Train Loss: 0.272301 | Val Loss: 0.274577\n",
      "Epoch [44/500] | Train Loss: 0.271953 | Val Loss: 0.274353\n",
      "Epoch [45/500] | Train Loss: 0.271351 | Val Loss: 0.276286\n",
      "Epoch [46/500] | Train Loss: 0.270921 | Val Loss: 0.273235\n",
      "Epoch [47/500] | Train Loss: 0.270375 | Val Loss: 0.272674\n",
      "Epoch [48/500] | Train Loss: 0.269944 | Val Loss: 0.272011\n",
      "Epoch [49/500] | Train Loss: 0.269678 | Val Loss: 0.272618\n",
      "Epoch [50/500] | Train Loss: 0.269467 | Val Loss: 0.271149\n",
      "Epoch [51/500] | Train Loss: 0.269191 | Val Loss: 0.271341\n",
      "Epoch [52/500] | Train Loss: 0.268914 | Val Loss: 0.271804\n",
      "Epoch [53/500] | Train Loss: 0.268689 | Val Loss: 0.270819\n",
      "Epoch [54/500] | Train Loss: 0.268454 | Val Loss: 0.271048\n",
      "Epoch [55/500] | Train Loss: 0.268266 | Val Loss: 0.270936\n",
      "Epoch [56/500] | Train Loss: 0.268087 | Val Loss: 0.270727\n",
      "Epoch [57/500] | Train Loss: 0.267941 | Val Loss: 0.272053\n",
      "Epoch [58/500] | Train Loss: 0.267797 | Val Loss: 0.269666\n",
      "Epoch [59/500] | Train Loss: 0.267626 | Val Loss: 0.270117\n",
      "Epoch [60/500] | Train Loss: 0.267309 | Val Loss: 0.269077\n",
      "Epoch [61/500] | Train Loss: 0.267107 | Val Loss: 0.269102\n",
      "Epoch [62/500] | Train Loss: 0.267000 | Val Loss: 0.269130\n",
      "Epoch [63/500] | Train Loss: 0.266806 | Val Loss: 0.268859\n",
      "Epoch [64/500] | Train Loss: 0.266683 | Val Loss: 0.269776\n",
      "Epoch [65/500] | Train Loss: 0.266461 | Val Loss: 0.268585\n",
      "Epoch [66/500] | Train Loss: 0.266373 | Val Loss: 0.268731\n",
      "Epoch [67/500] | Train Loss: 0.266405 | Val Loss: 0.268867\n",
      "Epoch [68/500] | Train Loss: 0.266329 | Val Loss: 0.268137\n",
      "Epoch [69/500] | Train Loss: 0.266176 | Val Loss: 0.268662\n",
      "Epoch [70/500] | Train Loss: 0.266087 | Val Loss: 0.268586\n",
      "Epoch [71/500] | Train Loss: 0.265894 | Val Loss: 0.268400\n",
      "Epoch [72/500] | Train Loss: 0.265782 | Val Loss: 0.268917\n",
      "Epoch [73/500] | Train Loss: 0.265715 | Val Loss: 0.267313\n",
      "Epoch [74/500] | Train Loss: 0.265609 | Val Loss: 0.268656\n",
      "Epoch [75/500] | Train Loss: 0.265552 | Val Loss: 0.267665\n",
      "Epoch [76/500] | Train Loss: 0.265441 | Val Loss: 0.268771\n",
      "Epoch [77/500] | Train Loss: 0.265411 | Val Loss: 0.267713\n",
      "Epoch [78/500] | Train Loss: 0.265410 | Val Loss: 0.268635\n",
      "Epoch [79/500] | Train Loss: 0.265335 | Val Loss: 0.267384\n",
      "Epoch [80/500] | Train Loss: 0.265355 | Val Loss: 0.266895\n",
      "Epoch [81/500] | Train Loss: 0.265385 | Val Loss: 0.268094\n",
      "Epoch [82/500] | Train Loss: 0.265376 | Val Loss: 0.269173\n",
      "Epoch [83/500] | Train Loss: 0.265288 | Val Loss: 0.267666\n",
      "Epoch [84/500] | Train Loss: 0.265247 | Val Loss: 0.267905\n",
      "Epoch [85/500] | Train Loss: 0.265236 | Val Loss: 0.268340\n",
      "Epoch [86/500] | Train Loss: 0.265161 | Val Loss: 0.267676\n",
      "Epoch [87/500] | Train Loss: 0.265123 | Val Loss: 0.266879\n",
      "Epoch [88/500] | Train Loss: 0.265132 | Val Loss: 0.267298\n",
      "Epoch [89/500] | Train Loss: 0.265084 | Val Loss: 0.267568\n",
      "Epoch [90/500] | Train Loss: 0.264961 | Val Loss: 0.267620\n",
      "Epoch [91/500] | Train Loss: 0.264972 | Val Loss: 0.267152\n",
      "Epoch [92/500] | Train Loss: 0.264948 | Val Loss: 0.267095\n",
      "Epoch [93/500] | Train Loss: 0.264885 | Val Loss: 0.267433\n",
      "Epoch [94/500] | Train Loss: 0.264846 | Val Loss: 0.267288\n",
      "Epoch [95/500] | Train Loss: 0.264803 | Val Loss: 0.267614\n",
      "Epoch [96/500] | Train Loss: 0.264734 | Val Loss: 0.267054\n",
      "Epoch [97/500] | Train Loss: 0.264744 | Val Loss: 0.267549\n",
      "⏳ Early stopping at epoch 97\n",
      "✅ Training complete. Best model saved as sae_model_6_3072.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5646357463587044,\n",
       "  0.4296048907582445,\n",
       "  0.40433580629575494,\n",
       "  0.38094315248394534,\n",
       "  0.3633628173991632,\n",
       "  0.34898595275677174,\n",
       "  0.33746110880982455,\n",
       "  0.32824598680859096,\n",
       "  0.3212363291898616,\n",
       "  0.31587958567836155,\n",
       "  0.31136095165351074,\n",
       "  0.3078488080558182,\n",
       "  0.30524123651535423,\n",
       "  0.30303444220310743,\n",
       "  0.30038968253878895,\n",
       "  0.29811122687307695,\n",
       "  0.2960324708658065,\n",
       "  0.2936735098563109,\n",
       "  0.2912557582802728,\n",
       "  0.2893906635408765,\n",
       "  0.287565917916548,\n",
       "  0.2862232499049478,\n",
       "  0.2848637790549574,\n",
       "  0.2838606225624349,\n",
       "  0.28275578153982783,\n",
       "  0.28214090536986763,\n",
       "  0.28166227829243984,\n",
       "  0.28111695539001663,\n",
       "  0.2802673163875856,\n",
       "  0.2796682889982947,\n",
       "  0.2791917120979777,\n",
       "  0.278557852059529,\n",
       "  0.27818241567967966,\n",
       "  0.2777476436412907,\n",
       "  0.27731287658273557,\n",
       "  0.2767877736550519,\n",
       "  0.2759289249549226,\n",
       "  0.27511014672035866,\n",
       "  0.2744906287435728,\n",
       "  0.27377619896119093,\n",
       "  0.2733028698474803,\n",
       "  0.2727361684090899,\n",
       "  0.2723010436872749,\n",
       "  0.27195265582001843,\n",
       "  0.27135082290852086,\n",
       "  0.27092095107828135,\n",
       "  0.2703752946125169,\n",
       "  0.2699442437074374,\n",
       "  0.26967837742683265,\n",
       "  0.2694673181715606,\n",
       "  0.2691905266309009,\n",
       "  0.2689137939870623,\n",
       "  0.2686892131113994,\n",
       "  0.2684543676110319,\n",
       "  0.2682659799217411,\n",
       "  0.2680872174461899,\n",
       "  0.2679412541178298,\n",
       "  0.26779662186942554,\n",
       "  0.2676263501698991,\n",
       "  0.26730901496443404,\n",
       "  0.2671072213190254,\n",
       "  0.2669997060611542,\n",
       "  0.26680623968157186,\n",
       "  0.26668273267446324,\n",
       "  0.2664608709163916,\n",
       "  0.26637290877053665,\n",
       "  0.2664046311042263,\n",
       "  0.2663292747700586,\n",
       "  0.26617575510193486,\n",
       "  0.2660870746788207,\n",
       "  0.2658940004910177,\n",
       "  0.2657820140840629,\n",
       "  0.26571481784106127,\n",
       "  0.2656092237918935,\n",
       "  0.26555166049707885,\n",
       "  0.2654406718589008,\n",
       "  0.265411203141144,\n",
       "  0.26540981462130386,\n",
       "  0.265335163847165,\n",
       "  0.2653551867947845,\n",
       "  0.2653854974082412,\n",
       "  0.2653757482413193,\n",
       "  0.2652876593154117,\n",
       "  0.26524680699085845,\n",
       "  0.2652356056810782,\n",
       "  0.26516101938671904,\n",
       "  0.2651226803712005,\n",
       "  0.2651318215913905,\n",
       "  0.26508387609291406,\n",
       "  0.2649613736836053,\n",
       "  0.2649718995556744,\n",
       "  0.26494848035839985,\n",
       "  0.2648845446502024,\n",
       "  0.2648463848806855,\n",
       "  0.2648034993063277,\n",
       "  0.2647342116908824,\n",
       "  0.26474357151059097],\n",
       " [0.44362148103815896,\n",
       "  0.4155263757185541,\n",
       "  0.39197173499678883,\n",
       "  0.3716949506705081,\n",
       "  0.35510629619855705,\n",
       "  0.34169735055572736,\n",
       "  0.33313716414615924,\n",
       "  0.32516714549712167,\n",
       "  0.31915263381794107,\n",
       "  0.3141093912765795,\n",
       "  0.31088484081962975,\n",
       "  0.3075107424681673,\n",
       "  0.30599067831878035,\n",
       "  0.303366510374994,\n",
       "  0.301121090115019,\n",
       "  0.2982082970553175,\n",
       "  0.2963715865743659,\n",
       "  0.2934742219354675,\n",
       "  0.2916061370397187,\n",
       "  0.2926364850886465,\n",
       "  0.2892550211316862,\n",
       "  0.2883034121321443,\n",
       "  0.2882096365107454,\n",
       "  0.2850186766547384,\n",
       "  0.2851510963212775,\n",
       "  0.28356603008237996,\n",
       "  0.28263550379768826,\n",
       "  0.28213643283315354,\n",
       "  0.2816934321649565,\n",
       "  0.28059094998312123,\n",
       "  0.2805915000787932,\n",
       "  0.28168306472518034,\n",
       "  0.2803729541022546,\n",
       "  0.28059787825347055,\n",
       "  0.27921520575602354,\n",
       "  0.278286249480498,\n",
       "  0.27845347390733227,\n",
       "  0.27710353104747626,\n",
       "  0.27580151200719105,\n",
       "  0.27581329680879735,\n",
       "  0.2753053287231593,\n",
       "  0.2747602920816715,\n",
       "  0.2745767971127562,\n",
       "  0.2743527757489883,\n",
       "  0.27628638062589633,\n",
       "  0.2732351318678469,\n",
       "  0.2726738514612534,\n",
       "  0.2720107101666959,\n",
       "  0.2726182865571042,\n",
       "  0.27114946011019303,\n",
       "  0.2713412311681126,\n",
       "  0.2718041261603338,\n",
       "  0.2708186006450483,\n",
       "  0.27104802470810163,\n",
       "  0.27093632164961085,\n",
       "  0.2707271872889433,\n",
       "  0.2720533074116134,\n",
       "  0.26966633654978267,\n",
       "  0.27011674158241955,\n",
       "  0.2690774898574698,\n",
       "  0.26910245689237533,\n",
       "  0.26912975361701746,\n",
       "  0.26885870827539415,\n",
       "  0.26977553467398124,\n",
       "  0.26858518761603406,\n",
       "  0.268730537589384,\n",
       "  0.2688665236491554,\n",
       "  0.2681370379265567,\n",
       "  0.2686618452929856,\n",
       "  0.2685862370797384,\n",
       "  0.2683995225444396,\n",
       "  0.2689168220477134,\n",
       "  0.26731341570376926,\n",
       "  0.2686557235164719,\n",
       "  0.2676647111096769,\n",
       "  0.2687710843124458,\n",
       "  0.26771336484730296,\n",
       "  0.2686348551368459,\n",
       "  0.2673842526262399,\n",
       "  0.26689469180039177,\n",
       "  0.26809370469962074,\n",
       "  0.2691725642550554,\n",
       "  0.2676658817066853,\n",
       "  0.2679052762255962,\n",
       "  0.2683396537921732,\n",
       "  0.2676764553344791,\n",
       "  0.2668792163938257,\n",
       "  0.26729805689248676,\n",
       "  0.2675676069961312,\n",
       "  0.2676201311212932,\n",
       "  0.26715187383409283,\n",
       "  0.2670945551707505,\n",
       "  0.267432590639708,\n",
       "  0.2672877437231908,\n",
       "  0.2676141239895103,\n",
       "  0.2670538466977733,\n",
       "  0.2675492081091219])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "sae = SparseAutoencoder(input_dim=768, hidden_dim=3072, top_k=50)\n",
    "train_sae(layer6_embeddings, sae, model_prefix=\"sae_model_6_3072\", epochs=500,\n",
    "          batch_size=64, lr=5e-4, weight_decay=1e-6,\n",
    "          train_losses=[],val_losses=val_losses, device=device, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13db1e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.06 GiB. GPU 0 has a total capacity of 23.78 GiB of which 1.29 GiB is free. Process 1364620 has 609.79 MiB memory in use. Process 1098709 has 5.87 GiB memory in use. Process 2597354 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 11.44 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 188.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m train_losses, val_losses\n\u001b[1;32m      5\u001b[0m sae \u001b[38;5;241m=\u001b[39m SparseAutoencoder(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3072\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_sae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer12_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msae_model_12_3072\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GPTAndPrejudice/train_sae.py:19\u001b[0m, in \u001b[0;36mtrain_sae\u001b[0;34m(embeddings, sae, model_prefix, batch_size, epochs, lr, weight_decay, device, patience, train_losses, val_losses)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     17\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(embeddings, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 19\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m sae\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(sae\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.06 GiB. GPU 0 has a total capacity of 23.78 GiB of which 1.29 GiB is free. Process 1364620 has 609.79 MiB memory in use. Process 1098709 has 5.87 GiB memory in use. Process 2597354 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 11.44 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 188.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "sae = SparseAutoencoder(input_dim=768, hidden_dim=3072, top_k=60)\n",
    "train_sae(layer12_embeddings, sae, model_prefix=\"sae_model_12_3072\", epochs=500,\n",
    "          batch_size=64, lr=5e-3, weight_decay=1e-6, \n",
    "          train_losses=[],val_losses=val_losses, device=device, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae61ddeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 4.2220, Val Loss: 3.5712\n",
      "Epoch [2/500], Train Loss: 3.7283, Val Loss: 3.4367\n",
      "Epoch [3/500], Train Loss: 3.5819, Val Loss: 3.3105\n",
      "Epoch [4/500], Train Loss: 3.4556, Val Loss: 3.3198\n",
      "Epoch [5/500], Train Loss: 3.4234, Val Loss: 3.4495\n",
      "Epoch [6/500], Train Loss: 3.4090, Val Loss: 4.0589\n",
      "Epoch [7/500], Train Loss: 3.4005, Val Loss: 5.4380\n",
      "Epoch [8/500], Train Loss: 3.3950, Val Loss: 6.2446\n",
      "Epoch [9/500], Train Loss: 3.3905, Val Loss: 10.7519\n",
      "Epoch [10/500], Train Loss: 3.3870, Val Loss: 13.5389\n",
      "Epoch [11/500], Train Loss: 3.3847, Val Loss: 21.7800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m train_losses, val_losses\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_sae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer6_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msae_layer6_512.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-03\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GPTAndPrejudice/train_sae.py:59\u001b[0m, in \u001b[0;36mtrain_sae\u001b[0;34m(embeddings, sae, model_name, train_losses, val_losses, batch_size, epochs, lr, hidden_dim, weight_decay, device, patience)\u001b[0m\n\u001b[1;32m     57\u001b[0m sparsity_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(encoded, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-4\u001b[39m  \n\u001b[1;32m     58\u001b[0m total_loss_val \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m sparsity_loss\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtotal_loss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     61\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss_val\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "train_sae(layer6_embeddings, model_name=\"sae_layer6_512.pth\", epochs=500,\n",
    "          batch_size=32, lr=5e-5, hidden_dim=512, weight_decay=1e-03, \n",
    "          train_losses=[],val_losses=val_losses, device=device, patience=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6be99e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 9.3784, Val Loss: 8.6603\n",
      "Epoch [2/500], Train Loss: 8.5349, Val Loss: 8.2008\n",
      "Epoch [3/500], Train Loss: 8.1273, Val Loss: 7.8981\n",
      "Epoch [4/500], Train Loss: 7.8221, Val Loss: 7.6962\n",
      "Epoch [5/500], Train Loss: 7.5788, Val Loss: 7.4449\n",
      "Epoch [6/500], Train Loss: 7.3745, Val Loss: 7.2593\n",
      "Epoch [7/500], Train Loss: 7.1991, Val Loss: 7.0678\n",
      "Epoch [8/500], Train Loss: 7.0452, Val Loss: 6.8777\n",
      "Epoch [9/500], Train Loss: 6.9088, Val Loss: 6.7717\n",
      "Epoch [10/500], Train Loss: 6.7874, Val Loss: 6.6290\n",
      "Epoch [11/500], Train Loss: 6.6799, Val Loss: 6.5192\n",
      "Epoch [12/500], Train Loss: 6.5856, Val Loss: 6.3898\n",
      "Epoch [13/500], Train Loss: 6.5007, Val Loss: 6.3215\n",
      "Epoch [14/500], Train Loss: 6.4268, Val Loss: 6.2082\n",
      "Epoch [15/500], Train Loss: 6.3615, Val Loss: 6.1749\n",
      "Epoch [16/500], Train Loss: 6.3047, Val Loss: 6.1079\n",
      "Epoch [17/500], Train Loss: 6.2549, Val Loss: 6.0426\n",
      "Epoch [18/500], Train Loss: 6.2121, Val Loss: 6.0207\n",
      "Epoch [19/500], Train Loss: 6.1741, Val Loss: 5.9502\n",
      "Epoch [20/500], Train Loss: 6.1426, Val Loss: 5.9204\n",
      "Epoch [21/500], Train Loss: 6.1137, Val Loss: 5.9035\n",
      "Epoch [22/500], Train Loss: 6.0912, Val Loss: 5.8688\n",
      "Epoch [23/500], Train Loss: 6.0682, Val Loss: 5.8455\n",
      "Epoch [24/500], Train Loss: 6.0516, Val Loss: 5.8259\n",
      "Epoch [25/500], Train Loss: 6.0346, Val Loss: 5.8177\n",
      "Epoch [26/500], Train Loss: 6.0192, Val Loss: 5.8152\n",
      "Epoch [27/500], Train Loss: 6.0068, Val Loss: 5.7729\n",
      "Epoch [28/500], Train Loss: 5.9945, Val Loss: 5.7581\n",
      "Epoch [29/500], Train Loss: 5.9837, Val Loss: 5.7516\n",
      "Epoch [30/500], Train Loss: 5.9728, Val Loss: 5.7452\n",
      "Epoch [31/500], Train Loss: 5.9627, Val Loss: 5.7299\n",
      "Epoch [32/500], Train Loss: 5.9539, Val Loss: 5.7112\n",
      "Epoch [33/500], Train Loss: 5.9444, Val Loss: 5.7008\n",
      "Epoch [34/500], Train Loss: 5.9364, Val Loss: 5.7059\n",
      "Epoch [35/500], Train Loss: 5.9277, Val Loss: 5.7007\n",
      "Epoch [36/500], Train Loss: 5.9196, Val Loss: 5.6674\n",
      "Epoch [37/500], Train Loss: 5.9111, Val Loss: 5.6785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_79434/843324810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m train_sae(layer12_embeddings, \"sae_layer12.pth\", epochs=500,\n\u001b[0m\u001b[1;32m      6\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           train_losses=[],val_losses=val_losses, device=device)\n",
      "\u001b[0;32m~/development/Build GPT from scratch/train_sae.py\u001b[0m in \u001b[0;36mtrain_sae\u001b[0;34m(embeddings, model_name, train_losses, val_losses, batch_size, epochs, lr, hidden_dim, weight_decay, device, patience)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtotal_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msparsity_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                             )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    185\u001b[0m             )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "train_sae(layer12_embeddings, \"sae_layer12.pth\", epochs=500,\n",
    "          batch_size=32, lr=1e-5, hidden_dim=512, weight_decay=1e-02, \n",
    "          train_losses=[],val_losses=val_losses, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21bec9e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 10.2528, Val Loss: 9.4597\n",
      "Epoch [2/500], Train Loss: 9.1150, Val Loss: 8.7378\n",
      "Epoch [3/500], Train Loss: 8.4605, Val Loss: 8.2728\n",
      "Epoch [4/500], Train Loss: 7.9646, Val Loss: 7.8654\n",
      "Epoch [5/500], Train Loss: 7.5986, Val Loss: 7.6130\n",
      "Epoch [6/500], Train Loss: 7.3334, Val Loss: 7.4305\n",
      "Epoch [7/500], Train Loss: 7.1272, Val Loss: 7.0853\n",
      "Epoch [8/500], Train Loss: 6.9541, Val Loss: 7.0251\n",
      "Epoch [9/500], Train Loss: 6.8049, Val Loss: 6.7875\n",
      "Epoch [10/500], Train Loss: 6.6743, Val Loss: 6.6703\n",
      "Epoch [11/500], Train Loss: 6.5586, Val Loss: 6.5076\n",
      "Epoch [12/500], Train Loss: 6.4574, Val Loss: 6.4108\n",
      "Epoch [13/500], Train Loss: 6.3698, Val Loss: 6.2961\n",
      "Epoch [14/500], Train Loss: 6.2927, Val Loss: 6.1800\n",
      "Epoch [15/500], Train Loss: 6.2243, Val Loss: 6.0854\n",
      "Epoch [16/500], Train Loss: 6.1671, Val Loss: 6.0580\n",
      "Epoch [17/500], Train Loss: 6.1168, Val Loss: 5.9766\n",
      "Epoch [18/500], Train Loss: 6.0739, Val Loss: 5.9582\n",
      "Epoch [19/500], Train Loss: 6.0375, Val Loss: 5.8765\n",
      "Epoch [20/500], Train Loss: 6.0081, Val Loss: 5.8419\n",
      "Epoch [21/500], Train Loss: 5.9820, Val Loss: 5.8041\n",
      "Epoch [22/500], Train Loss: 5.9598, Val Loss: 5.7608\n",
      "Epoch [23/500], Train Loss: 5.9408, Val Loss: 5.7330\n",
      "Epoch [24/500], Train Loss: 5.9259, Val Loss: 5.7223\n",
      "Epoch [25/500], Train Loss: 5.9122, Val Loss: 5.7230\n",
      "Epoch [26/500], Train Loss: 5.8996, Val Loss: 5.6935\n",
      "Epoch [27/500], Train Loss: 5.8898, Val Loss: 5.7098\n",
      "Epoch [28/500], Train Loss: 5.8800, Val Loss: 5.6719\n",
      "Epoch [29/500], Train Loss: 5.8727, Val Loss: 5.6658\n",
      "Epoch [30/500], Train Loss: 5.8653, Val Loss: 5.6590\n",
      "Epoch [31/500], Train Loss: 5.8592, Val Loss: 5.6572\n",
      "Epoch [32/500], Train Loss: 5.8540, Val Loss: 5.6581\n",
      "Epoch [33/500], Train Loss: 5.8473, Val Loss: 5.6347\n",
      "Epoch [34/500], Train Loss: 5.8415, Val Loss: 5.6329\n",
      "Epoch [35/500], Train Loss: 5.8365, Val Loss: 5.6255\n",
      "Epoch [36/500], Train Loss: 5.8317, Val Loss: 5.6416\n",
      "Epoch [37/500], Train Loss: 5.8274, Val Loss: 5.6184\n",
      "Epoch [38/500], Train Loss: 5.8236, Val Loss: 5.6166\n",
      "Epoch [39/500], Train Loss: 5.8192, Val Loss: 5.6101\n",
      "Epoch [40/500], Train Loss: 5.8154, Val Loss: 5.6094\n",
      "Epoch [41/500], Train Loss: 5.8117, Val Loss: 5.6192\n",
      "Epoch [42/500], Train Loss: 5.8080, Val Loss: 5.6058\n",
      "Epoch [43/500], Train Loss: 5.8044, Val Loss: 5.6006\n",
      "Epoch [44/500], Train Loss: 5.8004, Val Loss: 5.5964\n",
      "Epoch [45/500], Train Loss: 5.7963, Val Loss: 5.5907\n",
      "Epoch [46/500], Train Loss: 5.7918, Val Loss: 5.5857\n",
      "Epoch [47/500], Train Loss: 5.7894, Val Loss: 5.5820\n",
      "Epoch [48/500], Train Loss: 5.7867, Val Loss: 5.5801\n",
      "Epoch [49/500], Train Loss: 5.7834, Val Loss: 5.5932\n",
      "Epoch [50/500], Train Loss: 5.7808, Val Loss: 5.5755\n",
      "Epoch [51/500], Train Loss: 5.7773, Val Loss: 5.5737\n",
      "Epoch [52/500], Train Loss: 5.7737, Val Loss: 5.5764\n",
      "Epoch [53/500], Train Loss: 5.7716, Val Loss: 5.5687\n",
      "Epoch [54/500], Train Loss: 5.7676, Val Loss: 5.5648\n",
      "Epoch [55/500], Train Loss: 5.7666, Val Loss: 5.5682\n",
      "Epoch [56/500], Train Loss: 5.7625, Val Loss: 5.5616\n",
      "Epoch [57/500], Train Loss: 5.7591, Val Loss: 5.5740\n",
      "Epoch [58/500], Train Loss: 5.7580, Val Loss: 5.5533\n",
      "Epoch [59/500], Train Loss: 5.7541, Val Loss: 5.5636\n",
      "Epoch [60/500], Train Loss: 5.7516, Val Loss: 5.5506\n",
      "Epoch [61/500], Train Loss: 5.7485, Val Loss: 5.5468\n",
      "Epoch [62/500], Train Loss: 5.7463, Val Loss: 5.5455\n",
      "Epoch [63/500], Train Loss: 5.7441, Val Loss: 5.5391\n",
      "Epoch [64/500], Train Loss: 5.7406, Val Loss: 5.5416\n",
      "Epoch [65/500], Train Loss: 5.7364, Val Loss: 5.5409\n",
      "Epoch [66/500], Train Loss: 5.7347, Val Loss: 5.5318\n",
      "Epoch [67/500], Train Loss: 5.7320, Val Loss: 5.5447\n",
      "Epoch [68/500], Train Loss: 5.7283, Val Loss: 5.5312\n",
      "Epoch [69/500], Train Loss: 5.7272, Val Loss: 5.5253\n",
      "Epoch [70/500], Train Loss: 5.7240, Val Loss: 5.5245\n",
      "Epoch [71/500], Train Loss: 5.7207, Val Loss: 5.5200\n",
      "Epoch [72/500], Train Loss: 5.7179, Val Loss: 5.5164\n",
      "Epoch [73/500], Train Loss: 5.7157, Val Loss: 5.5126\n",
      "Epoch [74/500], Train Loss: 5.7129, Val Loss: 5.5185\n",
      "Epoch [75/500], Train Loss: 5.7104, Val Loss: 5.5192\n",
      "Epoch [76/500], Train Loss: 5.7067, Val Loss: 5.5179\n",
      "Epoch [77/500], Train Loss: 5.7037, Val Loss: 5.5082\n",
      "Epoch [78/500], Train Loss: 5.7011, Val Loss: 5.5011\n",
      "Epoch [79/500], Train Loss: 5.6982, Val Loss: 5.5007\n",
      "Epoch [80/500], Train Loss: 5.6945, Val Loss: 5.4939\n",
      "Epoch [81/500], Train Loss: 5.6924, Val Loss: 5.4981\n",
      "Epoch [82/500], Train Loss: 5.6892, Val Loss: 5.4936\n",
      "Epoch [83/500], Train Loss: 5.6860, Val Loss: 5.4911\n",
      "Epoch [84/500], Train Loss: 5.6836, Val Loss: 5.4826\n",
      "Epoch [85/500], Train Loss: 5.6801, Val Loss: 5.4852\n",
      "Epoch [86/500], Train Loss: 5.6769, Val Loss: 5.4798\n",
      "Epoch [87/500], Train Loss: 5.6742, Val Loss: 5.4710\n",
      "Epoch [88/500], Train Loss: 5.6715, Val Loss: 5.4699\n",
      "Epoch [89/500], Train Loss: 5.6683, Val Loss: 5.4716\n",
      "Epoch [90/500], Train Loss: 5.6640, Val Loss: 5.4678\n",
      "Epoch [91/500], Train Loss: 5.6606, Val Loss: 5.4581\n",
      "Epoch [92/500], Train Loss: 5.6574, Val Loss: 5.4639\n",
      "Epoch [93/500], Train Loss: 5.6545, Val Loss: 5.4526\n",
      "Epoch [94/500], Train Loss: 5.6510, Val Loss: 5.4753\n",
      "Epoch [95/500], Train Loss: 5.6481, Val Loss: 5.4436\n",
      "Epoch [96/500], Train Loss: 5.6442, Val Loss: 5.4483\n",
      "Epoch [97/500], Train Loss: 5.6397, Val Loss: 5.4428\n",
      "Epoch [98/500], Train Loss: 5.6369, Val Loss: 5.4466\n",
      "Epoch [99/500], Train Loss: 5.6325, Val Loss: 5.4341\n",
      "Epoch [100/500], Train Loss: 5.6298, Val Loss: 5.4299\n",
      "Epoch [101/500], Train Loss: 5.6256, Val Loss: 5.4316\n",
      "Epoch [102/500], Train Loss: 5.6221, Val Loss: 5.4229\n",
      "Epoch [103/500], Train Loss: 5.6174, Val Loss: 5.4133\n",
      "Epoch [104/500], Train Loss: 5.6144, Val Loss: 5.4142\n",
      "Epoch [105/500], Train Loss: 5.6097, Val Loss: 5.4122\n",
      "Epoch [106/500], Train Loss: 5.6059, Val Loss: 5.4086\n",
      "Epoch [107/500], Train Loss: 5.6019, Val Loss: 5.4019\n",
      "Epoch [108/500], Train Loss: 5.5984, Val Loss: 5.3951\n",
      "Epoch [109/500], Train Loss: 5.5928, Val Loss: 5.3954\n",
      "Epoch [110/500], Train Loss: 5.5884, Val Loss: 5.3994\n",
      "Epoch [111/500], Train Loss: 5.5842, Val Loss: 5.3860\n",
      "Epoch [112/500], Train Loss: 5.5797, Val Loss: 5.3822\n",
      "Epoch [113/500], Train Loss: 5.5744, Val Loss: 5.3781\n",
      "Epoch [114/500], Train Loss: 5.5699, Val Loss: 5.3781\n",
      "Epoch [115/500], Train Loss: 5.5636, Val Loss: 5.3640\n",
      "Epoch [116/500], Train Loss: 5.5599, Val Loss: 5.3692\n",
      "Epoch [117/500], Train Loss: 5.5538, Val Loss: 5.3729\n",
      "Epoch [118/500], Train Loss: 5.5497, Val Loss: 5.3665\n",
      "Epoch [119/500], Train Loss: 5.5436, Val Loss: 5.3578\n",
      "Epoch [120/500], Train Loss: 5.5388, Val Loss: 5.3528\n",
      "Epoch [121/500], Train Loss: 5.5334, Val Loss: 5.3501\n",
      "Epoch [122/500], Train Loss: 5.5296, Val Loss: 5.3449\n",
      "Epoch [123/500], Train Loss: 5.5253, Val Loss: 5.3412\n",
      "Epoch [124/500], Train Loss: 5.5220, Val Loss: 5.3469\n",
      "Epoch [125/500], Train Loss: 5.5172, Val Loss: 5.3335\n",
      "Epoch [126/500], Train Loss: 5.5134, Val Loss: 5.3364\n",
      "Epoch [127/500], Train Loss: 5.5096, Val Loss: 5.3301\n",
      "Epoch [128/500], Train Loss: 5.5062, Val Loss: 5.3328\n",
      "Epoch [129/500], Train Loss: 5.5032, Val Loss: 5.3269\n",
      "Epoch [130/500], Train Loss: 5.4998, Val Loss: 5.3347\n",
      "Epoch [131/500], Train Loss: 5.4959, Val Loss: 5.3237\n",
      "Epoch [132/500], Train Loss: 5.4942, Val Loss: 5.3229\n",
      "Epoch [133/500], Train Loss: 5.4908, Val Loss: 5.3231\n",
      "Epoch [134/500], Train Loss: 5.4876, Val Loss: 5.3182\n",
      "Epoch [135/500], Train Loss: 5.4845, Val Loss: 5.3194\n",
      "Epoch [136/500], Train Loss: 5.4823, Val Loss: 5.3209\n",
      "Epoch [137/500], Train Loss: 5.4794, Val Loss: 5.3144\n",
      "Epoch [138/500], Train Loss: 5.4773, Val Loss: 5.3126\n",
      "Epoch [139/500], Train Loss: 5.4744, Val Loss: 5.3088\n",
      "Epoch [140/500], Train Loss: 5.4723, Val Loss: 5.3105\n",
      "Epoch [141/500], Train Loss: 5.4693, Val Loss: 5.3055\n",
      "Epoch [142/500], Train Loss: 5.4687, Val Loss: 5.3126\n",
      "Epoch [143/500], Train Loss: 5.4657, Val Loss: 5.3092\n",
      "Epoch [144/500], Train Loss: 5.4639, Val Loss: 5.3019\n",
      "Epoch [145/500], Train Loss: 5.4625, Val Loss: 5.3092\n",
      "Epoch [146/500], Train Loss: 5.4607, Val Loss: 5.3024\n",
      "Epoch [147/500], Train Loss: 5.4588, Val Loss: 5.3024\n",
      "Epoch [148/500], Train Loss: 5.4577, Val Loss: 5.3096\n",
      "Epoch [149/500], Train Loss: 5.4552, Val Loss: 5.3000\n",
      "Epoch [150/500], Train Loss: 5.4546, Val Loss: 5.2963\n",
      "Epoch [151/500], Train Loss: 5.4531, Val Loss: 5.2961\n",
      "Epoch [152/500], Train Loss: 5.4508, Val Loss: 5.3059\n",
      "Epoch [153/500], Train Loss: 5.4495, Val Loss: 5.2962\n",
      "Epoch [154/500], Train Loss: 5.4487, Val Loss: 5.2973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [155/500], Train Loss: 5.4472, Val Loss: 5.2968\n",
      "Epoch [156/500], Train Loss: 5.4463, Val Loss: 5.2974\n",
      "Epoch [157/500], Train Loss: 5.4433, Val Loss: 5.2900\n",
      "Epoch [158/500], Train Loss: 5.4432, Val Loss: 5.3019\n",
      "Epoch [159/500], Train Loss: 5.4421, Val Loss: 5.2970\n",
      "Epoch [160/500], Train Loss: 5.4419, Val Loss: 5.2957\n",
      "Epoch [161/500], Train Loss: 5.4415, Val Loss: 5.2974\n",
      "Epoch [162/500], Train Loss: 5.4405, Val Loss: 5.2977\n",
      "Epoch [163/500], Train Loss: 5.4403, Val Loss: 5.3023\n",
      "Epoch [164/500], Train Loss: 5.4392, Val Loss: 5.2934\n",
      "Epoch [165/500], Train Loss: 5.4390, Val Loss: 5.3098\n",
      "Epoch [166/500], Train Loss: 5.4385, Val Loss: 5.3026\n",
      "Epoch [167/500], Train Loss: 5.4382, Val Loss: 5.2926\n",
      "⏳ Early stopping at epoch 167. No improvement for 10 epochs.\n",
      "✅ SAE training completed. Best model saved as sae_layer12_expanded.pth.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "train_sae(layer12_embeddings, \"sae_layer12_expanded_1700.pth\", epochs=500,\n",
    "          batch_size=32, lr=1e-5, hidden_dim=1700, weight_decay=1e-04, \n",
    "          train_losses=[],val_losses=val_losses, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7c4eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"losses_sae.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "train_losses = data[\"train_losses\"]\n",
    "val_losses = data[\"val_losses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7032254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparse_auto_encoder import SparseAutoencoder\n",
    "\n",
    "sae_layer12_expanded = SparseAutoencoder(input_dim=768, hidden_dim=1700)\n",
    "sae_layer12_expanded.load_state_dict(torch.load(\"sae_layer12_expanded.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e49d48a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 10.7967, Val Loss: 9.8300\n",
      "Epoch [2/500], Train Loss: 9.4837, Val Loss: 9.0222\n",
      "Epoch [3/500], Train Loss: 8.7017, Val Loss: 8.3775\n",
      "Epoch [4/500], Train Loss: 8.1056, Val Loss: 8.0037\n",
      "Epoch [5/500], Train Loss: 7.6678, Val Loss: 7.7563\n",
      "Epoch [6/500], Train Loss: 7.3639, Val Loss: 7.4141\n",
      "Epoch [7/500], Train Loss: 7.1397, Val Loss: 7.2778\n",
      "Epoch [8/500], Train Loss: 6.9595, Val Loss: 7.0649\n",
      "Epoch [9/500], Train Loss: 6.8049, Val Loss: 6.8241\n",
      "Epoch [10/500], Train Loss: 6.6685, Val Loss: 6.6474\n",
      "Epoch [11/500], Train Loss: 6.5516, Val Loss: 6.5122\n",
      "Epoch [12/500], Train Loss: 6.4494, Val Loss: 6.3105\n",
      "Epoch [13/500], Train Loss: 6.3583, Val Loss: 6.2694\n",
      "Epoch [14/500], Train Loss: 6.2799, Val Loss: 6.1230\n",
      "Epoch [15/500], Train Loss: 6.2110, Val Loss: 6.0314\n",
      "Epoch [16/500], Train Loss: 6.1518, Val Loss: 5.9883\n",
      "Epoch [17/500], Train Loss: 6.1017, Val Loss: 5.9333\n",
      "Epoch [18/500], Train Loss: 6.0584, Val Loss: 5.8741\n",
      "Epoch [19/500], Train Loss: 6.0212, Val Loss: 5.8591\n",
      "Epoch [20/500], Train Loss: 5.9915, Val Loss: 5.8080\n",
      "Epoch [21/500], Train Loss: 5.9632, Val Loss: 5.7748\n",
      "Epoch [22/500], Train Loss: 5.9416, Val Loss: 5.7516\n",
      "Epoch [23/500], Train Loss: 5.9234, Val Loss: 5.7198\n",
      "Epoch [24/500], Train Loss: 5.9079, Val Loss: 5.6766\n",
      "Epoch [25/500], Train Loss: 5.8937, Val Loss: 5.6792\n",
      "Epoch [26/500], Train Loss: 5.8820, Val Loss: 5.6714\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_56525/3295904009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_sae(layer12_embeddings, model_name=\"sae_layer12_expanded_1.pth\", epochs=500,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2304\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-03\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           train_losses=[],val_losses=[], device=device)\n",
      "\u001b[0;32m~/development/Build GPT from scratch/train_sae.py\u001b[0m in \u001b[0;36mtrain_sae\u001b[0;34m(embeddings, sae, model_name, train_losses, val_losses, batch_size, epochs, lr, hidden_dim, weight_decay, device, patience)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0msparsity_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mtotal_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msparsity_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_sae(layer12_embeddings, model_name=\"sae_layer12_expanded_1.pth\", epochs=500,\n",
    "          patience=30, batch_size=32, lr=1e-5, hidden_dim=2304, weight_decay=1e-03, \n",
    "          train_losses=[],val_losses=[], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b9bdfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.419478  ,  1.097434  , -5.971122  , ..., -2.1528153 ,\n",
       "         4.6942825 ,  0.84316725],\n",
       "       [-0.55278456, -2.5868275 , -4.9650073 , ..., -4.4642773 ,\n",
       "         8.33209   , -6.3761005 ],\n",
       "       [11.52194   , -7.388887  , -7.3803024 , ..., -2.946519  ,\n",
       "        -0.92312884, -1.3777622 ],\n",
       "       ...,\n",
       "       [ 5.6442814 , -3.999446  , -2.193171  , ..., -2.052844  ,\n",
       "         0.05622917,  0.82508975],\n",
       "       [ 1.7008734 ,  1.0291717 ,  1.7188739 , ...,  0.36227274,\n",
       "         3.265392  , -0.6340358 ],\n",
       "       [ 4.86724   , -3.615003  , -0.6352348 , ...,  3.7851796 ,\n",
       "         1.1924815 , -1.6266764 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer12_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e31cb7e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariam/opt/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 10.7911, Val Loss: 9.8234\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [2/500], Train Loss: 9.4748, Val Loss: 8.9847\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [3/500], Train Loss: 8.6924, Val Loss: 8.4460\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [4/500], Train Loss: 8.0968, Val Loss: 7.9623\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [5/500], Train Loss: 7.6660, Val Loss: 7.7041\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [6/500], Train Loss: 7.3656, Val Loss: 7.4455\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [7/500], Train Loss: 7.1416, Val Loss: 7.1965\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [8/500], Train Loss: 6.9612, Val Loss: 7.0236\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [9/500], Train Loss: 6.8082, Val Loss: 6.9582\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [10/500], Train Loss: 6.6735, Val Loss: 6.6553\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [11/500], Train Loss: 6.5566, Val Loss: 6.5508\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [12/500], Train Loss: 6.4546, Val Loss: 6.4480\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [13/500], Train Loss: 6.3648, Val Loss: 6.2476\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [14/500], Train Loss: 6.2853, Val Loss: 6.1997\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [15/500], Train Loss: 6.2168, Val Loss: 6.1305\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [16/500], Train Loss: 6.1573, Val Loss: 6.0226\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [17/500], Train Loss: 6.1064, Val Loss: 5.9572\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [18/500], Train Loss: 6.0622, Val Loss: 5.8939\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [19/500], Train Loss: 6.0246, Val Loss: 5.8552\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [20/500], Train Loss: 5.9928, Val Loss: 5.8023\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [21/500], Train Loss: 5.9647, Val Loss: 5.8220\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [22/500], Train Loss: 5.9416, Val Loss: 5.7446\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [23/500], Train Loss: 5.9232, Val Loss: 5.7339\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [24/500], Train Loss: 5.9068, Val Loss: 5.6976\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [25/500], Train Loss: 5.8931, Val Loss: 5.6718\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [26/500], Train Loss: 5.8795, Val Loss: 5.6815\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [27/500], Train Loss: 5.8693, Val Loss: 5.6950\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [28/500], Train Loss: 5.8603, Val Loss: 5.6604\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [29/500], Train Loss: 5.8517, Val Loss: 5.6327\n",
      "🔹 New Learning Rate: 1e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_77814/3146884291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_sae(layer12_embeddings, model_name=\"sae_layer12_expanded_2.pth\", epochs=500,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2304\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           train_losses=[],val_losses=[], device=device)\n",
      "\u001b[0;32m~/development/Build GPT from scratch/train_sae.py\u001b[0m in \u001b[0;36mtrain_sae\u001b[0;34m(embeddings, sae, model_name, train_losses, val_losses, batch_size, epochs, lr, hidden_dim, weight_decay, device, patience)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mtotal_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msparsity_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                             )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    185\u001b[0m             )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_sae(layer12_embeddings, model_name=\"sae_layer12_expanded_2.pth\", epochs=500,\n",
    "          patience=30, batch_size=32, lr=1e-5, hidden_dim=2304, weight_decay=1e-02, \n",
    "          train_losses=[],val_losses=[], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4396b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 5.4032, Val Loss: 4.8838\n",
      "Epoch [2/500], Train Loss: 4.8314, Val Loss: 4.5803\n",
      "Epoch [3/500], Train Loss: 4.6227, Val Loss: 4.3705\n",
      "Epoch [4/500], Train Loss: 4.4661, Val Loss: 4.2232\n",
      "Epoch [5/500], Train Loss: 4.3308, Val Loss: 4.0835\n",
      "Epoch [6/500], Train Loss: 4.2070, Val Loss: 3.9584\n",
      "Epoch [7/500], Train Loss: 4.0939, Val Loss: 3.8387\n",
      "Epoch [8/500], Train Loss: 3.9889, Val Loss: 3.7355\n",
      "Epoch [9/500], Train Loss: 3.8890, Val Loss: 3.6505\n",
      "Epoch [10/500], Train Loss: 3.7948, Val Loss: 3.5522\n",
      "Epoch [11/500], Train Loss: 3.7048, Val Loss: 3.4585\n",
      "Epoch [12/500], Train Loss: 3.6188, Val Loss: 3.3752\n",
      "Epoch [13/500], Train Loss: 3.5362, Val Loss: 3.3095\n",
      "Epoch [14/500], Train Loss: 3.4574, Val Loss: 3.2342\n",
      "Epoch [15/500], Train Loss: 3.3810, Val Loss: 3.1437\n",
      "Epoch [16/500], Train Loss: 3.3091, Val Loss: 3.0838\n",
      "Epoch [17/500], Train Loss: 3.2393, Val Loss: 3.0454\n",
      "Epoch [18/500], Train Loss: 3.1739, Val Loss: 2.9571\n",
      "Epoch [19/500], Train Loss: 3.1122, Val Loss: 2.9221\n",
      "Epoch [20/500], Train Loss: 3.0534, Val Loss: 2.8719\n",
      "Epoch [21/500], Train Loss: 2.9997, Val Loss: 2.8324\n",
      "Epoch [22/500], Train Loss: 2.9484, Val Loss: 2.7815\n",
      "Epoch [23/500], Train Loss: 2.9033, Val Loss: 2.7553\n",
      "Epoch [24/500], Train Loss: 2.8621, Val Loss: 2.7304\n",
      "Epoch [25/500], Train Loss: 2.8245, Val Loss: 2.6951\n",
      "Epoch [26/500], Train Loss: 2.7932, Val Loss: 2.6664\n",
      "Epoch [27/500], Train Loss: 2.7636, Val Loss: 2.6491\n",
      "Epoch [28/500], Train Loss: 2.7386, Val Loss: 2.6487\n",
      "Epoch [29/500], Train Loss: 2.7154, Val Loss: 2.6069\n",
      "Epoch [30/500], Train Loss: 2.6948, Val Loss: 2.5818\n",
      "Epoch [31/500], Train Loss: 2.6770, Val Loss: 2.5935\n",
      "Epoch [32/500], Train Loss: 2.6596, Val Loss: 2.5670\n",
      "Epoch [33/500], Train Loss: 2.6443, Val Loss: 2.5564\n",
      "Epoch [34/500], Train Loss: 2.6305, Val Loss: 2.5508\n",
      "Epoch [35/500], Train Loss: 2.6181, Val Loss: 2.5182\n",
      "Epoch [36/500], Train Loss: 2.6063, Val Loss: 2.4987\n",
      "Epoch [37/500], Train Loss: 2.5954, Val Loss: 2.4808\n",
      "Epoch [38/500], Train Loss: 2.5850, Val Loss: 2.4758\n",
      "Epoch [39/500], Train Loss: 2.5749, Val Loss: 2.4758\n",
      "Epoch [40/500], Train Loss: 2.5662, Val Loss: 2.4595\n",
      "Epoch [41/500], Train Loss: 2.5578, Val Loss: 2.4502\n",
      "Epoch [42/500], Train Loss: 2.5498, Val Loss: 2.4327\n",
      "Epoch [43/500], Train Loss: 2.5421, Val Loss: 2.4309\n",
      "Epoch [44/500], Train Loss: 2.5348, Val Loss: 2.4060\n",
      "Epoch [45/500], Train Loss: 2.5281, Val Loss: 2.4051\n",
      "Epoch [46/500], Train Loss: 2.5213, Val Loss: 2.3890\n",
      "Epoch [47/500], Train Loss: 2.5153, Val Loss: 2.3934\n",
      "Epoch [48/500], Train Loss: 2.5096, Val Loss: 2.3822\n",
      "Epoch [49/500], Train Loss: 2.5038, Val Loss: 2.3713\n",
      "Epoch [50/500], Train Loss: 2.4981, Val Loss: 2.3746\n",
      "Epoch [51/500], Train Loss: 2.4932, Val Loss: 2.3545\n",
      "Epoch [52/500], Train Loss: 2.4882, Val Loss: 2.3585\n",
      "Epoch [53/500], Train Loss: 2.4830, Val Loss: 2.3612\n",
      "Epoch [54/500], Train Loss: 2.4789, Val Loss: 2.3486\n",
      "Epoch [55/500], Train Loss: 2.4747, Val Loss: 2.3362\n",
      "Epoch [56/500], Train Loss: 2.4704, Val Loss: 2.3300\n",
      "Epoch [57/500], Train Loss: 2.4665, Val Loss: 2.3159\n",
      "Epoch [58/500], Train Loss: 2.4629, Val Loss: 2.3163\n",
      "Epoch [59/500], Train Loss: 2.4587, Val Loss: 2.3144\n",
      "Epoch [60/500], Train Loss: 2.4556, Val Loss: 2.3000\n",
      "Epoch [61/500], Train Loss: 2.4523, Val Loss: 2.2937\n",
      "Epoch [62/500], Train Loss: 2.4492, Val Loss: 2.2939\n",
      "Epoch [63/500], Train Loss: 2.4456, Val Loss: 2.2936\n",
      "Epoch [64/500], Train Loss: 2.4427, Val Loss: 2.2853\n",
      "Epoch [65/500], Train Loss: 2.4397, Val Loss: 2.2831\n",
      "Epoch [66/500], Train Loss: 2.4367, Val Loss: 2.2918\n",
      "Epoch [67/500], Train Loss: 2.4341, Val Loss: 2.2811\n",
      "Epoch [68/500], Train Loss: 2.4312, Val Loss: 2.2792\n",
      "Epoch [69/500], Train Loss: 2.4291, Val Loss: 2.2743\n",
      "Epoch [70/500], Train Loss: 2.4268, Val Loss: 2.2785\n",
      "Epoch [71/500], Train Loss: 2.4242, Val Loss: 2.2710\n",
      "Epoch [72/500], Train Loss: 2.4219, Val Loss: 2.2713\n",
      "Epoch [73/500], Train Loss: 2.4197, Val Loss: 2.2678\n",
      "Epoch [74/500], Train Loss: 2.4178, Val Loss: 2.2679\n",
      "Epoch [75/500], Train Loss: 2.4161, Val Loss: 2.2676\n",
      "Epoch [76/500], Train Loss: 2.4137, Val Loss: 2.2637\n",
      "Epoch [77/500], Train Loss: 2.4119, Val Loss: 2.2652\n",
      "Epoch [78/500], Train Loss: 2.4099, Val Loss: 2.2636\n",
      "Epoch [79/500], Train Loss: 2.4085, Val Loss: 2.2616\n",
      "Epoch [80/500], Train Loss: 2.4068, Val Loss: 2.2599\n",
      "Epoch [81/500], Train Loss: 2.4046, Val Loss: 2.2588\n",
      "Epoch [82/500], Train Loss: 2.4035, Val Loss: 2.2614\n",
      "Epoch [83/500], Train Loss: 2.4013, Val Loss: 2.2563\n",
      "Epoch [84/500], Train Loss: 2.4001, Val Loss: 2.2587\n",
      "Epoch [85/500], Train Loss: 2.3987, Val Loss: 2.2576\n",
      "Epoch [86/500], Train Loss: 2.3965, Val Loss: 2.2611\n",
      "Epoch [87/500], Train Loss: 2.3957, Val Loss: 2.2602\n",
      "Epoch [88/500], Train Loss: 2.3942, Val Loss: 2.2544\n",
      "Epoch [89/500], Train Loss: 2.3930, Val Loss: 2.2612\n",
      "Epoch [90/500], Train Loss: 2.3918, Val Loss: 2.2553\n",
      "Epoch [91/500], Train Loss: 2.3904, Val Loss: 2.2672\n",
      "Epoch [92/500], Train Loss: 2.3892, Val Loss: 2.2577\n",
      "Epoch [93/500], Train Loss: 2.3883, Val Loss: 2.2595\n",
      "Epoch [94/500], Train Loss: 2.3867, Val Loss: 2.2592\n",
      "Epoch [95/500], Train Loss: 2.3857, Val Loss: 2.2560\n",
      "Epoch [96/500], Train Loss: 2.3849, Val Loss: 2.2554\n",
      "Epoch [97/500], Train Loss: 2.3838, Val Loss: 2.2577\n",
      "Epoch [98/500], Train Loss: 2.3825, Val Loss: 2.2665\n",
      "Epoch [99/500], Train Loss: 2.3814, Val Loss: 2.2623\n",
      "Epoch [100/500], Train Loss: 2.3807, Val Loss: 2.2632\n",
      "Epoch [101/500], Train Loss: 2.3795, Val Loss: 2.2535\n",
      "Epoch [102/500], Train Loss: 2.3786, Val Loss: 2.2654\n",
      "Epoch [103/500], Train Loss: 2.3777, Val Loss: 2.2649\n",
      "Epoch [104/500], Train Loss: 2.3771, Val Loss: 2.2706\n",
      "Epoch [105/500], Train Loss: 2.3759, Val Loss: 2.2680\n",
      "Epoch [106/500], Train Loss: 2.3751, Val Loss: 2.2704\n",
      "Epoch [107/500], Train Loss: 2.3746, Val Loss: 2.2621\n",
      "Epoch [108/500], Train Loss: 2.3739, Val Loss: 2.2782\n",
      "Epoch [109/500], Train Loss: 2.3723, Val Loss: 2.2634\n",
      "Epoch [110/500], Train Loss: 2.3717, Val Loss: 2.2713\n",
      "Epoch [111/500], Train Loss: 2.3711, Val Loss: 2.2798\n",
      "Epoch [112/500], Train Loss: 2.3704, Val Loss: 2.2711\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_62465/2350761703.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_sae(layer6_embeddings, model_name=\"sae_layer6_expanded.pth\", epochs=500,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3072\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           train_losses=[],val_losses=[], device=device)\n",
      "\u001b[0;32m~/development/Build GPT from scratch/train_sae.py\u001b[0m in \u001b[0;36mtrain_sae\u001b[0;34m(embeddings, sae, model_name, train_losses, val_losses, batch_size, epochs, lr, hidden_dim, weight_decay, device, patience)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0msparsity_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/Build GPT from scratch/sparse_auto_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_sae(layer6_embeddings, model_name=\"sae_layer6_expanded.pth\", epochs=500,\n",
    "          patience=30, batch_size=16, lr=1e-6, hidden_dim=3072, weight_decay=0.01, \n",
    "          train_losses=[],val_losses=[], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ce61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
