{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b225ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from clean_gutenberg_collected_work import clean_gutenberg_collected_work\n",
    "from sparse_auto_encoder import SparseAutoencoder\n",
    "from train_sae import train_sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b0d461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3191ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.2,\n",
    "    \"qkv_bias\": True,\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd8e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "checkpoint = torch.load(\"model_768_12_12_old_tok.pth\", weights_only=True, map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f4fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb48a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_and_clean_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads a text file and splits it into sentences while cleaning the text.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the text file.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of cleaned sentences from the book.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = clean_gutenberg_collected_work(file_path)\n",
    "\n",
    "    # Split text into sentences (simple heuristic using punctuation)\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "\n",
    "    # Remove very short or long sentences\n",
    "    sentences = [s.strip() for s in sentences if 5 < len(s.split()) < 60]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f25032d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25538\n"
     ]
    }
   ],
   "source": [
    "directory=\"original_texts/\"\n",
    "dataset = []\n",
    "\n",
    "sentences = load_and_clean_text(os.path.join(directory, 'complete_jane_austen.txt'))\n",
    "dataset += sentences\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f23472ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def get_token_embeddings(text, model, tokenizer, layers=[6, 12]):\n",
    "    \"\"\"\n",
    "    Extracts token embeddings from specified transformer layers.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "    - model: Custom GPT model.\n",
    "    - tokenizer: tiktoken encoding object.\n",
    "    - layers (list): Transformer layers to extract embeddings from.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Layer-wise token embeddings {layer_number: embeddings}\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = text_to_token_ids(text, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, hidden_states = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    embeddings = {} \n",
    "    for layer in layers:\n",
    "        if layer - 1 < len(hidden_states):\n",
    "            embeddings[layer] = hidden_states[layer - 1].squeeze(0).cpu().numpy()\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: Layer {layer} is out of range (max index {len(hidden_states) - 1})\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28bb562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token embeddings:\n",
      "Layer 6: (718536, 768)\n",
      "Layer 12: (718536, 768)\n"
     ]
    }
   ],
   "source": [
    "layer6_embeddings = []\n",
    "layer12_embeddings = []\n",
    "\n",
    "for sentence in dataset:\n",
    "    embeddings = get_token_embeddings(sentence, model, tokenizer)\n",
    "    layer6_embeddings.append(embeddings[6])\n",
    "    layer12_embeddings.append(embeddings[12])\n",
    "\n",
    "# Convert to NumPy and flatten tokens into dataset\n",
    "layer6_embeddings = np.vstack(layer6_embeddings)\n",
    "layer12_embeddings = np.vstack(layer12_embeddings)\n",
    "\n",
    "os.makedirs(\"sae_data\", exist_ok=True)\n",
    "np.save(\"sae_data/layer6_embeddings.npy\", layer6_embeddings)\n",
    "np.save(\"sae_data/layer12_embeddings.npy\", layer12_embeddings)\n",
    "\n",
    "print(\"Saved token embeddings:\")\n",
    "print(f\"Layer 6: {layer6_embeddings.shape}\")\n",
    "print(f\"Layer 12: {layer12_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c72b1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer6_embeddings = np.load(\"sae_data/layer6_embeddings.npy\")\n",
    "layer12_embeddings = np.load(\"sae_data/layer12_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c637e474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] | Train Loss: 0.564636 | Val Loss: 0.443621\n",
      "Epoch [2/500] | Train Loss: 0.429605 | Val Loss: 0.415526\n",
      "Epoch [3/500] | Train Loss: 0.404336 | Val Loss: 0.391972\n",
      "Epoch [4/500] | Train Loss: 0.380943 | Val Loss: 0.371695\n",
      "Epoch [5/500] | Train Loss: 0.363363 | Val Loss: 0.355106\n",
      "Epoch [6/500] | Train Loss: 0.348986 | Val Loss: 0.341697\n",
      "Epoch [7/500] | Train Loss: 0.337461 | Val Loss: 0.333137\n",
      "Epoch [8/500] | Train Loss: 0.328246 | Val Loss: 0.325167\n",
      "Epoch [9/500] | Train Loss: 0.321236 | Val Loss: 0.319153\n",
      "Epoch [10/500] | Train Loss: 0.315880 | Val Loss: 0.314109\n",
      "Epoch [11/500] | Train Loss: 0.311361 | Val Loss: 0.310885\n",
      "Epoch [12/500] | Train Loss: 0.307849 | Val Loss: 0.307511\n",
      "Epoch [13/500] | Train Loss: 0.305241 | Val Loss: 0.305991\n",
      "Epoch [14/500] | Train Loss: 0.303034 | Val Loss: 0.303367\n",
      "Epoch [15/500] | Train Loss: 0.300390 | Val Loss: 0.301121\n",
      "Epoch [16/500] | Train Loss: 0.298111 | Val Loss: 0.298208\n",
      "Epoch [17/500] | Train Loss: 0.296032 | Val Loss: 0.296372\n",
      "Epoch [18/500] | Train Loss: 0.293674 | Val Loss: 0.293474\n",
      "Epoch [19/500] | Train Loss: 0.291256 | Val Loss: 0.291606\n",
      "Epoch [20/500] | Train Loss: 0.289391 | Val Loss: 0.292636\n",
      "Epoch [21/500] | Train Loss: 0.287566 | Val Loss: 0.289255\n",
      "Epoch [22/500] | Train Loss: 0.286223 | Val Loss: 0.288303\n",
      "Epoch [23/500] | Train Loss: 0.284864 | Val Loss: 0.288210\n",
      "Epoch [24/500] | Train Loss: 0.283861 | Val Loss: 0.285019\n",
      "Epoch [25/500] | Train Loss: 0.282756 | Val Loss: 0.285151\n",
      "Epoch [26/500] | Train Loss: 0.282141 | Val Loss: 0.283566\n",
      "Epoch [27/500] | Train Loss: 0.281662 | Val Loss: 0.282636\n",
      "Epoch [28/500] | Train Loss: 0.281117 | Val Loss: 0.282136\n",
      "Epoch [29/500] | Train Loss: 0.280267 | Val Loss: 0.281693\n",
      "Epoch [30/500] | Train Loss: 0.279668 | Val Loss: 0.280591\n",
      "Epoch [31/500] | Train Loss: 0.279192 | Val Loss: 0.280592\n",
      "Epoch [32/500] | Train Loss: 0.278558 | Val Loss: 0.281683\n",
      "Epoch [33/500] | Train Loss: 0.278182 | Val Loss: 0.280373\n",
      "Epoch [34/500] | Train Loss: 0.277748 | Val Loss: 0.280598\n",
      "Epoch [35/500] | Train Loss: 0.277313 | Val Loss: 0.279215\n",
      "Epoch [36/500] | Train Loss: 0.276788 | Val Loss: 0.278286\n",
      "Epoch [37/500] | Train Loss: 0.275929 | Val Loss: 0.278453\n",
      "Epoch [38/500] | Train Loss: 0.275110 | Val Loss: 0.277104\n",
      "Epoch [39/500] | Train Loss: 0.274491 | Val Loss: 0.275802\n",
      "Epoch [40/500] | Train Loss: 0.273776 | Val Loss: 0.275813\n",
      "Epoch [41/500] | Train Loss: 0.273303 | Val Loss: 0.275305\n",
      "Epoch [42/500] | Train Loss: 0.272736 | Val Loss: 0.274760\n",
      "Epoch [43/500] | Train Loss: 0.272301 | Val Loss: 0.274577\n",
      "Epoch [44/500] | Train Loss: 0.271953 | Val Loss: 0.274353\n",
      "Epoch [45/500] | Train Loss: 0.271351 | Val Loss: 0.276286\n",
      "Epoch [46/500] | Train Loss: 0.270921 | Val Loss: 0.273235\n",
      "Epoch [47/500] | Train Loss: 0.270375 | Val Loss: 0.272674\n",
      "Epoch [48/500] | Train Loss: 0.269944 | Val Loss: 0.272011\n",
      "Epoch [49/500] | Train Loss: 0.269678 | Val Loss: 0.272618\n",
      "Epoch [50/500] | Train Loss: 0.269467 | Val Loss: 0.271149\n",
      "Epoch [51/500] | Train Loss: 0.269191 | Val Loss: 0.271341\n",
      "Epoch [52/500] | Train Loss: 0.268914 | Val Loss: 0.271804\n",
      "Epoch [53/500] | Train Loss: 0.268689 | Val Loss: 0.270819\n",
      "Epoch [54/500] | Train Loss: 0.268454 | Val Loss: 0.271048\n",
      "Epoch [55/500] | Train Loss: 0.268266 | Val Loss: 0.270936\n",
      "Epoch [56/500] | Train Loss: 0.268087 | Val Loss: 0.270727\n",
      "Epoch [57/500] | Train Loss: 0.267941 | Val Loss: 0.272053\n",
      "Epoch [58/500] | Train Loss: 0.267797 | Val Loss: 0.269666\n",
      "Epoch [59/500] | Train Loss: 0.267626 | Val Loss: 0.270117\n",
      "Epoch [60/500] | Train Loss: 0.267309 | Val Loss: 0.269077\n",
      "Epoch [61/500] | Train Loss: 0.267107 | Val Loss: 0.269102\n",
      "Epoch [62/500] | Train Loss: 0.267000 | Val Loss: 0.269130\n",
      "Epoch [63/500] | Train Loss: 0.266806 | Val Loss: 0.268859\n",
      "Epoch [64/500] | Train Loss: 0.266683 | Val Loss: 0.269776\n",
      "Epoch [65/500] | Train Loss: 0.266461 | Val Loss: 0.268585\n",
      "Epoch [66/500] | Train Loss: 0.266373 | Val Loss: 0.268731\n",
      "Epoch [67/500] | Train Loss: 0.266405 | Val Loss: 0.268867\n",
      "Epoch [68/500] | Train Loss: 0.266329 | Val Loss: 0.268137\n",
      "Epoch [69/500] | Train Loss: 0.266176 | Val Loss: 0.268662\n",
      "Epoch [70/500] | Train Loss: 0.266087 | Val Loss: 0.268586\n",
      "Epoch [71/500] | Train Loss: 0.265894 | Val Loss: 0.268400\n",
      "Epoch [72/500] | Train Loss: 0.265782 | Val Loss: 0.268917\n",
      "Epoch [73/500] | Train Loss: 0.265715 | Val Loss: 0.267313\n",
      "Epoch [74/500] | Train Loss: 0.265609 | Val Loss: 0.268656\n",
      "Epoch [75/500] | Train Loss: 0.265552 | Val Loss: 0.267665\n",
      "Epoch [76/500] | Train Loss: 0.265441 | Val Loss: 0.268771\n",
      "Epoch [77/500] | Train Loss: 0.265411 | Val Loss: 0.267713\n",
      "Epoch [78/500] | Train Loss: 0.265410 | Val Loss: 0.268635\n",
      "Epoch [79/500] | Train Loss: 0.265335 | Val Loss: 0.267384\n",
      "Epoch [80/500] | Train Loss: 0.265355 | Val Loss: 0.266895\n",
      "Epoch [81/500] | Train Loss: 0.265385 | Val Loss: 0.268094\n",
      "Epoch [82/500] | Train Loss: 0.265376 | Val Loss: 0.269173\n",
      "Epoch [83/500] | Train Loss: 0.265288 | Val Loss: 0.267666\n",
      "Epoch [84/500] | Train Loss: 0.265247 | Val Loss: 0.267905\n",
      "Epoch [85/500] | Train Loss: 0.265236 | Val Loss: 0.268340\n",
      "Epoch [86/500] | Train Loss: 0.265161 | Val Loss: 0.267676\n",
      "Epoch [87/500] | Train Loss: 0.265123 | Val Loss: 0.266879\n",
      "Epoch [88/500] | Train Loss: 0.265132 | Val Loss: 0.267298\n",
      "Epoch [89/500] | Train Loss: 0.265084 | Val Loss: 0.267568\n",
      "Epoch [90/500] | Train Loss: 0.264961 | Val Loss: 0.267620\n",
      "Epoch [91/500] | Train Loss: 0.264972 | Val Loss: 0.267152\n",
      "Epoch [92/500] | Train Loss: 0.264948 | Val Loss: 0.267095\n",
      "Epoch [93/500] | Train Loss: 0.264885 | Val Loss: 0.267433\n",
      "Epoch [94/500] | Train Loss: 0.264846 | Val Loss: 0.267288\n",
      "Epoch [95/500] | Train Loss: 0.264803 | Val Loss: 0.267614\n",
      "Epoch [96/500] | Train Loss: 0.264734 | Val Loss: 0.267054\n",
      "Epoch [97/500] | Train Loss: 0.264744 | Val Loss: 0.267549\n",
      "⏳ Early stopping at epoch 97\n",
      "✅ Training complete. Best model saved as sae_model_6_3072.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5646357463587044,\n",
       "  0.4296048907582445,\n",
       "  0.40433580629575494,\n",
       "  0.38094315248394534,\n",
       "  0.3633628173991632,\n",
       "  0.34898595275677174,\n",
       "  0.33746110880982455,\n",
       "  0.32824598680859096,\n",
       "  0.3212363291898616,\n",
       "  0.31587958567836155,\n",
       "  0.31136095165351074,\n",
       "  0.3078488080558182,\n",
       "  0.30524123651535423,\n",
       "  0.30303444220310743,\n",
       "  0.30038968253878895,\n",
       "  0.29811122687307695,\n",
       "  0.2960324708658065,\n",
       "  0.2936735098563109,\n",
       "  0.2912557582802728,\n",
       "  0.2893906635408765,\n",
       "  0.287565917916548,\n",
       "  0.2862232499049478,\n",
       "  0.2848637790549574,\n",
       "  0.2838606225624349,\n",
       "  0.28275578153982783,\n",
       "  0.28214090536986763,\n",
       "  0.28166227829243984,\n",
       "  0.28111695539001663,\n",
       "  0.2802673163875856,\n",
       "  0.2796682889982947,\n",
       "  0.2791917120979777,\n",
       "  0.278557852059529,\n",
       "  0.27818241567967966,\n",
       "  0.2777476436412907,\n",
       "  0.27731287658273557,\n",
       "  0.2767877736550519,\n",
       "  0.2759289249549226,\n",
       "  0.27511014672035866,\n",
       "  0.2744906287435728,\n",
       "  0.27377619896119093,\n",
       "  0.2733028698474803,\n",
       "  0.2727361684090899,\n",
       "  0.2723010436872749,\n",
       "  0.27195265582001843,\n",
       "  0.27135082290852086,\n",
       "  0.27092095107828135,\n",
       "  0.2703752946125169,\n",
       "  0.2699442437074374,\n",
       "  0.26967837742683265,\n",
       "  0.2694673181715606,\n",
       "  0.2691905266309009,\n",
       "  0.2689137939870623,\n",
       "  0.2686892131113994,\n",
       "  0.2684543676110319,\n",
       "  0.2682659799217411,\n",
       "  0.2680872174461899,\n",
       "  0.2679412541178298,\n",
       "  0.26779662186942554,\n",
       "  0.2676263501698991,\n",
       "  0.26730901496443404,\n",
       "  0.2671072213190254,\n",
       "  0.2669997060611542,\n",
       "  0.26680623968157186,\n",
       "  0.26668273267446324,\n",
       "  0.2664608709163916,\n",
       "  0.26637290877053665,\n",
       "  0.2664046311042263,\n",
       "  0.2663292747700586,\n",
       "  0.26617575510193486,\n",
       "  0.2660870746788207,\n",
       "  0.2658940004910177,\n",
       "  0.2657820140840629,\n",
       "  0.26571481784106127,\n",
       "  0.2656092237918935,\n",
       "  0.26555166049707885,\n",
       "  0.2654406718589008,\n",
       "  0.265411203141144,\n",
       "  0.26540981462130386,\n",
       "  0.265335163847165,\n",
       "  0.2653551867947845,\n",
       "  0.2653854974082412,\n",
       "  0.2653757482413193,\n",
       "  0.2652876593154117,\n",
       "  0.26524680699085845,\n",
       "  0.2652356056810782,\n",
       "  0.26516101938671904,\n",
       "  0.2651226803712005,\n",
       "  0.2651318215913905,\n",
       "  0.26508387609291406,\n",
       "  0.2649613736836053,\n",
       "  0.2649718995556744,\n",
       "  0.26494848035839985,\n",
       "  0.2648845446502024,\n",
       "  0.2648463848806855,\n",
       "  0.2648034993063277,\n",
       "  0.2647342116908824,\n",
       "  0.26474357151059097],\n",
       " [0.44362148103815896,\n",
       "  0.4155263757185541,\n",
       "  0.39197173499678883,\n",
       "  0.3716949506705081,\n",
       "  0.35510629619855705,\n",
       "  0.34169735055572736,\n",
       "  0.33313716414615924,\n",
       "  0.32516714549712167,\n",
       "  0.31915263381794107,\n",
       "  0.3141093912765795,\n",
       "  0.31088484081962975,\n",
       "  0.3075107424681673,\n",
       "  0.30599067831878035,\n",
       "  0.303366510374994,\n",
       "  0.301121090115019,\n",
       "  0.2982082970553175,\n",
       "  0.2963715865743659,\n",
       "  0.2934742219354675,\n",
       "  0.2916061370397187,\n",
       "  0.2926364850886465,\n",
       "  0.2892550211316862,\n",
       "  0.2883034121321443,\n",
       "  0.2882096365107454,\n",
       "  0.2850186766547384,\n",
       "  0.2851510963212775,\n",
       "  0.28356603008237996,\n",
       "  0.28263550379768826,\n",
       "  0.28213643283315354,\n",
       "  0.2816934321649565,\n",
       "  0.28059094998312123,\n",
       "  0.2805915000787932,\n",
       "  0.28168306472518034,\n",
       "  0.2803729541022546,\n",
       "  0.28059787825347055,\n",
       "  0.27921520575602354,\n",
       "  0.278286249480498,\n",
       "  0.27845347390733227,\n",
       "  0.27710353104747626,\n",
       "  0.27580151200719105,\n",
       "  0.27581329680879735,\n",
       "  0.2753053287231593,\n",
       "  0.2747602920816715,\n",
       "  0.2745767971127562,\n",
       "  0.2743527757489883,\n",
       "  0.27628638062589633,\n",
       "  0.2732351318678469,\n",
       "  0.2726738514612534,\n",
       "  0.2720107101666959,\n",
       "  0.2726182865571042,\n",
       "  0.27114946011019303,\n",
       "  0.2713412311681126,\n",
       "  0.2718041261603338,\n",
       "  0.2708186006450483,\n",
       "  0.27104802470810163,\n",
       "  0.27093632164961085,\n",
       "  0.2707271872889433,\n",
       "  0.2720533074116134,\n",
       "  0.26966633654978267,\n",
       "  0.27011674158241955,\n",
       "  0.2690774898574698,\n",
       "  0.26910245689237533,\n",
       "  0.26912975361701746,\n",
       "  0.26885870827539415,\n",
       "  0.26977553467398124,\n",
       "  0.26858518761603406,\n",
       "  0.268730537589384,\n",
       "  0.2688665236491554,\n",
       "  0.2681370379265567,\n",
       "  0.2686618452929856,\n",
       "  0.2685862370797384,\n",
       "  0.2683995225444396,\n",
       "  0.2689168220477134,\n",
       "  0.26731341570376926,\n",
       "  0.2686557235164719,\n",
       "  0.2676647111096769,\n",
       "  0.2687710843124458,\n",
       "  0.26771336484730296,\n",
       "  0.2686348551368459,\n",
       "  0.2673842526262399,\n",
       "  0.26689469180039177,\n",
       "  0.26809370469962074,\n",
       "  0.2691725642550554,\n",
       "  0.2676658817066853,\n",
       "  0.2679052762255962,\n",
       "  0.2683396537921732,\n",
       "  0.2676764553344791,\n",
       "  0.2668792163938257,\n",
       "  0.26729805689248676,\n",
       "  0.2675676069961312,\n",
       "  0.2676201311212932,\n",
       "  0.26715187383409283,\n",
       "  0.2670945551707505,\n",
       "  0.267432590639708,\n",
       "  0.2672877437231908,\n",
       "  0.2676141239895103,\n",
       "  0.2670538466977733,\n",
       "  0.2675492081091219])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "sae = SparseAutoencoder(input_dim=768, hidden_dim=3072, top_k=50)\n",
    "train_sae(layer6_embeddings, sae, model_prefix=\"sae_model_6_3072\", epochs=500,\n",
    "          batch_size=64, lr=5e-4, weight_decay=1e-6,\n",
    "          train_losses=[],val_losses=val_losses, device=device, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13db1e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] | Train Loss: 1.905818 | Val Loss: 1.158597\n",
      "Epoch [2/500] | Train Loss: 1.039364 | Val Loss: 0.968068\n",
      "Epoch [3/500] | Train Loss: 0.916584 | Val Loss: 0.893482\n",
      "Epoch [4/500] | Train Loss: 0.867206 | Val Loss: 0.858755\n",
      "Epoch [5/500] | Train Loss: 0.828626 | Val Loss: 0.813538\n",
      "Epoch [6/500] | Train Loss: 0.785492 | Val Loss: 0.781871\n",
      "Epoch [7/500] | Train Loss: 0.756617 | Val Loss: 0.753035\n",
      "Epoch [8/500] | Train Loss: 0.734986 | Val Loss: 0.736617\n",
      "Epoch [9/500] | Train Loss: 0.716969 | Val Loss: 0.718567\n",
      "Epoch [10/500] | Train Loss: 0.701612 | Val Loss: 0.706740\n",
      "Epoch [11/500] | Train Loss: 0.692045 | Val Loss: 0.697579\n",
      "Epoch [12/500] | Train Loss: 0.683404 | Val Loss: 0.690329\n",
      "Epoch [13/500] | Train Loss: 0.678386 | Val Loss: 0.686642\n",
      "Epoch [14/500] | Train Loss: 0.673057 | Val Loss: 0.680456\n",
      "Epoch [15/500] | Train Loss: 0.668214 | Val Loss: 0.678352\n",
      "Epoch [16/500] | Train Loss: 0.665458 | Val Loss: 0.673642\n",
      "Epoch [17/500] | Train Loss: 0.662169 | Val Loss: 0.673167\n",
      "Epoch [18/500] | Train Loss: 0.660586 | Val Loss: 0.669282\n",
      "Epoch [19/500] | Train Loss: 0.660033 | Val Loss: 0.672387\n",
      "Epoch [20/500] | Train Loss: 0.661410 | Val Loss: 0.671107\n",
      "Epoch [21/500] | Train Loss: 0.659010 | Val Loss: 0.669519\n",
      "Epoch [22/500] | Train Loss: 0.658214 | Val Loss: 0.670347\n",
      "Epoch [23/500] | Train Loss: 0.659185 | Val Loss: 0.668339\n",
      "Epoch [24/500] | Train Loss: 0.655584 | Val Loss: 0.664055\n",
      "Epoch [25/500] | Train Loss: 0.652139 | Val Loss: 0.661451\n",
      "Epoch [26/500] | Train Loss: 0.650980 | Val Loss: 0.662376\n",
      "Epoch [27/500] | Train Loss: 0.649302 | Val Loss: 0.658068\n",
      "Epoch [28/500] | Train Loss: 0.646254 | Val Loss: 0.655682\n",
      "Epoch [29/500] | Train Loss: 0.643894 | Val Loss: 0.653305\n",
      "Epoch [30/500] | Train Loss: 0.641921 | Val Loss: 0.651468\n",
      "Epoch [31/500] | Train Loss: 0.640295 | Val Loss: 0.650073\n",
      "Epoch [32/500] | Train Loss: 0.640843 | Val Loss: 0.652196\n",
      "Epoch [33/500] | Train Loss: 0.640179 | Val Loss: 0.650042\n",
      "Epoch [34/500] | Train Loss: 0.638515 | Val Loss: 0.648671\n",
      "Epoch [35/500] | Train Loss: 0.637196 | Val Loss: 0.647386\n",
      "Epoch [36/500] | Train Loss: 0.635957 | Val Loss: 0.646323\n",
      "Epoch [37/500] | Train Loss: 0.634891 | Val Loss: 0.645285\n",
      "Epoch [38/500] | Train Loss: 0.633866 | Val Loss: 0.644239\n",
      "Epoch [39/500] | Train Loss: 0.632909 | Val Loss: 0.643169\n",
      "Epoch [40/500] | Train Loss: 0.632070 | Val Loss: 0.642507\n",
      "Epoch [41/500] | Train Loss: 0.631308 | Val Loss: 0.642034\n",
      "Epoch [42/500] | Train Loss: 0.631728 | Val Loss: 0.643598\n",
      "Epoch [43/500] | Train Loss: 0.634575 | Val Loss: 0.648690\n",
      "Epoch [44/500] | Train Loss: 0.637016 | Val Loss: 0.649977\n",
      "Epoch [45/500] | Train Loss: 0.639569 | Val Loss: 0.650392\n",
      "Epoch [46/500] | Train Loss: 0.641151 | Val Loss: 0.651938\n",
      "Epoch [47/500] | Train Loss: 0.642193 | Val Loss: 0.653224\n",
      "Epoch [48/500] | Train Loss: 0.641001 | Val Loss: 0.651415\n",
      "Epoch [49/500] | Train Loss: 0.639419 | Val Loss: 0.649866\n",
      "Epoch [50/500] | Train Loss: 0.641245 | Val Loss: 0.655757\n",
      "Epoch [51/500] | Train Loss: 0.648688 | Val Loss: 0.662052\n",
      "⏳ Early stopping at epoch 51\n",
      "✅ Training complete. Best model saved as sae_model_12_3072.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.9058179149469605,\n",
       "  1.039364313548417,\n",
       "  0.9165840226668406,\n",
       "  0.8672063389489817,\n",
       "  0.8286255512775494,\n",
       "  0.7854920965147042,\n",
       "  0.7566170522094304,\n",
       "  0.7349864254665517,\n",
       "  0.716969497825532,\n",
       "  0.7016121615754797,\n",
       "  0.6920449247901714,\n",
       "  0.6834044909724972,\n",
       "  0.6783855521991546,\n",
       "  0.6730574318260795,\n",
       "  0.6682138484872726,\n",
       "  0.6654584197966195,\n",
       "  0.6621690611624588,\n",
       "  0.6605855575591724,\n",
       "  0.6600327727288553,\n",
       "  0.6614102297080501,\n",
       "  0.6590100276564561,\n",
       "  0.6582140301490172,\n",
       "  0.6591849146444924,\n",
       "  0.6555843895847758,\n",
       "  0.6521392760773451,\n",
       "  0.6509797522922602,\n",
       "  0.6493021730058917,\n",
       "  0.6462541953075763,\n",
       "  0.6438941036162549,\n",
       "  0.6419214325809998,\n",
       "  0.640295190851857,\n",
       "  0.6408433420507109,\n",
       "  0.6401786109588337,\n",
       "  0.6385145136326154,\n",
       "  0.637196177506671,\n",
       "  0.6359571195515826,\n",
       "  0.6348911339784836,\n",
       "  0.6338660208262293,\n",
       "  0.6329092795102093,\n",
       "  0.6320700638516005,\n",
       "  0.6313075651897174,\n",
       "  0.6317276368071572,\n",
       "  0.6345745387760381,\n",
       "  0.6370158464468569,\n",
       "  0.6395691596889543,\n",
       "  0.641150717809612,\n",
       "  0.6421931837349466,\n",
       "  0.6410013966044832,\n",
       "  0.6394186774879798,\n",
       "  0.641245280776149,\n",
       "  0.6486877279757037],\n",
       " [1.1585973914669543,\n",
       "  0.9680680218065624,\n",
       "  0.8934816576387876,\n",
       "  0.8587546902157214,\n",
       "  0.8135382675317814,\n",
       "  0.7818705814816754,\n",
       "  0.7530346324904519,\n",
       "  0.7366172552427223,\n",
       "  0.7185665764982745,\n",
       "  0.7067400700581042,\n",
       "  0.6975794312472017,\n",
       "  0.6903290396169797,\n",
       "  0.6866415362217972,\n",
       "  0.6804556587606483,\n",
       "  0.6783520555453755,\n",
       "  0.673641817917913,\n",
       "  0.6731670814120971,\n",
       "  0.6692815579586759,\n",
       "  0.6723872586116129,\n",
       "  0.6711071152190279,\n",
       "  0.6695186607241099,\n",
       "  0.6703472601868484,\n",
       "  0.6683392260930204,\n",
       "  0.6640546638734194,\n",
       "  0.6614505218164473,\n",
       "  0.6623759876376056,\n",
       "  0.6580676874303436,\n",
       "  0.6556821322717098,\n",
       "  0.6533050073222826,\n",
       "  0.6514676544725736,\n",
       "  0.6500730453488663,\n",
       "  0.6521962191627584,\n",
       "  0.6500416576915411,\n",
       "  0.6486713068621135,\n",
       "  0.6473860256871472,\n",
       "  0.646323444794462,\n",
       "  0.6452845310857014,\n",
       "  0.6442392195375074,\n",
       "  0.643168629675387,\n",
       "  0.6425071775064018,\n",
       "  0.642034453625989,\n",
       "  0.6435982153443279,\n",
       "  0.6486902132643617,\n",
       "  0.6499766310409892,\n",
       "  0.6503916866845142,\n",
       "  0.6519384542853304,\n",
       "  0.6532242157474121,\n",
       "  0.651415230648282,\n",
       "  0.6498659918696776,\n",
       "  0.6557566618558454,\n",
       "  0.6620520886733716])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "sae = SparseAutoencoder(input_dim=768, hidden_dim=3072, top_k=50)\n",
    "train_sae(layer12_embeddings, sae, model_prefix=\"sae_model_12_3072\", epochs=500,\n",
    "          batch_size=64, lr=1e-4, weight_decay=1e-5, \n",
    "          train_losses=[],val_losses=val_losses, device=device, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a40f722c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] | Train Loss: 0.192792 | Val Loss: 0.126466\n",
      "Epoch [2/500] | Train Loss: 0.115499 | Val Loss: 0.109232\n",
      "Epoch [3/500] | Train Loss: 0.104484 | Val Loss: 0.101956\n",
      "Epoch [4/500] | Train Loss: 0.098965 | Val Loss: 0.098186\n",
      "Epoch [5/500] | Train Loss: 0.095586 | Val Loss: 0.095307\n",
      "Epoch [6/500] | Train Loss: 0.093393 | Val Loss: 0.093630\n",
      "Epoch [7/500] | Train Loss: 0.091804 | Val Loss: 0.092291\n",
      "Epoch [8/500] | Train Loss: 0.090634 | Val Loss: 0.091361\n",
      "Epoch [9/500] | Train Loss: 0.089830 | Val Loss: 0.090632\n",
      "Epoch [10/500] | Train Loss: 0.089185 | Val Loss: 0.090091\n",
      "Epoch [11/500] | Train Loss: 0.088684 | Val Loss: 0.089712\n",
      "Epoch [12/500] | Train Loss: 0.088291 | Val Loss: 0.089273\n",
      "Epoch [13/500] | Train Loss: 0.087931 | Val Loss: 0.089011\n",
      "Epoch [14/500] | Train Loss: 0.087626 | Val Loss: 0.088705\n",
      "Epoch [15/500] | Train Loss: 0.087385 | Val Loss: 0.088471\n",
      "Epoch [16/500] | Train Loss: 0.087149 | Val Loss: 0.088205\n",
      "Epoch [17/500] | Train Loss: 0.086918 | Val Loss: 0.087966\n",
      "Epoch [18/500] | Train Loss: 0.086705 | Val Loss: 0.087823\n",
      "Epoch [19/500] | Train Loss: 0.086524 | Val Loss: 0.087637\n",
      "Epoch [20/500] | Train Loss: 0.086366 | Val Loss: 0.087522\n",
      "Epoch [21/500] | Train Loss: 0.086218 | Val Loss: 0.087368\n",
      "Epoch [22/500] | Train Loss: 0.086081 | Val Loss: 0.087269\n",
      "Epoch [23/500] | Train Loss: 0.085962 | Val Loss: 0.087164\n",
      "Epoch [24/500] | Train Loss: 0.085835 | Val Loss: 0.087058\n",
      "Epoch [25/500] | Train Loss: 0.085728 | Val Loss: 0.086912\n",
      "Epoch [26/500] | Train Loss: 0.085631 | Val Loss: 0.086769\n",
      "Epoch [27/500] | Train Loss: 0.085531 | Val Loss: 0.086801\n",
      "Epoch [28/500] | Train Loss: 0.085438 | Val Loss: 0.086642\n",
      "Epoch [29/500] | Train Loss: 0.085352 | Val Loss: 0.086532\n",
      "Epoch [30/500] | Train Loss: 0.085262 | Val Loss: 0.086449\n",
      "Epoch [31/500] | Train Loss: 0.085184 | Val Loss: 0.086423\n",
      "Epoch [32/500] | Train Loss: 0.085117 | Val Loss: 0.086373\n",
      "Epoch [33/500] | Train Loss: 0.085048 | Val Loss: 0.086278\n",
      "Epoch [34/500] | Train Loss: 0.084983 | Val Loss: 0.086199\n",
      "Epoch [35/500] | Train Loss: 0.084914 | Val Loss: 0.086031\n",
      "Epoch [36/500] | Train Loss: 0.084850 | Val Loss: 0.086062\n",
      "Epoch [37/500] | Train Loss: 0.084796 | Val Loss: 0.086068\n",
      "Epoch [38/500] | Train Loss: 0.084730 | Val Loss: 0.085973\n",
      "Epoch [39/500] | Train Loss: 0.084667 | Val Loss: 0.085944\n",
      "Epoch [40/500] | Train Loss: 0.084622 | Val Loss: 0.085832\n",
      "Epoch [41/500] | Train Loss: 0.084574 | Val Loss: 0.085815\n",
      "Epoch [42/500] | Train Loss: 0.084518 | Val Loss: 0.085758\n",
      "Epoch [43/500] | Train Loss: 0.084484 | Val Loss: 0.085645\n",
      "Epoch [44/500] | Train Loss: 0.084442 | Val Loss: 0.085660\n",
      "Epoch [45/500] | Train Loss: 0.084389 | Val Loss: 0.085614\n",
      "Epoch [46/500] | Train Loss: 0.084345 | Val Loss: 0.085559\n",
      "Epoch [47/500] | Train Loss: 0.084299 | Val Loss: 0.085539\n",
      "Epoch [48/500] | Train Loss: 0.084257 | Val Loss: 0.085457\n",
      "Epoch [49/500] | Train Loss: 0.084211 | Val Loss: 0.085472\n",
      "Epoch [50/500] | Train Loss: 0.084175 | Val Loss: 0.085360\n",
      "Epoch [51/500] | Train Loss: 0.084138 | Val Loss: 0.085335\n",
      "Epoch [52/500] | Train Loss: 0.084092 | Val Loss: 0.085356\n",
      "Epoch [53/500] | Train Loss: 0.084047 | Val Loss: 0.085307\n",
      "Epoch [54/500] | Train Loss: 0.084005 | Val Loss: 0.085232\n",
      "Epoch [55/500] | Train Loss: 0.083963 | Val Loss: 0.085169\n",
      "Epoch [56/500] | Train Loss: 0.083930 | Val Loss: 0.085167\n",
      "Epoch [57/500] | Train Loss: 0.083902 | Val Loss: 0.085117\n",
      "Epoch [58/500] | Train Loss: 0.083855 | Val Loss: 0.085093\n",
      "Epoch [59/500] | Train Loss: 0.083825 | Val Loss: 0.085037\n",
      "Epoch [60/500] | Train Loss: 0.083782 | Val Loss: 0.085014\n",
      "Epoch [61/500] | Train Loss: 0.083758 | Val Loss: 0.084998\n",
      "Epoch [62/500] | Train Loss: 0.083709 | Val Loss: 0.084980\n",
      "Epoch [63/500] | Train Loss: 0.083688 | Val Loss: 0.084956\n",
      "Epoch [64/500] | Train Loss: 0.083652 | Val Loss: 0.084930\n",
      "Epoch [65/500] | Train Loss: 0.083628 | Val Loss: 0.084835\n",
      "Epoch [66/500] | Train Loss: 0.083586 | Val Loss: 0.084824\n",
      "Epoch [67/500] | Train Loss: 0.083554 | Val Loss: 0.084848\n",
      "Epoch [68/500] | Train Loss: 0.083537 | Val Loss: 0.084806\n",
      "Epoch [69/500] | Train Loss: 0.083508 | Val Loss: 0.084728\n",
      "Epoch [70/500] | Train Loss: 0.083472 | Val Loss: 0.084725\n",
      "Epoch [71/500] | Train Loss: 0.083447 | Val Loss: 0.084732\n",
      "Epoch [72/500] | Train Loss: 0.083429 | Val Loss: 0.084742\n",
      "Epoch [73/500] | Train Loss: 0.083396 | Val Loss: 0.084640\n",
      "Epoch [74/500] | Train Loss: 0.083373 | Val Loss: 0.084587\n",
      "Epoch [75/500] | Train Loss: 0.083350 | Val Loss: 0.084656\n",
      "Epoch [76/500] | Train Loss: 0.083327 | Val Loss: 0.084591\n",
      "Epoch [77/500] | Train Loss: 0.083311 | Val Loss: 0.084545\n",
      "Epoch [78/500] | Train Loss: 0.083289 | Val Loss: 0.084492\n",
      "Epoch [79/500] | Train Loss: 0.083263 | Val Loss: 0.084521\n",
      "Epoch [80/500] | Train Loss: 0.083244 | Val Loss: 0.084452\n",
      "Epoch [81/500] | Train Loss: 0.083219 | Val Loss: 0.084474\n",
      "Epoch [82/500] | Train Loss: 0.083199 | Val Loss: 0.084459\n",
      "Epoch [83/500] | Train Loss: 0.083190 | Val Loss: 0.084397\n",
      "Epoch [84/500] | Train Loss: 0.083152 | Val Loss: 0.084461\n",
      "Epoch [85/500] | Train Loss: 0.083142 | Val Loss: 0.084413\n",
      "Epoch [86/500] | Train Loss: 0.083131 | Val Loss: 0.084337\n",
      "Epoch [87/500] | Train Loss: 0.083115 | Val Loss: 0.084328\n",
      "Epoch [88/500] | Train Loss: 0.083091 | Val Loss: 0.084325\n",
      "Epoch [89/500] | Train Loss: 0.083070 | Val Loss: 0.084372\n",
      "Epoch [90/500] | Train Loss: 0.083067 | Val Loss: 0.084314\n",
      "Epoch [91/500] | Train Loss: 0.083051 | Val Loss: 0.084314\n",
      "Epoch [92/500] | Train Loss: 0.083030 | Val Loss: 0.084275\n",
      "Epoch [93/500] | Train Loss: 0.083015 | Val Loss: 0.084224\n",
      "Epoch [94/500] | Train Loss: 0.083005 | Val Loss: 0.084272\n",
      "Epoch [95/500] | Train Loss: 0.082986 | Val Loss: 0.084277\n",
      "Epoch [96/500] | Train Loss: 0.082969 | Val Loss: 0.084201\n",
      "Epoch [97/500] | Train Loss: 0.082956 | Val Loss: 0.084211\n",
      "Epoch [98/500] | Train Loss: 0.082944 | Val Loss: 0.084158\n",
      "Epoch [99/500] | Train Loss: 0.082920 | Val Loss: 0.084168\n",
      "Epoch [100/500] | Train Loss: 0.082914 | Val Loss: 0.084146\n",
      "Epoch [101/500] | Train Loss: 0.082899 | Val Loss: 0.084141\n",
      "Epoch [102/500] | Train Loss: 0.082890 | Val Loss: 0.084115\n",
      "Epoch [103/500] | Train Loss: 0.082876 | Val Loss: 0.084099\n",
      "Epoch [104/500] | Train Loss: 0.082868 | Val Loss: 0.084100\n",
      "Epoch [105/500] | Train Loss: 0.082853 | Val Loss: 0.084086\n",
      "Epoch [106/500] | Train Loss: 0.082845 | Val Loss: 0.084075\n",
      "Epoch [107/500] | Train Loss: 0.082828 | Val Loss: 0.084066\n",
      "Epoch [108/500] | Train Loss: 0.082820 | Val Loss: 0.084031\n",
      "Epoch [109/500] | Train Loss: 0.082806 | Val Loss: 0.084073\n",
      "Epoch [110/500] | Train Loss: 0.082794 | Val Loss: 0.083984\n",
      "Epoch [111/500] | Train Loss: 0.082784 | Val Loss: 0.084015\n",
      "Epoch [112/500] | Train Loss: 0.082772 | Val Loss: 0.084038\n",
      "Epoch [113/500] | Train Loss: 0.082765 | Val Loss: 0.084005\n",
      "Epoch [114/500] | Train Loss: 0.082755 | Val Loss: 0.083977\n",
      "Epoch [115/500] | Train Loss: 0.082745 | Val Loss: 0.083958\n",
      "Epoch [116/500] | Train Loss: 0.082744 | Val Loss: 0.083950\n",
      "Epoch [117/500] | Train Loss: 0.082729 | Val Loss: 0.083985\n",
      "Epoch [118/500] | Train Loss: 0.082708 | Val Loss: 0.083936\n",
      "Epoch [119/500] | Train Loss: 0.082707 | Val Loss: 0.083953\n",
      "Epoch [120/500] | Train Loss: 0.082703 | Val Loss: 0.083896\n",
      "Epoch [121/500] | Train Loss: 0.082685 | Val Loss: 0.083902\n",
      "Epoch [122/500] | Train Loss: 0.082672 | Val Loss: 0.083960\n",
      "Epoch [123/500] | Train Loss: 0.082673 | Val Loss: 0.083880\n",
      "Epoch [124/500] | Train Loss: 0.082651 | Val Loss: 0.083875\n",
      "Epoch [125/500] | Train Loss: 0.082644 | Val Loss: 0.083858\n",
      "Epoch [126/500] | Train Loss: 0.082630 | Val Loss: 0.083834\n",
      "Epoch [127/500] | Train Loss: 0.082623 | Val Loss: 0.083801\n",
      "Epoch [128/500] | Train Loss: 0.082611 | Val Loss: 0.083779\n",
      "Epoch [129/500] | Train Loss: 0.082595 | Val Loss: 0.083814\n",
      "Epoch [130/500] | Train Loss: 0.082586 | Val Loss: 0.083787\n",
      "Epoch [131/500] | Train Loss: 0.082586 | Val Loss: 0.083804\n",
      "Epoch [132/500] | Train Loss: 0.082581 | Val Loss: 0.083795\n",
      "Epoch [133/500] | Train Loss: 0.082562 | Val Loss: 0.083804\n",
      "Epoch [134/500] | Train Loss: 0.082557 | Val Loss: 0.083736\n",
      "Epoch [135/500] | Train Loss: 0.082553 | Val Loss: 0.083761\n",
      "Epoch [136/500] | Train Loss: 0.082533 | Val Loss: 0.083738\n",
      "Epoch [137/500] | Train Loss: 0.082527 | Val Loss: 0.083813\n",
      "Epoch [138/500] | Train Loss: 0.082526 | Val Loss: 0.083703\n",
      "Epoch [139/500] | Train Loss: 0.082511 | Val Loss: 0.083700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/500] | Train Loss: 0.082505 | Val Loss: 0.083731\n",
      "Epoch [141/500] | Train Loss: 0.082500 | Val Loss: 0.083651\n",
      "Epoch [142/500] | Train Loss: 0.082493 | Val Loss: 0.083683\n",
      "Epoch [143/500] | Train Loss: 0.082474 | Val Loss: 0.083660\n",
      "Epoch [144/500] | Train Loss: 0.082470 | Val Loss: 0.083661\n",
      "Epoch [145/500] | Train Loss: 0.082462 | Val Loss: 0.083688\n",
      "Epoch [146/500] | Train Loss: 0.082456 | Val Loss: 0.083666\n",
      "Epoch [147/500] | Train Loss: 0.082444 | Val Loss: 0.083585\n",
      "Epoch [148/500] | Train Loss: 0.082431 | Val Loss: 0.083615\n",
      "Epoch [149/500] | Train Loss: 0.082423 | Val Loss: 0.083664\n",
      "Epoch [150/500] | Train Loss: 0.082416 | Val Loss: 0.083651\n",
      "Epoch [151/500] | Train Loss: 0.082414 | Val Loss: 0.083626\n",
      "Epoch [152/500] | Train Loss: 0.082399 | Val Loss: 0.083570\n",
      "Epoch [153/500] | Train Loss: 0.082401 | Val Loss: 0.083589\n",
      "Epoch [154/500] | Train Loss: 0.082387 | Val Loss: 0.083629\n",
      "Epoch [155/500] | Train Loss: 0.082374 | Val Loss: 0.083599\n",
      "Epoch [156/500] | Train Loss: 0.082367 | Val Loss: 0.083616\n",
      "Epoch [157/500] | Train Loss: 0.082367 | Val Loss: 0.083553\n",
      "Epoch [158/500] | Train Loss: 0.082355 | Val Loss: 0.083518\n",
      "Epoch [159/500] | Train Loss: 0.082352 | Val Loss: 0.083568\n",
      "Epoch [160/500] | Train Loss: 0.082336 | Val Loss: 0.083516\n",
      "Epoch [161/500] | Train Loss: 0.082340 | Val Loss: 0.083589\n",
      "Epoch [162/500] | Train Loss: 0.082332 | Val Loss: 0.083485\n",
      "Epoch [163/500] | Train Loss: 0.082326 | Val Loss: 0.083535\n",
      "Epoch [164/500] | Train Loss: 0.082318 | Val Loss: 0.083525\n",
      "Epoch [165/500] | Train Loss: 0.082307 | Val Loss: 0.083564\n",
      "Epoch [166/500] | Train Loss: 0.082310 | Val Loss: 0.083516\n",
      "Epoch [167/500] | Train Loss: 0.082295 | Val Loss: 0.083495\n",
      "Epoch [168/500] | Train Loss: 0.082283 | Val Loss: 0.083470\n",
      "Epoch [169/500] | Train Loss: 0.082287 | Val Loss: 0.083501\n",
      "Epoch [170/500] | Train Loss: 0.082271 | Val Loss: 0.083477\n",
      "Epoch [171/500] | Train Loss: 0.082271 | Val Loss: 0.083450\n",
      "Epoch [172/500] | Train Loss: 0.082260 | Val Loss: 0.083441\n",
      "Epoch [173/500] | Train Loss: 0.082258 | Val Loss: 0.083416\n",
      "Epoch [174/500] | Train Loss: 0.082246 | Val Loss: 0.083443\n",
      "Epoch [175/500] | Train Loss: 0.082241 | Val Loss: 0.083398\n",
      "Epoch [176/500] | Train Loss: 0.082245 | Val Loss: 0.083458\n",
      "Epoch [177/500] | Train Loss: 0.082234 | Val Loss: 0.083492\n",
      "Epoch [178/500] | Train Loss: 0.082230 | Val Loss: 0.083462\n",
      "Epoch [179/500] | Train Loss: 0.082221 | Val Loss: 0.083420\n",
      "Epoch [180/500] | Train Loss: 0.082215 | Val Loss: 0.083406\n",
      "Epoch [181/500] | Train Loss: 0.082213 | Val Loss: 0.083402\n",
      "Epoch [182/500] | Train Loss: 0.082194 | Val Loss: 0.083414\n",
      "Epoch [183/500] | Train Loss: 0.082200 | Val Loss: 0.083389\n",
      "Epoch [184/500] | Train Loss: 0.082196 | Val Loss: 0.083386\n",
      "Epoch [185/500] | Train Loss: 0.082182 | Val Loss: 0.083393\n",
      "Epoch [186/500] | Train Loss: 0.082173 | Val Loss: 0.083378\n",
      "Epoch [187/500] | Train Loss: 0.082179 | Val Loss: 0.083351\n",
      "Epoch [188/500] | Train Loss: 0.082157 | Val Loss: 0.083381\n",
      "Epoch [189/500] | Train Loss: 0.082164 | Val Loss: 0.083281\n",
      "Epoch [190/500] | Train Loss: 0.082150 | Val Loss: 0.083325\n",
      "Epoch [191/500] | Train Loss: 0.082133 | Val Loss: 0.083356\n",
      "Epoch [192/500] | Train Loss: 0.082150 | Val Loss: 0.083349\n",
      "Epoch [193/500] | Train Loss: 0.082132 | Val Loss: 0.083342\n",
      "Epoch [194/500] | Train Loss: 0.082124 | Val Loss: 0.083286\n",
      "Epoch [195/500] | Train Loss: 0.082121 | Val Loss: 0.083278\n",
      "Epoch [196/500] | Train Loss: 0.082124 | Val Loss: 0.083308\n",
      "Epoch [197/500] | Train Loss: 0.082113 | Val Loss: 0.083270\n",
      "Epoch [198/500] | Train Loss: 0.082108 | Val Loss: 0.083292\n",
      "Epoch [199/500] | Train Loss: 0.082100 | Val Loss: 0.083266\n",
      "Epoch [200/500] | Train Loss: 0.082087 | Val Loss: 0.083307\n",
      "Epoch [201/500] | Train Loss: 0.082093 | Val Loss: 0.083288\n",
      "Epoch [202/500] | Train Loss: 0.082080 | Val Loss: 0.083245\n",
      "Epoch [203/500] | Train Loss: 0.082071 | Val Loss: 0.083190\n",
      "Epoch [204/500] | Train Loss: 0.082066 | Val Loss: 0.083283\n",
      "Epoch [205/500] | Train Loss: 0.082055 | Val Loss: 0.083261\n",
      "Epoch [206/500] | Train Loss: 0.082053 | Val Loss: 0.083257\n",
      "Epoch [207/500] | Train Loss: 0.082049 | Val Loss: 0.083214\n",
      "Epoch [208/500] | Train Loss: 0.082040 | Val Loss: 0.083202\n",
      "Epoch [209/500] | Train Loss: 0.082039 | Val Loss: 0.083232\n",
      "Epoch [210/500] | Train Loss: 0.082032 | Val Loss: 0.083174\n",
      "Epoch [211/500] | Train Loss: 0.082021 | Val Loss: 0.083243\n",
      "Epoch [212/500] | Train Loss: 0.082008 | Val Loss: 0.083181\n",
      "Epoch [213/500] | Train Loss: 0.082011 | Val Loss: 0.083162\n",
      "Epoch [214/500] | Train Loss: 0.082008 | Val Loss: 0.083183\n",
      "Epoch [215/500] | Train Loss: 0.082004 | Val Loss: 0.083125\n",
      "Epoch [216/500] | Train Loss: 0.081991 | Val Loss: 0.083174\n",
      "Epoch [217/500] | Train Loss: 0.081985 | Val Loss: 0.083159\n",
      "Epoch [218/500] | Train Loss: 0.081987 | Val Loss: 0.083169\n",
      "Epoch [219/500] | Train Loss: 0.081985 | Val Loss: 0.083121\n",
      "Epoch [220/500] | Train Loss: 0.081967 | Val Loss: 0.083155\n",
      "Epoch [221/500] | Train Loss: 0.081966 | Val Loss: 0.083182\n",
      "Epoch [222/500] | Train Loss: 0.081963 | Val Loss: 0.083140\n",
      "Epoch [223/500] | Train Loss: 0.081963 | Val Loss: 0.083105\n",
      "Epoch [224/500] | Train Loss: 0.081957 | Val Loss: 0.083085\n",
      "Epoch [225/500] | Train Loss: 0.081952 | Val Loss: 0.083149\n",
      "Epoch [226/500] | Train Loss: 0.081944 | Val Loss: 0.083124\n",
      "Epoch [227/500] | Train Loss: 0.081935 | Val Loss: 0.083116\n",
      "Epoch [228/500] | Train Loss: 0.081926 | Val Loss: 0.083125\n",
      "Epoch [229/500] | Train Loss: 0.081934 | Val Loss: 0.083085\n",
      "Epoch [230/500] | Train Loss: 0.081917 | Val Loss: 0.083089\n",
      "Epoch [231/500] | Train Loss: 0.081918 | Val Loss: 0.083069\n",
      "Epoch [232/500] | Train Loss: 0.081911 | Val Loss: 0.083034\n",
      "Epoch [233/500] | Train Loss: 0.081910 | Val Loss: 0.083072\n",
      "Epoch [234/500] | Train Loss: 0.081893 | Val Loss: 0.083082\n",
      "Epoch [235/500] | Train Loss: 0.081889 | Val Loss: 0.083090\n",
      "Epoch [236/500] | Train Loss: 0.081886 | Val Loss: 0.083023\n",
      "Epoch [237/500] | Train Loss: 0.081882 | Val Loss: 0.083075\n",
      "Epoch [238/500] | Train Loss: 0.081873 | Val Loss: 0.083057\n",
      "Epoch [239/500] | Train Loss: 0.081862 | Val Loss: 0.082997\n",
      "Epoch [240/500] | Train Loss: 0.081859 | Val Loss: 0.083001\n",
      "Epoch [241/500] | Train Loss: 0.081853 | Val Loss: 0.082981\n",
      "Epoch [242/500] | Train Loss: 0.081840 | Val Loss: 0.082990\n",
      "Epoch [243/500] | Train Loss: 0.081841 | Val Loss: 0.083008\n",
      "Epoch [244/500] | Train Loss: 0.081838 | Val Loss: 0.083037\n",
      "Epoch [245/500] | Train Loss: 0.081832 | Val Loss: 0.082985\n",
      "Epoch [246/500] | Train Loss: 0.081836 | Val Loss: 0.082962\n",
      "Epoch [247/500] | Train Loss: 0.081823 | Val Loss: 0.082972\n",
      "Epoch [248/500] | Train Loss: 0.081806 | Val Loss: 0.083007\n",
      "Epoch [249/500] | Train Loss: 0.081813 | Val Loss: 0.082934\n",
      "Epoch [250/500] | Train Loss: 0.081812 | Val Loss: 0.082930\n",
      "Epoch [251/500] | Train Loss: 0.081803 | Val Loss: 0.082913\n",
      "Epoch [252/500] | Train Loss: 0.081812 | Val Loss: 0.082920\n",
      "Epoch [253/500] | Train Loss: 0.081803 | Val Loss: 0.082962\n",
      "Epoch [254/500] | Train Loss: 0.081797 | Val Loss: 0.082967\n",
      "Epoch [255/500] | Train Loss: 0.081797 | Val Loss: 0.082918\n",
      "Epoch [256/500] | Train Loss: 0.081790 | Val Loss: 0.082946\n",
      "Epoch [257/500] | Train Loss: 0.081791 | Val Loss: 0.082911\n",
      "Epoch [258/500] | Train Loss: 0.081779 | Val Loss: 0.082927\n",
      "Epoch [259/500] | Train Loss: 0.081781 | Val Loss: 0.082916\n",
      "Epoch [260/500] | Train Loss: 0.081777 | Val Loss: 0.082923\n",
      "Epoch [261/500] | Train Loss: 0.081773 | Val Loss: 0.082973\n",
      "Epoch [262/500] | Train Loss: 0.081770 | Val Loss: 0.082901\n",
      "Epoch [263/500] | Train Loss: 0.081763 | Val Loss: 0.082903\n",
      "Epoch [264/500] | Train Loss: 0.081756 | Val Loss: 0.082966\n",
      "Epoch [265/500] | Train Loss: 0.081762 | Val Loss: 0.082892\n",
      "Epoch [266/500] | Train Loss: 0.081752 | Val Loss: 0.082895\n",
      "Epoch [267/500] | Train Loss: 0.081745 | Val Loss: 0.082914\n",
      "Epoch [268/500] | Train Loss: 0.081749 | Val Loss: 0.082891\n",
      "Epoch [269/500] | Train Loss: 0.081745 | Val Loss: 0.082894\n",
      "Epoch [270/500] | Train Loss: 0.081745 | Val Loss: 0.082911\n",
      "Epoch [271/500] | Train Loss: 0.081742 | Val Loss: 0.082886\n",
      "Epoch [272/500] | Train Loss: 0.081736 | Val Loss: 0.082918\n",
      "Epoch [273/500] | Train Loss: 0.081737 | Val Loss: 0.082847\n",
      "Epoch [274/500] | Train Loss: 0.081728 | Val Loss: 0.082972\n",
      "Epoch [275/500] | Train Loss: 0.081733 | Val Loss: 0.082866\n",
      "Epoch [276/500] | Train Loss: 0.081730 | Val Loss: 0.082899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [277/500] | Train Loss: 0.081718 | Val Loss: 0.082847\n",
      "Epoch [278/500] | Train Loss: 0.081716 | Val Loss: 0.082832\n",
      "Epoch [279/500] | Train Loss: 0.081716 | Val Loss: 0.082881\n",
      "Epoch [280/500] | Train Loss: 0.081713 | Val Loss: 0.082851\n",
      "Epoch [281/500] | Train Loss: 0.081704 | Val Loss: 0.082873\n",
      "Epoch [282/500] | Train Loss: 0.081712 | Val Loss: 0.082850\n",
      "Epoch [283/500] | Train Loss: 0.081694 | Val Loss: 0.082877\n",
      "Epoch [284/500] | Train Loss: 0.081697 | Val Loss: 0.082864\n",
      "Epoch [285/500] | Train Loss: 0.081694 | Val Loss: 0.082846\n",
      "Epoch [286/500] | Train Loss: 0.081682 | Val Loss: 0.082839\n",
      "Epoch [287/500] | Train Loss: 0.081682 | Val Loss: 0.082818\n",
      "Epoch [288/500] | Train Loss: 0.081681 | Val Loss: 0.082806\n",
      "Epoch [289/500] | Train Loss: 0.081667 | Val Loss: 0.082846\n",
      "Epoch [290/500] | Train Loss: 0.081674 | Val Loss: 0.082797\n",
      "Epoch [291/500] | Train Loss: 0.081661 | Val Loss: 0.082837\n",
      "Epoch [292/500] | Train Loss: 0.081661 | Val Loss: 0.082817\n",
      "Epoch [293/500] | Train Loss: 0.081659 | Val Loss: 0.082844\n",
      "Epoch [294/500] | Train Loss: 0.081657 | Val Loss: 0.082784\n",
      "Epoch [295/500] | Train Loss: 0.081657 | Val Loss: 0.082806\n",
      "Epoch [296/500] | Train Loss: 0.081645 | Val Loss: 0.082793\n",
      "Epoch [297/500] | Train Loss: 0.081638 | Val Loss: 0.082791\n",
      "Epoch [298/500] | Train Loss: 0.081639 | Val Loss: 0.082827\n",
      "Epoch [299/500] | Train Loss: 0.081638 | Val Loss: 0.082763\n",
      "Epoch [300/500] | Train Loss: 0.081626 | Val Loss: 0.082779\n",
      "Epoch [301/500] | Train Loss: 0.081637 | Val Loss: 0.082808\n",
      "Epoch [302/500] | Train Loss: 0.081627 | Val Loss: 0.082807\n",
      "Epoch [303/500] | Train Loss: 0.081622 | Val Loss: 0.082775\n",
      "Epoch [304/500] | Train Loss: 0.081623 | Val Loss: 0.082726\n",
      "Epoch [305/500] | Train Loss: 0.081618 | Val Loss: 0.082739\n",
      "Epoch [306/500] | Train Loss: 0.081609 | Val Loss: 0.082769\n",
      "Epoch [307/500] | Train Loss: 0.081616 | Val Loss: 0.082794\n",
      "Epoch [308/500] | Train Loss: 0.081596 | Val Loss: 0.082745\n",
      "Epoch [309/500] | Train Loss: 0.081609 | Val Loss: 0.082794\n",
      "Epoch [310/500] | Train Loss: 0.081599 | Val Loss: 0.082767\n",
      "Epoch [311/500] | Train Loss: 0.081595 | Val Loss: 0.082775\n",
      "Epoch [312/500] | Train Loss: 0.081588 | Val Loss: 0.082797\n",
      "Epoch [313/500] | Train Loss: 0.081583 | Val Loss: 0.082725\n",
      "Epoch [314/500] | Train Loss: 0.081589 | Val Loss: 0.082781\n",
      "Epoch [315/500] | Train Loss: 0.081588 | Val Loss: 0.082765\n",
      "Epoch [316/500] | Train Loss: 0.081581 | Val Loss: 0.082758\n",
      "Epoch [317/500] | Train Loss: 0.081574 | Val Loss: 0.082769\n",
      "Epoch [318/500] | Train Loss: 0.081565 | Val Loss: 0.082725\n",
      "Epoch [319/500] | Train Loss: 0.081579 | Val Loss: 0.082757\n",
      "Epoch [320/500] | Train Loss: 0.081573 | Val Loss: 0.082731\n",
      "Epoch [321/500] | Train Loss: 0.081567 | Val Loss: 0.082784\n",
      "Epoch [322/500] | Train Loss: 0.081565 | Val Loss: 0.082724\n",
      "Epoch [323/500] | Train Loss: 0.081557 | Val Loss: 0.082726\n",
      "Epoch [324/500] | Train Loss: 0.081568 | Val Loss: 0.082696\n",
      "Epoch [325/500] | Train Loss: 0.081556 | Val Loss: 0.082765\n",
      "Epoch [326/500] | Train Loss: 0.081563 | Val Loss: 0.082725\n",
      "Epoch [327/500] | Train Loss: 0.081560 | Val Loss: 0.082732\n",
      "Epoch [328/500] | Train Loss: 0.081560 | Val Loss: 0.082766\n",
      "Epoch [329/500] | Train Loss: 0.081552 | Val Loss: 0.082747\n",
      "Epoch [330/500] | Train Loss: 0.081552 | Val Loss: 0.082777\n",
      "Epoch [331/500] | Train Loss: 0.081551 | Val Loss: 0.082733\n",
      "Epoch [332/500] | Train Loss: 0.081539 | Val Loss: 0.082691\n",
      "Epoch [333/500] | Train Loss: 0.081544 | Val Loss: 0.082696\n",
      "Epoch [334/500] | Train Loss: 0.081542 | Val Loss: 0.082729\n",
      "Epoch [335/500] | Train Loss: 0.081533 | Val Loss: 0.082743\n",
      "Epoch [336/500] | Train Loss: 0.081542 | Val Loss: 0.082696\n",
      "Epoch [337/500] | Train Loss: 0.081529 | Val Loss: 0.082743\n",
      "Epoch [338/500] | Train Loss: 0.081535 | Val Loss: 0.082690\n",
      "Epoch [339/500] | Train Loss: 0.081520 | Val Loss: 0.082744\n",
      "Epoch [340/500] | Train Loss: 0.081526 | Val Loss: 0.082711\n",
      "Epoch [341/500] | Train Loss: 0.081519 | Val Loss: 0.082736\n",
      "Epoch [342/500] | Train Loss: 0.081518 | Val Loss: 0.082731\n",
      "Epoch [343/500] | Train Loss: 0.081520 | Val Loss: 0.082736\n",
      "Epoch [344/500] | Train Loss: 0.081517 | Val Loss: 0.082760\n",
      "Epoch [345/500] | Train Loss: 0.081504 | Val Loss: 0.082701\n",
      "Epoch [346/500] | Train Loss: 0.081509 | Val Loss: 0.082695\n",
      "Epoch [347/500] | Train Loss: 0.081511 | Val Loss: 0.082739\n",
      "Epoch [348/500] | Train Loss: 0.081518 | Val Loss: 0.082707\n",
      "⏳ Early stopping at epoch 348\n",
      "✅ Training complete. Best model saved as sae_model_12_3072_after_norm.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.19279175708563479,\n",
       "  0.11549860926944981,\n",
       "  0.10448358910234773,\n",
       "  0.09896527336286413,\n",
       "  0.09558597063108401,\n",
       "  0.0933927520932625,\n",
       "  0.09180383104047582,\n",
       "  0.09063366981889576,\n",
       "  0.08982952392149951,\n",
       "  0.08918457192832854,\n",
       "  0.08868416035635408,\n",
       "  0.08829125295154001,\n",
       "  0.087930630713716,\n",
       "  0.08762624014513673,\n",
       "  0.08738450747589732,\n",
       "  0.0871489303381003,\n",
       "  0.08691816166972301,\n",
       "  0.08670491596950215,\n",
       "  0.08652362758307573,\n",
       "  0.08636559554731002,\n",
       "  0.08621830559496477,\n",
       "  0.08608146039036345,\n",
       "  0.08596179679567008,\n",
       "  0.08583505128144864,\n",
       "  0.08572848315679278,\n",
       "  0.0856309494904101,\n",
       "  0.08553074438720278,\n",
       "  0.08543761171868743,\n",
       "  0.08535219094559794,\n",
       "  0.08526241912302376,\n",
       "  0.0851844198588892,\n",
       "  0.08511690838259674,\n",
       "  0.08504816432663675,\n",
       "  0.08498335382516345,\n",
       "  0.08491367107457244,\n",
       "  0.0848497730012078,\n",
       "  0.08479576572104883,\n",
       "  0.08473036008739991,\n",
       "  0.0846672472875346,\n",
       "  0.0846223310834255,\n",
       "  0.08457404956855673,\n",
       "  0.08451776735923303,\n",
       "  0.08448367451254475,\n",
       "  0.08444233427302546,\n",
       "  0.08438877105078799,\n",
       "  0.08434493852053934,\n",
       "  0.08429927256277207,\n",
       "  0.08425726424174577,\n",
       "  0.08421057373145203,\n",
       "  0.08417528237869161,\n",
       "  0.0841382383894708,\n",
       "  0.08409178451234489,\n",
       "  0.08404713862422136,\n",
       "  0.08400530307740342,\n",
       "  0.08396317865708747,\n",
       "  0.08393008225458734,\n",
       "  0.08390156924872041,\n",
       "  0.08385503950654605,\n",
       "  0.083825159791022,\n",
       "  0.08378168453399289,\n",
       "  0.08375843433328692,\n",
       "  0.08370853111641882,\n",
       "  0.08368774017135676,\n",
       "  0.08365176145921893,\n",
       "  0.08362821905418884,\n",
       "  0.08358615430487612,\n",
       "  0.08355426540001969,\n",
       "  0.08353748713121857,\n",
       "  0.08350818181859274,\n",
       "  0.08347227856303016,\n",
       "  0.08344667199068233,\n",
       "  0.08342866740313454,\n",
       "  0.08339560625319845,\n",
       "  0.08337308750153767,\n",
       "  0.08334979470865317,\n",
       "  0.08332675409193148,\n",
       "  0.08331095182448847,\n",
       "  0.08328896897970775,\n",
       "  0.08326319916259528,\n",
       "  0.08324430094039245,\n",
       "  0.0832186926489918,\n",
       "  0.08319906532019745,\n",
       "  0.08318964620883484,\n",
       "  0.08315223640679782,\n",
       "  0.0831417033735447,\n",
       "  0.08313148306412664,\n",
       "  0.08311531325449217,\n",
       "  0.08309096305795614,\n",
       "  0.08306986993120662,\n",
       "  0.08306741847526254,\n",
       "  0.08305104013047615,\n",
       "  0.08303017132334459,\n",
       "  0.0830146675189346,\n",
       "  0.08300527975309727,\n",
       "  0.08298564104898798,\n",
       "  0.08296917394164881,\n",
       "  0.08295574050907273,\n",
       "  0.08294367931233898,\n",
       "  0.08291952602052854,\n",
       "  0.0829141306053554,\n",
       "  0.08289876216188449,\n",
       "  0.08289020999949029,\n",
       "  0.08287573942176367,\n",
       "  0.08286751919508512,\n",
       "  0.08285315350096217,\n",
       "  0.0828445495384156,\n",
       "  0.08282812825159411,\n",
       "  0.08281975310666322,\n",
       "  0.0828058237573966,\n",
       "  0.08279432873584916,\n",
       "  0.08278422901671396,\n",
       "  0.08277201895288228,\n",
       "  0.08276478617918899,\n",
       "  0.08275464610378674,\n",
       "  0.08274476999423398,\n",
       "  0.08274375977734715,\n",
       "  0.08272932937635107,\n",
       "  0.0827080205587955,\n",
       "  0.08270722147794363,\n",
       "  0.08270314683393934,\n",
       "  0.08268483714161386,\n",
       "  0.08267183295058944,\n",
       "  0.08267327569008523,\n",
       "  0.08265087798916132,\n",
       "  0.08264425540695917,\n",
       "  0.08263000764583138,\n",
       "  0.08262260292114332,\n",
       "  0.08261050220057257,\n",
       "  0.08259513287158486,\n",
       "  0.0825858681995877,\n",
       "  0.08258564962108261,\n",
       "  0.08258135615931414,\n",
       "  0.08256195968607757,\n",
       "  0.0825572525688854,\n",
       "  0.08255325543377613,\n",
       "  0.08253270482320942,\n",
       "  0.08252665913382241,\n",
       "  0.0825261552013984,\n",
       "  0.08251145213532719,\n",
       "  0.08250476398469787,\n",
       "  0.08249988253756008,\n",
       "  0.08249294897896237,\n",
       "  0.08247393121494037,\n",
       "  0.08246997033551387,\n",
       "  0.08246192438200757,\n",
       "  0.08245583289606567,\n",
       "  0.08244391555304813,\n",
       "  0.0824307165217836,\n",
       "  0.08242309089899358,\n",
       "  0.08241645465321555,\n",
       "  0.08241352233104869,\n",
       "  0.08239916308475626,\n",
       "  0.08240075187590794,\n",
       "  0.08238696203830218,\n",
       "  0.08237432821965761,\n",
       "  0.08236702032634258,\n",
       "  0.08236711958790993,\n",
       "  0.08235471168369836,\n",
       "  0.08235206230000244,\n",
       "  0.08233636656996343,\n",
       "  0.08234045679345985,\n",
       "  0.08233189269465305,\n",
       "  0.08232576038314234,\n",
       "  0.0823184285395839,\n",
       "  0.08230742027873393,\n",
       "  0.08231036668917084,\n",
       "  0.08229519797029146,\n",
       "  0.08228279058478187,\n",
       "  0.08228743866948666,\n",
       "  0.08227147118237749,\n",
       "  0.08227086630066924,\n",
       "  0.08226013922679548,\n",
       "  0.08225793669213168,\n",
       "  0.082245711977089,\n",
       "  0.08224078702018262,\n",
       "  0.08224474816356833,\n",
       "  0.08223434196456066,\n",
       "  0.08222982159408822,\n",
       "  0.08222101848316275,\n",
       "  0.08221498169502606,\n",
       "  0.08221295927787169,\n",
       "  0.08219402717930559,\n",
       "  0.08219979900547858,\n",
       "  0.08219637105528049,\n",
       "  0.08218206478589717,\n",
       "  0.08217325391434963,\n",
       "  0.08217858132242213,\n",
       "  0.08215656580420959,\n",
       "  0.08216383743115076,\n",
       "  0.08214973051032767,\n",
       "  0.08213283791916905,\n",
       "  0.08215017835952927,\n",
       "  0.08213174443937127,\n",
       "  0.08212395716368238,\n",
       "  0.08212128148913266,\n",
       "  0.08212354102829551,\n",
       "  0.08211346853972366,\n",
       "  0.08210846287618175,\n",
       "  0.08210003148139497,\n",
       "  0.08208705240041239,\n",
       "  0.08209332006917561,\n",
       "  0.08207973391591825,\n",
       "  0.08207062534630329,\n",
       "  0.0820655626528249,\n",
       "  0.08205505762063059,\n",
       "  0.08205302658424939,\n",
       "  0.08204862002075043,\n",
       "  0.082039566034288,\n",
       "  0.0820391799205751,\n",
       "  0.08203162839696114,\n",
       "  0.08202068654434154,\n",
       "  0.08200783540431607,\n",
       "  0.08201121926484634,\n",
       "  0.08200768073269603,\n",
       "  0.08200360919795432,\n",
       "  0.08199126389705268,\n",
       "  0.08198479052148262,\n",
       "  0.08198723525089063,\n",
       "  0.08198456087586078,\n",
       "  0.08196736188372251,\n",
       "  0.08196620058926363,\n",
       "  0.08196335372315487,\n",
       "  0.08196339765724896,\n",
       "  0.08195680412714992,\n",
       "  0.08195246225983302,\n",
       "  0.08194363516437896,\n",
       "  0.08193539589171596,\n",
       "  0.08192631138146719,\n",
       "  0.08193369670552171,\n",
       "  0.08191669448686673,\n",
       "  0.0819178425774664,\n",
       "  0.08191050073589977,\n",
       "  0.08190962929335754,\n",
       "  0.08189259372293979,\n",
       "  0.08188894065276696,\n",
       "  0.08188605547644372,\n",
       "  0.08188225966442758,\n",
       "  0.08187336529476522,\n",
       "  0.08186170053970158,\n",
       "  0.08185900155489896,\n",
       "  0.08185269881384553,\n",
       "  0.08183952795654048,\n",
       "  0.08184097351110967,\n",
       "  0.08183754462599459,\n",
       "  0.08183215246496668,\n",
       "  0.08183610398305755,\n",
       "  0.08182293741361508,\n",
       "  0.0818061477883006,\n",
       "  0.08181346319007556,\n",
       "  0.08181177488335108,\n",
       "  0.08180332807718027,\n",
       "  0.08181181670687743,\n",
       "  0.08180252920019633,\n",
       "  0.08179749700396088,\n",
       "  0.08179704842776547,\n",
       "  0.08179007068954736,\n",
       "  0.08179061831791232,\n",
       "  0.08177861355057098,\n",
       "  0.0817808160885527,\n",
       "  0.08177682724530186,\n",
       "  0.0817734359729001,\n",
       "  0.08177024641400042,\n",
       "  0.08176309399081892,\n",
       "  0.08175649090878802,\n",
       "  0.08176188832393146,\n",
       "  0.08175157476354801,\n",
       "  0.08174512748823551,\n",
       "  0.08174868515226759,\n",
       "  0.08174470841796135,\n",
       "  0.08174490784652041,\n",
       "  0.08174184659944791,\n",
       "  0.08173552901916785,\n",
       "  0.08173686144472585,\n",
       "  0.081728127362084,\n",
       "  0.08173278925229806,\n",
       "  0.08172957615937,\n",
       "  0.08171807614103041,\n",
       "  0.08171633610382993,\n",
       "  0.08171636838500952,\n",
       "  0.08171315296068751,\n",
       "  0.0817040619369871,\n",
       "  0.08171176784724837,\n",
       "  0.08169437766569136,\n",
       "  0.08169705094212001,\n",
       "  0.08169354988001232,\n",
       "  0.08168191289780813,\n",
       "  0.08168185332671125,\n",
       "  0.08168130416325388,\n",
       "  0.08166740449944503,\n",
       "  0.08167390665394253,\n",
       "  0.08166051575898357,\n",
       "  0.08166064651464543,\n",
       "  0.08165913084941238,\n",
       "  0.08165659453021247,\n",
       "  0.08165660095260557,\n",
       "  0.08164455616816266,\n",
       "  0.08163768598005242,\n",
       "  0.08163875998069248,\n",
       "  0.08163788082361044,\n",
       "  0.08162555211048088,\n",
       "  0.08163728797527188,\n",
       "  0.0816265651206903,\n",
       "  0.08162217672798139,\n",
       "  0.08162328812456379,\n",
       "  0.08161847708596563,\n",
       "  0.08160943144555437,\n",
       "  0.08161640156102853,\n",
       "  0.08159567230094901,\n",
       "  0.08160864583104628,\n",
       "  0.08159872003707008,\n",
       "  0.08159510746694094,\n",
       "  0.08158821340010698,\n",
       "  0.08158326748203724,\n",
       "  0.08158852706070231,\n",
       "  0.0815881384578098,\n",
       "  0.08158148114027097,\n",
       "  0.08157417038285102,\n",
       "  0.08156525644813357,\n",
       "  0.08157861294765864,\n",
       "  0.08157293989384369,\n",
       "  0.08156740928178433,\n",
       "  0.08156472230675726,\n",
       "  0.08155739706475978,\n",
       "  0.08156823306125559,\n",
       "  0.08155620794804128,\n",
       "  0.08156303765785865,\n",
       "  0.0815602350133973,\n",
       "  0.08156000279896569,\n",
       "  0.08155178784815348,\n",
       "  0.08155234020345285,\n",
       "  0.0815509008563491,\n",
       "  0.08153910170291273,\n",
       "  0.08154368045850888,\n",
       "  0.08154197959233958,\n",
       "  0.08153302547093785,\n",
       "  0.08154205105487278,\n",
       "  0.08152935709371123,\n",
       "  0.08153544858370837,\n",
       "  0.08152036715238062,\n",
       "  0.08152635437517046,\n",
       "  0.08151938455241084,\n",
       "  0.0815183552434573,\n",
       "  0.08151967581112687,\n",
       "  0.08151695538664265,\n",
       "  0.08150400069866187,\n",
       "  0.08150865804147803,\n",
       "  0.08151058212755813,\n",
       "  0.08151776162241131],\n",
       " [0.12646587855802088,\n",
       "  0.10923153539507494,\n",
       "  0.1019562948113664,\n",
       "  0.09818637911114221,\n",
       "  0.09530689670162762,\n",
       "  0.0936295515090519,\n",
       "  0.09229063680553054,\n",
       "  0.09136051693906343,\n",
       "  0.09063175858685935,\n",
       "  0.09009074927610367,\n",
       "  0.08971175361003392,\n",
       "  0.08927291995602746,\n",
       "  0.0890112626876347,\n",
       "  0.08870526890495156,\n",
       "  0.08847121304549602,\n",
       "  0.08820474416826946,\n",
       "  0.08796619728725082,\n",
       "  0.08782309959190611,\n",
       "  0.08763658488210778,\n",
       "  0.08752214146292008,\n",
       "  0.0873682124910085,\n",
       "  0.08726908967482758,\n",
       "  0.08716445220825084,\n",
       "  0.08705778191080077,\n",
       "  0.08691162621322107,\n",
       "  0.0867685047662481,\n",
       "  0.08680057842433718,\n",
       "  0.08664176958419867,\n",
       "  0.08653174084300243,\n",
       "  0.08644895795284906,\n",
       "  0.08642316271259863,\n",
       "  0.08637256362041722,\n",
       "  0.0862778695076306,\n",
       "  0.08619898122769323,\n",
       "  0.08603121768865644,\n",
       "  0.08606232711356979,\n",
       "  0.08606811431721716,\n",
       "  0.08597288636999903,\n",
       "  0.08594445580034413,\n",
       "  0.08583220786424163,\n",
       "  0.08581507118055572,\n",
       "  0.08575831475739806,\n",
       "  0.08564522869732373,\n",
       "  0.08565958896494294,\n",
       "  0.08561419713860734,\n",
       "  0.08555901954854479,\n",
       "  0.08553886485558902,\n",
       "  0.0854565650788621,\n",
       "  0.08547152801419303,\n",
       "  0.08535957744117836,\n",
       "  0.0853349347088821,\n",
       "  0.0853558824574268,\n",
       "  0.08530712772893885,\n",
       "  0.08523177879987714,\n",
       "  0.08516911163806704,\n",
       "  0.08516686428288531,\n",
       "  0.08511701129370254,\n",
       "  0.08509339700359271,\n",
       "  0.08503690041528678,\n",
       "  0.08501408814070274,\n",
       "  0.08499753144789464,\n",
       "  0.08498014101322379,\n",
       "  0.08495626076127097,\n",
       "  0.084930467119943,\n",
       "  0.08483548994148722,\n",
       "  0.08482369241484658,\n",
       "  0.08484820684522576,\n",
       "  0.08480611023553651,\n",
       "  0.08472806533080388,\n",
       "  0.08472491003977647,\n",
       "  0.08473236037489568,\n",
       "  0.08474210185974916,\n",
       "  0.08464030973208556,\n",
       "  0.0845867438492293,\n",
       "  0.08465640532153054,\n",
       "  0.08459064667299722,\n",
       "  0.08454492536579936,\n",
       "  0.08449245046223791,\n",
       "  0.08452068839159164,\n",
       "  0.08445219481272667,\n",
       "  0.0844741274311621,\n",
       "  0.08445865390880343,\n",
       "  0.08439682391890965,\n",
       "  0.08446146174270132,\n",
       "  0.08441286688323861,\n",
       "  0.08433702151529086,\n",
       "  0.08432842594698298,\n",
       "  0.08432493352215946,\n",
       "  0.08437189382840136,\n",
       "  0.08431408755673644,\n",
       "  0.08431446902579956,\n",
       "  0.08427462906295555,\n",
       "  0.0842241378748194,\n",
       "  0.08427233326904814,\n",
       "  0.08427659281507095,\n",
       "  0.08420142179432981,\n",
       "  0.08421118713364682,\n",
       "  0.08415777630314483,\n",
       "  0.08416790038824931,\n",
       "  0.08414595939371593,\n",
       "  0.08414089858518684,\n",
       "  0.0841154954259351,\n",
       "  0.08409874350535795,\n",
       "  0.08409971577295319,\n",
       "  0.08408577280419274,\n",
       "  0.08407458370735575,\n",
       "  0.08406588201969198,\n",
       "  0.08403099169050703,\n",
       "  0.08407301772203386,\n",
       "  0.08398379063033039,\n",
       "  0.08401484780166156,\n",
       "  0.08403799205832681,\n",
       "  0.08400534217445742,\n",
       "  0.08397719254078657,\n",
       "  0.08395808204371591,\n",
       "  0.08394966126881533,\n",
       "  0.0839852223293864,\n",
       "  0.08393594386422623,\n",
       "  0.08395291431305245,\n",
       "  0.08389579252947789,\n",
       "  0.08390155334758738,\n",
       "  0.08395963130338004,\n",
       "  0.08387993758118058,\n",
       "  0.08387468537300799,\n",
       "  0.08385753948722476,\n",
       "  0.0838340682011572,\n",
       "  0.0838014452468574,\n",
       "  0.08377888344579898,\n",
       "  0.08381367634354907,\n",
       "  0.08378720234744483,\n",
       "  0.08380376060600273,\n",
       "  0.0837952889906755,\n",
       "  0.0838039501745151,\n",
       "  0.08373590250261533,\n",
       "  0.08376138445358136,\n",
       "  0.08373838235837269,\n",
       "  0.08381307071020108,\n",
       "  0.08370281961263022,\n",
       "  0.08369956556989588,\n",
       "  0.08373050631180684,\n",
       "  0.08365067065040765,\n",
       "  0.08368302339344606,\n",
       "  0.08365992749257482,\n",
       "  0.08366131779546199,\n",
       "  0.08368803986339089,\n",
       "  0.08366608915258388,\n",
       "  0.08358457876918365,\n",
       "  0.08361507085411546,\n",
       "  0.08366406721893105,\n",
       "  0.08365141572505369,\n",
       "  0.08362587212268943,\n",
       "  0.08357034238819978,\n",
       "  0.08358879485238055,\n",
       "  0.08362873034918297,\n",
       "  0.08359921400170717,\n",
       "  0.08361561846847207,\n",
       "  0.08355259890348807,\n",
       "  0.08351841778929808,\n",
       "  0.0835682773632549,\n",
       "  0.08351553579458995,\n",
       "  0.08358925380615496,\n",
       "  0.08348466665587675,\n",
       "  0.0835353646888493,\n",
       "  0.08352483496839513,\n",
       "  0.08356371165675344,\n",
       "  0.08351606242723583,\n",
       "  0.08349539970818207,\n",
       "  0.08346954941457653,\n",
       "  0.08350143353732589,\n",
       "  0.0834771394563751,\n",
       "  0.08344999991708957,\n",
       "  0.08344141015068081,\n",
       "  0.08341620249453126,\n",
       "  0.08344318837471446,\n",
       "  0.08339798711870572,\n",
       "  0.08345775931197835,\n",
       "  0.08349222467610164,\n",
       "  0.08346156518199777,\n",
       "  0.08342009259989934,\n",
       "  0.08340601883132014,\n",
       "  0.0834015474015331,\n",
       "  0.08341434520676745,\n",
       "  0.08338938177773067,\n",
       "  0.08338620322271319,\n",
       "  0.0833934866495676,\n",
       "  0.08337823680385563,\n",
       "  0.08335051244375223,\n",
       "  0.08338071499766246,\n",
       "  0.08328112149785272,\n",
       "  0.0833253922473344,\n",
       "  0.08335585141503057,\n",
       "  0.08334873810423768,\n",
       "  0.08334174564478128,\n",
       "  0.08328555723385203,\n",
       "  0.08327762073342118,\n",
       "  0.08330814257755093,\n",
       "  0.08326986332557505,\n",
       "  0.08329164180058723,\n",
       "  0.08326592546815545,\n",
       "  0.08330672708665911,\n",
       "  0.08328783941701492,\n",
       "  0.08324489589923212,\n",
       "  0.08318983491772217,\n",
       "  0.08328277111809804,\n",
       "  0.08326086076515547,\n",
       "  0.0832567748415449,\n",
       "  0.08321361860072941,\n",
       "  0.08320152372426999,\n",
       "  0.08323229952837247,\n",
       "  0.08317448269784822,\n",
       "  0.08324346869507222,\n",
       "  0.0831813143561275,\n",
       "  0.08316185749662527,\n",
       "  0.08318324005587867,\n",
       "  0.08312494483571549,\n",
       "  0.08317417363144411,\n",
       "  0.08315904532042552,\n",
       "  0.08316949837221595,\n",
       "  0.08312113841502983,\n",
       "  0.08315533240353117,\n",
       "  0.08318245143215781,\n",
       "  0.08314032851654508,\n",
       "  0.08310511145982907,\n",
       "  0.08308543416464956,\n",
       "  0.08314877783652405,\n",
       "  0.08312383288797903,\n",
       "  0.08311607816711877,\n",
       "  0.0831250966835903,\n",
       "  0.08308496471836115,\n",
       "  0.08308929404971967,\n",
       "  0.08306915819883771,\n",
       "  0.08303436453148179,\n",
       "  0.08307210400448457,\n",
       "  0.08308206886743076,\n",
       "  0.08309009670879516,\n",
       "  0.08302301797727231,\n",
       "  0.083075479976667,\n",
       "  0.08305693820835433,\n",
       "  0.08299744459580653,\n",
       "  0.0830009765060534,\n",
       "  0.08298124037251765,\n",
       "  0.08298969006734773,\n",
       "  0.08300788527751012,\n",
       "  0.08303747753067836,\n",
       "  0.08298524603486486,\n",
       "  0.0829624923730629,\n",
       "  0.08297187271374522,\n",
       "  0.08300672151407067,\n",
       "  0.08293380536875444,\n",
       "  0.08292987868388847,\n",
       "  0.08291306567916569,\n",
       "  0.08291968403389065,\n",
       "  0.08296208789875649,\n",
       "  0.08296727079264521,\n",
       "  0.0829183362915329,\n",
       "  0.08294643183385593,\n",
       "  0.08291104639932606,\n",
       "  0.0829265724638691,\n",
       "  0.08291644659407522,\n",
       "  0.08292306068942999,\n",
       "  0.08297268980293006,\n",
       "  0.08290124174708356,\n",
       "  0.08290342413691783,\n",
       "  0.08296636103230295,\n",
       "  0.08289242132449935,\n",
       "  0.08289534307001217,\n",
       "  0.08291432328144037,\n",
       "  0.08289062626865541,\n",
       "  0.08289417743895167,\n",
       "  0.08291137229754048,\n",
       "  0.08288629997259575,\n",
       "  0.08291779469801502,\n",
       "  0.08284730806230226,\n",
       "  0.08297213778987805,\n",
       "  0.08286648260583754,\n",
       "  0.08289852793778783,\n",
       "  0.08284682574833703,\n",
       "  0.08283165052453535,\n",
       "  0.08288145319410233,\n",
       "  0.0828512971349996,\n",
       "  0.08287278747887666,\n",
       "  0.08284985127453282,\n",
       "  0.08287703725223232,\n",
       "  0.08286369636402742,\n",
       "  0.08284626676570807,\n",
       "  0.08283880652191378,\n",
       "  0.08281822095584467,\n",
       "  0.08280584358760319,\n",
       "  0.08284573932364915,\n",
       "  0.08279718529642,\n",
       "  0.08283700057216659,\n",
       "  0.08281682666892042,\n",
       "  0.08284391051130112,\n",
       "  0.08278431865790542,\n",
       "  0.08280589181402381,\n",
       "  0.08279276399053002,\n",
       "  0.08279137040686628,\n",
       "  0.08282696195311665,\n",
       "  0.08276345695261964,\n",
       "  0.0827785174520761,\n",
       "  0.08280757826587927,\n",
       "  0.08280724347075605,\n",
       "  0.08277457782655344,\n",
       "  0.08272557663179782,\n",
       "  0.08273885435924823,\n",
       "  0.08276896557885945,\n",
       "  0.08279421271379782,\n",
       "  0.08274517059392517,\n",
       "  0.08279395617962626,\n",
       "  0.08276653178666493,\n",
       "  0.08277531607758223,\n",
       "  0.08279698941514944,\n",
       "  0.08272537033099313,\n",
       "  0.08278109802667943,\n",
       "  0.08276458015584139,\n",
       "  0.08275811442191433,\n",
       "  0.0827693306573087,\n",
       "  0.08272467732747962,\n",
       "  0.08275700058997049,\n",
       "  0.08273136740814122,\n",
       "  0.08278417122531234,\n",
       "  0.0827237755982396,\n",
       "  0.08272591906326855,\n",
       "  0.08269587408946968,\n",
       "  0.08276524843245028,\n",
       "  0.08272510256123988,\n",
       "  0.08273244134332065,\n",
       "  0.08276601232263199,\n",
       "  0.08274690994954703,\n",
       "  0.08277672789626003,\n",
       "  0.08273269283192984,\n",
       "  0.0826910654893966,\n",
       "  0.08269585432187851,\n",
       "  0.0827291051617501,\n",
       "  0.08274289684988831,\n",
       "  0.08269562198716619,\n",
       "  0.08274335426776831,\n",
       "  0.08269043999225671,\n",
       "  0.0827436195363026,\n",
       "  0.08271093766403219,\n",
       "  0.08273642996486977,\n",
       "  0.0827314785000796,\n",
       "  0.082736260804181,\n",
       "  0.08276035686754162,\n",
       "  0.08270087396644739,\n",
       "  0.08269488833718605,\n",
       "  0.08273920926364108,\n",
       "  0.0827066357233433])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "sae = SparseAutoencoder(input_dim=768, hidden_dim=3072, top_k=50)\n",
    "train_sae(layer12_embeddings, sae, model_prefix=\"sae_model_12_3072_after_norm\", epochs=500,\n",
    "          batch_size=64, lr=1e-4, weight_decay=1e-5, \n",
    "          train_losses=[],val_losses=val_losses, device=device, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62df3de3-c92d-4399-9906-5675803c20f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sparse_auto_encoder import SparseAutoencoder\n",
    "\n",
    "def objective(trial, device=\"cpu\", embeddings_path=\"training_embeddings.npy\"):\n",
    "    # Hyperparameter search space\n",
    "    print(50*\"=\")\n",
    "    print(f\"Trial number {trial.number + 1}\")\n",
    "    print(50*\"=\")\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 512, step=64)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    # Load your embeddings\n",
    "    embeddings = np.load(embeddings_path)  # Replace with actual file\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "    input_dim = embeddings.shape[1]\n",
    "    sae = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(sae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    dataset = TensorDataset(embeddings)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience, early_stop_counter = 10, 0\n",
    "\n",
    "    for epoch in range(30):\n",
    "        sae.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, encoded = sae(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            sparsity_loss = torch.norm(encoded, p=1) * 1e-4\n",
    "            total_loss = loss + sparsity_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += total_loss.item()\n",
    "\n",
    "        sae.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch[0].to(device)\n",
    "                outputs, encoded = sae(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                sparsity_loss = torch.norm(encoded, p=1) * 1e-4\n",
    "                total_loss = loss + sparsity_loss\n",
    "                val_loss += total_loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch}: Train loss {train_loss:.4f}, Val loss {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            break\n",
    "\n",
    "    return best_val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ad1e0d-2ec6-4e76-b20a-85949d7930d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 05:49:50,242] A new study created in memory with name: no-name-debab08a-1ae8-4354-b1d7-2e8a93fcaf53\n",
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Trial number 1\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.5013, Val loss 2.7664\n",
      "Epoch 1: Train loss 2.6397, Val loss 2.5404\n",
      "Epoch 2: Train loss 2.5101, Val loss 2.4382\n",
      "Epoch 3: Train loss 2.4494, Val loss 2.3934\n",
      "Epoch 4: Train loss 2.4061, Val loss 2.3609\n",
      "Epoch 5: Train loss 2.3730, Val loss 2.3305\n",
      "Epoch 6: Train loss 2.3432, Val loss 2.3005\n",
      "Epoch 7: Train loss 2.3171, Val loss 2.2783\n",
      "Epoch 8: Train loss 2.2942, Val loss 2.2555\n",
      "Epoch 9: Train loss 2.2737, Val loss 2.2374\n",
      "Epoch 10: Train loss 2.2572, Val loss 2.2252\n",
      "Epoch 11: Train loss 2.2438, Val loss 2.2149\n",
      "Epoch 12: Train loss 2.2321, Val loss 2.2054\n",
      "Epoch 13: Train loss 2.2236, Val loss 2.2009\n",
      "Epoch 14: Train loss 2.2164, Val loss 2.1995\n",
      "Epoch 15: Train loss 2.2110, Val loss 2.1948\n",
      "Epoch 16: Train loss 2.2063, Val loss 2.1908\n",
      "Epoch 17: Train loss 2.2020, Val loss 2.1906\n",
      "Epoch 18: Train loss 2.1982, Val loss 2.1864\n",
      "Epoch 19: Train loss 2.1944, Val loss 2.1859\n",
      "Epoch 20: Train loss 2.1912, Val loss 2.1838\n",
      "Epoch 21: Train loss 2.1882, Val loss 2.1842\n",
      "Epoch 22: Train loss 2.1858, Val loss 2.1784\n",
      "Epoch 23: Train loss 2.1831, Val loss 2.1818\n",
      "Epoch 24: Train loss 2.1809, Val loss 2.1789\n",
      "Epoch 25: Train loss 2.1786, Val loss 2.1784\n",
      "Epoch 26: Train loss 2.1770, Val loss 2.1826\n",
      "Epoch 27: Train loss 2.1748, Val loss 2.1746\n",
      "Epoch 28: Train loss 2.1729, Val loss 2.1777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:02:12,080] Trial 0 finished with value: 2.170551270788366 and parameters: {'batch_size': 128, 'lr': 0.0002688498549528964, 'hidden_dim': 320, 'weight_decay': 0.00024172216807324387}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1711, Val loss 2.1706\n",
      "==================================================\n",
      "Trial number 2\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss 2.4140, Val loss 2.3188\n",
      "Epoch 1: Train loss 2.3013, Val loss 2.3061\n",
      "Epoch 2: Train loss 2.2921, Val loss 2.2908\n",
      "Epoch 3: Train loss 2.2870, Val loss 2.3128\n",
      "Epoch 4: Train loss 2.2838, Val loss 2.3026\n",
      "Epoch 5: Train loss 2.2817, Val loss 2.3122\n",
      "Epoch 6: Train loss 2.2804, Val loss 2.2955\n",
      "Epoch 7: Train loss 2.2793, Val loss 2.2935\n",
      "Epoch 8: Train loss 2.2782, Val loss 2.2808\n",
      "Epoch 9: Train loss 2.2774, Val loss 2.3122\n",
      "Epoch 10: Train loss 2.2773, Val loss 2.2876\n",
      "Epoch 11: Train loss 2.2768, Val loss 2.2953\n",
      "Epoch 12: Train loss 2.2759, Val loss 2.2960\n",
      "Epoch 13: Train loss 2.2757, Val loss 2.2897\n",
      "Epoch 14: Train loss 2.2754, Val loss 2.2881\n",
      "Epoch 15: Train loss 2.2601, Val loss 2.2521\n",
      "Epoch 16: Train loss 2.2585, Val loss 2.2471\n",
      "Epoch 17: Train loss 2.2585, Val loss 2.2550\n",
      "Epoch 18: Train loss 2.2587, Val loss 2.2596\n",
      "Epoch 19: Train loss 2.2585, Val loss 2.2635\n",
      "Epoch 20: Train loss 2.2582, Val loss 2.2584\n",
      "Epoch 21: Train loss 2.2581, Val loss 2.2611\n",
      "Epoch 22: Train loss 2.2582, Val loss 2.2607\n",
      "Epoch 23: Train loss 2.2479, Val loss 2.2250\n",
      "Epoch 24: Train loss 2.2469, Val loss 2.2332\n",
      "Epoch 25: Train loss 2.2468, Val loss 2.2317\n",
      "Epoch 26: Train loss 2.2467, Val loss 2.2267\n",
      "Epoch 27: Train loss 2.2465, Val loss 2.2312\n",
      "Epoch 28: Train loss 2.2465, Val loss 2.2292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:22:54,658] Trial 1 finished with value: 2.224995661976669 and parameters: {'batch_size': 64, 'lr': 0.004508739037375544, 'hidden_dim': 192, 'weight_decay': 0.0001467563404464736}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.2463, Val loss 2.2263\n",
      "==================================================\n",
      "Trial number 3\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.1771, Val loss 2.8172\n",
      "Epoch 1: Train loss 2.7800, Val loss 2.6882\n",
      "Epoch 2: Train loss 2.6947, Val loss 2.6175\n",
      "Epoch 3: Train loss 2.6386, Val loss 2.5764\n",
      "Epoch 4: Train loss 2.6049, Val loss 2.5540\n",
      "Epoch 5: Train loss 2.5865, Val loss 2.5463\n",
      "Epoch 6: Train loss 2.5802, Val loss 2.5443\n",
      "Epoch 7: Train loss 2.5765, Val loss 2.5453\n",
      "Epoch 8: Train loss 2.5741, Val loss 2.5435\n",
      "Epoch 9: Train loss 2.5720, Val loss 2.5434\n",
      "Epoch 10: Train loss 2.5707, Val loss 2.5442\n",
      "Epoch 11: Train loss 2.5698, Val loss 2.5418\n",
      "Epoch 12: Train loss 2.5687, Val loss 2.5428\n",
      "Epoch 13: Train loss 2.5678, Val loss 2.5433\n",
      "Epoch 14: Train loss 2.5671, Val loss 2.5422\n",
      "Epoch 15: Train loss 2.5665, Val loss 2.5457\n",
      "Epoch 16: Train loss 2.5662, Val loss 2.5436\n",
      "Epoch 17: Train loss 2.5657, Val loss 2.5425\n",
      "Epoch 18: Train loss 2.5633, Val loss 2.5331\n",
      "Epoch 19: Train loss 2.5630, Val loss 2.5343\n",
      "Epoch 20: Train loss 2.5628, Val loss 2.5353\n",
      "Epoch 21: Train loss 2.5624, Val loss 2.5335\n",
      "Epoch 22: Train loss 2.5624, Val loss 2.5336\n",
      "Epoch 23: Train loss 2.5620, Val loss 2.5333\n",
      "Epoch 24: Train loss 2.5618, Val loss 2.5328\n",
      "Epoch 25: Train loss 2.5608, Val loss 2.5299\n",
      "Epoch 26: Train loss 2.5606, Val loss 2.5301\n",
      "Epoch 27: Train loss 2.5606, Val loss 2.5311\n",
      "Epoch 28: Train loss 2.5605, Val loss 2.5301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:43:08,899] Trial 2 finished with value: 2.5284160660719817 and parameters: {'batch_size': 64, 'lr': 0.00020725071060021414, 'hidden_dim': 64, 'weight_decay': 1.9243970614633706e-06}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.5604, Val loss 2.5284\n",
      "==================================================\n",
      "Trial number 4\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.8168, Val loss 2.6210\n",
      "Epoch 1: Train loss 2.5877, Val loss 2.6165\n",
      "Epoch 2: Train loss 2.5764, Val loss 2.5997\n",
      "Epoch 3: Train loss 2.5724, Val loss 2.5933\n",
      "Epoch 4: Train loss 2.5704, Val loss 2.6060\n",
      "Epoch 5: Train loss 2.5694, Val loss 2.6090\n",
      "Epoch 6: Train loss 2.5685, Val loss 2.5808\n",
      "Epoch 7: Train loss 2.5677, Val loss 2.5884\n",
      "Epoch 8: Train loss 2.5670, Val loss 2.5909\n",
      "Epoch 9: Train loss 2.5670, Val loss 2.5738\n",
      "Epoch 10: Train loss 2.5666, Val loss 2.5881\n",
      "Epoch 11: Train loss 2.5661, Val loss 2.5779\n",
      "Epoch 12: Train loss 2.5656, Val loss 2.5883\n",
      "Epoch 13: Train loss 2.5655, Val loss 2.5677\n",
      "Epoch 14: Train loss 2.5651, Val loss 2.5784\n",
      "Epoch 15: Train loss 2.5650, Val loss 2.5797\n",
      "Epoch 16: Train loss 2.5648, Val loss 2.5900\n",
      "Epoch 17: Train loss 2.5645, Val loss 2.5777\n",
      "Epoch 18: Train loss 2.5645, Val loss 2.5801\n",
      "Epoch 19: Train loss 2.5645, Val loss 2.5810\n",
      "Epoch 20: Train loss 2.5585, Val loss 2.5615\n",
      "Epoch 21: Train loss 2.5579, Val loss 2.5576\n",
      "Epoch 22: Train loss 2.5575, Val loss 2.5573\n",
      "Epoch 23: Train loss 2.5575, Val loss 2.5587\n",
      "Epoch 24: Train loss 2.5575, Val loss 2.5647\n",
      "Epoch 25: Train loss 2.5574, Val loss 2.5576\n",
      "Epoch 26: Train loss 2.5575, Val loss 2.5544\n",
      "Epoch 27: Train loss 2.5573, Val loss 2.5627\n",
      "Epoch 28: Train loss 2.5575, Val loss 2.5561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:55:02,432] Trial 3 finished with value: 2.5543929240920327 and parameters: {'batch_size': 128, 'lr': 0.0017955378450738243, 'hidden_dim': 64, 'weight_decay': 1.980034613217017e-05}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.5572, Val loss 2.5640\n",
      "==================================================\n",
      "Trial number 5\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.7470, Val loss 2.3950\n",
      "Epoch 1: Train loss 2.3955, Val loss 2.2924\n",
      "Epoch 2: Train loss 2.3048, Val loss 2.2097\n",
      "Epoch 3: Train loss 2.2311, Val loss 2.1628\n",
      "Epoch 4: Train loss 2.1922, Val loss 2.1437\n",
      "Epoch 5: Train loss 2.1740, Val loss 2.1422\n",
      "Epoch 6: Train loss 2.1642, Val loss 2.1383\n",
      "Epoch 7: Train loss 2.1578, Val loss 2.1427\n",
      "Epoch 8: Train loss 2.1526, Val loss 2.1437\n",
      "Epoch 9: Train loss 2.1479, Val loss 2.1372\n",
      "Epoch 10: Train loss 2.1445, Val loss 2.1387\n",
      "Epoch 11: Train loss 2.1415, Val loss 2.1432\n",
      "Epoch 12: Train loss 2.1381, Val loss 2.1517\n",
      "Epoch 13: Train loss 2.1356, Val loss 2.1578\n",
      "Epoch 14: Train loss 2.1332, Val loss 2.1525\n",
      "Epoch 15: Train loss 2.1310, Val loss 2.1545\n",
      "Epoch 16: Train loss 2.1234, Val loss 2.1080\n",
      "Epoch 17: Train loss 2.1219, Val loss 2.1100\n",
      "Epoch 18: Train loss 2.1206, Val loss 2.1085\n",
      "Epoch 19: Train loss 2.1194, Val loss 2.1072\n",
      "Epoch 20: Train loss 2.1182, Val loss 2.1102\n",
      "Epoch 21: Train loss 2.1174, Val loss 2.1108\n",
      "Epoch 22: Train loss 2.1164, Val loss 2.1132\n",
      "Epoch 23: Train loss 2.1155, Val loss 2.1107\n",
      "Epoch 24: Train loss 2.1145, Val loss 2.1094\n",
      "Epoch 25: Train loss 2.1136, Val loss 2.1119\n",
      "Epoch 26: Train loss 2.1095, Val loss 2.0839\n",
      "Epoch 27: Train loss 2.1087, Val loss 2.0843\n",
      "Epoch 28: Train loss 2.1081, Val loss 2.0863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 07:16:17,436] Trial 4 finished with value: 2.083881515848338 and parameters: {'batch_size': 64, 'lr': 0.0003881008304162215, 'hidden_dim': 384, 'weight_decay': 2.3923055206292162e-05}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1077, Val loss 2.0868\n",
      "==================================================\n",
      "Trial number 6\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.3573, Val loss 2.2031\n",
      "Epoch 1: Train loss 2.2544, Val loss 2.1934\n",
      "Epoch 2: Train loss 2.2470, Val loss 2.1944\n",
      "Epoch 3: Train loss 2.2429, Val loss 2.1903\n",
      "Epoch 4: Train loss 2.2405, Val loss 2.1887\n",
      "Epoch 5: Train loss 2.2391, Val loss 2.1896\n",
      "Epoch 6: Train loss 2.2378, Val loss 2.1853\n",
      "Epoch 7: Train loss 2.2372, Val loss 2.1904\n",
      "Epoch 8: Train loss 2.2364, Val loss 2.1852\n",
      "Epoch 9: Train loss 2.2354, Val loss 2.1833\n",
      "Epoch 10: Train loss 2.2351, Val loss 2.1861\n",
      "Epoch 11: Train loss 2.2346, Val loss 2.1805\n",
      "Epoch 12: Train loss 2.2338, Val loss 2.1858\n",
      "Epoch 13: Train loss 2.2336, Val loss 2.1823\n",
      "Epoch 14: Train loss 2.2334, Val loss 2.1866\n",
      "Epoch 15: Train loss 2.2330, Val loss 2.1816\n",
      "Epoch 16: Train loss 2.2328, Val loss 2.1794\n",
      "Epoch 17: Train loss 2.2324, Val loss 2.1818\n",
      "Epoch 18: Train loss 2.2323, Val loss 2.1867\n",
      "Epoch 19: Train loss 2.2321, Val loss 2.1766\n",
      "Epoch 20: Train loss 2.2320, Val loss 2.1891\n",
      "Epoch 21: Train loss 2.2317, Val loss 2.1924\n",
      "Epoch 22: Train loss 2.2316, Val loss 2.1749\n",
      "Epoch 23: Train loss 2.2314, Val loss 2.1798\n",
      "Epoch 24: Train loss 2.2308, Val loss 2.1811\n",
      "Epoch 25: Train loss 2.2312, Val loss 2.1879\n",
      "Epoch 26: Train loss 2.2308, Val loss 2.1752\n",
      "Epoch 27: Train loss 2.2309, Val loss 2.1794\n",
      "Epoch 28: Train loss 2.2306, Val loss 2.1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 07:55:47,399] Trial 5 finished with value: 2.1656301614224165 and parameters: {'batch_size': 32, 'lr': 0.0021729419335939316, 'hidden_dim': 256, 'weight_decay': 0.00048608168804415455}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.2222, Val loss 2.1656\n",
      "==================================================\n",
      "Trial number 7\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.5723, Val loss 2.2847\n",
      "Epoch 1: Train loss 2.2485, Val loss 2.2205\n",
      "Epoch 2: Train loss 2.1990, Val loss 2.2098\n",
      "Epoch 3: Train loss 2.1830, Val loss 2.1955\n",
      "Epoch 4: Train loss 2.1738, Val loss 2.2090\n",
      "Epoch 5: Train loss 2.1684, Val loss 2.2012\n",
      "Epoch 6: Train loss 2.1642, Val loss 2.2004\n",
      "Epoch 7: Train loss 2.1604, Val loss 2.2010\n",
      "Epoch 8: Train loss 2.1579, Val loss 2.1832\n",
      "Epoch 9: Train loss 2.1553, Val loss 2.1774\n",
      "Epoch 10: Train loss 2.1524, Val loss 2.2113\n",
      "Epoch 11: Train loss 2.1509, Val loss 2.2076\n",
      "Epoch 12: Train loss 2.1490, Val loss 2.2049\n",
      "Epoch 13: Train loss 2.1477, Val loss 2.2049\n",
      "Epoch 14: Train loss 2.1454, Val loss 2.1735\n",
      "Epoch 15: Train loss 2.1450, Val loss 2.1901\n",
      "Epoch 16: Train loss 2.1440, Val loss 2.1863\n",
      "Epoch 17: Train loss 2.1427, Val loss 2.2090\n",
      "Epoch 18: Train loss 2.1412, Val loss 2.1864\n",
      "Epoch 19: Train loss 2.1403, Val loss 2.2033\n",
      "Epoch 20: Train loss 2.1400, Val loss 2.5091\n",
      "Epoch 21: Train loss 2.1314, Val loss 2.1538\n",
      "Epoch 22: Train loss 2.1290, Val loss 2.1983\n",
      "Epoch 23: Train loss 2.1280, Val loss 2.1678\n",
      "Epoch 24: Train loss 2.1274, Val loss 2.1459\n",
      "Epoch 25: Train loss 2.1267, Val loss 2.1677\n",
      "Epoch 26: Train loss 2.1267, Val loss 2.1792\n",
      "Epoch 27: Train loss 2.1258, Val loss 2.1469\n",
      "Epoch 28: Train loss 2.1254, Val loss 2.1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 08:08:39,194] Trial 6 finished with value: 2.1458648746663873 and parameters: {'batch_size': 128, 'lr': 0.002262804895465889, 'hidden_dim': 512, 'weight_decay': 1.2074671078795074e-05}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1242, Val loss 2.1671\n",
      "==================================================\n",
      "Trial number 8\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.6830, Val loss 2.9287\n",
      "Epoch 1: Train loss 2.6973, Val loss 2.5486\n",
      "Epoch 2: Train loss 2.5121, Val loss 2.4390\n",
      "Epoch 3: Train loss 2.4410, Val loss 2.3816\n",
      "Epoch 4: Train loss 2.3982, Val loss 2.3592\n",
      "Epoch 5: Train loss 2.3705, Val loss 2.3356\n",
      "Epoch 6: Train loss 2.3475, Val loss 2.3143\n",
      "Epoch 7: Train loss 2.3279, Val loss 2.2913\n",
      "Epoch 8: Train loss 2.3100, Val loss 2.2707\n",
      "Epoch 9: Train loss 2.2940, Val loss 2.2571\n",
      "Epoch 10: Train loss 2.2788, Val loss 2.2455\n",
      "Epoch 11: Train loss 2.2643, Val loss 2.2292\n",
      "Epoch 12: Train loss 2.2505, Val loss 2.2204\n",
      "Epoch 13: Train loss 2.2369, Val loss 2.2035\n",
      "Epoch 14: Train loss 2.2259, Val loss 2.1959\n",
      "Epoch 15: Train loss 2.2149, Val loss 2.1837\n",
      "Epoch 16: Train loss 2.2055, Val loss 2.1777\n",
      "Epoch 17: Train loss 2.1969, Val loss 2.1710\n",
      "Epoch 18: Train loss 2.1900, Val loss 2.1665\n",
      "Epoch 19: Train loss 2.1841, Val loss 2.1618\n",
      "Epoch 20: Train loss 2.1790, Val loss 2.1631\n",
      "Epoch 21: Train loss 2.1749, Val loss 2.1544\n",
      "Epoch 22: Train loss 2.1704, Val loss 2.1520\n",
      "Epoch 23: Train loss 2.1667, Val loss 2.1490\n",
      "Epoch 24: Train loss 2.1631, Val loss 2.1474\n",
      "Epoch 25: Train loss 2.1603, Val loss 2.1428\n",
      "Epoch 26: Train loss 2.1573, Val loss 2.1437\n",
      "Epoch 27: Train loss 2.1547, Val loss 2.1403\n",
      "Epoch 28: Train loss 2.1524, Val loss 2.1394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 08:21:38,398] Trial 7 finished with value: 2.1394470236518166 and parameters: {'batch_size': 128, 'lr': 0.00019293742668148652, 'hidden_dim': 448, 'weight_decay': 0.00022579186946369568}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1504, Val loss 2.1417\n",
      "==================================================\n",
      "Trial number 9\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4441, Val loss 2.1521\n",
      "Epoch 1: Train loss 2.1927, Val loss 2.1148\n",
      "Epoch 2: Train loss 2.1680, Val loss 2.1177\n",
      "Epoch 3: Train loss 2.1613, Val loss 2.1051\n",
      "Epoch 4: Train loss 2.1571, Val loss 2.1005\n",
      "Epoch 5: Train loss 2.1540, Val loss 2.1026\n",
      "Epoch 6: Train loss 2.1515, Val loss 2.0959\n",
      "Epoch 7: Train loss 2.1492, Val loss 2.0967\n",
      "Epoch 8: Train loss 2.1474, Val loss 2.0888\n",
      "Epoch 9: Train loss 2.1458, Val loss 2.0864\n",
      "Epoch 10: Train loss 2.1449, Val loss 2.0961\n",
      "Epoch 11: Train loss 2.1438, Val loss 2.0909\n",
      "Epoch 12: Train loss 2.1426, Val loss 2.0885\n",
      "Epoch 13: Train loss 2.1418, Val loss 2.0868\n",
      "Epoch 14: Train loss 2.1407, Val loss 2.0861\n",
      "Epoch 15: Train loss 2.1403, Val loss 2.0882\n",
      "Epoch 16: Train loss 2.1397, Val loss 2.0880\n",
      "Epoch 17: Train loss 2.1392, Val loss 2.0777\n",
      "Epoch 18: Train loss 2.1386, Val loss 2.0790\n",
      "Epoch 19: Train loss 2.1383, Val loss 2.0814\n",
      "Epoch 20: Train loss 2.1380, Val loss 2.0770\n",
      "Epoch 21: Train loss 2.1374, Val loss 2.0794\n",
      "Epoch 22: Train loss 2.1374, Val loss 2.0781\n",
      "Epoch 23: Train loss 2.1368, Val loss 2.0749\n",
      "Epoch 24: Train loss 2.1363, Val loss 2.0786\n",
      "Epoch 25: Train loss 2.1359, Val loss 2.0803\n",
      "Epoch 26: Train loss 2.1354, Val loss 2.0753\n",
      "Epoch 27: Train loss 2.1348, Val loss 2.0771\n",
      "Epoch 28: Train loss 2.1344, Val loss 2.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 09:00:44,986] Trial 8 finished with value: 2.07491034575172 and parameters: {'batch_size': 32, 'lr': 0.000816547713285386, 'hidden_dim': 512, 'weight_decay': 2.9090240560596477e-06}. Best is trial 8 with value: 2.07491034575172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1341, Val loss 2.0782\n",
      "==================================================\n",
      "Trial number 10\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.6428, Val loss 2.9041\n",
      "Epoch 1: Train loss 2.6809, Val loss 2.5343\n",
      "Epoch 2: Train loss 2.5082, Val loss 2.4249\n",
      "Epoch 3: Train loss 2.4365, Val loss 2.3805\n",
      "Epoch 4: Train loss 2.3950, Val loss 2.3383\n",
      "Epoch 5: Train loss 2.3643, Val loss 2.3227\n",
      "Epoch 6: Train loss 2.3415, Val loss 2.3036\n",
      "Epoch 7: Train loss 2.3214, Val loss 2.2759\n",
      "Epoch 8: Train loss 2.3022, Val loss 2.2607\n",
      "Epoch 9: Train loss 2.2851, Val loss 2.2427\n",
      "Epoch 10: Train loss 2.2683, Val loss 2.2313\n",
      "Epoch 11: Train loss 2.2530, Val loss 2.2129\n",
      "Epoch 12: Train loss 2.2384, Val loss 2.2007\n",
      "Epoch 13: Train loss 2.2255, Val loss 2.1905\n",
      "Epoch 14: Train loss 2.2141, Val loss 2.1795\n",
      "Epoch 15: Train loss 2.2038, Val loss 2.1727\n",
      "Epoch 16: Train loss 2.1950, Val loss 2.1652\n",
      "Epoch 17: Train loss 2.1873, Val loss 2.1620\n",
      "Epoch 18: Train loss 2.1811, Val loss 2.1559\n",
      "Epoch 19: Train loss 2.1750, Val loss 2.1516\n",
      "Epoch 20: Train loss 2.1699, Val loss 2.1459\n",
      "Epoch 21: Train loss 2.1655, Val loss 2.1439\n",
      "Epoch 22: Train loss 2.1620, Val loss 2.1426\n",
      "Epoch 23: Train loss 2.1590, Val loss 2.1400\n",
      "Epoch 24: Train loss 2.1558, Val loss 2.1384\n",
      "Epoch 25: Train loss 2.1535, Val loss 2.1373\n",
      "Epoch 26: Train loss 2.1515, Val loss 2.1362\n",
      "Epoch 27: Train loss 2.1494, Val loss 2.1331\n",
      "Epoch 28: Train loss 2.1475, Val loss 2.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 09:13:30,328] Trial 9 finished with value: 2.133140113136985 and parameters: {'batch_size': 128, 'lr': 0.00020385491002032069, 'hidden_dim': 448, 'weight_decay': 0.0009293870195375112}. Best is trial 8 with value: 2.07491034575172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1461, Val loss 2.1341\n",
      "==================================================\n",
      "Trial number 11\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4641, Val loss 2.1610\n",
      "Epoch 1: Train loss 2.1981, Val loss 2.1177\n",
      "Epoch 2: Train loss 2.1680, Val loss 2.1113\n",
      "Epoch 3: Train loss 2.1603, Val loss 2.1073\n",
      "Epoch 4: Train loss 2.1552, Val loss 2.1021\n",
      "Epoch 5: Train loss 2.1526, Val loss 2.1009\n",
      "Epoch 6: Train loss 2.1501, Val loss 2.1042\n",
      "Epoch 7: Train loss 2.1482, Val loss 2.0891\n",
      "Epoch 8: Train loss 2.1468, Val loss 2.0926\n",
      "Epoch 9: Train loss 2.1453, Val loss 2.0885\n",
      "Epoch 10: Train loss 2.1440, Val loss 2.0865\n",
      "Epoch 11: Train loss 2.1432, Val loss 2.0876\n",
      "Epoch 12: Train loss 2.1423, Val loss 2.0805\n",
      "Epoch 13: Train loss 2.1417, Val loss 2.0836\n",
      "Epoch 14: Train loss 2.1412, Val loss 2.0824\n",
      "Epoch 15: Train loss 2.1406, Val loss 2.0840\n",
      "Epoch 16: Train loss 2.1400, Val loss 2.0843\n",
      "Epoch 17: Train loss 2.1391, Val loss 2.0755\n",
      "Epoch 18: Train loss 2.1386, Val loss 2.0769\n",
      "Epoch 19: Train loss 2.1380, Val loss 2.0733\n",
      "Epoch 20: Train loss 2.1372, Val loss 2.0794\n",
      "Epoch 21: Train loss 2.1368, Val loss 2.0805\n",
      "Epoch 22: Train loss 2.1367, Val loss 2.0795\n",
      "Epoch 23: Train loss 2.1365, Val loss 2.0813\n",
      "Epoch 24: Train loss 2.1361, Val loss 2.0719\n",
      "Epoch 25: Train loss 2.1360, Val loss 2.0764\n",
      "Epoch 26: Train loss 2.1357, Val loss 2.0776\n",
      "Epoch 27: Train loss 2.1354, Val loss 2.0761\n",
      "Epoch 28: Train loss 2.1354, Val loss 2.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 09:52:38,080] Trial 10 finished with value: 2.070989271797911 and parameters: {'batch_size': 32, 'lr': 0.0007246691209707045, 'hidden_dim': 512, 'weight_decay': 1.0552397856143292e-06}. Best is trial 10 with value: 2.070989271797911.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1352, Val loss 2.0710\n",
      "==================================================\n",
      "Trial number 12\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4702, Val loss 2.1798\n",
      "Epoch 1: Train loss 2.2019, Val loss 2.1129\n",
      "Epoch 2: Train loss 2.1672, Val loss 2.1201\n",
      "Epoch 3: Train loss 2.1594, Val loss 2.1079\n",
      "Epoch 4: Train loss 2.1543, Val loss 2.1025\n",
      "Epoch 5: Train loss 2.1510, Val loss 2.1049\n",
      "Epoch 6: Train loss 2.1492, Val loss 2.0979\n",
      "Epoch 7: Train loss 2.1475, Val loss 2.0987\n",
      "Epoch 8: Train loss 2.1465, Val loss 2.0943\n",
      "Epoch 9: Train loss 2.1456, Val loss 2.0965\n",
      "Epoch 10: Train loss 2.1447, Val loss 2.0882\n",
      "Epoch 11: Train loss 2.1438, Val loss 2.0918\n",
      "Epoch 12: Train loss 2.1429, Val loss 2.0872\n",
      "Epoch 13: Train loss 2.1420, Val loss 2.0878\n",
      "Epoch 14: Train loss 2.1413, Val loss 2.0828\n",
      "Epoch 15: Train loss 2.1404, Val loss 2.0836\n",
      "Epoch 16: Train loss 2.1395, Val loss 2.0862\n",
      "Epoch 17: Train loss 2.1389, Val loss 2.0900\n",
      "Epoch 18: Train loss 2.1385, Val loss 2.0852\n",
      "Epoch 19: Train loss 2.1379, Val loss 2.0827\n",
      "Epoch 20: Train loss 2.1371, Val loss 2.0838\n",
      "Epoch 21: Train loss 2.1289, Val loss 2.0680\n",
      "Epoch 22: Train loss 2.1273, Val loss 2.0689\n",
      "Epoch 23: Train loss 2.1269, Val loss 2.0667\n",
      "Epoch 24: Train loss 2.1266, Val loss 2.0674\n",
      "Epoch 25: Train loss 2.1263, Val loss 2.0709\n",
      "Epoch 26: Train loss 2.1263, Val loss 2.0661\n",
      "Epoch 27: Train loss 2.1258, Val loss 2.0678\n",
      "Epoch 28: Train loss 2.1256, Val loss 2.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 10:31:43,896] Trial 11 finished with value: 2.0660949138034574 and parameters: {'batch_size': 32, 'lr': 0.0007016224567866245, 'hidden_dim': 512, 'weight_decay': 1.1004487564277892e-06}. Best is trial 11 with value: 2.0660949138034574.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1256, Val loss 2.0701\n",
      "==================================================\n",
      "Trial number 13\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4893, Val loss 2.1768\n",
      "Epoch 1: Train loss 2.2109, Val loss 2.1531\n",
      "Epoch 2: Train loss 2.1821, Val loss 2.1659\n",
      "Epoch 3: Train loss 2.1730, Val loss 2.1861\n",
      "Epoch 4: Train loss 2.1683, Val loss 2.1886\n",
      "Epoch 5: Train loss 2.1654, Val loss 2.2205\n",
      "Epoch 6: Train loss 2.1636, Val loss 2.2249\n",
      "Epoch 7: Train loss 2.1614, Val loss 2.2632\n",
      "Epoch 8: Train loss 2.1478, Val loss 2.1724\n",
      "Epoch 9: Train loss 2.1455, Val loss 2.1609\n",
      "Epoch 10: Train loss 2.1442, Val loss 2.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 10:47:19,730] Trial 12 finished with value: 2.153145467405852 and parameters: {'batch_size': 32, 'lr': 0.0006288969424953504, 'hidden_dim': 384, 'weight_decay': 1.0034603972640884e-06}. Best is trial 11 with value: 2.0660949138034574.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train loss 2.1431, Val loss 2.2010\n",
      "==================================================\n",
      "Trial number 14\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.5095, Val loss 2.2279\n",
      "Epoch 1: Train loss 2.2277, Val loss 2.1206\n",
      "Epoch 2: Train loss 2.1678, Val loss 2.1205\n",
      "Epoch 3: Train loss 2.1543, Val loss 2.1307\n",
      "Epoch 4: Train loss 2.1478, Val loss 2.1580\n",
      "Epoch 5: Train loss 2.1434, Val loss 2.1555\n",
      "Epoch 6: Train loss 2.1400, Val loss 2.1728\n",
      "Epoch 7: Train loss 2.1374, Val loss 2.1752\n",
      "Epoch 8: Train loss 2.1244, Val loss 2.0986\n",
      "Epoch 9: Train loss 2.1213, Val loss 2.1100\n",
      "Epoch 10: Train loss 2.1196, Val loss 2.1081\n",
      "Epoch 11: Train loss 2.1185, Val loss 2.1205\n",
      "Epoch 12: Train loss 2.1173, Val loss 2.1234\n",
      "Epoch 13: Train loss 2.1164, Val loss 2.1237\n",
      "Epoch 14: Train loss 2.1156, Val loss 2.1255\n",
      "Epoch 15: Train loss 2.1085, Val loss 2.0756\n",
      "Epoch 16: Train loss 2.1080, Val loss 2.0798\n",
      "Epoch 17: Train loss 2.1071, Val loss 2.0785\n",
      "Epoch 18: Train loss 2.1066, Val loss 2.0757\n",
      "Epoch 19: Train loss 2.1058, Val loss 2.0778\n",
      "Epoch 20: Train loss 2.1053, Val loss 2.0760\n",
      "Epoch 21: Train loss 2.1048, Val loss 2.0761\n",
      "Epoch 22: Train loss 2.1011, Val loss 2.0572\n",
      "Epoch 23: Train loss 2.1008, Val loss 2.0589\n",
      "Epoch 24: Train loss 2.1005, Val loss 2.0608\n",
      "Epoch 25: Train loss 2.1003, Val loss 2.0577\n",
      "Epoch 26: Train loss 2.1000, Val loss 2.0595\n",
      "Epoch 27: Train loss 2.0998, Val loss 2.0610\n",
      "Epoch 28: Train loss 2.0993, Val loss 2.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 11:26:55,833] Trial 13 finished with value: 2.05181546670681 and parameters: {'batch_size': 32, 'lr': 0.0005587614855267997, 'hidden_dim': 512, 'weight_decay': 6.056437420027853e-06}. Best is trial 13 with value: 2.05181546670681.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.0975, Val loss 2.0518\n",
      "==================================================\n",
      "Trial number 15\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.8744, Val loss 2.4504\n",
      "Epoch 1: Train loss 2.4838, Val loss 2.3448\n",
      "Epoch 2: Train loss 2.4184, Val loss 2.2957\n",
      "Epoch 3: Train loss 2.3694, Val loss 2.2535\n",
      "Epoch 4: Train loss 2.3175, Val loss 2.1969\n",
      "Epoch 5: Train loss 2.2606, Val loss 2.1474\n",
      "Epoch 6: Train loss 2.2106, Val loss 2.1180\n",
      "Epoch 7: Train loss 2.1856, Val loss 2.1069\n",
      "Epoch 8: Train loss 2.1742, Val loss 2.1009\n",
      "Epoch 9: Train loss 2.1669, Val loss 2.0953\n",
      "Epoch 10: Train loss 2.1617, Val loss 2.0941\n",
      "Epoch 11: Train loss 2.1573, Val loss 2.0918\n",
      "Epoch 12: Train loss 2.1538, Val loss 2.0895\n",
      "Epoch 13: Train loss 2.1505, Val loss 2.0892\n",
      "Epoch 14: Train loss 2.1486, Val loss 2.0874\n",
      "Epoch 15: Train loss 2.1460, Val loss 2.0858\n",
      "Epoch 16: Train loss 2.1435, Val loss 2.0855\n",
      "Epoch 17: Train loss 2.1417, Val loss 2.0863\n",
      "Epoch 18: Train loss 2.1397, Val loss 2.0826\n",
      "Epoch 19: Train loss 2.1383, Val loss 2.0815\n",
      "Epoch 20: Train loss 2.1371, Val loss 2.0823\n",
      "Epoch 21: Train loss 2.1358, Val loss 2.0834\n",
      "Epoch 22: Train loss 2.1352, Val loss 2.0832\n",
      "Epoch 23: Train loss 2.1346, Val loss 2.0843\n",
      "Epoch 24: Train loss 2.1339, Val loss 2.0844\n",
      "Epoch 25: Train loss 2.1330, Val loss 2.0864\n",
      "Epoch 26: Train loss 2.1294, Val loss 2.0745\n",
      "Epoch 27: Train loss 2.1293, Val loss 2.0760\n",
      "Epoch 28: Train loss 2.1289, Val loss 2.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 12:07:36,156] Trial 14 finished with value: 2.0745204591968585 and parameters: {'batch_size': 32, 'lr': 0.00010999744095048953, 'hidden_dim': 384, 'weight_decay': 5.383174846292139e-06}. Best is trial 13 with value: 2.05181546670681.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1285, Val loss 2.0749\n",
      "Best hyperparameters: {'batch_size': 32, 'lr': 0.0005587614855267997, 'hidden_dim': 512, 'weight_decay': 6.056437420027853e-06}\n",
      "Search completed in 377.77 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda trial: objective(trial, device, embeddings_path=\"sae_data/layer6_embeddings.npy\"), n_trials=15)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Search completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f23c70f3-d02d-4323-b01f-639eb9886fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    " \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ce61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
