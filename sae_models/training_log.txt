— Layer 1 —
  Found 23 shard(s); dim=896
  -> shard 1/23: (250007, 896) (sae_data/layer1_part0.npy)
[sae_models/sae_layer1] Ep 0001: train 0.299101 | val 0.206544
[sae_models/sae_layer1] Early stopping at epoch 45. Best val=0.157151
  -> shard 2/23: (250018, 896) (sae_data/layer1_part1.npy)
[sae_models/sae_layer1] Ep 0001: train 0.155318 | val 0.148650
[sae_models/sae_layer1] Ep 0016: train 0.136514 | val 0.144714
[sae_models/sae_layer1] Ep 0032: train 0.135074 | val 0.144083
[sae_models/sae_layer1] Ep 0048: train 0.134366 | val 0.143388
[sae_models/sae_layer1] Early stopping at epoch 62. Best val=0.143310
  -> shard 3/23: (250021, 896) (sae_data/layer1_part10.npy)
[sae_models/sae_layer1] Ep 0001: train 0.175905 | val 0.171946
[sae_models/sae_layer1] Ep 0016: train 0.155077 | val 0.165349
[sae_models/sae_layer1] Ep 0032: train 0.153522 | val 0.165021
[sae_models/sae_layer1] Ep 0048: train 0.152762 | val 0.164787
[sae_models/sae_layer1] Early stopping at epoch 52. Best val=0.164619
  -> shard 4/23: (250030, 896) (sae_data/layer1_part11.npy)
[sae_models/sae_layer1] Ep 0001: train 0.164892 | val 0.161000
[sae_models/sae_layer1] Ep 0016: train 0.147132 | val 0.156538
[sae_models/sae_layer1] Ep 0032: train 0.145635 | val 0.155683
[sae_models/sae_layer1] Ep 0048: train 0.144982 | val 0.155618
[sae_models/sae_layer1] Early stopping at epoch 62. Best val=0.154861
  -> shard 5/23: (250005, 896) (sae_data/layer1_part12.npy)
[sae_models/sae_layer1] Ep 0001: train 0.151162 | val 0.146978
[sae_models/sae_layer1] Ep 0016: train 0.136319 | val 0.142771
[sae_models/sae_layer1] Ep 0032: train 0.135189 | val 0.142306
[sae_models/sae_layer1] Early stopping at epoch 32. Best val=0.142238
  -> shard 6/23: (250006, 896) (sae_data/layer1_part13.npy)
[sae_models/sae_layer1] Ep 0001: train 0.170370 | val 0.164175
[sae_models/sae_layer1] Ep 0016: train 0.151677 | val 0.158914
[sae_models/sae_layer1] Ep 0032: train 0.150395 | val 0.158073
[sae_models/sae_layer1] Ep 0048: train 0.149655 | val 0.157710
[sae_models/sae_layer1] Early stopping at epoch 59. Best val=0.157531
  -> shard 7/23: (250015, 896) (sae_data/layer1_part14.npy)
[sae_models/sae_layer1] Ep 0001: train 0.164968 | val 0.160069
[sae_models/sae_layer1] Ep 0016: train 0.151176 | val 0.155132
[sae_models/sae_layer1] Ep 0032: train 0.150231 | val 0.155322
[sae_models/sae_layer1] Early stopping at epoch 43. Best val=0.154568
  -> shard 8/23: (250010, 896) (sae_data/layer1_part15.npy)
[sae_models/sae_layer1] Ep 0001: train 0.132019 | val 0.127619
[sae_models/sae_layer1] Ep 0016: train 0.122638 | val 0.124664
[sae_models/sae_layer1] Early stopping at epoch 23. Best val=0.124541
  -> shard 9/23: (250017, 896) (sae_data/layer1_part16.npy)
[sae_models/sae_layer1] Ep 0001: train 0.164123 | val 0.159884
[sae_models/sae_layer1] Ep 0016: train 0.149595 | val 0.154003
[sae_models/sae_layer1] Early stopping at epoch 18. Best val=0.153909
  -> shard 10/23: (250009, 896) (sae_data/layer1_part17.npy)
[sae_models/sae_layer1] Ep 0001: train 0.154985 | val 0.149758
[sae_models/sae_layer1] Ep 0016: train 0.143905 | val 0.146003
[sae_models/sae_layer1] Early stopping at epoch 29. Best val=0.145632
  -> shard 11/23: (250047, 896) (sae_data/layer1_part18.npy)
[sae_models/sae_layer1] Ep 0001: train 0.152966 | val 0.148921
[sae_models/sae_layer1] Ep 0016: train 0.140855 | val 0.145050
[sae_models/sae_layer1] Ep 0032: train 0.140128 | val 0.144731
[sae_models/sae_layer1] Early stopping at epoch 42. Best val=0.144731
  -> shard 12/23: (250061, 896) (sae_data/layer1_part19.npy)
[sae_models/sae_layer1] Ep 0001: train 0.153706 | val 0.146758
[sae_models/sae_layer1] Ep 0016: train 0.137322 | val 0.140621
[sae_models/sae_layer1] Early stopping at epoch 26. Best val=0.140621
  -> shard 13/23: (250056, 896) (sae_data/layer1_part2.npy)
[sae_models/sae_layer1] Ep 0001: train 0.168010 | val 0.161193
[sae_models/sae_layer1] Ep 0016: train 0.151260 | val 0.155933
[sae_models/sae_layer1] Early stopping at epoch 30. Best val=0.155551
  -> shard 14/23: (250013, 896) (sae_data/layer1_part20.npy)
[sae_models/sae_layer1] Ep 0001: train 0.164457 | val 0.156269
[sae_models/sae_layer1] Ep 0016: train 0.147696 | val 0.150013
[sae_models/sae_layer1] Ep 0032: train 0.146708 | val 0.149955
[sae_models/sae_layer1] Ep 0048: train 0.146160 | val 0.149490
[sae_models/sae_layer1] Ep 0064: train 0.145404 | val 0.149040
[sae_models/sae_layer1] Ep 0080: train 0.144988 | val 0.148722
[sae_models/sae_layer1] Ep 0096: train 0.144784 | val 0.148582
[sae_models/sae_layer1] Early stopping at epoch 96. Best val=0.148149
  -> shard 15/23: (250011, 896) (sae_data/layer1_part21.npy)
[sae_models/sae_layer1] Ep 0001: train 0.160630 | val 0.157873
[sae_models/sae_layer1] Ep 0016: train 0.148234 | val 0.153244
[sae_models/sae_layer1] Ep 0032: train 0.147460 | val 0.152661
[sae_models/sae_layer1] Early stopping at epoch 38. Best val=0.152248
  -> shard 16/23: (133717, 896) (sae_data/layer1_part22.npy)
[sae_models/sae_layer1] Ep 0001: train 0.163771 | val 0.164983
[sae_models/sae_layer1] Ep 0016: train 0.150322 | val 0.160564
[sae_models/sae_layer1] Ep 0032: train 0.149583 | val 0.160448
[sae_models/sae_layer1] Ep 0048: train 0.149008 | val 0.160055
[sae_models/sae_layer1] Early stopping at epoch 54. Best val=0.159635
  -> shard 17/23: (250032, 896) (sae_data/layer1_part3.npy)
[sae_models/sae_layer1] Ep 0001: train 0.169408 | val 0.162394
[sae_models/sae_layer1] Ep 0016: train 0.153310 | val 0.157026
[sae_models/sae_layer1] Ep 0032: train 0.152746 | val 0.156794
[sae_models/sae_layer1] Ep 0048: train 0.151989 | val 0.156523
[sae_models/sae_layer1] Early stopping at epoch 63. Best val=0.156012
  -> shard 18/23: (250013, 896) (sae_data/layer1_part4.npy)
[sae_models/sae_layer1] Ep 0001: train 0.167541 | val 0.162591
[sae_models/sae_layer1] Ep 0016: train 0.156234 | val 0.158490
[sae_models/sae_layer1] Early stopping at epoch 26. Best val=0.158490
  -> shard 19/23: (250017, 896) (sae_data/layer1_part5.npy)
[sae_models/sae_layer1] Ep 0001: train 0.177894 | val 0.169771
[sae_models/sae_layer1] Ep 0016: train 0.161444 | val 0.164247
[sae_models/sae_layer1] Ep 0032: train 0.160561 | val 0.163498
[sae_models/sae_layer1] Ep 0048: train 0.160011 | val 0.162942
[sae_models/sae_layer1] Early stopping at epoch 61. Best val=0.162510
  -> shard 20/23: (250002, 896) (sae_data/layer1_part6.npy)
[sae_models/sae_layer1] Ep 0001: train 0.160442 | val 0.151624
[sae_models/sae_layer1] Ep 0016: train 0.144379 | val 0.146177
[sae_models/sae_layer1] Ep 0032: train 0.143648 | val 0.145543
[sae_models/sae_layer1] Early stopping at epoch 33. Best val=0.145312
  -> shard 21/23: (250034, 896) (sae_data/layer1_part7.npy)
[sae_models/sae_layer1] Ep 0001: train 0.143502 | val 0.142015
[sae_models/sae_layer1] Ep 0016: train 0.132493 | val 0.138866
[sae_models/sae_layer1] Ep 0032: train 0.131864 | val 0.137752
[sae_models/sae_layer1] Ep 0048: train 0.131374 | val 0.137732
[sae_models/sae_layer1] Early stopping at epoch 52. Best val=0.137134
  -> shard 22/23: (250023, 896) (sae_data/layer1_part8.npy)
[sae_models/sae_layer1] Ep 0001: train 0.149112 | val 0.144492
[sae_models/sae_layer1] Ep 0016: train 0.137429 | val 0.139292
[sae_models/sae_layer1] Early stopping at epoch 27. Best val=0.138940
  -> shard 23/23: (250029, 896) (sae_data/layer1_part9.npy)
[sae_models/sae_layer1] Ep 0001: train 0.203356 | val 0.191788
[sae_models/sae_layer1] Ep 0016: train 0.186909 | val 0.186622
[sae_models/sae_layer1] Ep 0032: train 0.185894 | val 0.186297
[sae_models/sae_layer1] Early stopping at epoch 32. Best val=0.186157
  Saved model & metadata for layer 1

— Layer 2 —
  Found 23 shard(s); dim=896
  -> shard 1/23: (250007, 896) (sae_data/layer2_part0.npy)
[sae_models/sae_layer2] Ep 0001: train 0.437818 | val 0.313137
[sae_models/sae_layer2] Early stopping at epoch 47. Best val=0.246997
  -> shard 2/23: (250018, 896) (sae_data/layer2_part1.npy)
[sae_models/sae_layer2] Ep 0001: train 0.246190 | val 0.240157
[sae_models/sae_layer2] Ep 0016: train 0.223241 | val 0.232817
[sae_models/sae_layer2] Ep 0032: train 0.221365 | val 0.232316
[sae_models/sae_layer2] Ep 0048: train 0.220460 | val 0.232620
[sae_models/sae_layer2] Early stopping at epoch 55. Best val=0.232043
  -> shard 3/23: (250021, 896) (sae_data/layer2_part10.npy)
[sae_models/sae_layer2] Ep 0001: train 0.272144 | val 0.268294
[sae_models/sae_layer2] Ep 0016: train 0.248388 | val 0.260510
[sae_models/sae_layer2] Ep 0032: train 0.246696 | val 0.259832
[sae_models/sae_layer2] Early stopping at epoch 43. Best val=0.259622
  -> shard 4/23: (250030, 896) (sae_data/layer2_part11.npy)
[sae_models/sae_layer2] Ep 0001: train 0.259128 | val 0.254921
[sae_models/sae_layer2] Ep 0016: train 0.238019 | val 0.248373
[sae_models/sae_layer2] Ep 0032: train 0.236588 | val 0.248425
[sae_models/sae_layer2] Ep 0048: train 0.235879 | val 0.248488
[sae_models/sae_layer2] Early stopping at epoch 48. Best val=0.247794
  -> shard 5/23: (250005, 896) (sae_data/layer2_part12.npy)
[sae_models/sae_layer2] Ep 0001: train 0.237933 | val 0.232462
[sae_models/sae_layer2] Ep 0016: train 0.222279 | val 0.228439
[sae_models/sae_layer2] Ep 0032: train 0.221133 | val 0.227785
[sae_models/sae_layer2] Early stopping at epoch 42. Best val=0.227785
  -> shard 6/23: (250006, 896) (sae_data/layer2_part13.npy)
[sae_models/sae_layer2] Ep 0001: train 0.261856 | val 0.256086
[sae_models/sae_layer2] Ep 0016: train 0.242058 | val 0.250942
[sae_models/sae_layer2] Ep 0032: train 0.240499 | val 0.249980
[sae_models/sae_layer2] Early stopping at epoch 36. Best val=0.249835
  -> shard 7/23: (250015, 896) (sae_data/layer2_part14.npy)
[sae_models/sae_layer2] Ep 0001: train 0.257268 | val 0.251498
[sae_models/sae_layer2] Ep 0016: train 0.242421 | val 0.245858
[sae_models/sae_layer2] Ep 0032: train 0.241450 | val 0.245977
[sae_models/sae_layer2] Early stopping at epoch 47. Best val=0.245457
  -> shard 8/23: (250010, 896) (sae_data/layer2_part15.npy)
[sae_models/sae_layer2] Ep 0001: train 0.215297 | val 0.210188
[sae_models/sae_layer2] Ep 0016: train 0.205402 | val 0.207424
[sae_models/sae_layer2] Early stopping at epoch 21. Best val=0.207326
  -> shard 9/23: (250017, 896) (sae_data/layer2_part16.npy)
[sae_models/sae_layer2] Ep 0001: train 0.252579 | val 0.248505
[sae_models/sae_layer2] Ep 0016: train 0.237446 | val 0.243894
[sae_models/sae_layer2] Ep 0032: train 0.236509 | val 0.242600
[sae_models/sae_layer2] Ep 0048: train 0.236086 | val 0.242515
[sae_models/sae_layer2] Early stopping at epoch 53. Best val=0.242067
  -> shard 10/23: (250009, 896) (sae_data/layer2_part17.npy)
[sae_models/sae_layer2] Ep 0001: train 0.243549 | val 0.237084
[sae_models/sae_layer2] Ep 0016: train 0.231908 | val 0.232910
[sae_models/sae_layer2] Early stopping at epoch 31. Best val=0.232733
  -> shard 11/23: (250047, 896) (sae_data/layer2_part18.npy)
[sae_models/sae_layer2] Ep 0001: train 0.240676 | val 0.236401
[sae_models/sae_layer2] Ep 0016: train 0.227474 | val 0.232500
[sae_models/sae_layer2] Early stopping at epoch 21. Best val=0.231727
  -> shard 12/23: (250061, 896) (sae_data/layer2_part19.npy)
[sae_models/sae_layer2] Ep 0001: train 0.241136 | val 0.234859
[sae_models/sae_layer2] Ep 0016: train 0.224314 | val 0.227626
[sae_models/sae_layer2] Early stopping at epoch 26. Best val=0.227626
  -> shard 13/23: (250056, 896) (sae_data/layer2_part2.npy)
[sae_models/sae_layer2] Ep 0001: train 0.256925 | val 0.249562
[sae_models/sae_layer2] Ep 0016: train 0.239458 | val 0.244285
[sae_models/sae_layer2] Ep 0032: train 0.238328 | val 0.243754
[sae_models/sae_layer2] Early stopping at epoch 43. Best val=0.243165
  -> shard 14/23: (250013, 896) (sae_data/layer2_part20.npy)
[sae_models/sae_layer2] Ep 0001: train 0.253544 | val 0.241280
[sae_models/sae_layer2] Ep 0016: train 0.235786 | val 0.236130
[sae_models/sae_layer2] Ep 0032: train 0.234805 | val 0.235197
[sae_models/sae_layer2] Early stopping at epoch 34. Best val=0.234512
  -> shard 15/23: (250011, 896) (sae_data/layer2_part21.npy)
[sae_models/sae_layer2] Ep 0001: train 0.249122 | val 0.245771
[sae_models/sae_layer2] Ep 0016: train 0.235607 | val 0.240910
[sae_models/sae_layer2] Early stopping at epoch 31. Best val=0.240277
  -> shard 16/23: (133717, 896) (sae_data/layer2_part22.npy)
[sae_models/sae_layer2] Ep 0001: train 0.253840 | val 0.253878
[sae_models/sae_layer2] Ep 0016: train 0.238421 | val 0.250447
[sae_models/sae_layer2] Early stopping at epoch 28. Best val=0.249924
  -> shard 17/23: (250032, 896) (sae_data/layer2_part3.npy)
[sae_models/sae_layer2] Ep 0001: train 0.257145 | val 0.250113
[sae_models/sae_layer2] Ep 0016: train 0.240886 | val 0.245651
[sae_models/sae_layer2] Early stopping at epoch 23. Best val=0.244711
  -> shard 18/23: (250013, 896) (sae_data/layer2_part4.npy)
[sae_models/sae_layer2] Ep 0001: train 0.256226 | val 0.254627
[sae_models/sae_layer2] Ep 0016: train 0.245524 | val 0.252158
[sae_models/sae_layer2] Ep 0032: train 0.244726 | val 0.251373
[sae_models/sae_layer2] Early stopping at epoch 44. Best val=0.250627
  -> shard 19/23: (250017, 896) (sae_data/layer2_part5.npy)
[sae_models/sae_layer2] Ep 0001: train 0.270017 | val 0.258329
[sae_models/sae_layer2] Ep 0016: train 0.253100 | val 0.252415
[sae_models/sae_layer2] Ep 0032: train 0.252120 | val 0.251399
[sae_models/sae_layer2] Ep 0048: train 0.251664 | val 0.251740
[sae_models/sae_layer2] Early stopping at epoch 48. Best val=0.251139
  -> shard 20/23: (250002, 896) (sae_data/layer2_part6.npy)
[sae_models/sae_layer2] Ep 0001: train 0.247917 | val 0.240042
[sae_models/sae_layer2] Ep 0016: train 0.231644 | val 0.235650
[sae_models/sae_layer2] Early stopping at epoch 19. Best val=0.235046
  -> shard 21/23: (250034, 896) (sae_data/layer2_part7.npy)
[sae_models/sae_layer2] Ep 0001: train 0.230321 | val 0.228691
[sae_models/sae_layer2] Ep 0016: train 0.218353 | val 0.226423
[sae_models/sae_layer2] Early stopping at epoch 20. Best val=0.225367
  -> shard 22/23: (250023, 896) (sae_data/layer2_part8.npy)
[sae_models/sae_layer2] Ep 0001: train 0.240108 | val 0.233972
[sae_models/sae_layer2] Ep 0016: train 0.226255 | val 0.229487
[sae_models/sae_layer2] Ep 0032: train 0.225535 | val 0.228458
[sae_models/sae_layer2] Ep 0048: train 0.224779 | val 0.227904
[sae_models/sae_layer2] Early stopping at epoch 56. Best val=0.227534
  -> shard 23/23: (250029, 896) (sae_data/layer2_part9.npy)
[sae_models/sae_layer2] Ep 0001: train 0.301016 | val 0.289115
[sae_models/sae_layer2] Ep 0016: train 0.282575 | val 0.284101
[sae_models/sae_layer2] Early stopping at epoch 21. Best val=0.283670
  Saved model & metadata for layer 2

— Layer 3 —
  Found 23 shard(s); dim=896
  -> shard 1/23: (250007, 896) (sae_data/layer3_part0.npy)
[sae_models/sae_layer3] Ep 0001: train 0.622705 | val 0.454424
[sae_models/sae_layer3] Ep 0050: train 0.348774 | val 0.367186
[sae_models/sae_layer3] Early stopping at epoch 50. Best val=0.366826
  -> shard 2/23: (250018, 896) (sae_data/layer3_part1.npy)
[sae_models/sae_layer3] Ep 0001: train 0.366136 | val 0.359791
[sae_models/sae_layer3] Ep 0016: train 0.338467 | val 0.351082
[sae_models/sae_layer3] Ep 0032: train 0.336363 | val 0.350630
[sae_models/sae_layer3] Early stopping at epoch 43. Best val=0.350015
  -> shard 3/23: (250021, 896) (sae_data/layer3_part10.npy)
[sae_models/sae_layer3] Ep 0001: train 0.397846 | val 0.395151
[sae_models/sae_layer3] Ep 0016: train 0.369890 | val 0.384737
[sae_models/sae_layer3] Ep 0032: train 0.367966 | val 0.384066
[sae_models/sae_layer3] Early stopping at epoch 32. Best val=0.384009
  -> shard 4/23: (250030, 896) (sae_data/layer3_part11.npy)
[sae_models/sae_layer3] Ep 0001: train 0.379063 | val 0.375011
[sae_models/sae_layer3] Ep 0016: train 0.354721 | val 0.367578
[sae_models/sae_layer3] Ep 0032: train 0.352874 | val 0.367209
[sae_models/sae_layer3] Early stopping at epoch 34. Best val=0.366485
  -> shard 5/23: (250005, 896) (sae_data/layer3_part12.npy)
[sae_models/sae_layer3] Ep 0001: train 0.350036 | val 0.343230
[sae_models/sae_layer3] Ep 0016: train 0.330283 | val 0.338367
[sae_models/sae_layer3] Early stopping at epoch 25. Best val=0.338160
  -> shard 6/23: (250006, 896) (sae_data/layer3_part13.npy)
[sae_models/sae_layer3] Ep 0001: train 0.381524 | val 0.372724
[sae_models/sae_layer3] Ep 0016: train 0.356913 | val 0.364590
[sae_models/sae_layer3] Early stopping at epoch 26. Best val=0.364590
  -> shard 7/23: (250015, 896) (sae_data/layer3_part14.npy)
[sae_models/sae_layer3] Ep 0001: train 0.376446 | val 0.367796
[sae_models/sae_layer3] Ep 0016: train 0.357565 | val 0.363295
[sae_models/sae_layer3] Ep 0032: train 0.356264 | val 0.361834
[sae_models/sae_layer3] Early stopping at epoch 42. Best val=0.361834
  -> shard 8/23: (250010, 896) (sae_data/layer3_part15.npy)
[sae_models/sae_layer3] Ep 0001: train 0.320384 | val 0.311943
[sae_models/sae_layer3] Ep 0016: train 0.305231 | val 0.307733
[sae_models/sae_layer3] Early stopping at epoch 21. Best val=0.306995
  -> shard 9/23: (250017, 896) (sae_data/layer3_part16.npy)
[sae_models/sae_layer3] Ep 0001: train 0.364196 | val 0.356961
[sae_models/sae_layer3] Ep 0016: train 0.343711 | val 0.349931
[sae_models/sae_layer3] Early stopping at epoch 27. Best val=0.349687
  -> shard 10/23: (250009, 896) (sae_data/layer3_part17.npy)
[sae_models/sae_layer3] Ep 0001: train 0.354721 | val 0.347078
[sae_models/sae_layer3] Ep 0016: train 0.340088 | val 0.343507
[sae_models/sae_layer3] Early stopping at epoch 23. Best val=0.342912
  -> shard 11/23: (250047, 896) (sae_data/layer3_part18.npy)
[sae_models/sae_layer3] Ep 0001: train 0.349215 | val 0.345679
[sae_models/sae_layer3] Ep 0016: train 0.331719 | val 0.340915
[sae_models/sae_layer3] Early stopping at epoch 22. Best val=0.340125
  -> shard 12/23: (250061, 896) (sae_data/layer3_part19.npy)
[sae_models/sae_layer3] Ep 0001: train 0.351326 | val 0.341988
[sae_models/sae_layer3] Ep 0016: train 0.330742 | val 0.335218
[sae_models/sae_layer3] Early stopping at epoch 30. Best val=0.334587
  -> shard 13/23: (250056, 896) (sae_data/layer3_part2.npy)
[sae_models/sae_layer3] Ep 0001: train 0.374698 | val 0.364658
[sae_models/sae_layer3] Ep 0016: train 0.352048 | val 0.359303
[sae_models/sae_layer3] Early stopping at epoch 25. Best val=0.358415
  -> shard 14/23: (250013, 896) (sae_data/layer3_part20.npy)
[sae_models/sae_layer3] Ep 0001: train 0.363223 | val 0.348029
[sae_models/sae_layer3] Ep 0016: train 0.341356 | val 0.341643
[sae_models/sae_layer3] Early stopping at epoch 31. Best val=0.340833
  -> shard 15/23: (250011, 896) (sae_data/layer3_part21.npy)
[sae_models/sae_layer3] Ep 0001: train 0.357516 | val 0.351087
[sae_models/sae_layer3] Ep 0016: train 0.340950 | val 0.346492
[sae_models/sae_layer3] Ep 0032: train 0.340244 | val 0.346097
[sae_models/sae_layer3] Early stopping at epoch 35. Best val=0.345133
  -> shard 16/23: (133717, 896) (sae_data/layer3_part22.npy)
[sae_models/sae_layer3] Ep 0001: train 0.363592 | val 0.361486
[sae_models/sae_layer3] Ep 0016: train 0.344259 | val 0.360436
[sae_models/sae_layer3] Early stopping at epoch 17. Best val=0.358151
  -> shard 17/23: (250032, 896) (sae_data/layer3_part3.npy)
[sae_models/sae_layer3] Ep 0001: train 0.369284 | val 0.361413
[sae_models/sae_layer3] Ep 0016: train 0.349654 | val 0.355968
[sae_models/sae_layer3] Early stopping at epoch 23. Best val=0.355014
  -> shard 18/23: (250013, 896) (sae_data/layer3_part4.npy)
[sae_models/sae_layer3] Ep 0001: train 0.371921 | val 0.368390
[sae_models/sae_layer3] Ep 0016: train 0.357636 | val 0.363885
[sae_models/sae_layer3] Early stopping at epoch 26. Best val=0.363885
  -> shard 19/23: (250017, 896) (sae_data/layer3_part5.npy)
[sae_models/sae_layer3] Ep 0001: train 0.384814 | val 0.372976
[sae_models/sae_layer3] Ep 0016: train 0.363330 | val 0.364575
[sae_models/sae_layer3] Early stopping at epoch 24. Best val=0.363755
  -> shard 20/23: (250002, 896) (sae_data/layer3_part6.npy)
[sae_models/sae_layer3] Ep 0001: train 0.358339 | val 0.349501
[sae_models/sae_layer3] Ep 0016: train 0.340554 | val 0.344871
[sae_models/sae_layer3] Early stopping at epoch 31. Best val=0.343702
  -> shard 21/23: (250034, 896) (sae_data/layer3_part7.npy)
[sae_models/sae_layer3] Ep 0001: train 0.338134 | val 0.337426
[sae_models/sae_layer3] Ep 0016: train 0.323514 | val 0.334718
[sae_models/sae_layer3] Early stopping at epoch 23. Best val=0.333449
  -> shard 22/23: (250023, 896) (sae_data/layer3_part8.npy)
[sae_models/sae_layer3] Ep 0001: train 0.351098 | val 0.345014
[sae_models/sae_layer3] Ep 0016: train 0.333550 | val 0.338921
[sae_models/sae_layer3] Ep 0032: train 0.332951 | val 0.339050
[sae_models/sae_layer3] Ep 0048: train 0.332249 | val 0.338454
[sae_models/sae_layer3] Early stopping at epoch 55. Best val=0.337400
  -> shard 23/23: (250029, 896) (sae_data/layer3_part9.npy)
[sae_models/sae_layer3] Ep 0001: train 0.417738 | val 0.405954
[sae_models/sae_layer3] Ep 0016: train 0.397031 | val 0.397747
[sae_models/sae_layer3] Ep 0032: train 0.396094 | val 0.398341
[sae_models/sae_layer3] Early stopping at epoch 45. Best val=0.396607
  Saved model & metadata for layer 3

— Layer 4 —
  Found 23 shard(s); dim=896
  -> shard 1/23: (250007, 896) (sae_data/layer4_part0.npy)
[sae_models/sae_layer4] Ep 0001: train 0.904838 | val 0.661042
[sae_models/sae_layer4] Early stopping at epoch 48. Best val=0.545307
  -> shard 2/23: (250018, 896) (sae_data/layer4_part1.npy)
[sae_models/sae_layer4] Ep 0001: train 0.543913 | val 0.535980
[sae_models/sae_layer4] Ep 0016: train 0.507454 | val 0.522869
[sae_models/sae_layer4] Ep 0032: train 0.505364 | val 0.521967
[sae_models/sae_layer4] Early stopping at epoch 35. Best val=0.521453
  -> shard 3/23: (250021, 896) (sae_data/layer4_part10.npy)
[sae_models/sae_layer4] Ep 0001: train 0.583177 | val 0.579843
[sae_models/sae_layer4] Ep 0016: train 0.548835 | val 0.569242
[sae_models/sae_layer4] Ep 0032: train 0.546138 | val 0.568352
[sae_models/sae_layer4] Early stopping at epoch 39. Best val=0.566322
  -> shard 4/23: (250030, 896) (sae_data/layer4_part11.npy)
[sae_models/sae_layer4] Ep 0001: train 0.557558 | val 0.550773
[sae_models/sae_layer4] Ep 0016: train 0.528798 | val 0.542106
[sae_models/sae_layer4] Ep 0032: train 0.526502 | val 0.541919
[sae_models/sae_layer4] Early stopping at epoch 40. Best val=0.540683
  -> shard 5/23: (250005, 896) (sae_data/layer4_part12.npy)
[sae_models/sae_layer4] Ep 0001: train 0.519299 | val 0.514163
[sae_models/sae_layer4] Ep 0016: train 0.495021 | val 0.506331
[sae_models/sae_layer4] Ep 0032: train 0.493511 | val 0.504901
[sae_models/sae_layer4] Early stopping at epoch 38. Best val=0.503969
  -> shard 6/23: (250006, 896) (sae_data/layer4_part13.npy)
[sae_models/sae_layer4] Ep 0001: train 0.557234 | val 0.546504
[sae_models/sae_layer4] Ep 0016: train 0.530830 | val 0.537982
[sae_models/sae_layer4] Ep 0032: train 0.529317 | val 0.537173
[sae_models/sae_layer4] Ep 0048: train 0.528555 | val 0.537401
[sae_models/sae_layer4] Early stopping at epoch 52. Best val=0.536499
  -> shard 7/23: (250015, 896) (sae_data/layer4_part14.npy)
[sae_models/sae_layer4] Ep 0001: train 0.556233 | val 0.546260
[sae_models/sae_layer4] Ep 0016: train 0.535755 | val 0.544255
[sae_models/sae_layer4] Ep 0032: train 0.534346 | val 0.544333
[sae_models/sae_layer4] Early stopping at epoch 33. Best val=0.542688
  -> shard 8/23: (250010, 896) (sae_data/layer4_part15.npy)
[sae_models/sae_layer4] Ep 0001: train 0.483389 | val 0.469534
[sae_models/sae_layer4] Ep 0016: train 0.464501 | val 0.465917
[sae_models/sae_layer4] Early stopping at epoch 30. Best val=0.464713
  -> shard 9/23: (250017, 896) (sae_data/layer4_part16.npy)
[sae_models/sae_layer4] Ep 0001: train 0.539705 | val 0.529019
[sae_models/sae_layer4] Ep 0016: train 0.514181 | val 0.521406
[sae_models/sae_layer4] Early stopping at epoch 22. Best val=0.520098
  -> shard 10/23: (250009, 896) (sae_data/layer4_part17.npy)
[sae_models/sae_layer4] Ep 0001: train 0.529894 | val 0.522484
[sae_models/sae_layer4] Ep 0016: train 0.513323 | val 0.519593
[sae_models/sae_layer4] Early stopping at epoch 18. Best val=0.518521
  -> shard 11/23: (250047, 896) (sae_data/layer4_part18.npy)
[sae_models/sae_layer4] Ep 0001: train 0.526840 | val 0.522372
[sae_models/sae_layer4] Ep 0016: train 0.506177 | val 0.517761
[sae_models/sae_layer4] Ep 0032: train 0.505142 | val 0.516471
[sae_models/sae_layer4] Early stopping at epoch 38. Best val=0.516388
  -> shard 12/23: (250061, 896) (sae_data/layer4_part19.npy)
[sae_models/sae_layer4] Ep 0001: train 0.536930 | val 0.522167
[sae_models/sae_layer4] Ep 0016: train 0.508886 | val 0.514189
[sae_models/sae_layer4] Early stopping at epoch 26. Best val=0.514189
  -> shard 13/23: (250056, 896) (sae_data/layer4_part2.npy)
[sae_models/sae_layer4] Ep 0001: train 0.560979 | val 0.549042
[sae_models/sae_layer4] Ep 0016: train 0.532711 | val 0.541972
[sae_models/sae_layer4] Ep 0032: train 0.531521 | val 0.541530
[sae_models/sae_layer4] Ep 0048: train 0.530695 | val 0.539526
[sae_models/sae_layer4] Early stopping at epoch 48. Best val=0.539182
  -> shard 14/23: (250013, 896) (sae_data/layer4_part20.npy)
[sae_models/sae_layer4] Ep 0001: train 0.548615 | val 0.532480
[sae_models/sae_layer4] Ep 0016: train 0.520734 | val 0.524603
[sae_models/sae_layer4] Early stopping at epoch 29. Best val=0.524061
  -> shard 15/23: (250011, 896) (sae_data/layer4_part21.npy)
[sae_models/sae_layer4] Ep 0001: train 0.537811 | val 0.530111
[sae_models/sae_layer4] Ep 0016: train 0.517521 | val 0.525689
[sae_models/sae_layer4] Ep 0032: train 0.516355 | val 0.524908
[sae_models/sae_layer4] Early stopping at epoch 35. Best val=0.524308
  -> shard 16/23: (133717, 896) (sae_data/layer4_part22.npy)
[sae_models/sae_layer4] Ep 0001: train 0.549937 | val 0.545082
[sae_models/sae_layer4] Early stopping at epoch 12. Best val=0.543009
  -> shard 17/23: (250032, 896) (sae_data/layer4_part3.npy)
[sae_models/sae_layer4] Ep 0001: train 0.550333 | val 0.540926
[sae_models/sae_layer4] Ep 0016: train 0.526380 | val 0.536670
[sae_models/sae_layer4] Early stopping at epoch 21. Best val=0.535391
  -> shard 18/23: (250013, 896) (sae_data/layer4_part4.npy)
[sae_models/sae_layer4] Ep 0001: train 0.553074 | val 0.547887
[sae_models/sae_layer4] Ep 0016: train 0.534546 | val 0.546200
[sae_models/sae_layer4] Early stopping at epoch 19. Best val=0.544935
  -> shard 19/23: (250017, 896) (sae_data/layer4_part5.npy)
[sae_models/sae_layer4] Ep 0001: train 0.573571 | val 0.558161
[sae_models/sae_layer4] Ep 0016: train 0.546929 | val 0.551060
[sae_models/sae_layer4] Early stopping at epoch 19. Best val=0.550344
  -> shard 20/23: (250002, 896) (sae_data/layer4_part6.npy)
[sae_models/sae_layer4] Ep 0001: train 0.539892 | val 0.528582
[sae_models/sae_layer4] Ep 0016: train 0.517630 | val 0.524633
[sae_models/sae_layer4] Ep 0032: train 0.516247 | val 0.524094
[sae_models/sae_layer4] Early stopping at epoch 39. Best val=0.522535
  -> shard 21/23: (250034, 896) (sae_data/layer4_part7.npy)
[sae_models/sae_layer4] Ep 0001: train 0.512981 | val 0.512732
[sae_models/sae_layer4] Ep 0016: train 0.492555 | val 0.508588
[sae_models/sae_layer4] Early stopping at epoch 30. Best val=0.508318
  -> shard 22/23: (250023, 896) (sae_data/layer4_part8.npy)
[sae_models/sae_layer4] Ep 0001: train 0.532423 | val 0.527320
[sae_models/sae_layer4] Ep 0016: train 0.511306 | val 0.523660
[sae_models/sae_layer4] Early stopping at epoch 17. Best val=0.522906
  -> shard 23/23: (250029, 896) (sae_data/layer4_part9.npy)
[sae_models/sae_layer4] Ep 0001: train 0.610436 | val 0.588631
[sae_models/sae_layer4] Ep 0016: train 0.584253 | val 0.584903
[sae_models/sae_layer4] Ep 0032: train 0.583132 | val 0.584630
[sae_models/sae_layer4] Early stopping at epoch 43. Best val=0.582848
  Saved model & metadata for layer 4

— Layer 5 —
  Found 23 shard(s); dim=896
  -> shard 1/23: (250007, 896) (sae_data/layer5_part0.npy)
[sae_models/sae_layer5] Ep 0001: train 1.276221 | val 0.933615
[sae_models/sae_layer5] Ep 0050: train 0.754475 | val 0.782117
[sae_models/sae_layer5] Early stopping at epoch 54. Best val=0.781584
  -> shard 2/23: (250018, 896) (sae_data/layer5_part1.npy)
[sae_models/sae_layer5] Ep 0001: train 0.780659 | val 0.768813
[sae_models/sae_layer5] Ep 0016: train 0.735365 | val 0.753620
[sae_models/sae_layer5] Ep 0032: train 0.732683 | val 0.754273
[sae_models/sae_layer5] Early stopping at epoch 37. Best val=0.751983
  -> shard 3/23: (250021, 896) (sae_data/layer5_part10.npy)
[sae_models/sae_layer5] Ep 0001: train 0.828880 | val 0.825169
[sae_models/sae_layer5] Ep 0016: train 0.786621 | val 0.812381
[sae_models/sae_layer5] Early stopping at epoch 28. Best val=0.809215
  -> shard 4/23: (250030, 896) (sae_data/layer5_part11.npy)
[sae_models/sae_layer5] Ep 0001: train 0.797397 | val 0.785157
[sae_models/sae_layer5] Ep 0016: train 0.761728 | val 0.777012
[sae_models/sae_layer5] Early stopping at epoch 23. Best val=0.775245
  -> shard 5/23: (250005, 896) (sae_data/layer5_part12.npy)
[sae_models/sae_layer5] Ep 0001: train 0.752304 | val 0.743855
[sae_models/sae_layer5] Ep 0016: train 0.722596 | val 0.737971
[sae_models/sae_layer5] Early stopping at epoch 22. Best val=0.735461
  -> shard 6/23: (250006, 896) (sae_data/layer5_part13.npy)
[sae_models/sae_layer5] Ep 0001: train 0.797569 | val 0.785041
[sae_models/sae_layer5] Ep 0016: train 0.765842 | val 0.777823
[sae_models/sae_layer5] Early stopping at epoch 24. Best val=0.776552
  -> shard 7/23: (250015, 896) (sae_data/layer5_part14.npy)
[sae_models/sae_layer5] Ep 0001: train 0.798678 | val 0.786745
[sae_models/sae_layer5] Ep 0016: train 0.772210 | val 0.783980
[sae_models/sae_layer5] Early stopping at epoch 17. Best val=0.782220
  -> shard 8/23: (250010, 896) (sae_data/layer5_part15.npy)
[sae_models/sae_layer5] Ep 0001: train 0.707707 | val 0.695707
[sae_models/sae_layer5] Ep 0016: train 0.683672 | val 0.690750
[sae_models/sae_layer5] Early stopping at epoch 18. Best val=0.689452
  -> shard 9/23: (250017, 896) (sae_data/layer5_part16.npy)
[sae_models/sae_layer5] Ep 0001: train 0.770477 | val 0.760415
[sae_models/sae_layer5] Ep 0016: train 0.738457 | val 0.750747
[sae_models/sae_layer5] Early stopping at epoch 19. Best val=0.750119
  -> shard 10/23: (250009, 896) (sae_data/layer5_part17.npy)
[sae_models/sae_layer5] Ep 0001: train 0.765041 | val 0.753744
[sae_models/sae_layer5] Early stopping at epoch 15. Best val=0.750348
  -> shard 11/23: (250047, 896) (sae_data/layer5_part18.npy)
[sae_models/sae_layer5] Ep 0001: train 0.760792 | val 0.757616
[sae_models/sae_layer5] Ep 0016: train 0.734959 | val 0.751216
[sae_models/sae_layer5] Early stopping at epoch 29. Best val=0.749256
  -> shard 12/23: (250061, 896) (sae_data/layer5_part19.npy)
[sae_models/sae_layer5] Ep 0001: train 0.772610 | val 0.753668
[sae_models/sae_layer5] Ep 0016: train 0.740311 | val 0.743464
[sae_models/sae_layer5] Early stopping at epoch 28. Best val=0.742051
  -> shard 13/23: (250056, 896) (sae_data/layer5_part2.npy)
[sae_models/sae_layer5] Ep 0001: train 0.800809 | val 0.786250
[sae_models/sae_layer5] Ep 0016: train 0.765787 | val 0.779189
[sae_models/sae_layer5] Early stopping at epoch 21. Best val=0.778484
  -> shard 14/23: (250013, 896) (sae_data/layer5_part20.npy)
[sae_models/sae_layer5] Ep 0001: train 0.781417 | val 0.758187
[sae_models/sae_layer5] Ep 0016: train 0.748337 | val 0.751709
[sae_models/sae_layer5] Early stopping at epoch 27. Best val=0.749879
  -> shard 15/23: (250011, 896) (sae_data/layer5_part21.npy)
[sae_models/sae_layer5] Ep 0001: train 0.772435 | val 0.759665
[sae_models/sae_layer5] Ep 0016: train 0.745505 | val 0.755437
[sae_models/sae_layer5] Early stopping at epoch 19. Best val=0.753207
  -> shard 16/23: (133717, 896) (sae_data/layer5_part22.npy)
[sae_models/sae_layer5] Ep 0001: train 0.785162 | val 0.779336
[sae_models/sae_layer5] Ep 0016: train 0.754158 | val 0.777214
[sae_models/sae_layer5] Early stopping at epoch 22. Best val=0.775350
  -> shard 17/23: (250032, 896) (sae_data/layer5_part3.npy)
[sae_models/sae_layer5] Ep 0001: train 0.789740 | val 0.778829
[sae_models/sae_layer5] Ep 0016: train 0.757584 | val 0.773298
[sae_models/sae_layer5] Early stopping at epoch 25. Best val=0.772317
  -> shard 18/23: (250013, 896) (sae_data/layer5_part4.npy)
[sae_models/sae_layer5] Ep 0001: train 0.792840 | val 0.786930
[sae_models/sae_layer5] Ep 0016: train 0.770218 | val 0.785278
[sae_models/sae_layer5] Early stopping at epoch 22. Best val=0.783936
  -> shard 19/23: (250017, 896) (sae_data/layer5_part5.npy)
[sae_models/sae_layer5] Ep 0001: train 0.812177 | val 0.786834
[sae_models/sae_layer5] Ep 0016: train 0.777309 | val 0.781637
[sae_models/sae_layer5] Early stopping at epoch 31. Best val=0.779915
  -> shard 20/23: (250002, 896) (sae_data/layer5_part6.npy)
[sae_models/sae_layer5] Ep 0001: train 0.771412 | val 0.756978
[sae_models/sae_layer5] Ep 0016: train 0.742239 | val 0.751917
[sae_models/sae_layer5] Early stopping at epoch 30. Best val=0.750742
  -> shard 21/23: (250034, 896) (sae_data/layer5_part7.npy)
[sae_models/sae_layer5] Ep 0001: train 0.737737 | val 0.735532
[sae_models/sae_layer5] Ep 0016: train 0.714069 | val 0.734848
[sae_models/sae_layer5] Early stopping at epoch 18. Best val=0.731839
  -> shard 22/23: (250023, 896) (sae_data/layer5_part8.npy)
[sae_models/sae_layer5] Ep 0001: train 0.766272 | val 0.760530
[sae_models/sae_layer5] Ep 0016: train 0.738422 | val 0.755522
[sae_models/sae_layer5] Early stopping at epoch 19. Best val=0.754072
  -> shard 23/23: (250029, 896) (sae_data/layer5_part9.npy)
[sae_models/sae_layer5] Ep 0001: train 0.856573 | val 0.833291
[sae_models/sae_layer5] Ep 0016: train 0.821806 | val 0.825987
[sae_models/sae_layer5] Early stopping at epoch 22. Best val=0.824220
  Saved model & metadata for layer 5

— Layer 6 —
  Found 23 shard(s); dim=896
  -> shard 1/23: (250007, 896) (sae_data/layer6_part0.npy)
[sae_models/sae_layer6] Ep 0001: train 1.863183 | val 1.355676
[sae_models/sae_layer6] Ep 0050: train 1.094124 | val 1.127519
[sae_models/sae_layer6] Early stopping at epoch 66. Best val=1.126537
  -> shard 2/23: (250018, 896) (sae_data/layer6_part1.npy)
[sae_models/sae_layer6] Ep 0001: train 1.144423 | val 1.126059
[sae_models/sae_layer6] Ep 0016: train 1.082977 | val 1.109032
[sae_models/sae_layer6] Ep 0032: train 1.078936 | val 1.110316
[sae_models/sae_layer6] Early stopping at epoch 34. Best val=1.107600
  -> shard 3/23: (250021, 896) (sae_data/layer6_part10.npy)
[sae_models/sae_layer6] Ep 0001: train 1.196131 | val 1.185448
[sae_models/sae_layer6] Ep 0016: train 1.140990 | val 1.170563
[sae_models/sae_layer6] Early stopping at epoch 25. Best val=1.169231
  -> shard 4/23: (250030, 896) (sae_data/layer6_part11.npy)
[sae_models/sae_layer6] Ep 0001: train 1.157646 | val 1.142208
[sae_models/sae_layer6] Ep 0016: train 1.109697 | val 1.134041
[sae_models/sae_layer6] Early stopping at epoch 17. Best val=1.131117
  -> shard 5/23: (250005, 896) (sae_data/layer6_part12.npy)
[sae_models/sae_layer6] Ep 0001: train 1.107596 | val 1.093430
[sae_models/sae_layer6] Ep 0016: train 1.065384 | val 1.083860
[sae_models/sae_layer6] Early stopping at epoch 19. Best val=1.082999
  -> shard 6/23: (250006, 896) (sae_data/layer6_part13.npy)
[sae_models/sae_layer6] Ep 0001: train 1.151537 | val 1.132003
[sae_models/sae_layer6] Ep 0016: train 1.106508 | val 1.124357
[sae_models/sae_layer6] Early stopping at epoch 19. Best val=1.124344
  -> shard 7/23: (250015, 896) (sae_data/layer6_part14.npy)
[sae_models/sae_layer6] Ep 0001: train 1.164316 | val 1.147266
[sae_models/sae_layer6] Early stopping at epoch 15. Best val=1.141917
  -> shard 8/23: (250010, 896) (sae_data/layer6_part15.npy)
[sae_models/sae_layer6] Ep 0001: train 1.050725 | val 1.031818
[sae_models/sae_layer6] Ep 0016: train 1.015296 | val 1.025847
[sae_models/sae_layer6] Early stopping at epoch 26. Best val=1.025847
  -> shard 9/23: (250017, 896) (sae_data/layer6_part16.npy)
[sae_models/sae_layer6] Ep 0001: train 1.118071 | val 1.100586
[sae_models/sae_layer6] Ep 0016: train 1.074171 | val 1.091008
[sae_models/sae_layer6] Early stopping at epoch 29. Best val=1.089518
  -> shard 10/23: (250009, 896) (sae_data/layer6_part17.npy)
[sae_models/sae_layer6] Ep 0001: train 1.123249 | val 1.105711
[sae_models/sae_layer6] Ep 0016: train 1.090899 | val 1.101141
[sae_models/sae_layer6] Early stopping at epoch 22. Best val=1.100947
  -> shard 11/23: (250047, 896) (sae_data/layer6_part18.npy)
[sae_models/sae_layer6] Ep 0001: train 1.123685 | val 1.114699
[sae_models/sae_layer6] Ep 0016: train 1.084687 | val 1.110173
[sae_models/sae_layer6] Early stopping at epoch 21. Best val=1.107484
  -> shard 12/23: (250061, 896) (sae_data/layer6_part19.npy)
[sae_models/sae_layer6] Ep 0001: train 1.138396 | val 1.111492
[sae_models/sae_layer6] Ep 0016: train 1.094029 | val 1.103109
[sae_models/sae_layer6] Early stopping at epoch 25. Best val=1.101492
  -> shard 13/23: (250056, 896) (sae_data/layer6_part2.npy)
[sae_models/sae_layer6] Ep 0001: train 1.176049 | val 1.152266
[sae_models/sae_layer6] Ep 0016: train 1.124994 | val 1.144148
[sae_models/sae_layer6] Early stopping at epoch 24. Best val=1.142331
  -> shard 14/23: (250013, 896) (sae_data/layer6_part20.npy)
[sae_models/sae_layer6] Ep 0001: train 1.154373 | val 1.125391
[sae_models/sae_layer6] Ep 0016: train 1.104389 | val 1.114897
[sae_models/sae_layer6] Ep 0032: train 1.101654 | val 1.115528
[sae_models/sae_layer6] Early stopping at epoch 32. Best val=1.113452
  -> shard 15/23: (250011, 896) (sae_data/layer6_part21.npy)
[sae_models/sae_layer6] Ep 0001: train 1.138016 | val 1.113751
[sae_models/sae_layer6] Ep 0016: train 1.098306 | val 1.110944
[sae_models/sae_layer6] Early stopping at epoch 18. Best val=1.107982
  -> shard 16/23: (133717, 896) (sae_data/layer6_part22.npy)
[sae_models/sae_layer6] Ep 0001: train 1.166206 | val 1.153038
[sae_models/sae_layer6] Early stopping at epoch 13. Best val=1.151431
  -> shard 17/23: (250032, 896) (sae_data/layer6_part3.npy)
[sae_models/sae_layer6] Ep 0001: train 1.160735 | val 1.144038
[sae_models/sae_layer6] Early stopping at epoch 15. Best val=1.138225
  -> shard 18/23: (250013, 896) (sae_data/layer6_part4.npy)
[sae_models/sae_layer6] Ep 0001: train 1.163911 | val 1.153892
[sae_models/sae_layer6] Ep 0016: train 1.130065 | val 1.150540
[sae_models/sae_layer6] Early stopping at epoch 19. Best val=1.148643
  -> shard 19/23: (250017, 896) (sae_data/layer6_part5.npy)
[sae_models/sae_layer6] Ep 0001: train 1.174042 | val 1.150870
[sae_models/sae_layer6] Ep 0016: train 1.130219 | val 1.145676
[sae_models/sae_layer6] Early stopping at epoch 27. Best val=1.139857
  -> shard 20/23: (250002, 896) (sae_data/layer6_part6.npy)
[sae_models/sae_layer6] Ep 0001: train 1.145833 | val 1.128731
[sae_models/sae_layer6] Ep 0016: train 1.105667 | val 1.124413
[sae_models/sae_layer6] Early stopping at epoch 21. Best val=1.123116
  -> shard 21/23: (250034, 896) (sae_data/layer6_part7.npy)
[sae_models/sae_layer6] Ep 0001: train 1.110083 | val 1.105526
[sae_models/sae_layer6] Early stopping at epoch 15. Best val=1.102531
  -> shard 22/23: (250023, 896) (sae_data/layer6_part8.npy)
[sae_models/sae_layer6] Ep 0001: train 1.144985 | val 1.135446
[sae_models/sae_layer6] Ep 0016: train 1.107317 | val 1.131292
[sae_models/sae_layer6] Early stopping at epoch 18. Best val=1.129121
  -> shard 23/23: (250029, 896) (sae_data/layer6_part9.npy)
[sae_models/sae_layer6] Ep 0001: train 1.237595 | val 1.211557
[sae_models/sae_layer6] Ep 0016: train 1.187131 | val 1.196662
[sae_models/sae_layer6] Early stopping at epoch 29. Best val=1.194895
  Saved model & metadata for layer 6

— Layer 7 —
  Found 23 shard(s); dim=896
  -> shard 1/23: (250007, 896) (sae_data/layer7_part0.npy)
[sae_models/sae_layer7] Ep 0001: train 2.600822 | val 1.869152
[sae_models/sae_layer7] Ep 0050: train 1.492487 | val 1.544578
[sae_models/sae_layer7] Early stopping at epoch 56. Best val=1.542507
  -> shard 2/23: (250018, 896) (sae_data/layer7_part1.npy)
[sae_models/sae_layer7] Ep 0001: train 1.583374 | val 1.553740
[sae_models/sae_layer7] Ep 0016: train 1.494938 | val 1.535353
[sae_models/sae_layer7] Early stopping at epoch 20. Best val=1.529723
  -> shard 3/23: (250021, 896) (sae_data/layer7_part10.npy)
[sae_models/sae_layer7] Ep 0001: train 1.647508 | val 1.627429
[sae_models/sae_layer7] Ep 0016: train 1.563177 | val 1.613632
[sae_models/sae_layer7] Early stopping at epoch 17. Best val=1.610170
  -> shard 4/23: (250030, 896) (sae_data/layer7_part11.npy)
[sae_models/sae_layer7] Ep 0001: train 1.593113 | val 1.563420
[sae_models/sae_layer7] Early stopping at epoch 15. Best val=1.551129
  -> shard 5/23: (250005, 896) (sae_data/layer7_part12.npy)
[sae_models/sae_layer7] Ep 0001: train 1.535299 | val 1.509926
[sae_models/sae_layer7] Ep 0016: train 1.467541 | val 1.502054
[sae_models/sae_layer7] Early stopping at epoch 18. Best val=1.498718
  -> shard 6/23: (250006, 896) (sae_data/layer7_part13.npy)
[sae_models/sae_layer7] Ep 0001: train 1.588469 | val 1.558676
[sae_models/sae_layer7] Ep 0016: train 1.516513 | val 1.546657
[sae_models/sae_layer7] Early stopping at epoch 21. Best val=1.542846
  -> shard 7/23: (250015, 896) (sae_data/layer7_part14.npy)
[sae_models/sae_layer7] Ep 0001: train 1.621443 | val 1.592816
[sae_models/sae_layer7] Ep 0016: train 1.556106 | val 1.585289
[sae_models/sae_layer7] Early stopping at epoch 28. Best val=1.582411
  -> shard 8/23: (250010, 896) (sae_data/layer7_part15.npy)
[sae_models/sae_layer7] Ep 0001: train 1.460676 | val 1.430751
[sae_models/sae_layer7] Early stopping at epoch 15. Best val=1.422405
  -> shard 9/23: (250017, 896) (sae_data/layer7_part16.npy)
[sae_models/sae_layer7] Ep 0001: train 1.527416 | val 1.501649
[sae_models/sae_layer7] Ep 0016: train 1.460501 | val 1.493904
[sae_models/sae_layer7] Early stopping at epoch 27. Best val=1.490632
  -> shard 10/23: (250009, 896) (sae_data/layer7_part17.npy)
[sae_models/sae_layer7] Ep 0001: train 1.555695 | val 1.534631
[sae_models/sae_layer7] Ep 0016: train 1.502343 | val 1.529824
[sae_models/sae_layer7] Early stopping at epoch 20. Best val=1.526150
  -> shard 11/23: (250047, 896) (sae_data/layer7_part18.npy)
[sae_models/sae_layer7] Ep 0001: train 1.557552 | val 1.545830
[sae_models/sae_layer7] Ep 0016: train 1.498681 | val 1.539100
[sae_models/sae_layer7] Early stopping at epoch 16. Best val=1.537514
  -> shard 12/23: (250061, 896) (sae_data/layer7_part19.npy)
[sae_models/sae_layer7] Ep 0001: train 1.579244 | val 1.540707
[sae_models/sae_layer7] Ep 0016: train 1.507000 | val 1.524803
[sae_models/sae_layer7] Early stopping at epoch 26. Best val=1.524803
  -> shard 13/23: (250056, 896) (sae_data/layer7_part2.npy)
[sae_models/sae_layer7] Ep 0001: train 1.618499 | val 1.584433
[sae_models/sae_layer7] Ep 0016: train 1.537838 | val 1.570565
[sae_models/sae_layer7] Ep 0032: train 1.533441 | val 1.570106
[sae_models/sae_layer7] Early stopping at epoch 39. Best val=1.567433
  -> shard 14/23: (250013, 896) (sae_data/layer7_part20.npy)
[sae_models/sae_layer7] Ep 0001: train 1.603596 | val 1.561600
[sae_models/sae_layer7] Ep 0016: train 1.518616 | val 1.544393
[sae_models/sae_layer7] Early stopping at epoch 18. Best val=1.543492
  -> shard 15/23: (250011, 896) (sae_data/layer7_part21.npy)
[sae_models/sae_layer7] Ep 0001: train 1.568790 | val 1.541183
[sae_models/sae_layer7] Ep 0016: train 1.506178 | val 1.532878
[sae_models/sae_layer7] Early stopping at epoch 19. Best val=1.530872
  -> shard 16/23: (133717, 896) (sae_data/layer7_part22.npy)
[sae_models/sae_layer7] Ep 0001: train 1.608286 | val 1.589611
[sae_models/sae_layer7] Ep 0016: train 1.528129 | val 1.594702
[sae_models/sae_layer7] Early stopping at epoch 16. Best val=1.586775
  -> shard 17/23: (250032, 896) (sae_data/layer7_part3.npy)
[sae_models/sae_layer7] Ep 0001: train 1.610886 | val 1.582155
[sae_models/sae_layer7] Ep 0016: train 1.537353 | val 1.571863
[sae_models/sae_layer7] Early stopping at epoch 28. Best val=1.569372
  -> shard 18/23: (250013, 896) (sae_data/layer7_part4.npy)
[sae_models/sae_layer7] Ep 0001: train 1.607354 | val 1.591926
[sae_models/sae_layer7] Ep 0016: train 1.550867 | val 1.586344
[sae_models/sae_layer7] Ep 0032: train 1.547431 | val 1.587917
[sae_models/sae_layer7] Early stopping at epoch 39. Best val=1.582342
  -> shard 19/23: (250017, 896) (sae_data/layer7_part5.npy)
[sae_models/sae_layer7] Ep 0001: train 1.609365 | val 1.575534
[sae_models/sae_layer7] Ep 0016: train 1.536713 | val 1.565442
[sae_models/sae_layer7] Early stopping at epoch 22. Best val=1.562081
  -> shard 20/23: (250002, 896) (sae_data/layer7_part6.npy)
[sae_models/sae_layer7] Ep 0001: train 1.580651 | val 1.559221
[sae_models/sae_layer7] Ep 0016: train 1.513044 | val 1.546020
[sae_models/sae_layer7] Early stopping at epoch 26. Best val=1.546020
  -> shard 21/23: (250034, 896) (sae_data/layer7_part7.npy)
[sae_models/sae_layer7] Ep 0001: train 1.526170 | val 1.516876
[sae_models/sae_layer7] Early stopping at epoch 13. Best val=1.513270
  -> shard 22/23: (250023, 896) (sae_data/layer7_part8.npy)
[sae_models/sae_layer7] Ep 0001: train 1.582739 | val 1.565399
[sae_models/sae_layer7] Early stopping at epoch 13. Best val=1.557852
  -> shard 23/23: (250029, 896) (sae_data/layer7_part9.npy)
[sae_models/sae_layer7] Ep 0001: train 1.692865 | val 1.645373
[sae_models/sae_layer7] Ep 0016: train 1.617198 | val 1.631230
[sae_models/sae_layer7] Early stopping at epoch 24. Best val=1.629755
  Saved model & metadata for layer 7

— Layer 8 —
  Found 23 shard(s); dim=896
  -> shard 1/23: (250007, 896) (sae_data/layer8_part0.npy)
[sae_models/sae_layer8] Ep 0001: train 3.314873 | val 2.386342
[sae_models/sae_layer8] Ep 0050: train 1.821307 | val 1.895806
[sae_models/sae_layer8] Early stopping at epoch 72. Best val=1.892415
  -> shard 2/23: (250018, 896) (sae_data/layer8_part1.npy)
[sae_models/sae_layer8] Ep 0001: train 1.971209 | val 1.918445
[sae_models/sae_layer8] Ep 0016: train 1.827153 | val 1.888791
[sae_models/sae_layer8] Early stopping at epoch 28. Best val=1.887998
  -> shard 3/23: (250021, 896) (sae_data/layer8_part10.npy)
[sae_models/sae_layer8] Ep 0001: train 2.038542 | val 1.992213
[sae_models/sae_layer8] Ep 0016: train 1.894051 | val 1.964710
[sae_models/sae_layer8] Early stopping at epoch 28. Best val=1.962529
  -> shard 4/23: (250030, 896) (sae_data/layer8_part11.npy)
[sae_models/sae_layer8] Ep 0001: train 1.971466 | val 1.930493
[sae_models/sae_layer8] Ep 0016: train 1.846776 | val 1.909646
[sae_models/sae_layer8] Early stopping at epoch 28. Best val=1.902797
  -> shard 5/23: (250005, 896) (sae_data/layer8_part12.npy)
[sae_models/sae_layer8] Ep 0001: train 1.904929 | val 1.868896
[sae_models/sae_layer8] Ep 0016: train 1.802899 | val 1.856788
[sae_models/sae_layer8] Ep 0032: train 1.796050 | val 1.852421
[sae_models/sae_layer8] Early stopping at epoch 47. Best val=1.851982
  -> shard 6/23: (250006, 896) (sae_data/layer8_part13.npy)
[sae_models/sae_layer8] Ep 0001: train 1.963503 | val 1.917532
[sae_models/sae_layer8] Ep 0016: train 1.845392 | val 1.897981
[sae_models/sae_layer8] Early stopping at epoch 22. Best val=1.893147
  -> shard 7/23: (250015, 896) (sae_data/layer8_part14.npy)
[sae_models/sae_layer8] Ep 0001: train 2.007832 | val 1.967751
[sae_models/sae_layer8] Early stopping at epoch 14. Best val=1.954506
  -> shard 8/23: (250010, 896) (sae_data/layer8_part15.npy)
[sae_models/sae_layer8] Ep 0001: train 1.811577 | val 1.772893
[sae_models/sae_layer8] Ep 0016: train 1.716580 | val 1.754138
[sae_models/sae_layer8] Early stopping at epoch 26. Best val=1.754138
  -> shard 9/23: (250017, 896) (sae_data/layer8_part16.npy)
[sae_models/sae_layer8] Ep 0001: train 1.884104 | val 1.844855
[sae_models/sae_layer8] Ep 0016: train 1.781636 | val 1.829437
[sae_models/sae_layer8] Early stopping at epoch 16. Best val=1.825897
  -> shard 10/23: (250009, 896) (sae_data/layer8_part17.npy)
[sae_models/sae_layer8] Ep 0001: train 1.920912 | val 1.891414
[sae_models/sae_layer8] Early stopping at epoch 15. Best val=1.880766
  -> shard 11/23: (250047, 896) (sae_data/layer8_part18.npy)
[sae_models/sae_layer8] Ep 0001: train 1.934686 | val 1.906179
[sae_models/sae_layer8] Early stopping at epoch 15. Best val=1.892231
  -> shard 12/23: (250061, 896) (sae_data/layer8_part19.npy)
[sae_models/sae_layer8] Ep 0001: train 1.940940 | val 1.890800
[sae_models/sae_layer8] Ep 0016: train 1.831689 | val 1.873340
[sae_models/sae_layer8] Ep 0032: train 1.826157 | val 1.870332
[sae_models/sae_layer8] Early stopping at epoch 35. Best val=1.866427
  -> shard 13/23: (250056, 896) (sae_data/layer8_part2.npy)
[sae_models/sae_layer8] Ep 0001: train 1.983956 | val 1.931333
[sae_models/sae_layer8] Ep 0016: train 1.857735 | val 1.907498
[sae_models/sae_layer8] Early stopping at epoch 28. Best val=1.903475
  -> shard 14/23: (250013, 896) (sae_data/layer8_part20.npy)
[sae_models/sae_layer8] Ep 0001: train 1.985106 | val 1.922034
[sae_models/sae_layer8] Ep 0016: train 1.854698 | val 1.893152
[sae_models/sae_layer8] Ep 0032: train 1.848041 | val 1.893908
[sae_models/sae_layer8] Early stopping at epoch 38. Best val=1.890199
  -> shard 15/23: (250011, 896) (sae_data/layer8_part21.npy)
[sae_models/sae_layer8] Ep 0001: train 1.933466 | val 1.894527
[sae_models/sae_layer8] Ep 0016: train 1.833053 | val 1.876098
[sae_models/sae_layer8] Ep 0032: train 1.824847 | val 1.879207
[sae_models/sae_layer8] Early stopping at epoch 34. Best val=1.873735
  -> shard 16/23: (133717, 896) (sae_data/layer8_part22.npy)
[sae_models/sae_layer8] Ep 0001: train 1.982644 | val 1.946896
[sae_models/sae_layer8] Early stopping at epoch 13. Best val=1.934576
  -> shard 17/23: (250032, 896) (sae_data/layer8_part3.npy)
[sae_models/sae_layer8] Ep 0001: train 1.998033 | val 1.954491
[sae_models/sae_layer8] Early stopping at epoch 15. Best val=1.931408
  -> shard 18/23: (250013, 896) (sae_data/layer8_part4.npy)
[sae_models/sae_layer8] Ep 0001: train 1.963107 | val 1.938622
[sae_models/sae_layer8] Ep 0016: train 1.878734 | val 1.928022
[sae_models/sae_layer8] Early stopping at epoch 19. Best val=1.925469
  -> shard 19/23: (250017, 896) (sae_data/layer8_part5.npy)
[sae_models/sae_layer8] Ep 0001: train 1.969957 | val 1.924625
[sae_models/sae_layer8] Ep 0016: train 1.863628 | val 1.906372
[sae_models/sae_layer8] Early stopping at epoch 20. Best val=1.901006
  -> shard 20/23: (250002, 896) (sae_data/layer8_part6.npy)
[sae_models/sae_layer8] Ep 0001: train 1.928227 | val 1.897081
[sae_models/sae_layer8] Early stopping at epoch 15. Best val=1.884015
  -> shard 21/23: (250034, 896) (sae_data/layer8_part7.npy)
[sae_models/sae_layer8] Ep 0001: train 1.863251 | val 1.849601
[sae_models/sae_layer8] Ep 0016: train 1.783082 | val 1.839911
[sae_models/sae_layer8] Early stopping at epoch 27. Best val=1.836689
  -> shard 22/23: (250023, 896) (sae_data/layer8_part8.npy)
[sae_models/sae_layer8] Ep 0001: train 1.931704 | val 1.899701
[sae_models/sae_layer8] Ep 0016: train 1.839756 | val 1.886994
[sae_models/sae_layer8] Early stopping at epoch 26. Best val=1.886994
  -> shard 23/23: (250029, 896) (sae_data/layer8_part9.npy)
[sae_models/sae_layer8] Ep 0001: train 2.078721 | val 2.005394
[sae_models/sae_layer8] Ep 0016: train 1.944785 | val 1.976504
[sae_models/sae_layer8] Ep 0032: train 1.941062 | val 1.975528
[sae_models/sae_layer8] Early stopping at epoch 36. Best val=1.972415
  Saved model & metadata for layer 8