{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d339",
   "metadata": {},
   "source": [
    "### Detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d281a",
   "metadata": {},
   "source": [
    "### Set up model configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72561797-a3f0-4d84-9883-64c447482389",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "#   \"vocab_size\": 14000,    # Vocabulary size (custom tokenizer)\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.2,       # Dropout rate\n",
    "    \"qkv_bias\": True,      # Query-Key-Value bias\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383eb92",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d1c16",
   "metadata": {},
   "source": [
    "#### GPT-2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75227c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6642b10",
   "metadata": {},
   "source": [
    "#### Custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b2a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2281b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='all_books.txt',\n",
    "    model_prefix='gpt_custom_tokenizer',\n",
    "    vocab_size=GPT_CONFIG_124M[\"vocab_size\"],\n",
    "    model_type='bpe',\n",
    "    character_coverage=0.9995,\n",
    "    hard_vocab_limit=False,\n",
    "    bos_id=-1,\n",
    "    eos_id=-1,\n",
    "    user_defined_symbols=[\"<|endoftext|>\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d8fac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.load('gpt_custom_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88bc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_used_in_this_trial=\"GPT2\"\n",
    "# tokenizer_used_in_this_trial=\"CUSTOM\"\n",
    "\n",
    "def encode(full_text):\n",
    "    if tokenizer_used_in_this_trial == \"GPT2\":\n",
    "        return tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})\n",
    "    else:\n",
    "        return tokenizer.encode(full_text, out_type=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fbf11",
   "metadata": {},
   "source": [
    "### Load training and validation data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2b562de-efe1-40d2-a5ba-350b1edb7a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data.txt'\n",
    "val_file_path = 'val_text_data.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf608bf",
   "metadata": {},
   "source": [
    "### Initialize data loaders for training\n",
    "Data loaders implementation can be found in `./data_loader_v1.py`.\n",
    "\n",
    "This implementation follows the omplementation detailed in _Raschka, Sebastian. Build a Large Language Model (From Scratch). Manning Publications, 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bddf6dae-302d-4fc7-853b-2806a0c7d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f915a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 5789730\n",
      "Characters: 32372094\n",
      "Tokens: 7608098\n",
      "Unique Tokens Used: 28960\n"
     ]
    }
   ],
   "source": [
    "full_text = train_data + val_data\n",
    "\n",
    "word_count = len(full_text.split())\n",
    "char_count = len(full_text)\n",
    "\n",
    "# tiktoken tokenizer ->\n",
    "tokens = tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "# Custom tokenizer ->\n",
    "# tokens = tokenizer.encode(full_text, out_type=int)\n",
    "\n",
    "token_count = len(tokens)\n",
    "unique_token_count = len(set(tokens))\n",
    "\n",
    "print(\"Words:\", word_count)\n",
    "print(\"Characters:\", char_count)\n",
    "print(\"Tokens:\", token_count)\n",
    "print(\"Unique Tokens Used:\", unique_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f969edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean(): \n",
    "    \"\"\"\n",
    "    This is a function for GPU data claening before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    \n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.mps.empty_cache()  # Attempt to release MPS memory\n",
    "    \n",
    "    # Move tensors to CPU\n",
    "    for tensor in list(globals().values()):\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device == torch.device(\"mps\"):\n",
    "            tensor.to(\"cpu\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    del tensor\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "    print(\"Allocated Memory:\", torch.mps.current_allocated_memory() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da82d2c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f2c9bfd-5c57-4af6-98e8-5da47988d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train import train_model_simple\n",
    "import time\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\n",
    "def train(train_loader, val_loader,\n",
    "          num_epochs=10, eval_iter=5, lr=0.0002,\n",
    "          generate_sample_text=False,\n",
    "          sample_text=\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be\",\n",
    "          model_prefix=\"model_and_optimizer\"):\n",
    "\n",
    "    global train_losses, val_losses, track_tokens_seen  # Ensure these are updated globally\n",
    "\n",
    "    if device == \"mps\":\n",
    "        clean()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-08, weight_decay=0.05)\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=num_epochs, eval_iter=eval_iter,\n",
    "        start_context=sample_text, cfg=GPT_CONFIG_124M,\n",
    "        generate_sample_text=generate_sample_text,\n",
    "        model_prefix=model_prefix,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        print(50 * \"=\")\n",
    "        clean()\n",
    "    if device == \"cuda\":\n",
    "        print(50 * \"=\")\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d594966-9781-4ea2-9a68-01196f5111b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae6fc",
   "metadata": {},
   "source": [
    "### Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dda45148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.224, Val loss 10.277\n",
      "Ep 1 (Step 000010): Train loss 8.387, Val loss 8.419\n",
      "Ep 1 (Step 000020): Train loss 7.312, Val loss 7.419\n",
      "Ep 1 (Step 000030): Train loss 7.008, Val loss 7.153\n",
      "Ep 1 (Step 000040): Train loss 6.892, Val loss 7.060\n",
      "Ep 1 (Step 000050): Train loss 6.792, Val loss 6.963\n",
      "Ep 1 (Step 000060): Train loss 6.714, Val loss 6.843\n",
      "Ep 1 (Step 000070): Train loss 6.625, Val loss 6.806\n",
      "Ep 1 (Step 000080): Train loss 6.568, Val loss 6.665\n",
      "Ep 1 (Step 000090): Train loss 6.534, Val loss 6.587\n",
      "Ep 1 (Step 000100): Train loss 6.251, Val loss 6.490\n",
      "Ep 1 (Step 000110): Train loss 6.366, Val loss 6.421\n",
      "Ep 1 (Step 000120): Train loss 6.359, Val loss 6.381\n",
      "Ep 1 (Step 000130): Train loss 6.303, Val loss 6.307\n",
      "Ep 1 (Step 000140): Train loss 6.151, Val loss 6.261\n",
      "Ep 1 (Step 000150): Train loss 6.147, Val loss 6.217\n",
      "Ep 1 (Step 000160): Train loss 6.123, Val loss 6.189\n",
      "Ep 1 (Step 000170): Train loss 6.036, Val loss 6.147\n",
      "Ep 1 (Step 000180): Train loss 6.163, Val loss 6.148\n",
      "Ep 1 (Step 000190): Train loss 6.007, Val loss 6.086\n",
      "Ep 1 (Step 000200): Train loss 6.082, Val loss 6.079\n",
      "Ep 1 (Step 000210): Train loss 5.993, Val loss 6.083\n",
      "Ep 1 (Step 000220): Train loss 6.025, Val loss 6.054\n",
      "Ep 1 (Step 000230): Train loss 5.883, Val loss 6.050\n",
      "Ep 1 (Step 000240): Train loss 5.883, Val loss 6.037\n",
      "Ep 1 (Step 000250): Train loss 5.933, Val loss 6.008\n",
      "Ep 1 (Step 000260): Train loss 5.961, Val loss 5.978\n",
      "Ep 1 (Step 000270): Train loss 5.946, Val loss 5.973\n",
      "Ep 1 (Step 000280): Train loss 5.885, Val loss 5.948\n",
      "Ep 1 (Step 000290): Train loss 5.985, Val loss 5.933\n",
      "Ep 1 (Step 000300): Train loss 5.977, Val loss 5.935\n",
      "Ep 1 (Step 000310): Train loss 5.839, Val loss 5.910\n",
      "Ep 1 (Step 000320): Train loss 5.787, Val loss 5.906\n",
      "Ep 1 (Step 000330): Train loss 5.774, Val loss 5.917\n",
      "Ep 1 (Step 000340): Train loss 5.855, Val loss 5.889\n",
      "Ep 1 (Step 000350): Train loss 5.723, Val loss 5.848\n",
      "Ep 1 (Step 000360): Train loss 5.751, Val loss 5.857\n",
      "Ep 1 (Step 000370): Train loss 5.922, Val loss 5.813\n",
      "Ep 1 (Step 000380): Train loss 5.873, Val loss 5.849\n",
      "Ep 1 (Step 000390): Train loss 5.864, Val loss 5.831\n",
      "Ep 1 (Step 000400): Train loss 5.854, Val loss 5.821\n",
      "Ep 1 (Step 000410): Train loss 5.964, Val loss 5.817\n",
      "Ep 1 (Step 000420): Train loss 5.684, Val loss 5.808\n",
      "Ep 1 (Step 000430): Train loss 5.895, Val loss 5.792\n",
      "Ep 1 (Step 000440): Train loss 5.856, Val loss 5.791\n",
      "Ep 1 (Step 000450): Train loss 5.712, Val loss 5.813\n",
      "Ep 1 (Step 000460): Train loss 5.695, Val loss 5.781\n",
      "Ep 1 (Step 000470): Train loss 5.748, Val loss 5.762\n",
      "Ep 1 (Step 000480): Train loss 5.710, Val loss 5.739\n",
      "Ep 1 (Step 000490): Train loss 5.748, Val loss 5.727\n",
      "Ep 1 (Step 000500): Train loss 5.792, Val loss 5.744\n",
      "Ep 1 (Step 000510): Train loss 5.669, Val loss 5.727\n",
      "Ep 1 (Step 000520): Train loss 5.605, Val loss 5.733\n",
      "Ep 1 (Step 000530): Train loss 5.608, Val loss 5.724\n",
      "Ep 1 (Step 000540): Train loss 5.643, Val loss 5.718\n",
      "Ep 1 (Step 000550): Train loss 5.765, Val loss 5.704\n",
      "Ep 1 (Step 000560): Train loss 5.617, Val loss 5.713\n",
      "Ep 1 (Step 000570): Train loss 5.648, Val loss 5.708\n",
      "Ep 1 (Step 000580): Train loss 5.639, Val loss 5.702\n",
      "Ep 1 (Step 000590): Train loss 5.494, Val loss 5.686\n",
      "Ep 1 (Step 000600): Train loss 5.621, Val loss 5.675\n",
      "Ep 1 (Step 000610): Train loss 5.508, Val loss 5.673\n",
      "Ep 1 (Step 000620): Train loss 5.530, Val loss 5.681\n",
      "Ep 1 (Step 000630): Train loss 5.515, Val loss 5.666\n",
      "Ep 1 (Step 000640): Train loss 5.532, Val loss 5.641\n",
      "Ep 1 (Step 000650): Train loss 5.601, Val loss 5.651\n",
      "Ep 1 (Step 000660): Train loss 5.547, Val loss 5.640\n",
      "Ep 1 (Step 000670): Train loss 5.470, Val loss 5.633\n",
      "Ep 1 (Step 000680): Train loss 5.604, Val loss 5.632\n",
      "Ep 1 (Step 000690): Train loss 5.618, Val loss 5.613\n",
      "Ep 1 (Step 000700): Train loss 5.627, Val loss 5.607\n",
      "Ep 1 (Step 000710): Train loss 5.581, Val loss 5.597\n",
      "Ep 1 (Step 000720): Train loss 5.475, Val loss 5.618\n",
      "Ep 1 (Step 000730): Train loss 5.511, Val loss 5.578\n",
      "Ep 1 (Step 000740): Train loss 5.457, Val loss 5.560\n",
      "Ep 1 (Step 000750): Train loss 5.561, Val loss 5.578\n",
      "Ep 1 (Step 000760): Train loss 5.516, Val loss 5.537\n",
      "Ep 1 (Step 000770): Train loss 5.559, Val loss 5.549\n",
      "Ep 1 (Step 000780): Train loss 5.441, Val loss 5.550\n",
      "Ep 1 (Step 000790): Train loss 5.597, Val loss 5.575\n",
      "Ep 1 (Step 000800): Train loss 5.601, Val loss 5.564\n",
      "Ep 1 (Step 000810): Train loss 5.531, Val loss 5.548\n",
      "Ep 1 (Step 000820): Train loss 5.467, Val loss 5.532\n",
      "Ep 1 (Step 000830): Train loss 5.511, Val loss 5.548\n",
      "Ep 1 (Step 000840): Train loss 5.476, Val loss 5.535\n",
      "Ep 1 (Step 000850): Train loss 5.488, Val loss 5.519\n",
      "Ep 1 (Step 000860): Train loss 5.414, Val loss 5.532\n",
      "Ep 1 (Step 000870): Train loss 5.620, Val loss 5.502\n",
      "Ep 1 (Step 000880): Train loss 5.369, Val loss 5.528\n",
      "Ep 1 (Step 000890): Train loss 5.443, Val loss 5.516\n",
      "Ep 1 (Step 000900): Train loss 5.440, Val loss 5.513\n",
      "Ep 1 (Step 000910): Train loss 5.544, Val loss 5.517\n",
      "Ep 1 (Step 000920): Train loss 5.613, Val loss 5.509\n",
      "Ep 1 (Step 000930): Train loss 5.434, Val loss 5.513\n",
      "Ep 1 (Step 000940): Train loss 5.434, Val loss 5.500\n",
      "Ep 1 (Step 000950): Train loss 5.531, Val loss 5.491\n",
      "Ep 1 (Step 000960): Train loss 5.366, Val loss 5.473\n",
      "Ep 1 (Step 000970): Train loss 5.490, Val loss 5.464\n",
      "Ep 1 (Step 000980): Train loss 5.305, Val loss 5.454\n",
      "Ep 1 (Step 000990): Train loss 5.372, Val loss 5.447\n",
      "Ep 1 (Step 001000): Train loss 5.423, Val loss 5.430\n",
      "Ep 1 (Step 001010): Train loss 5.491, Val loss 5.402\n",
      "Ep 1 (Step 001020): Train loss 5.324, Val loss 5.402\n",
      "Ep 1 (Step 001030): Train loss 5.540, Val loss 5.383\n",
      "Ep 1 (Step 001040): Train loss 5.384, Val loss 5.383\n",
      "Ep 1 (Step 001050): Train loss 5.536, Val loss 5.386\n",
      "Ep 1 (Step 001060): Train loss 5.295, Val loss 5.389\n",
      "Ep 1 (Step 001070): Train loss 5.342, Val loss 5.404\n",
      "Ep 1 (Step 001080): Train loss 5.504, Val loss 5.421\n",
      "Ep 1 (Step 001090): Train loss 5.410, Val loss 5.386\n",
      "Ep 1 (Step 001100): Train loss 5.308, Val loss 5.374\n",
      "Ep 1 (Step 001110): Train loss 5.328, Val loss 5.387\n",
      "Ep 1 (Step 001120): Train loss 5.455, Val loss 5.346\n",
      "Ep 1 (Step 001130): Train loss 5.360, Val loss 5.339\n",
      "Ep 1 (Step 001140): Train loss 5.285, Val loss 5.337\n",
      "Ep 1 (Step 001150): Train loss 5.326, Val loss 5.320\n",
      "Ep 1 (Step 001160): Train loss 5.437, Val loss 5.315\n",
      "Ep 1 (Step 001170): Train loss 5.284, Val loss 5.320\n",
      "Ep 1 (Step 001180): Train loss 5.320, Val loss 5.324\n",
      "Ep 1 (Step 001190): Train loss 5.402, Val loss 5.321\n",
      "Ep 1 (Step 001200): Train loss 5.238, Val loss 5.314\n",
      "Ep 1 (Step 001210): Train loss 5.225, Val loss 5.299\n",
      "Ep 1 (Step 001220): Train loss 5.312, Val loss 5.319\n",
      "Ep 1 (Step 001230): Train loss 5.219, Val loss 5.315\n",
      "Ep 1 (Step 001240): Train loss 5.316, Val loss 5.313\n",
      "Ep 1 (Step 001250): Train loss 5.284, Val loss 5.315\n",
      "Ep 1 (Step 001260): Train loss 5.262, Val loss 5.319\n",
      "Ep 1 (Step 001270): Train loss 5.400, Val loss 5.305\n",
      "Ep 1 (Step 001280): Train loss 5.246, Val loss 5.292\n",
      "Ep 1 (Step 001290): Train loss 5.227, Val loss 5.294\n",
      "Ep 1 (Step 001300): Train loss 5.334, Val loss 5.304\n",
      "Ep 1 (Step 001310): Train loss 5.299, Val loss 5.292\n",
      "Ep 1 (Step 001320): Train loss 5.330, Val loss 5.286\n",
      "Ep 1 (Step 001330): Train loss 5.216, Val loss 5.296\n",
      "Ep 1 (Step 001340): Train loss 5.254, Val loss 5.292\n",
      "Ep 1 (Step 001350): Train loss 5.149, Val loss 5.299\n",
      "Ep 1 (Step 001360): Train loss 5.250, Val loss 5.301\n",
      "Ep 1 (Step 001370): Train loss 5.188, Val loss 5.288\n",
      "Ep 1 (Step 001380): Train loss 5.366, Val loss 5.268\n",
      "Ep 1 (Step 001390): Train loss 5.277, Val loss 5.276\n",
      "Ep 1 (Step 001400): Train loss 5.095, Val loss 5.273\n",
      "Ep 1 (Step 001410): Train loss 5.363, Val loss 5.252\n",
      "Ep 1 (Step 001420): Train loss 5.315, Val loss 5.257\n",
      "Ep 1 (Step 001430): Train loss 5.355, Val loss 5.254\n",
      "Ep 1 (Step 001440): Train loss 5.274, Val loss 5.256\n",
      "Ep 1 (Step 001450): Train loss 5.226, Val loss 5.251\n",
      "Ep 1 (Step 001460): Train loss 5.331, Val loss 5.267\n",
      "Ep 1 (Step 001470): Train loss 5.292, Val loss 5.265\n",
      "Ep 1 (Step 001480): Train loss 5.317, Val loss 5.275\n",
      "Ep 1 (Step 001490): Train loss 5.258, Val loss 5.265\n",
      "Ep 1 (Step 001500): Train loss 5.269, Val loss 5.247\n",
      "Ep 1 (Step 001510): Train loss 5.359, Val loss 5.262\n",
      "Ep 1 (Step 001520): Train loss 5.342, Val loss 5.253\n",
      "Ep 1 (Step 001530): Train loss 5.235, Val loss 5.254\n",
      "Ep 1 (Step 001540): Train loss 5.192, Val loss 5.255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 001550): Train loss 5.263, Val loss 5.267\n",
      "Ep 1 (Step 001560): Train loss 5.208, Val loss 5.271\n",
      "Ep 1 (Step 001570): Train loss 5.102, Val loss 5.253\n",
      "Ep 1 (Step 001580): Train loss 5.162, Val loss 5.266\n",
      "Ep 1 (Step 001590): Train loss 5.241, Val loss 5.256\n",
      "Ep 1 (Step 001600): Train loss 5.168, Val loss 5.257\n",
      "Ep 1 (Step 001610): Train loss 5.086, Val loss 5.260\n",
      "Ep 1 (Step 001620): Train loss 5.098, Val loss 5.264\n",
      "Ep 1 (Step 001630): Train loss 5.291, Val loss 5.280\n",
      "Ep 1 (Step 001640): Train loss 5.251, Val loss 5.283\n",
      "Ep 1 (Step 001650): Train loss 5.114, Val loss 5.265\n",
      "Ep 1 (Step 001660): Train loss 5.033, Val loss 5.242\n",
      "Ep 1 (Step 001670): Train loss 5.217, Val loss 5.251\n",
      "Ep 1 (Step 001680): Train loss 5.138, Val loss 5.242\n",
      "Ep 1 (Step 001690): Train loss 4.987, Val loss 5.246\n",
      "Ep 1 (Step 001700): Train loss 5.201, Val loss 5.246\n",
      "Ep 1 (Step 001710): Train loss 5.170, Val loss 5.249\n",
      "Ep 1 (Step 001720): Train loss 5.082, Val loss 5.243\n",
      "Ep 1 (Step 001730): Train loss 5.198, Val loss 5.190\n",
      "Ep 1 (Step 001740): Train loss 5.143, Val loss 5.187\n",
      "Ep 1 (Step 001750): Train loss 5.320, Val loss 5.182\n",
      "Ep 1 (Step 001760): Train loss 5.022, Val loss 5.169\n",
      "Ep 1 (Step 001770): Train loss 5.059, Val loss 5.156\n",
      "Ep 1 (Step 001780): Train loss 5.123, Val loss 5.121\n",
      "Ep 1 (Step 001790): Train loss 5.033, Val loss 5.129\n",
      "Ep 1 (Step 001800): Train loss 5.110, Val loss 5.125\n",
      "Ep 1 (Step 001810): Train loss 5.291, Val loss 5.134\n",
      "Ep 1 (Step 001820): Train loss 5.265, Val loss 5.139\n",
      "Ep 1 (Step 001830): Train loss 5.043, Val loss 5.137\n",
      "Ep 1 (Step 001840): Train loss 5.202, Val loss 5.141\n",
      "Ep 1 (Step 001850): Train loss 5.287, Val loss 5.146\n",
      "Ep 1 (Step 001860): Train loss 5.163, Val loss 5.148\n",
      "Ep 1 (Step 001870): Train loss 5.153, Val loss 5.127\n",
      "Ep 1 (Step 001880): Train loss 5.174, Val loss 5.139\n",
      "Ep 1 (Step 001890): Train loss 5.112, Val loss 5.139\n",
      "Ep 1 (Step 001900): Train loss 5.179, Val loss 5.146\n",
      "Ep 1 (Step 001910): Train loss 5.033, Val loss 5.157\n",
      "Ep 1 (Step 001920): Train loss 5.204, Val loss 5.145\n",
      "Ep 1 (Step 001930): Train loss 5.313, Val loss 5.177\n",
      "Ep 1 (Step 001940): Train loss 4.965, Val loss 5.165\n",
      "Ep 1 (Step 001950): Train loss 5.088, Val loss 5.170\n",
      "Ep 1 (Step 001960): Train loss 5.294, Val loss 5.157\n",
      "Ep 1 (Step 001970): Train loss 5.050, Val loss 5.143\n",
      "Ep 1 (Step 001980): Train loss 5.072, Val loss 5.151\n",
      "Ep 1 (Step 001990): Train loss 5.079, Val loss 5.165\n",
      "Ep 1 (Step 002000): Train loss 5.104, Val loss 5.158\n",
      "Ep 1 (Step 002010): Train loss 5.099, Val loss 5.168\n",
      "Ep 1 (Step 002020): Train loss 5.065, Val loss 5.150\n",
      "Ep 1 (Step 002030): Train loss 5.082, Val loss 5.158\n",
      "Ep 1 (Step 002040): Train loss 5.060, Val loss 5.147\n",
      "Ep 1 (Step 002050): Train loss 5.161, Val loss 5.164\n",
      "Ep 1 (Step 002060): Train loss 5.097, Val loss 5.158\n",
      "Ep 1 (Step 002070): Train loss 5.067, Val loss 5.167\n",
      "Ep 1 (Step 002080): Train loss 4.972, Val loss 5.140\n",
      "Ep 1 (Step 002090): Train loss 4.922, Val loss 5.073\n",
      "Ep 1 (Step 002100): Train loss 5.128, Val loss 5.076\n",
      "Ep 1 (Step 002110): Train loss 5.187, Val loss 5.090\n",
      "Ep 1 (Step 002120): Train loss 5.094, Val loss 5.045\n",
      "Ep 1 (Step 002130): Train loss 4.884, Val loss 5.027\n",
      "Ep 1 (Step 002140): Train loss 5.080, Val loss 5.027\n",
      "Ep 1 (Step 002150): Train loss 5.230, Val loss 5.041\n",
      "Ep 1 (Step 002160): Train loss 5.130, Val loss 5.031\n",
      "Ep 1 (Step 002170): Train loss 4.914, Val loss 5.034\n",
      "Ep 1 (Step 002180): Train loss 5.074, Val loss 5.049\n",
      "Ep 1 (Step 002190): Train loss 4.881, Val loss 5.035\n",
      "Ep 1 (Step 002200): Train loss 5.113, Val loss 5.038\n",
      "Ep 1 (Step 002210): Train loss 5.075, Val loss 5.029\n",
      "Ep 1 (Step 002220): Train loss 5.054, Val loss 5.037\n",
      "Ep 1 (Step 002230): Train loss 5.006, Val loss 5.037\n",
      "Ep 1 (Step 002240): Train loss 5.084, Val loss 5.040\n",
      "Ep 1 (Step 002250): Train loss 4.946, Val loss 5.041\n",
      "Ep 1 (Step 002260): Train loss 5.026, Val loss 5.035\n",
      "Ep 1 (Step 002270): Train loss 5.056, Val loss 5.011\n",
      "Ep 1 (Step 002280): Train loss 5.008, Val loss 5.010\n",
      "Ep 1 (Step 002290): Train loss 5.117, Val loss 5.016\n",
      "Ep 1 (Step 002300): Train loss 5.049, Val loss 5.036\n",
      "Ep 1 (Step 002310): Train loss 5.156, Val loss 5.050\n",
      "Ep 1 (Step 002320): Train loss 4.895, Val loss 5.034\n",
      "Ep 1 (Step 002330): Train loss 4.957, Val loss 5.036\n",
      "Ep 1 (Step 002340): Train loss 5.048, Val loss 5.047\n",
      "Ep 1 (Step 002350): Train loss 4.951, Val loss 5.027\n",
      "Ep 1 (Step 002360): Train loss 5.066, Val loss 5.021\n",
      "Ep 1 (Step 002370): Train loss 5.011, Val loss 5.011\n",
      "Ep 1 (Step 002380): Train loss 4.973, Val loss 4.986\n",
      "Ep 1 (Step 002390): Train loss 5.029, Val loss 4.972\n",
      "Ep 1 (Step 002400): Train loss 4.935, Val loss 4.970\n",
      "Ep 1 (Step 002410): Train loss 5.044, Val loss 4.971\n",
      "Ep 1 (Step 002420): Train loss 5.112, Val loss 4.975\n",
      "Ep 1 (Step 002430): Train loss 4.974, Val loss 4.966\n",
      "Ep 1 (Step 002440): Train loss 4.996, Val loss 4.949\n",
      "Ep 1 (Step 002450): Train loss 4.947, Val loss 4.931\n",
      "Ep 1 (Step 002460): Train loss 5.029, Val loss 4.936\n",
      "Ep 1 (Step 002470): Train loss 5.087, Val loss 4.950\n",
      "Ep 1 (Step 002480): Train loss 5.054, Val loss 4.955\n",
      "Ep 1 (Step 002490): Train loss 4.934, Val loss 4.949\n",
      "Ep 1 (Step 002500): Train loss 5.107, Val loss 4.988\n",
      "Ep 1 (Step 002510): Train loss 5.041, Val loss 4.976\n",
      "Ep 1 (Step 002520): Train loss 5.099, Val loss 4.992\n",
      "Ep 1 (Step 002530): Train loss 4.929, Val loss 4.973\n",
      "Ep 1 (Step 002540): Train loss 4.912, Val loss 4.974\n",
      "Ep 1 (Step 002550): Train loss 4.846, Val loss 4.967\n",
      "Ep 1 (Step 002560): Train loss 4.947, Val loss 4.986\n",
      "Ep 1 (Step 002570): Train loss 4.922, Val loss 4.982\n",
      "Ep 1 (Step 002580): Train loss 5.092, Val loss 4.974\n",
      "Ep 1 (Step 002590): Train loss 4.969, Val loss 4.960\n",
      "Ep 1 (Step 002600): Train loss 5.058, Val loss 4.982\n",
      "Ep 1 (Step 002610): Train loss 4.930, Val loss 4.975\n",
      "Ep 1 (Step 002620): Train loss 4.907, Val loss 4.961\n",
      "Ep 1 (Step 002630): Train loss 4.979, Val loss 4.973\n",
      "Ep 1 (Step 002640): Train loss 5.005, Val loss 4.933\n",
      "Ep 1 (Step 002650): Train loss 5.044, Val loss 4.952\n",
      "Ep 1 (Step 002660): Train loss 4.951, Val loss 4.963\n",
      "Ep 1 (Step 002670): Train loss 5.010, Val loss 4.953\n",
      "Ep 1 (Step 002680): Train loss 4.859, Val loss 4.961\n",
      "Ep 1 (Step 002690): Train loss 5.017, Val loss 4.960\n",
      "Ep 1 (Step 002700): Train loss 4.850, Val loss 4.953\n",
      "Ep 1 (Step 002710): Train loss 4.870, Val loss 4.917\n",
      "Ep 1 (Step 002720): Train loss 5.085, Val loss 4.876\n",
      "Ep 1 (Step 002730): Train loss 5.103, Val loss 4.905\n",
      "Ep 1 (Step 002740): Train loss 4.927, Val loss 4.891\n",
      "Ep 1 (Step 002750): Train loss 4.859, Val loss 4.851\n",
      "Ep 1 (Step 002760): Train loss 4.924, Val loss 4.837\n",
      "Ep 1 (Step 002770): Train loss 5.074, Val loss 4.839\n",
      "Ep 1 (Step 002780): Train loss 5.000, Val loss 4.860\n",
      "Ep 1 (Step 002790): Train loss 4.964, Val loss 4.853\n",
      "Ep 1 (Step 002800): Train loss 4.893, Val loss 4.857\n",
      "Ep 1 (Step 002810): Train loss 4.958, Val loss 4.861\n",
      "Ep 1 (Step 002820): Train loss 4.773, Val loss 4.872\n",
      "Ep 1 (Step 002830): Train loss 5.038, Val loss 4.883\n",
      "Ep 1 (Step 002840): Train loss 4.973, Val loss 4.883\n",
      "Ep 1 (Step 002850): Train loss 5.119, Val loss 4.871\n",
      "Ep 1 (Step 002860): Train loss 5.089, Val loss 4.869\n",
      "Ep 1 (Step 002870): Train loss 5.087, Val loss 4.866\n",
      "Ep 1 (Step 002880): Train loss 4.736, Val loss 4.863\n",
      "Ep 1 (Step 002890): Train loss 5.026, Val loss 4.864\n",
      "Ep 1 (Step 002900): Train loss 4.903, Val loss 4.844\n",
      "Ep 1 (Step 002910): Train loss 4.867, Val loss 4.849\n",
      "Ep 1 (Step 002920): Train loss 5.021, Val loss 4.843\n",
      "Ep 1 (Step 002930): Train loss 4.771, Val loss 4.858\n",
      "Ep 1 (Step 002940): Train loss 4.943, Val loss 4.853\n",
      "Ep 1 (Step 002950): Train loss 4.938, Val loss 4.853\n",
      "Ep 1 (Step 002960): Train loss 4.883, Val loss 4.847\n",
      "Ep 1 (Step 002970): Train loss 4.935, Val loss 4.848\n",
      "Ep 1 (Step 002980): Train loss 4.965, Val loss 4.833\n",
      "Ep 1 (Step 002990): Train loss 4.850, Val loss 4.829\n",
      "Ep 1 (Step 003000): Train loss 4.814, Val loss 4.820\n",
      "Ep 1 (Step 003010): Train loss 4.913, Val loss 4.833\n",
      "Ep 1 (Step 003020): Train loss 4.944, Val loss 4.827\n",
      "Ep 1 (Step 003030): Train loss 5.005, Val loss 4.814\n",
      "Ep 1 (Step 003040): Train loss 4.905, Val loss 4.786\n",
      "Ep 1 (Step 003050): Train loss 4.898, Val loss 4.784\n",
      "Ep 1 (Step 003060): Train loss 4.761, Val loss 4.802\n",
      "Ep 1 (Step 003070): Train loss 4.744, Val loss 4.794\n",
      "Ep 1 (Step 003080): Train loss 4.775, Val loss 4.795\n",
      "Ep 1 (Step 003090): Train loss 4.952, Val loss 4.799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 003100): Train loss 4.917, Val loss 4.813\n",
      "Ep 1 (Step 003110): Train loss 4.776, Val loss 4.810\n",
      "Ep 1 (Step 003120): Train loss 4.824, Val loss 4.813\n",
      "Ep 1 (Step 003130): Train loss 4.892, Val loss 4.813\n",
      "Ep 1 (Step 003140): Train loss 4.905, Val loss 4.799\n",
      "Ep 1 (Step 003150): Train loss 4.881, Val loss 4.839\n",
      "Ep 1 (Step 003160): Train loss 5.012, Val loss 4.846\n",
      "Ep 1 (Step 003170): Train loss 4.854, Val loss 4.840\n",
      "Ep 1 (Step 003180): Train loss 4.638, Val loss 4.826\n",
      "Ep 1 (Step 003190): Train loss 4.838, Val loss 4.823\n",
      "Ep 1 (Step 003200): Train loss 4.878, Val loss 4.836\n",
      "Ep 1 (Step 003210): Train loss 4.811, Val loss 4.823\n",
      "Ep 1 (Step 003220): Train loss 4.907, Val loss 4.823\n",
      "Ep 1 (Step 003230): Train loss 4.767, Val loss 4.856\n",
      "Ep 1 (Step 003240): Train loss 4.755, Val loss 4.835\n",
      "Ep 1 (Step 003250): Train loss 4.886, Val loss 4.820\n",
      "Ep 1 (Step 003260): Train loss 4.828, Val loss 4.820\n",
      "Ep 1 (Step 003270): Train loss 4.688, Val loss 4.829\n",
      "Ep 1 (Step 003280): Train loss 4.854, Val loss 4.826\n",
      "Ep 1 (Step 003290): Train loss 4.841, Val loss 4.848\n",
      "Ep 1 (Step 003300): Train loss 4.852, Val loss 4.832\n",
      "Ep 1 (Step 003310): Train loss 4.968, Val loss 4.855\n",
      "Ep 1 (Step 003320): Train loss 4.898, Val loss 4.846\n",
      "Ep 1 (Step 003330): Train loss 4.792, Val loss 4.842\n",
      "Ep 1 (Step 003340): Train loss 4.985, Val loss 4.817\n",
      "Ep 1 (Step 003350): Train loss 4.691, Val loss 4.832\n",
      "Ep 1 (Step 003360): Train loss 4.932, Val loss 4.813\n",
      "Ep 1 (Step 003370): Train loss 4.795, Val loss 4.813\n",
      "Ep 1 (Step 003380): Train loss 4.747, Val loss 4.829\n",
      "Ep 1 (Step 003390): Train loss 4.901, Val loss 4.815\n",
      "Ep 1 (Step 003400): Train loss 4.733, Val loss 4.844\n",
      "Ep 1 (Step 003410): Train loss 4.911, Val loss 4.834\n",
      "Ep 1 (Step 003420): Train loss 4.752, Val loss 4.830\n",
      "Ep 1 (Step 003430): Train loss 4.999, Val loss 4.827\n",
      "Ep 1 (Step 003440): Train loss 4.799, Val loss 4.835\n",
      "Ep 1 (Step 003450): Train loss 4.769, Val loss 4.834\n",
      "Ep 1 (Step 003460): Train loss 4.843, Val loss 4.843\n",
      "Ep 1 (Step 003470): Train loss 5.071, Val loss 4.847\n",
      "Ep 1 (Step 003480): Train loss 4.930, Val loss 4.820\n",
      "Ep 1 (Step 003490): Train loss 4.874, Val loss 4.827\n",
      "Ep 1 (Step 003500): Train loss 4.879, Val loss 4.820\n",
      "Ep 1 (Step 003510): Train loss 4.766, Val loss 4.799\n",
      "Ep 1 (Step 003520): Train loss 4.941, Val loss 4.801\n",
      "Ep 1 (Step 003530): Train loss 4.713, Val loss 4.796\n",
      "Ep 1 (Step 003540): Train loss 4.871, Val loss 4.792\n",
      "Ep 1 (Step 003550): Train loss 4.754, Val loss 4.799\n",
      "Ep 1 (Step 003560): Train loss 4.985, Val loss 4.809\n",
      "Ep 1 (Step 003570): Train loss 4.739, Val loss 4.815\n",
      "Ep 1 (Step 003580): Train loss 4.800, Val loss 4.803\n",
      "Ep 1 (Step 003590): Train loss 4.757, Val loss 4.817\n",
      "Ep 1 (Step 003600): Train loss 4.730, Val loss 4.821\n",
      "Ep 1 (Step 003610): Train loss 4.745, Val loss 4.829\n",
      "Ep 1 (Step 003620): Train loss 4.989, Val loss 4.829\n",
      "Ep 1 (Step 003630): Train loss 4.763, Val loss 4.841\n",
      "Ep 1 (Step 003640): Train loss 4.761, Val loss 4.841\n",
      "Ep 1 (Step 003650): Train loss 4.875, Val loss 4.832\n",
      "Ep 1 (Step 003660): Train loss 4.879, Val loss 4.843\n",
      "Ep 1 (Step 003670): Train loss 4.943, Val loss 4.846\n",
      "Ep 1 (Step 003680): Train loss 4.818, Val loss 4.840\n",
      "Ep 1 (Step 003690): Train loss 4.856, Val loss 4.838\n",
      "Ep 1 (Step 003700): Train loss 4.766, Val loss 4.863\n",
      "Ep 1 (Step 003710): Train loss 4.735, Val loss 4.843\n",
      "Ep 1 (Step 003720): Train loss 4.606, Val loss 4.831\n",
      "Ep 1 (Step 003730): Train loss 4.794, Val loss 4.830\n",
      "Ep 1 (Step 003740): Train loss 4.914, Val loss 4.830\n",
      "Ep 1 (Step 003750): Train loss 4.832, Val loss 4.809\n",
      "Ep 1 (Step 003760): Train loss 4.854, Val loss 4.823\n",
      "Ep 1 (Step 003770): Train loss 4.680, Val loss 4.825\n",
      "Ep 1 (Step 003780): Train loss 4.736, Val loss 4.824\n",
      "Ep 1 (Step 003790): Train loss 4.826, Val loss 4.821\n",
      "Ep 1 (Step 003800): Train loss 4.685, Val loss 4.773\n",
      "Ep 1 (Step 003810): Train loss 4.818, Val loss 4.772\n",
      "Ep 1 (Step 003820): Train loss 4.709, Val loss 4.778\n",
      "Ep 1 (Step 003830): Train loss 4.806, Val loss 4.784\n",
      "Ep 1 (Step 003840): Train loss 4.775, Val loss 4.789\n",
      "Ep 1 (Step 003850): Train loss 4.818, Val loss 4.778\n",
      "Ep 1 (Step 003860): Train loss 4.807, Val loss 4.767\n",
      "Ep 1 (Step 003870): Train loss 4.827, Val loss 4.780\n",
      "Ep 1 (Step 003880): Train loss 4.511, Val loss 4.779\n",
      "Ep 1 (Step 003890): Train loss 4.810, Val loss 4.744\n",
      "Ep 1 (Step 003900): Train loss 4.765, Val loss 4.740\n",
      "Ep 1 (Step 003910): Train loss 4.809, Val loss 4.734\n",
      "Ep 1 (Step 003920): Train loss 4.646, Val loss 4.713\n",
      "Ep 1 (Step 003930): Train loss 4.698, Val loss 4.707\n",
      "Ep 1 (Step 003940): Train loss 4.853, Val loss 4.713\n",
      "Ep 1 (Step 003950): Train loss 4.832, Val loss 4.707\n",
      "Ep 1 (Step 003960): Train loss 4.757, Val loss 4.707\n",
      "Ep 1 (Step 003970): Train loss 4.686, Val loss 4.699\n",
      "Ep 1 (Step 003980): Train loss 4.795, Val loss 4.736\n",
      "Ep 1 (Step 003990): Train loss 4.809, Val loss 4.723\n",
      "Ep 1 (Step 004000): Train loss 4.877, Val loss 4.714\n",
      "Ep 1 (Step 004010): Train loss 4.679, Val loss 4.712\n",
      "Ep 1 (Step 004020): Train loss 4.660, Val loss 4.701\n",
      "Ep 1 (Step 004030): Train loss 4.735, Val loss 4.692\n",
      "Ep 1 (Step 004040): Train loss 4.750, Val loss 4.684\n",
      "Ep 1 (Step 004050): Train loss 4.762, Val loss 4.703\n",
      "Ep 1 (Step 004060): Train loss 4.810, Val loss 4.672\n",
      "Ep 1 (Step 004070): Train loss 4.653, Val loss 4.674\n",
      "Ep 1 (Step 004080): Train loss 4.758, Val loss 4.667\n",
      "Ep 1 (Step 004090): Train loss 4.751, Val loss 4.667\n",
      "Ep 1 (Step 004100): Train loss 4.838, Val loss 4.652\n",
      "Ep 1 (Step 004110): Train loss 4.855, Val loss 4.651\n",
      "Ep 1 (Step 004120): Train loss 4.703, Val loss 4.660\n",
      "Ep 1 (Step 004130): Train loss 4.686, Val loss 4.649\n",
      "Ep 1 (Step 004140): Train loss 4.843, Val loss 4.664\n",
      "Ep 1 (Step 004150): Train loss 4.739, Val loss 4.676\n",
      "Ep 1 (Step 004160): Train loss 4.857, Val loss 4.657\n",
      "Ep 1 (Step 004170): Train loss 4.709, Val loss 4.653\n",
      "Ep 1 (Step 004180): Train loss 4.759, Val loss 4.644\n",
      "Ep 1 (Step 004190): Train loss 4.809, Val loss 4.646\n",
      "Ep 1 (Step 004200): Train loss 4.687, Val loss 4.657\n",
      "Ep 1 (Step 004210): Train loss 4.762, Val loss 4.656\n",
      "Ep 1 (Step 004220): Train loss 4.588, Val loss 4.628\n",
      "Ep 1 (Step 004230): Train loss 4.741, Val loss 4.627\n",
      "Ep 1 (Step 004240): Train loss 4.817, Val loss 4.644\n",
      "Ep 1 (Step 004250): Train loss 4.704, Val loss 4.655\n",
      "Ep 1 (Step 004260): Train loss 4.591, Val loss 4.644\n",
      "Ep 1 (Step 004270): Train loss 4.644, Val loss 4.625\n",
      "Ep 1 (Step 004280): Train loss 4.759, Val loss 4.638\n",
      "Ep 1 (Step 004290): Train loss 4.634, Val loss 4.626\n",
      "Ep 1 (Step 004300): Train loss 4.687, Val loss 4.629\n",
      "Ep 1 (Step 004310): Train loss 4.762, Val loss 4.638\n",
      "Ep 1 (Step 004320): Train loss 4.655, Val loss 4.635\n",
      "Ep 1 (Step 004330): Train loss 4.689, Val loss 4.638\n",
      "Ep 1 (Step 004340): Train loss 4.607, Val loss 4.658\n",
      "Ep 1 (Step 004350): Train loss 4.775, Val loss 4.650\n",
      "Ep 1 (Step 004360): Train loss 4.736, Val loss 4.665\n",
      "Ep 1 (Step 004370): Train loss 4.608, Val loss 4.655\n",
      "Ep 1 (Step 004380): Train loss 4.792, Val loss 4.646\n",
      "Ep 1 (Step 004390): Train loss 4.677, Val loss 4.642\n",
      "Ep 1 (Step 004400): Train loss 4.777, Val loss 4.634\n",
      "Ep 1 (Step 004410): Train loss 4.816, Val loss 4.633\n",
      "Ep 1 (Step 004420): Train loss 4.522, Val loss 4.647\n",
      "Ep 1 (Step 004430): Train loss 4.839, Val loss 4.651\n",
      "Ep 1 (Step 004440): Train loss 4.621, Val loss 4.651\n",
      "Ep 1 (Step 004450): Train loss 4.598, Val loss 4.649\n",
      "Ep 1 (Step 004460): Train loss 4.848, Val loss 4.655\n",
      "Ep 1 (Step 004470): Train loss 4.831, Val loss 4.646\n",
      "Ep 1 (Step 004480): Train loss 4.778, Val loss 4.652\n",
      "Ep 1 (Step 004490): Train loss 4.794, Val loss 4.644\n",
      "Ep 1 (Step 004500): Train loss 4.756, Val loss 4.629\n",
      "Ep 1 (Step 004510): Train loss 4.629, Val loss 4.599\n",
      "Ep 1 (Step 004520): Train loss 4.765, Val loss 4.591\n",
      "Ep 1 (Step 004530): Train loss 4.685, Val loss 4.596\n",
      "Ep 1 (Step 004540): Train loss 4.661, Val loss 4.598\n",
      "Ep 1 (Step 004550): Train loss 4.658, Val loss 4.603\n",
      "Ep 1 (Step 004560): Train loss 4.641, Val loss 4.610\n",
      "Ep 1 (Step 004570): Train loss 4.736, Val loss 4.592\n",
      "Ep 1 (Step 004580): Train loss 4.778, Val loss 4.582\n",
      "Ep 1 (Step 004590): Train loss 4.784, Val loss 4.578\n",
      "Ep 1 (Step 004600): Train loss 4.672, Val loss 4.585\n",
      "Ep 1 (Step 004610): Train loss 4.779, Val loss 4.572\n",
      "Ep 1 (Step 004620): Train loss 4.643, Val loss 4.557\n",
      "Ep 1 (Step 004630): Train loss 4.676, Val loss 4.555\n",
      "Ep 1 (Step 004640): Train loss 4.685, Val loss 4.553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 004650): Train loss 4.591, Val loss 4.567\n",
      "Ep 1 (Step 004660): Train loss 4.766, Val loss 4.574\n",
      "Ep 1 (Step 004670): Train loss 4.684, Val loss 4.567\n",
      "Ep 1 (Step 004680): Train loss 4.544, Val loss 4.557\n",
      "Ep 1 (Step 004690): Train loss 4.661, Val loss 4.560\n",
      "Ep 1 (Step 004700): Train loss 4.630, Val loss 4.587\n",
      "Ep 1 (Step 004710): Train loss 4.802, Val loss 4.582\n",
      "Ep 1 (Step 004720): Train loss 4.602, Val loss 4.576\n",
      "Ep 1 (Step 004730): Train loss 4.672, Val loss 4.582\n",
      "Ep 1 (Step 004740): Train loss 4.649, Val loss 4.605\n",
      "Ep 1 (Step 004750): Train loss 4.534, Val loss 4.601\n",
      "Ep 1 (Step 004760): Train loss 4.610, Val loss 4.595\n",
      "Ep 1 (Step 004770): Train loss 4.772, Val loss 4.599\n",
      "Ep 1 (Step 004780): Train loss 4.569, Val loss 4.611\n",
      "Ep 1 (Step 004790): Train loss 4.531, Val loss 4.595\n",
      "Ep 1 (Step 004800): Train loss 4.728, Val loss 4.611\n",
      "Ep 1 (Step 004810): Train loss 4.762, Val loss 4.623\n",
      "Ep 1 (Step 004820): Train loss 4.710, Val loss 4.618\n",
      "Ep 1 (Step 004830): Train loss 4.759, Val loss 4.625\n",
      "Ep 1 (Step 004840): Train loss 4.606, Val loss 4.616\n",
      "Ep 1 (Step 004850): Train loss 4.545, Val loss 4.626\n",
      "Ep 1 (Step 004860): Train loss 4.691, Val loss 4.608\n",
      "Ep 1 (Step 004870): Train loss 4.731, Val loss 4.618\n",
      "Ep 1 (Step 004880): Train loss 4.606, Val loss 4.630\n",
      "Ep 1 (Step 004890): Train loss 4.648, Val loss 4.641\n",
      "Ep 1 (Step 004900): Train loss 4.615, Val loss 4.639\n",
      "Ep 1 (Step 004910): Train loss 4.673, Val loss 4.634\n",
      "Ep 1 (Step 004920): Train loss 4.635, Val loss 4.643\n",
      "Ep 1 (Step 004930): Train loss 4.740, Val loss 4.645\n",
      "Ep 1 (Step 004940): Train loss 4.615, Val loss 4.648\n",
      "Ep 1 (Step 004950): Train loss 4.655, Val loss 4.632\n",
      "Ep 1 (Step 004960): Train loss 4.687, Val loss 4.608\n",
      "Ep 1 (Step 004970): Train loss 4.584, Val loss 4.638\n",
      "Ep 1 (Step 004980): Train loss 4.671, Val loss 4.617\n",
      "Ep 1 (Step 004990): Train loss 4.607, Val loss 4.614\n",
      "Ep 1 (Step 005000): Train loss 4.634, Val loss 4.603\n",
      "Ep 1 (Step 005010): Train loss 4.477, Val loss 4.618\n",
      "Ep 1 (Step 005020): Train loss 4.693, Val loss 4.604\n",
      "Ep 1 (Step 005030): Train loss 4.586, Val loss 4.605\n",
      "Ep 1 (Step 005040): Train loss 4.643, Val loss 4.621\n",
      "Ep 1 (Step 005050): Train loss 4.730, Val loss 4.630\n",
      "Ep 1 (Step 005060): Train loss 4.671, Val loss 4.631\n",
      "Ep 1 (Step 005070): Train loss 4.638, Val loss 4.587\n",
      "Ep 1 (Step 005080): Train loss 4.592, Val loss 4.583\n",
      "Ep 1 (Step 005090): Train loss 4.537, Val loss 4.582\n",
      "Ep 1 (Step 005100): Train loss 4.742, Val loss 4.575\n",
      "Ep 1 (Step 005110): Train loss 4.686, Val loss 4.575\n",
      "Ep 1 (Step 005120): Train loss 4.733, Val loss 4.577\n",
      "Ep 1 (Step 005130): Train loss 4.615, Val loss 4.599\n",
      "Ep 1 (Step 005140): Train loss 4.577, Val loss 4.591\n",
      "Ep 1 (Step 005150): Train loss 4.648, Val loss 4.579\n",
      "Ep 1 (Step 005160): Train loss 4.680, Val loss 4.597\n",
      "Ep 1 (Step 005170): Train loss 4.610, Val loss 4.588\n",
      "Ep 1 (Step 005180): Train loss 4.708, Val loss 4.603\n",
      "Ep 1 (Step 005190): Train loss 4.652, Val loss 4.610\n",
      "Ep 1 (Step 005200): Train loss 4.629, Val loss 4.628\n",
      "Ep 1 (Step 005210): Train loss 4.747, Val loss 4.617\n",
      "Ep 1 (Step 005220): Train loss 4.609, Val loss 4.633\n",
      "Ep 1 (Step 005230): Train loss 4.611, Val loss 4.626\n",
      "Ep 1 (Step 005240): Train loss 4.539, Val loss 4.601\n",
      "Ep 1 (Step 005250): Train loss 4.575, Val loss 4.599\n",
      "Ep 1 (Step 005260): Train loss 4.540, Val loss 4.594\n",
      "Ep 1 (Step 005270): Train loss 4.566, Val loss 4.608\n",
      "Ep 1 (Step 005280): Train loss 4.754, Val loss 4.572\n",
      "Ep 1 (Step 005290): Train loss 4.685, Val loss 4.585\n",
      "Ep 1 (Step 005300): Train loss 4.462, Val loss 4.568\n",
      "Ep 1 (Step 005310): Train loss 4.670, Val loss 4.566\n",
      "Ep 1 (Step 005320): Train loss 4.675, Val loss 4.575\n",
      "Ep 1 (Step 005330): Train loss 4.519, Val loss 4.548\n",
      "Ep 1 (Step 005340): Train loss 4.624, Val loss 4.537\n",
      "Ep 1 (Step 005350): Train loss 4.610, Val loss 4.527\n",
      "Ep 1 (Step 005360): Train loss 4.829, Val loss 4.551\n",
      "Ep 1 (Step 005370): Train loss 4.658, Val loss 4.553\n",
      "Ep 1 (Step 005380): Train loss 4.473, Val loss 4.555\n",
      "Ep 1 (Step 005390): Train loss 4.645, Val loss 4.558\n",
      "Ep 1 (Step 005400): Train loss 4.545, Val loss 4.539\n",
      "Ep 1 (Step 005410): Train loss 4.611, Val loss 4.540\n",
      "Ep 1 (Step 005420): Train loss 4.609, Val loss 4.552\n",
      "Ep 1 (Step 005430): Train loss 4.586, Val loss 4.576\n",
      "Ep 1 (Step 005440): Train loss 4.815, Val loss 4.544\n",
      "Ep 1 (Step 005450): Train loss 4.617, Val loss 4.537\n",
      "Ep 1 (Step 005460): Train loss 4.725, Val loss 4.567\n",
      "Ep 1 (Step 005470): Train loss 4.568, Val loss 4.547\n",
      "Ep 1 (Step 005480): Train loss 4.476, Val loss 4.562\n",
      "Ep 1 (Step 005490): Train loss 4.554, Val loss 4.531\n",
      "Ep 1 (Step 005500): Train loss 4.533, Val loss 4.543\n",
      "Ep 1 (Step 005510): Train loss 4.537, Val loss 4.518\n",
      "Ep 1 (Step 005520): Train loss 4.533, Val loss 4.536\n",
      "Ep 1 (Step 005530): Train loss 4.593, Val loss 4.531\n",
      "Ep 1 (Step 005540): Train loss 4.575, Val loss 4.534\n",
      "Ep 1 (Step 005550): Train loss 4.661, Val loss 4.536\n",
      "Ep 1 (Step 005560): Train loss 4.593, Val loss 4.507\n",
      "Ep 1 (Step 005570): Train loss 4.697, Val loss 4.508\n",
      "Ep 1 (Step 005580): Train loss 4.585, Val loss 4.513\n",
      "Ep 1 (Step 005590): Train loss 4.767, Val loss 4.528\n",
      "Ep 1 (Step 005600): Train loss 4.547, Val loss 4.548\n",
      "Ep 1 (Step 005610): Train loss 4.612, Val loss 4.524\n",
      "Ep 1 (Step 005620): Train loss 4.689, Val loss 4.516\n",
      "Ep 1 (Step 005630): Train loss 4.577, Val loss 4.497\n",
      "Ep 1 (Step 005640): Train loss 4.573, Val loss 4.501\n",
      "Ep 1 (Step 005650): Train loss 4.525, Val loss 4.515\n",
      "Ep 1 (Step 005660): Train loss 4.701, Val loss 4.505\n",
      "Ep 1 (Step 005670): Train loss 4.614, Val loss 4.478\n",
      "Ep 1 (Step 005680): Train loss 4.486, Val loss 4.499\n",
      "Ep 1 (Step 005690): Train loss 4.452, Val loss 4.497\n",
      "Ep 1 (Step 005700): Train loss 4.536, Val loss 4.499\n",
      "Ep 1 (Step 005710): Train loss 4.614, Val loss 4.486\n",
      "Ep 1 (Step 005720): Train loss 4.557, Val loss 4.506\n",
      "Ep 1 (Step 005730): Train loss 4.562, Val loss 4.491\n",
      "Ep 1 (Step 005740): Train loss 4.524, Val loss 4.509\n",
      "Ep 1 (Step 005750): Train loss 4.666, Val loss 4.500\n",
      "Ep 1 (Step 005760): Train loss 4.590, Val loss 4.491\n",
      "Ep 1 (Step 005770): Train loss 4.386, Val loss 4.514\n",
      "Ep 1 (Step 005780): Train loss 4.573, Val loss 4.498\n",
      "Ep 1 (Step 005790): Train loss 4.568, Val loss 4.476\n",
      "Ep 1 (Step 005800): Train loss 4.596, Val loss 4.458\n",
      "Ep 1 (Step 005810): Train loss 4.458, Val loss 4.465\n",
      "Ep 1 (Step 005820): Train loss 4.601, Val loss 4.476\n",
      "Ep 1 (Step 005830): Train loss 4.559, Val loss 4.480\n",
      "Ep 1 (Step 005840): Train loss 4.569, Val loss 4.491\n",
      "Ep 1 (Step 005850): Train loss 4.539, Val loss 4.496\n",
      "Ep 1 (Step 005860): Train loss 4.537, Val loss 4.463\n",
      "Ep 1 (Step 005870): Train loss 4.658, Val loss 4.472\n",
      "Ep 1 (Step 005880): Train loss 4.477, Val loss 4.480\n",
      "Ep 1 (Step 005890): Train loss 4.523, Val loss 4.478\n",
      "Ep 1 (Step 005900): Train loss 4.451, Val loss 4.465\n",
      "Ep 1 (Step 005910): Train loss 4.567, Val loss 4.456\n",
      "Ep 1 (Step 005920): Train loss 4.674, Val loss 4.459\n",
      "Ep 1 (Step 005930): Train loss 4.528, Val loss 4.475\n",
      "Ep 1 (Step 005940): Train loss 4.516, Val loss 4.471\n",
      "Ep 1 (Step 005950): Train loss 4.553, Val loss 4.490\n",
      "Ep 1 (Step 005960): Train loss 4.471, Val loss 4.494\n",
      "Ep 1 (Step 005970): Train loss 4.528, Val loss 4.498\n",
      "Ep 1 (Step 005980): Train loss 4.404, Val loss 4.493\n",
      "Ep 1 (Step 005990): Train loss 4.613, Val loss 4.487\n",
      "Ep 1 (Step 006000): Train loss 4.406, Val loss 4.483\n",
      "Ep 1 (Step 006010): Train loss 4.547, Val loss 4.467\n",
      "Ep 1 (Step 006020): Train loss 4.470, Val loss 4.451\n",
      "Ep 1 (Step 006030): Train loss 4.454, Val loss 4.470\n",
      "Ep 1 (Step 006040): Train loss 4.596, Val loss 4.463\n",
      "Ep 1 (Step 006050): Train loss 4.557, Val loss 4.463\n",
      "Ep 1 (Step 006060): Train loss 4.570, Val loss 4.491\n",
      "Ep 1 (Step 006070): Train loss 4.578, Val loss 4.495\n",
      "Ep 1 (Step 006080): Train loss 4.582, Val loss 4.479\n",
      "Ep 1 (Step 006090): Train loss 4.565, Val loss 4.487\n",
      "Ep 1 (Step 006100): Train loss 4.675, Val loss 4.477\n",
      "Ep 1 (Step 006110): Train loss 4.552, Val loss 4.493\n",
      "Ep 1 (Step 006120): Train loss 4.555, Val loss 4.502\n",
      "Ep 1 (Step 006130): Train loss 4.621, Val loss 4.498\n",
      "Ep 1 (Step 006140): Train loss 4.577, Val loss 4.493\n",
      "Ep 1 (Step 006150): Train loss 4.504, Val loss 4.516\n",
      "Ep 1 (Step 006160): Train loss 4.624, Val loss 4.500\n",
      "Ep 1 (Step 006170): Train loss 4.555, Val loss 4.481\n",
      "Ep 1 (Step 006180): Train loss 4.487, Val loss 4.456\n",
      "Ep 1 (Step 006190): Train loss 4.456, Val loss 4.496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 006200): Train loss 4.448, Val loss 4.508\n",
      "Ep 1 (Step 006210): Train loss 4.541, Val loss 4.484\n",
      "Ep 1 (Step 006220): Train loss 4.479, Val loss 4.480\n",
      "Ep 1 (Step 006230): Train loss 4.461, Val loss 4.473\n",
      "Ep 1 (Step 006240): Train loss 4.475, Val loss 4.447\n",
      "Ep 1 (Step 006250): Train loss 4.463, Val loss 4.452\n",
      "Ep 1 (Step 006260): Train loss 4.368, Val loss 4.447\n",
      "Ep 1 (Step 006270): Train loss 4.656, Val loss 4.444\n",
      "Ep 1 (Step 006280): Train loss 4.537, Val loss 4.444\n",
      "Ep 1 (Step 006290): Train loss 4.602, Val loss 4.463\n",
      "Ep 1 (Step 006300): Train loss 4.689, Val loss 4.469\n",
      "Ep 1 (Step 006310): Train loss 4.491, Val loss 4.455\n",
      "Ep 1 (Step 006320): Train loss 4.529, Val loss 4.431\n",
      "Ep 1 (Step 006330): Train loss 4.551, Val loss 4.430\n",
      "Ep 1 (Step 006340): Train loss 4.557, Val loss 4.443\n",
      "Ep 1 (Step 006350): Train loss 4.386, Val loss 4.447\n",
      "Ep 1 (Step 006360): Train loss 4.550, Val loss 4.441\n",
      "Ep 1 (Step 006370): Train loss 4.451, Val loss 4.446\n",
      "Ep 1 (Step 006380): Train loss 4.470, Val loss 4.460\n",
      "Ep 1 (Step 006390): Train loss 4.523, Val loss 4.461\n",
      "Ep 1 (Step 006400): Train loss 4.435, Val loss 4.454\n",
      "Ep 1 (Step 006410): Train loss 4.533, Val loss 4.466\n",
      "Ep 1 (Step 006420): Train loss 4.635, Val loss 4.461\n",
      "Ep 1 (Step 006430): Train loss 4.606, Val loss 4.437\n",
      "Ep 1 (Step 006440): Train loss 4.489, Val loss 4.434\n",
      "Ep 1 (Step 006450): Train loss 4.410, Val loss 4.434\n",
      "Ep 1 (Step 006460): Train loss 4.395, Val loss 4.422\n",
      "Ep 1 (Step 006470): Train loss 4.530, Val loss 4.420\n",
      "Ep 1 (Step 006480): Train loss 4.427, Val loss 4.421\n",
      "Ep 1 (Step 006490): Train loss 4.554, Val loss 4.423\n",
      "Ep 1 (Step 006500): Train loss 4.496, Val loss 4.432\n",
      "Ep 1 (Step 006510): Train loss 4.700, Val loss 4.413\n",
      "Ep 1 (Step 006520): Train loss 4.467, Val loss 4.421\n",
      "Ep 1 (Step 006530): Train loss 4.610, Val loss 4.412\n",
      "Ep 1 (Step 006540): Train loss 4.499, Val loss 4.416\n",
      "Ep 1 (Step 006550): Train loss 4.435, Val loss 4.426\n",
      "Ep 1 (Step 006560): Train loss 4.383, Val loss 4.414\n",
      "Ep 1 (Step 006570): Train loss 4.505, Val loss 4.432\n",
      "Ep 1 (Step 006580): Train loss 4.663, Val loss 4.414\n",
      "Ep 1 (Step 006590): Train loss 4.439, Val loss 4.410\n",
      "Ep 1 (Step 006600): Train loss 4.368, Val loss 4.405\n",
      "Ep 1 (Step 006610): Train loss 4.436, Val loss 4.413\n",
      "Ep 1 (Step 006620): Train loss 4.576, Val loss 4.445\n",
      "Ep 1 (Step 006630): Train loss 4.553, Val loss 4.427\n",
      "Ep 1 (Step 006640): Train loss 4.463, Val loss 4.428\n",
      "Ep 1 (Step 006650): Train loss 4.537, Val loss 4.425\n",
      "Ep 1 (Step 006660): Train loss 4.600, Val loss 4.428\n",
      "Ep 1 (Step 006670): Train loss 4.583, Val loss 4.427\n",
      "Ep 2 (Step 006680): Train loss 4.473, Val loss 4.427\n",
      "Ep 2 (Step 006690): Train loss 4.569, Val loss 4.429\n",
      "Ep 2 (Step 006700): Train loss 4.465, Val loss 4.449\n",
      "Ep 2 (Step 006710): Train loss 4.487, Val loss 4.436\n",
      "Ep 2 (Step 006720): Train loss 4.456, Val loss 4.444\n",
      "Ep 2 (Step 006730): Train loss 4.610, Val loss 4.454\n",
      "Ep 2 (Step 006740): Train loss 4.525, Val loss 4.456\n",
      "Ep 2 (Step 006750): Train loss 4.539, Val loss 4.443\n",
      "Ep 2 (Step 006760): Train loss 4.446, Val loss 4.433\n",
      "Ep 2 (Step 006770): Train loss 4.347, Val loss 4.438\n",
      "Ep 2 (Step 006780): Train loss 4.499, Val loss 4.434\n",
      "Ep 2 (Step 006790): Train loss 4.711, Val loss 4.443\n",
      "Ep 2 (Step 006800): Train loss 4.425, Val loss 4.425\n",
      "Ep 2 (Step 006810): Train loss 4.467, Val loss 4.432\n",
      "Ep 2 (Step 006820): Train loss 4.370, Val loss 4.436\n",
      "Ep 2 (Step 006830): Train loss 4.380, Val loss 4.452\n",
      "Ep 2 (Step 006840): Train loss 4.541, Val loss 4.439\n",
      "Ep 2 (Step 006850): Train loss 4.449, Val loss 4.434\n",
      "Ep 2 (Step 006860): Train loss 4.412, Val loss 4.446\n",
      "Ep 2 (Step 006870): Train loss 4.440, Val loss 4.458\n",
      "Ep 2 (Step 006880): Train loss 4.644, Val loss 4.453\n",
      "Ep 2 (Step 006890): Train loss 4.564, Val loss 4.453\n",
      "Ep 2 (Step 006900): Train loss 4.433, Val loss 4.441\n",
      "Ep 2 (Step 006910): Train loss 4.370, Val loss 4.421\n",
      "Ep 2 (Step 006920): Train loss 4.450, Val loss 4.410\n",
      "Ep 2 (Step 006930): Train loss 4.546, Val loss 4.420\n",
      "Ep 2 (Step 006940): Train loss 4.505, Val loss 4.429\n",
      "Ep 2 (Step 006950): Train loss 4.520, Val loss 4.425\n",
      "Ep 2 (Step 006960): Train loss 4.589, Val loss 4.449\n",
      "Ep 2 (Step 006970): Train loss 4.429, Val loss 4.441\n",
      "Ep 2 (Step 006980): Train loss 4.618, Val loss 4.456\n",
      "Ep 2 (Step 006990): Train loss 4.519, Val loss 4.452\n",
      "Ep 2 (Step 007000): Train loss 4.398, Val loss 4.438\n",
      "Ep 2 (Step 007010): Train loss 4.498, Val loss 4.453\n",
      "Ep 2 (Step 007020): Train loss 4.537, Val loss 4.453\n",
      "Ep 2 (Step 007030): Train loss 4.671, Val loss 4.448\n",
      "Ep 2 (Step 007040): Train loss 4.414, Val loss 4.464\n",
      "Ep 2 (Step 007050): Train loss 4.571, Val loss 4.436\n",
      "Ep 2 (Step 007060): Train loss 4.362, Val loss 4.447\n",
      "Ep 2 (Step 007070): Train loss 4.354, Val loss 4.438\n",
      "Ep 2 (Step 007080): Train loss 4.441, Val loss 4.447\n",
      "Ep 2 (Step 007090): Train loss 4.315, Val loss 4.453\n",
      "Ep 2 (Step 007100): Train loss 4.409, Val loss 4.409\n",
      "Ep 2 (Step 007110): Train loss 4.518, Val loss 4.396\n",
      "Ep 2 (Step 007120): Train loss 4.455, Val loss 4.407\n",
      "Ep 2 (Step 007130): Train loss 4.520, Val loss 4.398\n",
      "Ep 2 (Step 007140): Train loss 4.353, Val loss 4.389\n",
      "Ep 2 (Step 007150): Train loss 4.476, Val loss 4.401\n",
      "Ep 2 (Step 007160): Train loss 4.350, Val loss 4.404\n",
      "Ep 2 (Step 007170): Train loss 4.385, Val loss 4.408\n",
      "Ep 2 (Step 007180): Train loss 4.348, Val loss 4.397\n",
      "Ep 2 (Step 007190): Train loss 4.438, Val loss 4.408\n",
      "Ep 2 (Step 007200): Train loss 4.345, Val loss 4.393\n",
      "Ep 2 (Step 007210): Train loss 4.601, Val loss 4.408\n",
      "Ep 2 (Step 007220): Train loss 4.459, Val loss 4.404\n",
      "Ep 2 (Step 007230): Train loss 4.454, Val loss 4.424\n",
      "Ep 2 (Step 007240): Train loss 4.427, Val loss 4.403\n",
      "Ep 2 (Step 007250): Train loss 4.505, Val loss 4.408\n",
      "Ep 2 (Step 007260): Train loss 4.344, Val loss 4.418\n",
      "Ep 2 (Step 007270): Train loss 4.392, Val loss 4.424\n",
      "Ep 2 (Step 007280): Train loss 4.536, Val loss 4.412\n",
      "Ep 2 (Step 007290): Train loss 4.465, Val loss 4.432\n",
      "Ep 2 (Step 007300): Train loss 4.526, Val loss 4.427\n",
      "Ep 2 (Step 007310): Train loss 4.496, Val loss 4.392\n",
      "Ep 2 (Step 007320): Train loss 4.547, Val loss 4.398\n",
      "Ep 2 (Step 007330): Train loss 4.597, Val loss 4.402\n",
      "Ep 2 (Step 007340): Train loss 4.304, Val loss 4.392\n",
      "Ep 2 (Step 007350): Train loss 4.363, Val loss 4.392\n",
      "Ep 2 (Step 007360): Train loss 4.392, Val loss 4.403\n",
      "Ep 2 (Step 007370): Train loss 4.324, Val loss 4.392\n",
      "Ep 2 (Step 007380): Train loss 4.382, Val loss 4.418\n",
      "Ep 2 (Step 007390): Train loss 4.353, Val loss 4.418\n",
      "Ep 2 (Step 007400): Train loss 4.556, Val loss 4.412\n",
      "Ep 2 (Step 007410): Train loss 4.297, Val loss 4.411\n",
      "Ep 2 (Step 007420): Train loss 4.522, Val loss 4.403\n",
      "Ep 2 (Step 007430): Train loss 4.456, Val loss 4.395\n",
      "Ep 2 (Step 007440): Train loss 4.418, Val loss 4.392\n",
      "Ep 2 (Step 007450): Train loss 4.375, Val loss 4.403\n",
      "Ep 2 (Step 007460): Train loss 4.454, Val loss 4.404\n",
      "Ep 2 (Step 007470): Train loss 4.426, Val loss 4.384\n",
      "Ep 2 (Step 007480): Train loss 4.420, Val loss 4.379\n",
      "Ep 2 (Step 007490): Train loss 4.388, Val loss 4.399\n",
      "Ep 2 (Step 007500): Train loss 4.369, Val loss 4.397\n",
      "Ep 2 (Step 007510): Train loss 4.420, Val loss 4.404\n",
      "Ep 2 (Step 007520): Train loss 4.503, Val loss 4.402\n",
      "Ep 2 (Step 007530): Train loss 4.409, Val loss 4.417\n",
      "Ep 2 (Step 007540): Train loss 4.395, Val loss 4.392\n",
      "Ep 2 (Step 007550): Train loss 4.371, Val loss 4.379\n",
      "Ep 2 (Step 007560): Train loss 4.448, Val loss 4.380\n",
      "Ep 2 (Step 007570): Train loss 4.489, Val loss 4.376\n",
      "Ep 2 (Step 007580): Train loss 4.478, Val loss 4.394\n",
      "Ep 2 (Step 007590): Train loss 4.408, Val loss 4.401\n",
      "Ep 2 (Step 007600): Train loss 4.462, Val loss 4.394\n",
      "Ep 2 (Step 007610): Train loss 4.551, Val loss 4.405\n",
      "Ep 2 (Step 007620): Train loss 4.381, Val loss 4.400\n",
      "Ep 2 (Step 007630): Train loss 4.517, Val loss 4.385\n",
      "Ep 2 (Step 007640): Train loss 4.550, Val loss 4.406\n",
      "Ep 2 (Step 007650): Train loss 4.477, Val loss 4.417\n",
      "Ep 2 (Step 007660): Train loss 4.250, Val loss 4.402\n",
      "Ep 2 (Step 007670): Train loss 4.429, Val loss 4.390\n",
      "Ep 2 (Step 007680): Train loss 4.320, Val loss 4.411\n",
      "Ep 2 (Step 007690): Train loss 4.489, Val loss 4.412\n",
      "Ep 2 (Step 007700): Train loss 4.369, Val loss 4.406\n",
      "Ep 2 (Step 007710): Train loss 4.367, Val loss 4.398\n",
      "Ep 2 (Step 007720): Train loss 4.503, Val loss 4.413\n",
      "Ep 2 (Step 007730): Train loss 4.409, Val loss 4.418\n",
      "Ep 2 (Step 007740): Train loss 4.386, Val loss 4.420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2 (Step 007750): Train loss 4.314, Val loss 4.385\n",
      "Ep 2 (Step 007760): Train loss 4.390, Val loss 4.408\n",
      "Ep 2 (Step 007770): Train loss 4.386, Val loss 4.398\n",
      "Ep 2 (Step 007780): Train loss 4.331, Val loss 4.403\n",
      "Ep 2 (Step 007790): Train loss 4.429, Val loss 4.403\n",
      "Ep 2 (Step 007800): Train loss 4.414, Val loss 4.391\n",
      "Ep 2 (Step 007810): Train loss 4.296, Val loss 4.410\n",
      "Ep 2 (Step 007820): Train loss 4.413, Val loss 4.409\n",
      "Ep 2 (Step 007830): Train loss 4.470, Val loss 4.412\n",
      "Ep 2 (Step 007840): Train loss 4.344, Val loss 4.417\n",
      "Ep 2 (Step 007850): Train loss 4.502, Val loss 4.397\n",
      "Ep 2 (Step 007860): Train loss 4.366, Val loss 4.402\n",
      "Ep 2 (Step 007870): Train loss 4.312, Val loss 4.414\n",
      "Ep 2 (Step 007880): Train loss 4.311, Val loss 4.403\n",
      "Ep 2 (Step 007890): Train loss 4.519, Val loss 4.422\n",
      "Ep 2 (Step 007900): Train loss 4.441, Val loss 4.419\n",
      "Ep 2 (Step 007910): Train loss 4.364, Val loss 4.404\n",
      "Ep 2 (Step 007920): Train loss 4.441, Val loss 4.413\n",
      "Ep 2 (Step 007930): Train loss 4.354, Val loss 4.390\n",
      "Ep 2 (Step 007940): Train loss 4.298, Val loss 4.421\n",
      "Ep 2 (Step 007950): Train loss 4.452, Val loss 4.413\n",
      "Ep 2 (Step 007960): Train loss 4.334, Val loss 4.389\n",
      "Ep 2 (Step 007970): Train loss 4.358, Val loss 4.376\n",
      "Ep 2 (Step 007980): Train loss 4.370, Val loss 4.385\n",
      "Ep 2 (Step 007990): Train loss 4.409, Val loss 4.363\n",
      "Ep 2 (Step 008000): Train loss 4.418, Val loss 4.361\n",
      "Ep 2 (Step 008010): Train loss 4.510, Val loss 4.361\n",
      "Ep 2 (Step 008020): Train loss 4.405, Val loss 4.353\n",
      "Ep 2 (Step 008030): Train loss 4.386, Val loss 4.366\n",
      "Ep 2 (Step 008040): Train loss 4.401, Val loss 4.376\n",
      "Ep 2 (Step 008050): Train loss 4.389, Val loss 4.358\n",
      "Ep 2 (Step 008060): Train loss 4.384, Val loss 4.346\n",
      "Ep 2 (Step 008070): Train loss 4.559, Val loss 4.360\n",
      "Ep 2 (Step 008080): Train loss 4.443, Val loss 4.349\n",
      "Ep 2 (Step 008090): Train loss 4.422, Val loss 4.362\n",
      "Ep 2 (Step 008100): Train loss 4.540, Val loss 4.364\n",
      "Ep 2 (Step 008110): Train loss 4.283, Val loss 4.349\n",
      "Ep 2 (Step 008120): Train loss 4.307, Val loss 4.366\n",
      "Ep 2 (Step 008130): Train loss 4.325, Val loss 4.353\n",
      "Ep 2 (Step 008140): Train loss 4.446, Val loss 4.358\n",
      "Ep 2 (Step 008150): Train loss 4.513, Val loss 4.371\n",
      "Ep 2 (Step 008160): Train loss 4.419, Val loss 4.378\n",
      "Ep 2 (Step 008170): Train loss 4.371, Val loss 4.351\n",
      "Ep 2 (Step 008180): Train loss 4.512, Val loss 4.380\n",
      "Ep 2 (Step 008190): Train loss 4.434, Val loss 4.378\n",
      "Ep 2 (Step 008200): Train loss 4.345, Val loss 4.388\n",
      "Ep 2 (Step 008210): Train loss 4.369, Val loss 4.378\n",
      "Ep 2 (Step 008220): Train loss 4.441, Val loss 4.376\n",
      "Ep 2 (Step 008230): Train loss 4.278, Val loss 4.380\n",
      "Ep 2 (Step 008240): Train loss 4.308, Val loss 4.378\n",
      "Ep 2 (Step 008250): Train loss 4.377, Val loss 4.382\n",
      "Ep 2 (Step 008260): Train loss 4.424, Val loss 4.370\n",
      "Ep 2 (Step 008270): Train loss 4.337, Val loss 4.368\n",
      "Ep 2 (Step 008280): Train loss 4.277, Val loss 4.379\n",
      "Ep 2 (Step 008290): Train loss 4.444, Val loss 4.398\n",
      "Ep 2 (Step 008300): Train loss 4.363, Val loss 4.353\n",
      "Ep 2 (Step 008310): Train loss 4.412, Val loss 4.353\n",
      "Ep 2 (Step 008320): Train loss 4.420, Val loss 4.339\n",
      "Ep 2 (Step 008330): Train loss 4.457, Val loss 4.350\n",
      "Ep 2 (Step 008340): Train loss 4.420, Val loss 4.363\n",
      "Ep 2 (Step 008350): Train loss 4.371, Val loss 4.345\n",
      "Ep 2 (Step 008360): Train loss 4.495, Val loss 4.337\n",
      "Ep 2 (Step 008370): Train loss 4.415, Val loss 4.363\n",
      "Ep 2 (Step 008380): Train loss 4.304, Val loss 4.332\n",
      "Ep 2 (Step 008390): Train loss 4.262, Val loss 4.341\n",
      "Ep 2 (Step 008400): Train loss 4.476, Val loss 4.340\n",
      "Ep 2 (Step 008410): Train loss 4.369, Val loss 4.351\n",
      "Ep 2 (Step 008420): Train loss 4.243, Val loss 4.344\n",
      "Ep 2 (Step 008430): Train loss 4.433, Val loss 4.362\n",
      "Ep 2 (Step 008440): Train loss 4.360, Val loss 4.321\n",
      "Ep 2 (Step 008450): Train loss 4.449, Val loss 4.340\n",
      "Ep 2 (Step 008460): Train loss 4.469, Val loss 4.331\n",
      "Ep 2 (Step 008470): Train loss 4.416, Val loss 4.328\n",
      "Ep 2 (Step 008480): Train loss 4.344, Val loss 4.343\n",
      "Ep 2 (Step 008490): Train loss 4.368, Val loss 4.342\n",
      "Ep 2 (Step 008500): Train loss 4.388, Val loss 4.333\n",
      "Ep 2 (Step 008510): Train loss 4.452, Val loss 4.333\n",
      "Ep 2 (Step 008520): Train loss 4.285, Val loss 4.333\n",
      "Ep 2 (Step 008530): Train loss 4.423, Val loss 4.352\n",
      "Ep 2 (Step 008540): Train loss 4.433, Val loss 4.335\n",
      "Ep 2 (Step 008550): Train loss 4.314, Val loss 4.351\n",
      "Ep 2 (Step 008560): Train loss 4.397, Val loss 4.341\n",
      "Ep 2 (Step 008570): Train loss 4.232, Val loss 4.336\n",
      "Ep 2 (Step 008580): Train loss 4.347, Val loss 4.352\n",
      "Ep 2 (Step 008590): Train loss 4.313, Val loss 4.336\n",
      "Ep 2 (Step 008600): Train loss 4.288, Val loss 4.345\n",
      "Ep 2 (Step 008610): Train loss 4.329, Val loss 4.342\n",
      "Ep 2 (Step 008620): Train loss 4.384, Val loss 4.327\n",
      "Ep 2 (Step 008630): Train loss 4.288, Val loss 4.344\n",
      "Ep 2 (Step 008640): Train loss 4.419, Val loss 4.341\n",
      "Ep 2 (Step 008650): Train loss 4.410, Val loss 4.339\n",
      "Ep 2 (Step 008660): Train loss 4.278, Val loss 4.344\n",
      "Ep 2 (Step 008670): Train loss 4.432, Val loss 4.365\n",
      "Ep 2 (Step 008680): Train loss 4.291, Val loss 4.335\n",
      "Ep 2 (Step 008690): Train loss 4.341, Val loss 4.338\n",
      "Ep 2 (Step 008700): Train loss 4.318, Val loss 4.346\n",
      "Ep 2 (Step 008710): Train loss 4.490, Val loss 4.340\n",
      "Ep 2 (Step 008720): Train loss 4.255, Val loss 4.338\n",
      "Ep 2 (Step 008730): Train loss 4.310, Val loss 4.348\n",
      "Ep 2 (Step 008740): Train loss 4.370, Val loss 4.343\n",
      "Ep 2 (Step 008750): Train loss 4.470, Val loss 4.342\n",
      "Ep 2 (Step 008760): Train loss 4.309, Val loss 4.351\n",
      "Ep 2 (Step 008770): Train loss 4.337, Val loss 4.340\n",
      "Ep 2 (Step 008780): Train loss 4.415, Val loss 4.350\n",
      "Ep 2 (Step 008790): Train loss 4.449, Val loss 4.343\n",
      "Ep 2 (Step 008800): Train loss 4.269, Val loss 4.343\n",
      "Ep 2 (Step 008810): Train loss 4.439, Val loss 4.351\n",
      "Ep 2 (Step 008820): Train loss 4.251, Val loss 4.337\n",
      "Ep 2 (Step 008830): Train loss 4.256, Val loss 4.332\n",
      "Ep 2 (Step 008840): Train loss 4.290, Val loss 4.333\n",
      "Ep 2 (Step 008850): Train loss 4.338, Val loss 4.360\n",
      "Ep 2 (Step 008860): Train loss 4.419, Val loss 4.338\n",
      "Ep 2 (Step 008870): Train loss 4.410, Val loss 4.324\n",
      "Ep 2 (Step 008880): Train loss 4.348, Val loss 4.316\n",
      "Ep 2 (Step 008890): Train loss 4.424, Val loss 4.328\n",
      "Ep 2 (Step 008900): Train loss 4.296, Val loss 4.290\n",
      "Ep 2 (Step 008910): Train loss 4.412, Val loss 4.296\n",
      "Ep 2 (Step 008920): Train loss 4.337, Val loss 4.297\n",
      "Ep 2 (Step 008930): Train loss 4.280, Val loss 4.288\n",
      "Ep 2 (Step 008940): Train loss 4.325, Val loss 4.289\n",
      "Ep 2 (Step 008950): Train loss 4.282, Val loss 4.295\n",
      "Ep 2 (Step 008960): Train loss 4.309, Val loss 4.297\n",
      "Ep 2 (Step 008970): Train loss 4.370, Val loss 4.309\n",
      "Ep 2 (Step 008980): Train loss 4.357, Val loss 4.296\n",
      "Ep 2 (Step 008990): Train loss 4.411, Val loss 4.319\n",
      "Ep 2 (Step 009000): Train loss 4.376, Val loss 4.323\n",
      "Ep 2 (Step 009010): Train loss 4.322, Val loss 4.312\n",
      "Ep 2 (Step 009020): Train loss 4.435, Val loss 4.321\n",
      "Ep 2 (Step 009030): Train loss 4.314, Val loss 4.321\n",
      "Ep 2 (Step 009040): Train loss 4.382, Val loss 4.310\n",
      "Ep 2 (Step 009050): Train loss 4.314, Val loss 4.307\n",
      "Ep 2 (Step 009060): Train loss 4.311, Val loss 4.305\n",
      "Ep 2 (Step 009070): Train loss 4.354, Val loss 4.313\n",
      "Ep 2 (Step 009080): Train loss 4.384, Val loss 4.303\n",
      "Ep 2 (Step 009090): Train loss 4.358, Val loss 4.331\n",
      "Ep 2 (Step 009100): Train loss 4.216, Val loss 4.333\n",
      "Ep 2 (Step 009110): Train loss 4.319, Val loss 4.308\n",
      "Ep 2 (Step 009120): Train loss 4.389, Val loss 4.327\n",
      "Ep 2 (Step 009130): Train loss 4.216, Val loss 4.334\n",
      "Ep 2 (Step 009140): Train loss 4.363, Val loss 4.330\n",
      "Ep 2 (Step 009150): Train loss 4.382, Val loss 4.319\n",
      "Ep 2 (Step 009160): Train loss 4.315, Val loss 4.317\n",
      "Ep 2 (Step 009170): Train loss 4.470, Val loss 4.325\n",
      "Ep 2 (Step 009180): Train loss 4.359, Val loss 4.333\n",
      "Ep 2 (Step 009190): Train loss 4.309, Val loss 4.330\n",
      "Ep 2 (Step 009200): Train loss 4.233, Val loss 4.323\n",
      "Ep 2 (Step 009210): Train loss 4.257, Val loss 4.316\n",
      "Ep 2 (Step 009220): Train loss 4.409, Val loss 4.328\n",
      "Ep 2 (Step 009230): Train loss 4.446, Val loss 4.325\n",
      "Ep 2 (Step 009240): Train loss 4.333, Val loss 4.325\n",
      "Ep 2 (Step 009250): Train loss 4.373, Val loss 4.304\n",
      "Ep 2 (Step 009260): Train loss 4.303, Val loss 4.304\n",
      "Ep 2 (Step 009270): Train loss 4.320, Val loss 4.304\n",
      "Ep 2 (Step 009280): Train loss 4.406, Val loss 4.317\n",
      "Ep 2 (Step 009290): Train loss 4.379, Val loss 4.322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2 (Step 009300): Train loss 4.248, Val loss 4.307\n",
      "Ep 2 (Step 009310): Train loss 4.544, Val loss 4.311\n",
      "Ep 2 (Step 009320): Train loss 4.301, Val loss 4.316\n",
      "Ep 2 (Step 009330): Train loss 4.232, Val loss 4.313\n",
      "Ep 2 (Step 009340): Train loss 4.464, Val loss 4.304\n",
      "Ep 2 (Step 009350): Train loss 4.387, Val loss 4.290\n",
      "Ep 2 (Step 009360): Train loss 4.212, Val loss 4.277\n",
      "Ep 2 (Step 009370): Train loss 4.461, Val loss 4.274\n",
      "Ep 2 (Step 009380): Train loss 4.341, Val loss 4.259\n",
      "Ep 2 (Step 009390): Train loss 4.426, Val loss 4.267\n",
      "Ep 2 (Step 009400): Train loss 4.291, Val loss 4.258\n",
      "Ep 2 (Step 009410): Train loss 4.321, Val loss 4.257\n",
      "Ep 2 (Step 009420): Train loss 4.614, Val loss 4.245\n",
      "Ep 2 (Step 009430): Train loss 4.248, Val loss 4.252\n",
      "Ep 2 (Step 009440): Train loss 4.277, Val loss 4.269\n",
      "Ep 2 (Step 009450): Train loss 4.285, Val loss 4.255\n",
      "Ep 2 (Step 009460): Train loss 4.357, Val loss 4.264\n",
      "Ep 2 (Step 009470): Train loss 4.498, Val loss 4.278\n",
      "Ep 2 (Step 009480): Train loss 4.568, Val loss 4.264\n",
      "Ep 2 (Step 009490): Train loss 4.241, Val loss 4.252\n",
      "Ep 2 (Step 009500): Train loss 4.444, Val loss 4.263\n",
      "Ep 2 (Step 009510): Train loss 4.215, Val loss 4.254\n",
      "Ep 2 (Step 009520): Train loss 4.300, Val loss 4.246\n",
      "Ep 2 (Step 009530): Train loss 4.345, Val loss 4.284\n",
      "Ep 2 (Step 009540): Train loss 4.385, Val loss 4.273\n",
      "Ep 2 (Step 009550): Train loss 4.257, Val loss 4.269\n",
      "Ep 2 (Step 009560): Train loss 4.321, Val loss 4.280\n",
      "Ep 2 (Step 009570): Train loss 4.207, Val loss 4.266\n",
      "Ep 2 (Step 009580): Train loss 4.198, Val loss 4.284\n",
      "Ep 2 (Step 009590): Train loss 4.296, Val loss 4.268\n",
      "Ep 2 (Step 009600): Train loss 4.347, Val loss 4.284\n",
      "Ep 2 (Step 009610): Train loss 4.412, Val loss 4.273\n",
      "Ep 2 (Step 009620): Train loss 4.444, Val loss 4.278\n",
      "Ep 2 (Step 009630): Train loss 4.402, Val loss 4.279\n",
      "Ep 2 (Step 009640): Train loss 4.190, Val loss 4.296\n",
      "Ep 2 (Step 009650): Train loss 4.300, Val loss 4.289\n",
      "Ep 2 (Step 009660): Train loss 4.308, Val loss 4.285\n",
      "Ep 2 (Step 009670): Train loss 4.218, Val loss 4.290\n",
      "Ep 2 (Step 009680): Train loss 4.207, Val loss 4.287\n",
      "Ep 2 (Step 009690): Train loss 4.364, Val loss 4.292\n",
      "Ep 2 (Step 009700): Train loss 4.446, Val loss 4.297\n",
      "Ep 2 (Step 009710): Train loss 4.389, Val loss 4.291\n",
      "Ep 2 (Step 009720): Train loss 4.272, Val loss 4.281\n",
      "Ep 2 (Step 009730): Train loss 4.336, Val loss 4.278\n",
      "Ep 2 (Step 009740): Train loss 4.273, Val loss 4.290\n",
      "Ep 2 (Step 009750): Train loss 4.180, Val loss 4.285\n",
      "Ep 2 (Step 009760): Train loss 4.303, Val loss 4.284\n",
      "Ep 2 (Step 009770): Train loss 4.425, Val loss 4.279\n",
      "Ep 2 (Step 009780): Train loss 4.196, Val loss 4.279\n",
      "Ep 2 (Step 009790): Train loss 4.190, Val loss 4.286\n",
      "Ep 2 (Step 009800): Train loss 4.331, Val loss 4.306\n",
      "Ep 2 (Step 009810): Train loss 4.376, Val loss 4.279\n",
      "Ep 2 (Step 009820): Train loss 4.268, Val loss 4.273\n",
      "Ep 2 (Step 009830): Train loss 4.372, Val loss 4.263\n",
      "Ep 2 (Step 009840): Train loss 4.349, Val loss 4.271\n",
      "Ep 2 (Step 009850): Train loss 4.318, Val loss 4.283\n",
      "Ep 2 (Step 009860): Train loss 4.245, Val loss 4.284\n",
      "Ep 2 (Step 009870): Train loss 4.234, Val loss 4.269\n",
      "Ep 2 (Step 009880): Train loss 4.179, Val loss 4.286\n",
      "Ep 2 (Step 009890): Train loss 4.389, Val loss 4.298\n",
      "Ep 2 (Step 009900): Train loss 4.294, Val loss 4.271\n",
      "Ep 2 (Step 009910): Train loss 4.251, Val loss 4.275\n",
      "Ep 2 (Step 009920): Train loss 4.250, Val loss 4.287\n",
      "Ep 2 (Step 009930): Train loss 4.270, Val loss 4.281\n",
      "Ep 2 (Step 009940): Train loss 4.252, Val loss 4.291\n",
      "Ep 2 (Step 009950): Train loss 4.345, Val loss 4.305\n",
      "Ep 2 (Step 009960): Train loss 4.303, Val loss 4.307\n",
      "Ep 2 (Step 009970): Train loss 4.331, Val loss 4.308\n",
      "Ep 2 (Step 009980): Train loss 4.134, Val loss 4.316\n",
      "Ep 2 (Step 009990): Train loss 4.332, Val loss 4.315\n",
      "Ep 2 (Step 010000): Train loss 4.326, Val loss 4.301\n",
      "Ep 2 (Step 010010): Train loss 4.241, Val loss 4.303\n",
      "Ep 2 (Step 010020): Train loss 4.186, Val loss 4.289\n",
      "Ep 2 (Step 010030): Train loss 4.312, Val loss 4.298\n",
      "Ep 2 (Step 010040): Train loss 4.298, Val loss 4.307\n",
      "Ep 2 (Step 010050): Train loss 4.371, Val loss 4.304\n",
      "Ep 2 (Step 010060): Train loss 4.286, Val loss 4.314\n",
      "Ep 2 (Step 010070): Train loss 4.236, Val loss 4.309\n",
      "Ep 2 (Step 010080): Train loss 4.363, Val loss 4.306\n",
      "Ep 2 (Step 010090): Train loss 4.277, Val loss 4.291\n",
      "Ep 2 (Step 010100): Train loss 4.246, Val loss 4.298\n",
      "Ep 2 (Step 010110): Train loss 4.326, Val loss 4.291\n",
      "Ep 2 (Step 010120): Train loss 4.286, Val loss 4.282\n",
      "Ep 2 (Step 010130): Train loss 4.293, Val loss 4.304\n",
      "Ep 2 (Step 010140): Train loss 4.370, Val loss 4.290\n",
      "Ep 2 (Step 010150): Train loss 4.356, Val loss 4.277\n",
      "Ep 2 (Step 010160): Train loss 4.434, Val loss 4.294\n",
      "Ep 2 (Step 010170): Train loss 4.413, Val loss 4.276\n",
      "Ep 2 (Step 010180): Train loss 4.251, Val loss 4.291\n",
      "Ep 2 (Step 010190): Train loss 4.209, Val loss 4.300\n",
      "Ep 2 (Step 010200): Train loss 4.476, Val loss 4.294\n",
      "Ep 2 (Step 010210): Train loss 4.133, Val loss 4.290\n",
      "Ep 2 (Step 010220): Train loss 4.215, Val loss 4.281\n",
      "Ep 2 (Step 010230): Train loss 4.304, Val loss 4.301\n",
      "Ep 2 (Step 010240): Train loss 4.282, Val loss 4.292\n",
      "Ep 2 (Step 010250): Train loss 4.250, Val loss 4.289\n",
      "Ep 2 (Step 010260): Train loss 4.346, Val loss 4.295\n",
      "Ep 2 (Step 010270): Train loss 4.315, Val loss 4.303\n",
      "Ep 2 (Step 010280): Train loss 4.388, Val loss 4.305\n",
      "Ep 2 (Step 010290): Train loss 4.185, Val loss 4.296\n",
      "Ep 2 (Step 010300): Train loss 4.279, Val loss 4.294\n",
      "Ep 2 (Step 010310): Train loss 4.292, Val loss 4.297\n",
      "Ep 2 (Step 010320): Train loss 4.213, Val loss 4.308\n",
      "Ep 2 (Step 010330): Train loss 4.194, Val loss 4.323\n",
      "Ep 2 (Step 010340): Train loss 4.277, Val loss 4.327\n",
      "Ep 2 (Step 010350): Train loss 4.292, Val loss 4.322\n",
      "Ep 2 (Step 010360): Train loss 4.303, Val loss 4.319\n",
      "Ep 2 (Step 010370): Train loss 4.307, Val loss 4.312\n",
      "Ep 2 (Step 010380): Train loss 4.289, Val loss 4.312\n",
      "Ep 2 (Step 010390): Train loss 4.370, Val loss 4.302\n",
      "Ep 2 (Step 010400): Train loss 4.301, Val loss 4.313\n",
      "Ep 2 (Step 010410): Train loss 4.258, Val loss 4.309\n",
      "Ep 2 (Step 010420): Train loss 4.338, Val loss 4.313\n",
      "Ep 2 (Step 010430): Train loss 4.125, Val loss 4.288\n",
      "Ep 2 (Step 010440): Train loss 4.260, Val loss 4.323\n",
      "Ep 2 (Step 010450): Train loss 4.208, Val loss 4.313\n",
      "Ep 2 (Step 010460): Train loss 4.279, Val loss 4.310\n",
      "Ep 2 (Step 010470): Train loss 4.258, Val loss 4.314\n",
      "Ep 2 (Step 010480): Train loss 4.165, Val loss 4.305\n",
      "Ep 2 (Step 010490): Train loss 4.246, Val loss 4.327\n",
      "Ep 2 (Step 010500): Train loss 4.285, Val loss 4.301\n",
      "Ep 2 (Step 010510): Train loss 4.160, Val loss 4.303\n",
      "Ep 2 (Step 010520): Train loss 4.198, Val loss 4.313\n",
      "Ep 2 (Step 010530): Train loss 4.192, Val loss 4.300\n",
      "Ep 2 (Step 010540): Train loss 4.330, Val loss 4.318\n",
      "Ep 2 (Step 010550): Train loss 4.416, Val loss 4.319\n",
      "Ep 2 (Step 010560): Train loss 4.233, Val loss 4.311\n",
      "Ep 2 (Step 010570): Train loss 4.337, Val loss 4.280\n",
      "Ep 2 (Step 010580): Train loss 4.280, Val loss 4.269\n",
      "Ep 2 (Step 010590): Train loss 4.349, Val loss 4.283\n",
      "Ep 2 (Step 010600): Train loss 4.258, Val loss 4.293\n",
      "Ep 2 (Step 010610): Train loss 4.273, Val loss 4.305\n",
      "Ep 2 (Step 010620): Train loss 4.254, Val loss 4.299\n",
      "Ep 2 (Step 010630): Train loss 4.356, Val loss 4.286\n",
      "Ep 2 (Step 010640): Train loss 4.255, Val loss 4.290\n",
      "Ep 2 (Step 010650): Train loss 4.191, Val loss 4.282\n",
      "Ep 2 (Step 010660): Train loss 4.426, Val loss 4.296\n",
      "Ep 2 (Step 010670): Train loss 4.300, Val loss 4.287\n",
      "Ep 2 (Step 010680): Train loss 4.270, Val loss 4.288\n",
      "Ep 2 (Step 010690): Train loss 4.209, Val loss 4.263\n",
      "Ep 2 (Step 010700): Train loss 4.167, Val loss 4.256\n",
      "Ep 2 (Step 010710): Train loss 4.309, Val loss 4.239\n",
      "Ep 2 (Step 010720): Train loss 4.381, Val loss 4.267\n",
      "Ep 2 (Step 010730): Train loss 4.288, Val loss 4.270\n",
      "Ep 2 (Step 010740): Train loss 4.273, Val loss 4.279\n",
      "Ep 2 (Step 010750): Train loss 4.197, Val loss 4.245\n",
      "Ep 2 (Step 010760): Train loss 4.141, Val loss 4.257\n",
      "Ep 2 (Step 010770): Train loss 4.106, Val loss 4.284\n",
      "Ep 2 (Step 010780): Train loss 4.204, Val loss 4.272\n",
      "Ep 2 (Step 010790): Train loss 4.172, Val loss 4.271\n",
      "Ep 2 (Step 010800): Train loss 4.228, Val loss 4.295\n",
      "Ep 2 (Step 010810): Train loss 4.136, Val loss 4.269\n",
      "Ep 2 (Step 010820): Train loss 4.439, Val loss 4.238\n",
      "Ep 2 (Step 010830): Train loss 4.277, Val loss 4.242\n",
      "Ep 2 (Step 010840): Train loss 4.257, Val loss 4.245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2 (Step 010850): Train loss 4.194, Val loss 4.236\n",
      "Ep 2 (Step 010860): Train loss 4.141, Val loss 4.245\n",
      "Ep 2 (Step 010870): Train loss 4.301, Val loss 4.233\n",
      "Ep 2 (Step 010880): Train loss 4.202, Val loss 4.243\n",
      "Ep 2 (Step 010890): Train loss 4.225, Val loss 4.244\n",
      "Ep 2 (Step 010900): Train loss 4.234, Val loss 4.262\n",
      "Ep 2 (Step 010910): Train loss 4.370, Val loss 4.257\n",
      "Ep 2 (Step 010920): Train loss 4.086, Val loss 4.229\n",
      "Ep 2 (Step 010930): Train loss 4.140, Val loss 4.257\n",
      "Ep 2 (Step 010940): Train loss 4.096, Val loss 4.255\n",
      "Ep 2 (Step 010950): Train loss 4.226, Val loss 4.250\n",
      "Ep 2 (Step 010960): Train loss 4.275, Val loss 4.269\n",
      "Ep 2 (Step 010970): Train loss 4.159, Val loss 4.263\n",
      "Ep 2 (Step 010980): Train loss 4.209, Val loss 4.266\n",
      "Ep 2 (Step 010990): Train loss 4.258, Val loss 4.270\n",
      "Ep 2 (Step 011000): Train loss 4.236, Val loss 4.266\n",
      "Ep 2 (Step 011010): Train loss 4.213, Val loss 4.272\n",
      "Ep 2 (Step 011020): Train loss 4.320, Val loss 4.266\n",
      "Ep 2 (Step 011030): Train loss 4.167, Val loss 4.268\n",
      "Ep 2 (Step 011040): Train loss 4.237, Val loss 4.257\n",
      "Ep 2 (Step 011050): Train loss 4.212, Val loss 4.262\n",
      "Ep 2 (Step 011060): Train loss 4.217, Val loss 4.251\n",
      "Ep 2 (Step 011070): Train loss 4.329, Val loss 4.251\n",
      "Ep 2 (Step 011080): Train loss 4.230, Val loss 4.261\n",
      "Ep 2 (Step 011090): Train loss 4.177, Val loss 4.272\n",
      "Ep 2 (Step 011100): Train loss 4.291, Val loss 4.254\n",
      "Ep 2 (Step 011110): Train loss 4.189, Val loss 4.251\n",
      "Ep 2 (Step 011120): Train loss 4.251, Val loss 4.227\n",
      "Ep 2 (Step 011130): Train loss 4.293, Val loss 4.220\n",
      "Ep 2 (Step 011140): Train loss 4.085, Val loss 4.228\n",
      "Ep 2 (Step 011150): Train loss 4.166, Val loss 4.232\n",
      "Ep 2 (Step 011160): Train loss 4.088, Val loss 4.224\n",
      "Ep 2 (Step 011170): Train loss 4.291, Val loss 4.212\n",
      "Ep 2 (Step 011180): Train loss 4.282, Val loss 4.227\n",
      "Ep 2 (Step 011190): Train loss 4.269, Val loss 4.234\n",
      "Ep 2 (Step 011200): Train loss 4.295, Val loss 4.235\n",
      "Ep 2 (Step 011210): Train loss 4.190, Val loss 4.229\n",
      "Ep 2 (Step 011220): Train loss 4.243, Val loss 4.236\n",
      "Ep 2 (Step 011230): Train loss 4.295, Val loss 4.223\n",
      "Ep 2 (Step 011240): Train loss 4.257, Val loss 4.233\n",
      "Ep 2 (Step 011250): Train loss 4.303, Val loss 4.223\n",
      "Ep 2 (Step 011260): Train loss 4.197, Val loss 4.227\n",
      "Ep 2 (Step 011270): Train loss 4.252, Val loss 4.219\n",
      "Ep 2 (Step 011280): Train loss 4.244, Val loss 4.240\n",
      "Ep 2 (Step 011290): Train loss 4.399, Val loss 4.228\n",
      "Ep 2 (Step 011300): Train loss 4.219, Val loss 4.225\n",
      "Ep 2 (Step 011310): Train loss 4.145, Val loss 4.228\n",
      "Ep 2 (Step 011320): Train loss 4.261, Val loss 4.215\n",
      "Ep 2 (Step 011330): Train loss 4.276, Val loss 4.213\n",
      "Ep 2 (Step 011340): Train loss 4.420, Val loss 4.206\n",
      "Ep 2 (Step 011350): Train loss 4.235, Val loss 4.203\n",
      "Ep 2 (Step 011360): Train loss 4.181, Val loss 4.213\n",
      "Ep 2 (Step 011370): Train loss 4.300, Val loss 4.212\n",
      "Ep 2 (Step 011380): Train loss 4.241, Val loss 4.202\n",
      "Ep 2 (Step 011390): Train loss 4.201, Val loss 4.214\n",
      "Ep 2 (Step 011400): Train loss 4.157, Val loss 4.207\n",
      "Ep 2 (Step 011410): Train loss 4.179, Val loss 4.217\n",
      "Ep 2 (Step 011420): Train loss 4.340, Val loss 4.224\n",
      "Ep 2 (Step 011430): Train loss 4.268, Val loss 4.218\n",
      "Ep 2 (Step 011440): Train loss 4.185, Val loss 4.223\n",
      "Ep 2 (Step 011450): Train loss 4.186, Val loss 4.222\n",
      "Ep 2 (Step 011460): Train loss 4.209, Val loss 4.221\n",
      "Ep 2 (Step 011470): Train loss 4.234, Val loss 4.225\n",
      "Ep 2 (Step 011480): Train loss 4.266, Val loss 4.226\n",
      "Ep 2 (Step 011490): Train loss 4.310, Val loss 4.219\n",
      "Ep 2 (Step 011500): Train loss 4.287, Val loss 4.233\n",
      "Ep 2 (Step 011510): Train loss 4.265, Val loss 4.227\n",
      "Ep 2 (Step 011520): Train loss 4.299, Val loss 4.220\n",
      "Ep 2 (Step 011530): Train loss 4.319, Val loss 4.226\n",
      "Ep 2 (Step 011540): Train loss 4.174, Val loss 4.235\n",
      "Ep 2 (Step 011550): Train loss 4.254, Val loss 4.228\n",
      "Ep 2 (Step 011560): Train loss 4.246, Val loss 4.230\n",
      "Ep 2 (Step 011570): Train loss 4.376, Val loss 4.223\n",
      "Ep 2 (Step 011580): Train loss 4.383, Val loss 4.234\n",
      "Ep 2 (Step 011590): Train loss 4.142, Val loss 4.227\n",
      "Ep 2 (Step 011600): Train loss 4.075, Val loss 4.240\n",
      "Ep 2 (Step 011610): Train loss 4.362, Val loss 4.233\n",
      "Ep 2 (Step 011620): Train loss 4.119, Val loss 4.241\n",
      "Ep 2 (Step 011630): Train loss 4.247, Val loss 4.231\n",
      "Ep 2 (Step 011640): Train loss 4.132, Val loss 4.256\n",
      "Ep 2 (Step 011650): Train loss 4.288, Val loss 4.232\n",
      "Ep 2 (Step 011660): Train loss 4.091, Val loss 4.236\n",
      "Ep 2 (Step 011670): Train loss 4.235, Val loss 4.250\n",
      "Ep 2 (Step 011680): Train loss 4.290, Val loss 4.252\n",
      "Ep 2 (Step 011690): Train loss 4.238, Val loss 4.255\n",
      "Ep 2 (Step 011700): Train loss 4.223, Val loss 4.250\n",
      "Ep 2 (Step 011710): Train loss 4.207, Val loss 4.250\n",
      "Ep 2 (Step 011720): Train loss 4.342, Val loss 4.258\n",
      "Ep 2 (Step 011730): Train loss 4.148, Val loss 4.252\n",
      "Ep 2 (Step 011740): Train loss 4.104, Val loss 4.260\n",
      "Ep 2 (Step 011750): Train loss 4.327, Val loss 4.264\n",
      "Ep 2 (Step 011760): Train loss 4.151, Val loss 4.260\n",
      "Ep 2 (Step 011770): Train loss 4.257, Val loss 4.251\n",
      "Ep 2 (Step 011780): Train loss 4.217, Val loss 4.239\n",
      "Ep 2 (Step 011790): Train loss 4.197, Val loss 4.234\n",
      "Ep 2 (Step 011800): Train loss 4.018, Val loss 4.240\n",
      "Ep 2 (Step 011810): Train loss 4.262, Val loss 4.242\n",
      "Ep 2 (Step 011820): Train loss 4.170, Val loss 4.216\n",
      "Ep 2 (Step 011830): Train loss 4.208, Val loss 4.225\n",
      "Ep 2 (Step 011840): Train loss 4.266, Val loss 4.230\n",
      "Ep 2 (Step 011850): Train loss 4.161, Val loss 4.256\n",
      "Ep 2 (Step 011860): Train loss 4.323, Val loss 4.231\n",
      "Ep 2 (Step 011870): Train loss 4.202, Val loss 4.233\n",
      "Ep 2 (Step 011880): Train loss 4.213, Val loss 4.238\n",
      "Ep 2 (Step 011890): Train loss 4.247, Val loss 4.233\n",
      "Ep 2 (Step 011900): Train loss 4.159, Val loss 4.233\n",
      "Ep 2 (Step 011910): Train loss 4.199, Val loss 4.232\n",
      "Ep 2 (Step 011920): Train loss 4.253, Val loss 4.236\n",
      "Ep 2 (Step 011930): Train loss 4.137, Val loss 4.233\n",
      "Ep 2 (Step 011940): Train loss 4.300, Val loss 4.234\n",
      "Ep 2 (Step 011950): Train loss 4.330, Val loss 4.245\n",
      "Ep 2 (Step 011960): Train loss 4.215, Val loss 4.242\n",
      "Ep 2 (Step 011970): Train loss 4.153, Val loss 4.246\n",
      "Ep 2 (Step 011980): Train loss 4.310, Val loss 4.246\n",
      "Ep 2 (Step 011990): Train loss 4.310, Val loss 4.254\n",
      "Ep 2 (Step 012000): Train loss 4.297, Val loss 4.237\n",
      "Ep 2 (Step 012010): Train loss 4.207, Val loss 4.230\n",
      "Ep 2 (Step 012020): Train loss 4.203, Val loss 4.232\n",
      "Ep 2 (Step 012030): Train loss 4.297, Val loss 4.251\n",
      "Ep 2 (Step 012040): Train loss 4.244, Val loss 4.244\n",
      "Ep 2 (Step 012050): Train loss 4.178, Val loss 4.248\n",
      "Ep 2 (Step 012060): Train loss 4.150, Val loss 4.244\n",
      "Ep 2 (Step 012070): Train loss 4.229, Val loss 4.257\n",
      "Ep 2 (Step 012080): Train loss 4.325, Val loss 4.238\n",
      "Ep 2 (Step 012090): Train loss 4.145, Val loss 4.240\n",
      "Ep 2 (Step 012100): Train loss 4.200, Val loss 4.238\n",
      "Ep 2 (Step 012110): Train loss 4.137, Val loss 4.239\n",
      "Ep 2 (Step 012120): Train loss 4.065, Val loss 4.237\n",
      "Ep 2 (Step 012130): Train loss 4.111, Val loss 4.220\n",
      "Ep 2 (Step 012140): Train loss 4.098, Val loss 4.240\n",
      "Ep 2 (Step 012150): Train loss 4.099, Val loss 4.215\n",
      "Ep 2 (Step 012160): Train loss 4.227, Val loss 4.247\n",
      "Ep 2 (Step 012170): Train loss 4.320, Val loss 4.223\n",
      "Ep 2 (Step 012180): Train loss 4.108, Val loss 4.220\n",
      "Ep 2 (Step 012190): Train loss 4.216, Val loss 4.224\n",
      "Ep 2 (Step 012200): Train loss 4.090, Val loss 4.221\n",
      "Ep 2 (Step 012210): Train loss 4.216, Val loss 4.210\n",
      "Ep 2 (Step 012220): Train loss 4.116, Val loss 4.218\n",
      "Ep 2 (Step 012230): Train loss 4.120, Val loss 4.210\n",
      "Ep 2 (Step 012240): Train loss 4.036, Val loss 4.212\n",
      "Ep 2 (Step 012250): Train loss 4.360, Val loss 4.244\n",
      "Ep 2 (Step 012260): Train loss 4.141, Val loss 4.220\n",
      "Ep 2 (Step 012270): Train loss 4.203, Val loss 4.231\n",
      "Ep 2 (Step 012280): Train loss 4.280, Val loss 4.223\n",
      "Ep 2 (Step 012290): Train loss 4.146, Val loss 4.221\n",
      "Ep 2 (Step 012300): Train loss 4.250, Val loss 4.212\n",
      "Ep 2 (Step 012310): Train loss 4.120, Val loss 4.227\n",
      "Ep 2 (Step 012320): Train loss 4.197, Val loss 4.219\n",
      "Ep 2 (Step 012330): Train loss 4.183, Val loss 4.215\n",
      "Ep 2 (Step 012340): Train loss 4.054, Val loss 4.225\n",
      "Ep 2 (Step 012350): Train loss 4.184, Val loss 4.225\n",
      "Ep 2 (Step 012360): Train loss 4.156, Val loss 4.225\n",
      "Ep 2 (Step 012370): Train loss 4.253, Val loss 4.224\n",
      "Ep 2 (Step 012380): Train loss 4.131, Val loss 4.196\n",
      "Ep 2 (Step 012390): Train loss 4.079, Val loss 4.182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2 (Step 012400): Train loss 4.150, Val loss 4.201\n",
      "Ep 2 (Step 012410): Train loss 4.200, Val loss 4.194\n",
      "Ep 2 (Step 012420): Train loss 4.133, Val loss 4.198\n",
      "Ep 2 (Step 012430): Train loss 4.126, Val loss 4.194\n",
      "Ep 2 (Step 012440): Train loss 4.151, Val loss 4.194\n",
      "Ep 2 (Step 012450): Train loss 4.149, Val loss 4.194\n",
      "Ep 2 (Step 012460): Train loss 4.269, Val loss 4.204\n",
      "Ep 2 (Step 012470): Train loss 4.309, Val loss 4.212\n",
      "Ep 2 (Step 012480): Train loss 4.176, Val loss 4.195\n",
      "Ep 2 (Step 012490): Train loss 4.060, Val loss 4.191\n",
      "Ep 2 (Step 012500): Train loss 4.206, Val loss 4.204\n",
      "Ep 2 (Step 012510): Train loss 4.279, Val loss 4.193\n",
      "Ep 2 (Step 012520): Train loss 4.206, Val loss 4.202\n",
      "Ep 2 (Step 012530): Train loss 4.193, Val loss 4.201\n",
      "Ep 2 (Step 012540): Train loss 4.253, Val loss 4.207\n",
      "Ep 2 (Step 012550): Train loss 4.069, Val loss 4.196\n",
      "Ep 2 (Step 012560): Train loss 4.224, Val loss 4.183\n",
      "Ep 2 (Step 012570): Train loss 4.114, Val loss 4.207\n",
      "Ep 2 (Step 012580): Train loss 4.127, Val loss 4.208\n",
      "Ep 2 (Step 012590): Train loss 4.220, Val loss 4.195\n",
      "Ep 2 (Step 012600): Train loss 4.200, Val loss 4.202\n",
      "Ep 2 (Step 012610): Train loss 4.364, Val loss 4.211\n",
      "Ep 2 (Step 012620): Train loss 4.218, Val loss 4.208\n",
      "Ep 2 (Step 012630): Train loss 4.272, Val loss 4.223\n",
      "Ep 2 (Step 012640): Train loss 4.221, Val loss 4.213\n",
      "Ep 2 (Step 012650): Train loss 4.240, Val loss 4.213\n",
      "Ep 2 (Step 012660): Train loss 4.251, Val loss 4.206\n",
      "Ep 2 (Step 012670): Train loss 4.219, Val loss 4.211\n",
      "Ep 2 (Step 012680): Train loss 4.109, Val loss 4.196\n",
      "Ep 2 (Step 012690): Train loss 4.105, Val loss 4.180\n",
      "Ep 2 (Step 012700): Train loss 4.135, Val loss 4.180\n",
      "Ep 2 (Step 012710): Train loss 4.237, Val loss 4.193\n",
      "Ep 2 (Step 012720): Train loss 4.267, Val loss 4.173\n",
      "Ep 2 (Step 012730): Train loss 4.138, Val loss 4.192\n",
      "Ep 2 (Step 012740): Train loss 4.169, Val loss 4.183\n",
      "Ep 2 (Step 012750): Train loss 4.189, Val loss 4.168\n",
      "Ep 2 (Step 012760): Train loss 4.060, Val loss 4.177\n",
      "Ep 2 (Step 012770): Train loss 4.254, Val loss 4.171\n",
      "Ep 2 (Step 012780): Train loss 4.159, Val loss 4.180\n",
      "Ep 2 (Step 012790): Train loss 4.074, Val loss 4.204\n",
      "Ep 2 (Step 012800): Train loss 4.184, Val loss 4.175\n",
      "Ep 2 (Step 012810): Train loss 4.099, Val loss 4.158\n",
      "Ep 2 (Step 012820): Train loss 4.058, Val loss 4.162\n",
      "Ep 2 (Step 012830): Train loss 4.175, Val loss 4.155\n",
      "Ep 2 (Step 012840): Train loss 4.229, Val loss 4.150\n",
      "Ep 2 (Step 012850): Train loss 4.212, Val loss 4.144\n",
      "Ep 2 (Step 012860): Train loss 4.126, Val loss 4.153\n",
      "Ep 2 (Step 012870): Train loss 4.142, Val loss 4.162\n",
      "Ep 2 (Step 012880): Train loss 4.094, Val loss 4.164\n",
      "Ep 2 (Step 012890): Train loss 4.157, Val loss 4.177\n",
      "Ep 2 (Step 012900): Train loss 4.139, Val loss 4.149\n",
      "Ep 2 (Step 012910): Train loss 4.178, Val loss 4.132\n",
      "Ep 2 (Step 012920): Train loss 4.182, Val loss 4.159\n",
      "Ep 2 (Step 012930): Train loss 4.033, Val loss 4.142\n",
      "Ep 2 (Step 012940): Train loss 4.187, Val loss 4.154\n",
      "Ep 2 (Step 012950): Train loss 4.058, Val loss 4.129\n",
      "Ep 2 (Step 012960): Train loss 4.109, Val loss 4.163\n",
      "Ep 2 (Step 012970): Train loss 4.102, Val loss 4.144\n",
      "Ep 2 (Step 012980): Train loss 4.115, Val loss 4.152\n",
      "Ep 2 (Step 012990): Train loss 4.319, Val loss 4.141\n",
      "Ep 2 (Step 013000): Train loss 4.090, Val loss 4.142\n",
      "Ep 2 (Step 013010): Train loss 4.120, Val loss 4.135\n",
      "Ep 2 (Step 013020): Train loss 4.180, Val loss 4.132\n",
      "Ep 2 (Step 013030): Train loss 4.261, Val loss 4.154\n",
      "Ep 2 (Step 013040): Train loss 4.158, Val loss 4.142\n",
      "Ep 2 (Step 013050): Train loss 4.007, Val loss 4.143\n",
      "Ep 2 (Step 013060): Train loss 4.073, Val loss 4.141\n",
      "Ep 2 (Step 013070): Train loss 4.214, Val loss 4.161\n",
      "Ep 2 (Step 013080): Train loss 4.081, Val loss 4.149\n",
      "Ep 2 (Step 013090): Train loss 4.030, Val loss 4.138\n",
      "Ep 2 (Step 013100): Train loss 4.193, Val loss 4.133\n",
      "Ep 2 (Step 013110): Train loss 4.141, Val loss 4.154\n",
      "Ep 2 (Step 013120): Train loss 4.035, Val loss 4.160\n",
      "Ep 2 (Step 013130): Train loss 4.125, Val loss 4.146\n",
      "Ep 2 (Step 013140): Train loss 4.259, Val loss 4.114\n",
      "Ep 2 (Step 013150): Train loss 4.258, Val loss 4.131\n",
      "Ep 2 (Step 013160): Train loss 4.289, Val loss 4.132\n",
      "Ep 2 (Step 013170): Train loss 4.227, Val loss 4.145\n",
      "Ep 2 (Step 013180): Train loss 4.150, Val loss 4.135\n",
      "Ep 2 (Step 013190): Train loss 4.187, Val loss 4.146\n",
      "Ep 2 (Step 013200): Train loss 4.018, Val loss 4.127\n",
      "Ep 2 (Step 013210): Train loss 4.328, Val loss 4.130\n",
      "Ep 2 (Step 013220): Train loss 4.244, Val loss 4.131\n",
      "Ep 2 (Step 013230): Train loss 4.110, Val loss 4.124\n",
      "Ep 2 (Step 013240): Train loss 4.034, Val loss 4.125\n",
      "Ep 2 (Step 013250): Train loss 4.126, Val loss 4.128\n",
      "Ep 2 (Step 013260): Train loss 4.107, Val loss 4.115\n",
      "Ep 2 (Step 013270): Train loss 4.163, Val loss 4.119\n",
      "Ep 2 (Step 013280): Train loss 4.101, Val loss 4.127\n",
      "Ep 2 (Step 013290): Train loss 4.267, Val loss 4.124\n",
      "Ep 2 (Step 013300): Train loss 4.010, Val loss 4.108\n",
      "Ep 2 (Step 013310): Train loss 4.050, Val loss 4.115\n",
      "Ep 2 (Step 013320): Train loss 4.172, Val loss 4.118\n",
      "Ep 2 (Step 013330): Train loss 4.077, Val loss 4.122\n",
      "Ep 2 (Step 013340): Train loss 4.178, Val loss 4.123\n",
      "Ep 2 (Step 013350): Train loss 4.111, Val loss 4.127\n",
      "Ep 3 (Step 013360): Train loss 4.227, Val loss 4.130\n",
      "Ep 3 (Step 013370): Train loss 4.066, Val loss 4.130\n",
      "Ep 3 (Step 013380): Train loss 4.118, Val loss 4.126\n",
      "Ep 3 (Step 013390): Train loss 4.177, Val loss 4.123\n",
      "Ep 3 (Step 013400): Train loss 4.145, Val loss 4.141\n",
      "Ep 3 (Step 013410): Train loss 4.220, Val loss 4.139\n",
      "Ep 3 (Step 013420): Train loss 4.078, Val loss 4.121\n",
      "Ep 3 (Step 013430): Train loss 4.077, Val loss 4.128\n",
      "Ep 3 (Step 013440): Train loss 4.081, Val loss 4.138\n",
      "Ep 3 (Step 013450): Train loss 4.045, Val loss 4.132\n",
      "Ep 3 (Step 013460): Train loss 4.259, Val loss 4.140\n",
      "Ep 3 (Step 013470): Train loss 4.105, Val loss 4.143\n",
      "Ep 3 (Step 013480): Train loss 4.095, Val loss 4.168\n",
      "Ep 3 (Step 013490): Train loss 4.090, Val loss 4.142\n",
      "Ep 3 (Step 013500): Train loss 4.142, Val loss 4.132\n",
      "Ep 3 (Step 013510): Train loss 4.093, Val loss 4.141\n",
      "Ep 3 (Step 013520): Train loss 4.202, Val loss 4.152\n",
      "Ep 3 (Step 013530): Train loss 4.142, Val loss 4.152\n",
      "Ep 3 (Step 013540): Train loss 4.092, Val loss 4.161\n",
      "Ep 3 (Step 013550): Train loss 4.102, Val loss 4.158\n",
      "Ep 3 (Step 013560): Train loss 4.085, Val loss 4.144\n",
      "Ep 3 (Step 013570): Train loss 4.205, Val loss 4.132\n",
      "Ep 3 (Step 013580): Train loss 4.183, Val loss 4.132\n",
      "Ep 3 (Step 013590): Train loss 4.093, Val loss 4.132\n",
      "Ep 3 (Step 013600): Train loss 4.125, Val loss 4.133\n",
      "Ep 3 (Step 013610): Train loss 4.258, Val loss 4.120\n",
      "Ep 3 (Step 013620): Train loss 4.270, Val loss 4.123\n",
      "Ep 3 (Step 013630): Train loss 4.095, Val loss 4.137\n",
      "Ep 3 (Step 013640): Train loss 4.026, Val loss 4.133\n",
      "Ep 3 (Step 013650): Train loss 4.085, Val loss 4.132\n",
      "Ep 3 (Step 013660): Train loss 4.124, Val loss 4.136\n",
      "Ep 3 (Step 013670): Train loss 4.208, Val loss 4.128\n",
      "Ep 3 (Step 013680): Train loss 4.044, Val loss 4.135\n",
      "Ep 3 (Step 013690): Train loss 4.126, Val loss 4.113\n",
      "Ep 3 (Step 013700): Train loss 4.120, Val loss 4.128\n",
      "Ep 3 (Step 013710): Train loss 4.104, Val loss 4.112\n",
      "Ep 3 (Step 013720): Train loss 4.314, Val loss 4.132\n",
      "Ep 3 (Step 013730): Train loss 4.066, Val loss 4.119\n",
      "Ep 3 (Step 013740): Train loss 4.087, Val loss 4.135\n",
      "Ep 3 (Step 013750): Train loss 4.116, Val loss 4.126\n",
      "Ep 3 (Step 013760): Train loss 4.075, Val loss 4.127\n",
      "Ep 3 (Step 013770): Train loss 4.191, Val loss 4.127\n",
      "Ep 3 (Step 013780): Train loss 3.992, Val loss 4.126\n",
      "Ep 3 (Step 013790): Train loss 4.060, Val loss 4.144\n",
      "Ep 3 (Step 013800): Train loss 4.082, Val loss 4.124\n",
      "Ep 3 (Step 013810): Train loss 4.380, Val loss 4.127\n",
      "Ep 3 (Step 013820): Train loss 4.080, Val loss 4.128\n",
      "Ep 3 (Step 013830): Train loss 4.223, Val loss 4.134\n",
      "Ep 3 (Step 013840): Train loss 4.037, Val loss 4.134\n",
      "Ep 3 (Step 013850): Train loss 4.044, Val loss 4.128\n",
      "Ep 3 (Step 013860): Train loss 4.147, Val loss 4.138\n",
      "Ep 3 (Step 013870): Train loss 4.064, Val loss 4.134\n",
      "Ep 3 (Step 013880): Train loss 4.097, Val loss 4.148\n",
      "Ep 3 (Step 013890): Train loss 4.253, Val loss 4.119\n",
      "Ep 3 (Step 013900): Train loss 4.230, Val loss 4.135\n",
      "Ep 3 (Step 013910): Train loss 4.040, Val loss 4.155\n",
      "Ep 3 (Step 013920): Train loss 4.081, Val loss 4.142\n",
      "Ep 3 (Step 013930): Train loss 4.068, Val loss 4.148\n",
      "Ep 3 (Step 013940): Train loss 4.023, Val loss 4.151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3 (Step 013950): Train loss 4.148, Val loss 4.148\n",
      "Ep 3 (Step 013960): Train loss 4.185, Val loss 4.141\n",
      "Ep 3 (Step 013970): Train loss 3.996, Val loss 4.159\n",
      "Ep 3 (Step 013980): Train loss 4.210, Val loss 4.139\n",
      "Ep 3 (Step 013990): Train loss 3.994, Val loss 4.148\n",
      "Ep 3 (Step 014000): Train loss 4.187, Val loss 4.145\n",
      "Ep 3 (Step 014010): Train loss 4.171, Val loss 4.130\n",
      "Ep 3 (Step 014020): Train loss 4.134, Val loss 4.132\n",
      "Ep 3 (Step 014030): Train loss 4.222, Val loss 4.147\n",
      "Ep 3 (Step 014040): Train loss 4.137, Val loss 4.140\n",
      "Ep 3 (Step 014050): Train loss 4.181, Val loss 4.125\n",
      "Ep 3 (Step 014060): Train loss 4.233, Val loss 4.149\n",
      "Ep 3 (Step 014070): Train loss 4.129, Val loss 4.145\n",
      "Ep 3 (Step 014080): Train loss 3.997, Val loss 4.137\n",
      "Ep 3 (Step 014090): Train loss 4.162, Val loss 4.130\n",
      "Ep 3 (Step 014100): Train loss 4.212, Val loss 4.141\n",
      "Ep 3 (Step 014110): Train loss 4.230, Val loss 4.163\n",
      "Ep 3 (Step 014120): Train loss 4.163, Val loss 4.141\n",
      "Ep 3 (Step 014130): Train loss 4.094, Val loss 4.144\n",
      "Ep 3 (Step 014140): Train loss 4.017, Val loss 4.146\n",
      "Ep 3 (Step 014150): Train loss 4.275, Val loss 4.137\n",
      "Ep 3 (Step 014160): Train loss 4.230, Val loss 4.156\n",
      "Ep 3 (Step 014170): Train loss 4.174, Val loss 4.130\n",
      "Ep 3 (Step 014180): Train loss 4.075, Val loss 4.130\n",
      "Ep 3 (Step 014190): Train loss 4.244, Val loss 4.131\n",
      "Ep 3 (Step 014200): Train loss 4.135, Val loss 4.140\n",
      "Ep 3 (Step 014210): Train loss 3.984, Val loss 4.133\n",
      "Ep 3 (Step 014220): Train loss 4.050, Val loss 4.134\n",
      "Ep 3 (Step 014230): Train loss 4.159, Val loss 4.149\n",
      "Ep 3 (Step 014240): Train loss 4.120, Val loss 4.127\n",
      "Ep 3 (Step 014250): Train loss 4.201, Val loss 4.127\n",
      "Ep 3 (Step 014260): Train loss 4.110, Val loss 4.137\n",
      "Ep 3 (Step 014270): Train loss 4.044, Val loss 4.133\n",
      "Ep 3 (Step 014280): Train loss 4.019, Val loss 4.124\n",
      "Ep 3 (Step 014290): Train loss 4.116, Val loss 4.141\n",
      "Ep 3 (Step 014300): Train loss 4.027, Val loss 4.143\n",
      "Ep 3 (Step 014310): Train loss 4.068, Val loss 4.125\n",
      "Ep 3 (Step 014320): Train loss 4.200, Val loss 4.133\n",
      "Ep 3 (Step 014330): Train loss 4.141, Val loss 4.137\n",
      "Ep 3 (Step 014340): Train loss 4.075, Val loss 4.147\n",
      "Ep 3 (Step 014350): Train loss 4.019, Val loss 4.151\n",
      "Ep 3 (Step 014360): Train loss 4.024, Val loss 4.146\n",
      "Ep 3 (Step 014370): Train loss 4.117, Val loss 4.146\n",
      "Ep 3 (Step 014380): Train loss 4.029, Val loss 4.149\n",
      "Ep 3 (Step 014390): Train loss 4.081, Val loss 4.145\n",
      "Ep 3 (Step 014400): Train loss 4.026, Val loss 4.148\n",
      "Ep 3 (Step 014410): Train loss 4.096, Val loss 4.149\n",
      "Ep 3 (Step 014420): Train loss 4.169, Val loss 4.138\n",
      "Ep 3 (Step 014430): Train loss 4.120, Val loss 4.135\n",
      "Ep 3 (Step 014440): Train loss 4.086, Val loss 4.171\n",
      "Ep 3 (Step 014450): Train loss 4.039, Val loss 4.166\n",
      "Ep 3 (Step 014460): Train loss 4.122, Val loss 4.139\n",
      "Ep 3 (Step 014470): Train loss 4.061, Val loss 4.169\n",
      "Ep 3 (Step 014480): Train loss 4.224, Val loss 4.153\n",
      "Ep 3 (Step 014490): Train loss 4.078, Val loss 4.164\n",
      "Ep 3 (Step 014500): Train loss 4.105, Val loss 4.158\n",
      "Ep 3 (Step 014510): Train loss 4.119, Val loss 4.160\n",
      "Ep 3 (Step 014520): Train loss 4.118, Val loss 4.151\n",
      "Ep 3 (Step 014530): Train loss 4.133, Val loss 4.160\n",
      "Ep 3 (Step 014540): Train loss 4.072, Val loss 4.157\n",
      "Ep 3 (Step 014550): Train loss 4.053, Val loss 4.160\n",
      "Ep 3 (Step 014560): Train loss 3.956, Val loss 4.147\n",
      "Ep 3 (Step 014570): Train loss 4.131, Val loss 4.150\n",
      "Ep 3 (Step 014580): Train loss 4.170, Val loss 4.147\n",
      "Ep 3 (Step 014590): Train loss 4.102, Val loss 4.142\n",
      "Ep 3 (Step 014600): Train loss 4.111, Val loss 4.127\n",
      "Ep 3 (Step 014610): Train loss 4.182, Val loss 4.140\n",
      "Ep 3 (Step 014620): Train loss 4.132, Val loss 4.140\n",
      "Ep 3 (Step 014630): Train loss 3.990, Val loss 4.144\n",
      "Ep 3 (Step 014640): Train loss 3.920, Val loss 4.143\n",
      "Ep 3 (Step 014650): Train loss 4.065, Val loss 4.140\n",
      "Ep 3 (Step 014660): Train loss 4.102, Val loss 4.137\n",
      "Ep 3 (Step 014670): Train loss 4.078, Val loss 4.142\n",
      "Ep 3 (Step 014680): Train loss 4.159, Val loss 4.131\n",
      "Ep 3 (Step 014690): Train loss 4.080, Val loss 4.144\n",
      "Ep 3 (Step 014700): Train loss 3.960, Val loss 4.145\n",
      "Ep 3 (Step 014710): Train loss 4.015, Val loss 4.140\n",
      "Ep 3 (Step 014720): Train loss 4.241, Val loss 4.148\n",
      "Ep 3 (Step 014730): Train loss 4.201, Val loss 4.161\n",
      "Ep 3 (Step 014740): Train loss 4.122, Val loss 4.150\n",
      "Ep 3 (Step 014750): Train loss 4.113, Val loss 4.158\n",
      "Ep 3 (Step 014760): Train loss 4.127, Val loss 4.170\n",
      "Ep 3 (Step 014770): Train loss 4.061, Val loss 4.154\n",
      "Ep 3 (Step 014780): Train loss 4.042, Val loss 4.162\n",
      "Ep 3 (Step 014790): Train loss 4.205, Val loss 4.170\n",
      "Ep 3 (Step 014800): Train loss 4.143, Val loss 4.156\n",
      "Ep 3 (Step 014810): Train loss 4.022, Val loss 4.163\n",
      "Ep 3 (Step 014820): Train loss 4.048, Val loss 4.161\n",
      "Ep 3 (Step 014830): Train loss 4.012, Val loss 4.169\n",
      "Ep 3 (Step 014840): Train loss 3.983, Val loss 4.165\n",
      "Ep 3 (Step 014850): Train loss 4.080, Val loss 4.161\n",
      "Ep 3 (Step 014860): Train loss 4.101, Val loss 4.147\n",
      "Ep 3 (Step 014870): Train loss 4.132, Val loss 4.136\n",
      "Ep 3 (Step 014880): Train loss 4.086, Val loss 4.144\n",
      "Ep 3 (Step 014890): Train loss 4.000, Val loss 4.146\n",
      "Ep 3 (Step 014900): Train loss 4.139, Val loss 4.147\n",
      "Ep 3 (Step 014910): Train loss 3.995, Val loss 4.145\n",
      "Ep 3 (Step 014920): Train loss 4.125, Val loss 4.166\n",
      "Ep 3 (Step 014930): Train loss 4.069, Val loss 4.143\n",
      "Ep 3 (Step 014940): Train loss 4.117, Val loss 4.142\n",
      "Ep 3 (Step 014950): Train loss 4.112, Val loss 4.145\n",
      "Ep 3 (Step 014960): Train loss 4.158, Val loss 4.131\n",
      "Ep 3 (Step 014970): Train loss 4.017, Val loss 4.133\n",
      "Ep 3 (Step 014980): Train loss 4.004, Val loss 4.148\n",
      "Ep 3 (Step 014990): Train loss 4.017, Val loss 4.142\n",
      "Ep 3 (Step 015000): Train loss 4.101, Val loss 4.170\n",
      "Ep 3 (Step 015010): Train loss 3.967, Val loss 4.148\n",
      "Ep 3 (Step 015020): Train loss 4.112, Val loss 4.162\n",
      "Ep 3 (Step 015030): Train loss 4.031, Val loss 4.165\n",
      "Ep 3 (Step 015040): Train loss 4.071, Val loss 4.163\n",
      "Ep 3 (Step 015050): Train loss 4.058, Val loss 4.158\n",
      "Ep 3 (Step 015060): Train loss 4.125, Val loss 4.147\n",
      "Ep 3 (Step 015070): Train loss 4.128, Val loss 4.151\n",
      "Ep 3 (Step 015080): Train loss 4.300, Val loss 4.149\n",
      "Ep 3 (Step 015090): Train loss 4.116, Val loss 4.167\n",
      "Ep 3 (Step 015100): Train loss 4.094, Val loss 4.156\n",
      "Ep 3 (Step 015110): Train loss 4.042, Val loss 4.147\n",
      "Ep 3 (Step 015120): Train loss 4.234, Val loss 4.144\n",
      "Ep 3 (Step 015130): Train loss 4.026, Val loss 4.140\n",
      "Ep 3 (Step 015140): Train loss 4.056, Val loss 4.154\n",
      "Ep 3 (Step 015150): Train loss 4.180, Val loss 4.131\n",
      "Ep 3 (Step 015160): Train loss 4.166, Val loss 4.144\n",
      "Ep 3 (Step 015170): Train loss 4.090, Val loss 4.134\n",
      "Ep 3 (Step 015180): Train loss 4.145, Val loss 4.139\n",
      "Ep 3 (Step 015190): Train loss 4.152, Val loss 4.155\n",
      "Ep 3 (Step 015200): Train loss 3.929, Val loss 4.151\n",
      "Ep 3 (Step 015210): Train loss 3.995, Val loss 4.146\n",
      "Ep 3 (Step 015220): Train loss 4.043, Val loss 4.139\n",
      "Ep 3 (Step 015230): Train loss 4.136, Val loss 4.149\n",
      "Ep 3 (Step 015240): Train loss 3.995, Val loss 4.128\n",
      "Ep 3 (Step 015250): Train loss 4.162, Val loss 4.132\n",
      "Ep 3 (Step 015260): Train loss 4.005, Val loss 4.139\n",
      "Ep 3 (Step 015270): Train loss 3.995, Val loss 4.145\n",
      "Ep 3 (Step 015280): Train loss 3.959, Val loss 4.142\n",
      "Ep 3 (Step 015290): Train loss 4.154, Val loss 4.138\n",
      "Ep 3 (Step 015300): Train loss 4.085, Val loss 4.126\n",
      "Ep 3 (Step 015310): Train loss 4.024, Val loss 4.118\n",
      "Ep 3 (Step 015320): Train loss 4.073, Val loss 4.115\n",
      "Ep 3 (Step 015330): Train loss 4.072, Val loss 4.113\n",
      "Ep 3 (Step 015340): Train loss 4.062, Val loss 4.122\n",
      "Ep 3 (Step 015350): Train loss 4.142, Val loss 4.119\n",
      "Ep 3 (Step 015360): Train loss 4.147, Val loss 4.116\n",
      "Ep 3 (Step 015370): Train loss 4.021, Val loss 4.119\n",
      "Ep 3 (Step 015380): Train loss 4.097, Val loss 4.119\n",
      "Ep 3 (Step 015390): Train loss 3.989, Val loss 4.127\n",
      "Ep 3 (Step 015400): Train loss 4.001, Val loss 4.129\n",
      "Ep 3 (Step 015410): Train loss 4.113, Val loss 4.141\n",
      "Ep 3 (Step 015420): Train loss 4.098, Val loss 4.130\n",
      "Ep 3 (Step 015430): Train loss 4.024, Val loss 4.123\n",
      "Ep 3 (Step 015440): Train loss 3.881, Val loss 4.115\n",
      "Ep 3 (Step 015450): Train loss 4.000, Val loss 4.129\n",
      "Ep 3 (Step 015460): Train loss 4.050, Val loss 4.118\n",
      "Ep 3 (Step 015470): Train loss 3.940, Val loss 4.137\n",
      "Ep 3 (Step 015480): Train loss 4.046, Val loss 4.119\n",
      "Ep 3 (Step 015490): Train loss 3.956, Val loss 4.128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3 (Step 015500): Train loss 4.058, Val loss 4.125\n",
      "Ep 3 (Step 015510): Train loss 4.097, Val loss 4.116\n",
      "Ep 3 (Step 015520): Train loss 3.971, Val loss 4.132\n",
      "Ep 3 (Step 015530): Train loss 4.181, Val loss 4.115\n",
      "Ep 3 (Step 015540): Train loss 4.102, Val loss 4.127\n",
      "Ep 3 (Step 015550): Train loss 4.125, Val loss 4.118\n",
      "Ep 3 (Step 015560): Train loss 3.992, Val loss 4.122\n",
      "Ep 3 (Step 015570): Train loss 4.027, Val loss 4.111\n",
      "Ep 3 (Step 015580): Train loss 4.036, Val loss 4.120\n",
      "Ep 3 (Step 015590): Train loss 4.037, Val loss 4.109\n",
      "Ep 3 (Step 015600): Train loss 4.043, Val loss 4.121\n",
      "Ep 3 (Step 015610): Train loss 4.133, Val loss 4.123\n",
      "Ep 3 (Step 015620): Train loss 4.041, Val loss 4.112\n",
      "Ep 3 (Step 015630): Train loss 4.026, Val loss 4.115\n",
      "Ep 3 (Step 015640): Train loss 4.033, Val loss 4.120\n",
      "Ep 3 (Step 015650): Train loss 3.978, Val loss 4.123\n",
      "Ep 3 (Step 015660): Train loss 3.992, Val loss 4.121\n",
      "Ep 3 (Step 015670): Train loss 4.088, Val loss 4.129\n",
      "Ep 3 (Step 015680): Train loss 3.980, Val loss 4.124\n",
      "Ep 3 (Step 015690): Train loss 4.092, Val loss 4.110\n",
      "Ep 3 (Step 015700): Train loss 4.104, Val loss 4.111\n",
      "Ep 3 (Step 015710): Train loss 4.111, Val loss 4.120\n",
      "Ep 3 (Step 015720): Train loss 4.092, Val loss 4.115\n",
      "Ep 3 (Step 015730): Train loss 4.056, Val loss 4.123\n",
      "Ep 3 (Step 015740): Train loss 3.964, Val loss 4.130\n",
      "Ep 3 (Step 015750): Train loss 4.013, Val loss 4.124\n",
      "Ep 3 (Step 015760): Train loss 3.991, Val loss 4.119\n",
      "Ep 3 (Step 015770): Train loss 3.977, Val loss 4.118\n",
      "Ep 3 (Step 015780): Train loss 4.035, Val loss 4.131\n",
      "Ep 3 (Step 015790): Train loss 4.012, Val loss 4.129\n",
      "Ep 3 (Step 015800): Train loss 4.053, Val loss 4.121\n",
      "Ep 3 (Step 015810): Train loss 4.139, Val loss 4.144\n",
      "Ep 3 (Step 015820): Train loss 4.153, Val loss 4.135\n",
      "Ep 3 (Step 015830): Train loss 4.042, Val loss 4.125\n",
      "Ep 3 (Step 015840): Train loss 4.128, Val loss 4.118\n",
      "Ep 3 (Step 015850): Train loss 4.106, Val loss 4.113\n",
      "Ep 3 (Step 015860): Train loss 3.957, Val loss 4.121\n",
      "Ep 3 (Step 015870): Train loss 4.070, Val loss 4.108\n",
      "Ep 3 (Step 015880): Train loss 4.159, Val loss 4.120\n",
      "Ep 3 (Step 015890): Train loss 4.089, Val loss 4.131\n",
      "Ep 3 (Step 015900): Train loss 4.164, Val loss 4.132\n",
      "Ep 3 (Step 015910): Train loss 4.072, Val loss 4.116\n",
      "Ep 3 (Step 015920): Train loss 3.997, Val loss 4.136\n",
      "Ep 3 (Step 015930): Train loss 4.071, Val loss 4.123\n",
      "Ep 3 (Step 015940): Train loss 4.067, Val loss 4.133\n",
      "Ep 3 (Step 015950): Train loss 4.059, Val loss 4.132\n",
      "Ep 3 (Step 015960): Train loss 3.995, Val loss 4.130\n",
      "Ep 3 (Step 015970): Train loss 4.020, Val loss 4.118\n",
      "Ep 3 (Step 015980): Train loss 4.054, Val loss 4.128\n",
      "Ep 3 (Step 015990): Train loss 4.168, Val loss 4.134\n",
      "Ep 3 (Step 016000): Train loss 4.106, Val loss 4.114\n",
      "Ep 3 (Step 016010): Train loss 4.065, Val loss 4.107\n",
      "Ep 3 (Step 016020): Train loss 4.042, Val loss 4.101\n",
      "Ep 3 (Step 016030): Train loss 3.978, Val loss 4.107\n",
      "Ep 3 (Step 016040): Train loss 3.959, Val loss 4.104\n",
      "Ep 3 (Step 016050): Train loss 4.119, Val loss 4.109\n",
      "Ep 3 (Step 016060): Train loss 4.149, Val loss 4.113\n",
      "Ep 3 (Step 016070): Train loss 4.041, Val loss 4.118\n",
      "Ep 3 (Step 016080): Train loss 4.147, Val loss 4.129\n",
      "Ep 3 (Step 016090): Train loss 4.030, Val loss 4.111\n",
      "Ep 3 (Step 016100): Train loss 4.002, Val loss 4.118\n",
      "Ep 3 (Step 016110): Train loss 4.002, Val loss 4.114\n",
      "Ep 3 (Step 016120): Train loss 3.891, Val loss 4.118\n",
      "Ep 3 (Step 016130): Train loss 4.076, Val loss 4.113\n",
      "Ep 3 (Step 016140): Train loss 4.031, Val loss 4.119\n",
      "Ep 3 (Step 016150): Train loss 3.984, Val loss 4.118\n",
      "Ep 3 (Step 016160): Train loss 3.969, Val loss 4.124\n",
      "Ep 3 (Step 016170): Train loss 4.049, Val loss 4.101\n",
      "Ep 3 (Step 016180): Train loss 4.066, Val loss 4.087\n",
      "Ep 3 (Step 016190): Train loss 4.027, Val loss 4.105\n",
      "Ep 3 (Step 016200): Train loss 3.858, Val loss 4.101\n",
      "Ep 3 (Step 016210): Train loss 3.978, Val loss 4.094\n",
      "Ep 3 (Step 016220): Train loss 4.194, Val loss 4.091\n",
      "Ep 3 (Step 016230): Train loss 4.142, Val loss 4.090\n",
      "Ep 3 (Step 016240): Train loss 3.988, Val loss 4.075\n",
      "Ep 3 (Step 016250): Train loss 3.961, Val loss 4.095\n",
      "Ep 3 (Step 016260): Train loss 3.910, Val loss 4.079\n",
      "Ep 3 (Step 016270): Train loss 4.210, Val loss 4.088\n",
      "Ep 3 (Step 016280): Train loss 4.075, Val loss 4.081\n",
      "Ep 3 (Step 016290): Train loss 4.078, Val loss 4.079\n",
      "Ep 3 (Step 016300): Train loss 3.871, Val loss 4.084\n",
      "Ep 3 (Step 016310): Train loss 4.035, Val loss 4.096\n",
      "Ep 3 (Step 016320): Train loss 4.050, Val loss 4.066\n",
      "Ep 3 (Step 016330): Train loss 3.983, Val loss 4.057\n",
      "Ep 3 (Step 016340): Train loss 3.957, Val loss 4.059\n",
      "Ep 3 (Step 016350): Train loss 4.042, Val loss 4.057\n",
      "Ep 3 (Step 016360): Train loss 4.037, Val loss 4.055\n",
      "Ep 3 (Step 016370): Train loss 3.958, Val loss 4.060\n",
      "Ep 3 (Step 016380): Train loss 3.990, Val loss 4.059\n",
      "Ep 3 (Step 016390): Train loss 3.930, Val loss 4.048\n",
      "Ep 3 (Step 016400): Train loss 4.231, Val loss 4.062\n",
      "Ep 3 (Step 016410): Train loss 4.177, Val loss 4.054\n",
      "Ep 3 (Step 016420): Train loss 4.122, Val loss 4.057\n",
      "Ep 3 (Step 016430): Train loss 4.043, Val loss 4.056\n",
      "Ep 3 (Step 016440): Train loss 4.063, Val loss 4.051\n",
      "Ep 3 (Step 016450): Train loss 4.204, Val loss 4.058\n",
      "Ep 3 (Step 016460): Train loss 3.929, Val loss 4.059\n",
      "Ep 3 (Step 016470): Train loss 3.954, Val loss 4.055\n",
      "Ep 3 (Step 016480): Train loss 4.082, Val loss 4.073\n",
      "Ep 3 (Step 016490): Train loss 3.968, Val loss 4.063\n",
      "Ep 3 (Step 016500): Train loss 3.995, Val loss 4.072\n",
      "Ep 3 (Step 016510): Train loss 4.072, Val loss 4.061\n",
      "Ep 3 (Step 016520): Train loss 4.060, Val loss 4.060\n",
      "Ep 3 (Step 016530): Train loss 4.013, Val loss 4.058\n",
      "Ep 3 (Step 016540): Train loss 3.955, Val loss 4.054\n",
      "Ep 3 (Step 016550): Train loss 4.098, Val loss 4.063\n",
      "Ep 3 (Step 016560): Train loss 4.003, Val loss 4.057\n",
      "Ep 3 (Step 016570): Train loss 3.998, Val loss 4.061\n",
      "Ep 3 (Step 016580): Train loss 3.929, Val loss 4.055\n",
      "Ep 3 (Step 016590): Train loss 4.051, Val loss 4.060\n",
      "Ep 3 (Step 016600): Train loss 3.937, Val loss 4.058\n",
      "Ep 3 (Step 016610): Train loss 4.024, Val loss 4.060\n",
      "Ep 3 (Step 016620): Train loss 4.091, Val loss 4.049\n",
      "Ep 3 (Step 016630): Train loss 4.094, Val loss 4.054\n",
      "Ep 3 (Step 016640): Train loss 4.072, Val loss 4.052\n",
      "Ep 3 (Step 016650): Train loss 3.962, Val loss 4.066\n",
      "Ep 3 (Step 016660): Train loss 4.124, Val loss 4.056\n",
      "Ep 3 (Step 016670): Train loss 4.189, Val loss 4.066\n",
      "Ep 3 (Step 016680): Train loss 4.158, Val loss 4.068\n",
      "Ep 3 (Step 016690): Train loss 4.113, Val loss 4.067\n",
      "Ep 3 (Step 016700): Train loss 4.089, Val loss 4.072\n",
      "Ep 3 (Step 016710): Train loss 4.293, Val loss 4.073\n",
      "Ep 3 (Step 016720): Train loss 3.967, Val loss 4.068\n",
      "Ep 3 (Step 016730): Train loss 4.147, Val loss 4.066\n",
      "Ep 3 (Step 016740): Train loss 3.962, Val loss 4.069\n",
      "Ep 3 (Step 016750): Train loss 4.105, Val loss 4.078\n",
      "Ep 3 (Step 016760): Train loss 3.994, Val loss 4.053\n",
      "Ep 3 (Step 016770): Train loss 3.906, Val loss 4.049\n",
      "Ep 3 (Step 016780): Train loss 4.115, Val loss 4.058\n",
      "Ep 3 (Step 016790): Train loss 4.021, Val loss 4.060\n",
      "Ep 3 (Step 016800): Train loss 3.984, Val loss 4.063\n",
      "Ep 3 (Step 016810): Train loss 3.995, Val loss 4.071\n",
      "Ep 3 (Step 016820): Train loss 4.098, Val loss 4.055\n",
      "Ep 3 (Step 016830): Train loss 3.894, Val loss 4.053\n",
      "Ep 3 (Step 016840): Train loss 3.990, Val loss 4.045\n",
      "Ep 3 (Step 016850): Train loss 3.943, Val loss 4.045\n",
      "Ep 3 (Step 016860): Train loss 3.953, Val loss 4.047\n",
      "Ep 3 (Step 016870): Train loss 3.939, Val loss 4.052\n",
      "Ep 3 (Step 016880): Train loss 4.011, Val loss 4.055\n",
      "Ep 3 (Step 016890): Train loss 4.098, Val loss 4.064\n",
      "Ep 3 (Step 016900): Train loss 4.043, Val loss 4.062\n",
      "Ep 3 (Step 016910): Train loss 3.990, Val loss 4.062\n",
      "Ep 3 (Step 016920): Train loss 4.055, Val loss 4.065\n",
      "Ep 3 (Step 016930): Train loss 3.900, Val loss 4.055\n",
      "Ep 3 (Step 016940): Train loss 4.009, Val loss 4.047\n",
      "Ep 3 (Step 016950): Train loss 4.030, Val loss 4.049\n",
      "Ep 3 (Step 016960): Train loss 3.935, Val loss 4.054\n",
      "Ep 3 (Step 016970): Train loss 4.093, Val loss 4.059\n",
      "Ep 3 (Step 016980): Train loss 3.958, Val loss 4.045\n",
      "Ep 3 (Step 016990): Train loss 3.960, Val loss 4.049\n",
      "Ep 3 (Step 017000): Train loss 3.902, Val loss 4.042\n",
      "Ep 3 (Step 017010): Train loss 4.044, Val loss 4.041\n",
      "Ep 3 (Step 017020): Train loss 4.085, Val loss 4.031\n",
      "Ep 3 (Step 017030): Train loss 3.997, Val loss 4.045\n",
      "Ep 3 (Step 017040): Train loss 4.165, Val loss 4.031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3 (Step 017050): Train loss 4.123, Val loss 4.032\n",
      "Ep 3 (Step 017060): Train loss 4.193, Val loss 4.040\n",
      "Ep 3 (Step 017070): Train loss 4.006, Val loss 4.040\n",
      "Ep 3 (Step 017080): Train loss 4.055, Val loss 4.029\n",
      "Ep 3 (Step 017090): Train loss 3.968, Val loss 4.026\n",
      "Ep 3 (Step 017100): Train loss 4.151, Val loss 4.028\n",
      "Ep 3 (Step 017110): Train loss 4.032, Val loss 4.041\n",
      "Ep 3 (Step 017120): Train loss 3.987, Val loss 4.031\n",
      "Ep 3 (Step 017130): Train loss 4.055, Val loss 4.038\n",
      "Ep 3 (Step 017140): Train loss 4.062, Val loss 4.040\n",
      "Ep 3 (Step 017150): Train loss 4.035, Val loss 4.033\n",
      "Ep 3 (Step 017160): Train loss 4.128, Val loss 4.041\n",
      "Ep 3 (Step 017170): Train loss 3.909, Val loss 4.030\n",
      "Ep 3 (Step 017180): Train loss 3.952, Val loss 4.036\n",
      "Ep 3 (Step 017190): Train loss 3.905, Val loss 4.032\n",
      "Ep 3 (Step 017200): Train loss 4.046, Val loss 4.026\n",
      "Ep 3 (Step 017210): Train loss 4.146, Val loss 4.036\n",
      "Ep 3 (Step 017220): Train loss 4.047, Val loss 4.032\n",
      "Ep 3 (Step 017230): Train loss 3.968, Val loss 4.040\n",
      "Ep 3 (Step 017240): Train loss 3.977, Val loss 4.037\n",
      "Ep 3 (Step 017250): Train loss 3.871, Val loss 4.031\n",
      "Ep 3 (Step 017260): Train loss 4.101, Val loss 4.025\n",
      "Ep 3 (Step 017270): Train loss 3.914, Val loss 4.039\n",
      "Ep 3 (Step 017280): Train loss 3.972, Val loss 4.031\n",
      "Ep 3 (Step 017290): Train loss 4.050, Val loss 4.033\n",
      "Ep 3 (Step 017300): Train loss 4.057, Val loss 4.031\n",
      "Ep 3 (Step 017310): Train loss 4.103, Val loss 4.032\n",
      "Ep 3 (Step 017320): Train loss 4.040, Val loss 4.032\n",
      "Ep 3 (Step 017330): Train loss 4.046, Val loss 4.014\n",
      "Ep 3 (Step 017340): Train loss 4.004, Val loss 4.026\n",
      "Ep 3 (Step 017350): Train loss 3.904, Val loss 4.017\n",
      "Ep 3 (Step 017360): Train loss 4.032, Val loss 4.024\n",
      "Ep 3 (Step 017370): Train loss 3.993, Val loss 4.025\n",
      "Ep 3 (Step 017380): Train loss 3.938, Val loss 4.026\n",
      "Ep 3 (Step 017390): Train loss 3.870, Val loss 4.015\n",
      "Ep 3 (Step 017400): Train loss 3.992, Val loss 4.033\n",
      "Ep 3 (Step 017410): Train loss 3.848, Val loss 4.029\n",
      "Ep 3 (Step 017420): Train loss 4.014, Val loss 4.018\n",
      "Ep 3 (Step 017430): Train loss 4.027, Val loss 4.036\n",
      "Ep 3 (Step 017440): Train loss 3.899, Val loss 4.022\n",
      "Ep 3 (Step 017450): Train loss 3.960, Val loss 4.022\n",
      "Ep 3 (Step 017460): Train loss 3.979, Val loss 4.024\n",
      "Ep 3 (Step 017470): Train loss 3.956, Val loss 4.029\n",
      "Ep 3 (Step 017480): Train loss 3.956, Val loss 4.017\n",
      "Ep 3 (Step 017490): Train loss 4.045, Val loss 4.021\n",
      "Ep 3 (Step 017500): Train loss 3.993, Val loss 4.029\n",
      "Ep 3 (Step 017510): Train loss 4.029, Val loss 4.043\n",
      "Ep 3 (Step 017520): Train loss 3.882, Val loss 4.028\n",
      "Ep 3 (Step 017530): Train loss 3.979, Val loss 4.027\n",
      "Ep 3 (Step 017540): Train loss 4.164, Val loss 4.030\n",
      "Ep 3 (Step 017550): Train loss 3.930, Val loss 4.021\n",
      "Ep 3 (Step 017560): Train loss 3.907, Val loss 4.031\n",
      "Ep 3 (Step 017570): Train loss 3.946, Val loss 4.027\n",
      "Ep 3 (Step 017580): Train loss 4.035, Val loss 4.036\n",
      "Ep 3 (Step 017590): Train loss 3.904, Val loss 4.038\n",
      "Ep 3 (Step 017600): Train loss 3.982, Val loss 4.038\n",
      "Ep 3 (Step 017610): Train loss 3.934, Val loss 4.032\n",
      "Ep 3 (Step 017620): Train loss 4.147, Val loss 4.036\n",
      "Ep 3 (Step 017630): Train loss 4.053, Val loss 4.027\n",
      "Ep 3 (Step 017640): Train loss 4.131, Val loss 4.028\n",
      "Ep 3 (Step 017650): Train loss 4.101, Val loss 4.028\n",
      "Ep 3 (Step 017660): Train loss 4.093, Val loss 4.037\n",
      "Ep 3 (Step 017670): Train loss 4.172, Val loss 4.036\n",
      "Ep 3 (Step 017680): Train loss 4.002, Val loss 4.015\n",
      "Ep 3 (Step 017690): Train loss 3.966, Val loss 4.019\n",
      "Ep 3 (Step 017700): Train loss 3.820, Val loss 4.017\n",
      "Ep 3 (Step 017710): Train loss 4.021, Val loss 4.014\n",
      "Ep 3 (Step 017720): Train loss 4.052, Val loss 4.022\n",
      "Ep 3 (Step 017730): Train loss 4.000, Val loss 4.014\n",
      "Ep 3 (Step 017740): Train loss 3.886, Val loss 4.022\n",
      "Ep 3 (Step 017750): Train loss 4.024, Val loss 4.023\n",
      "Ep 3 (Step 017760): Train loss 4.112, Val loss 4.018\n",
      "Ep 3 (Step 017770): Train loss 4.067, Val loss 4.030\n",
      "Ep 3 (Step 017780): Train loss 4.002, Val loss 4.016\n",
      "Ep 3 (Step 017790): Train loss 3.952, Val loss 4.012\n",
      "Ep 3 (Step 017800): Train loss 3.953, Val loss 4.020\n",
      "Ep 3 (Step 017810): Train loss 3.953, Val loss 4.012\n",
      "Ep 3 (Step 017820): Train loss 3.991, Val loss 4.021\n",
      "Ep 3 (Step 017830): Train loss 4.100, Val loss 4.017\n",
      "Ep 3 (Step 017840): Train loss 4.059, Val loss 4.018\n",
      "Ep 3 (Step 017850): Train loss 4.010, Val loss 4.029\n",
      "Ep 3 (Step 017860): Train loss 4.085, Val loss 4.026\n",
      "Ep 3 (Step 017870): Train loss 4.014, Val loss 4.031\n",
      "Ep 3 (Step 017880): Train loss 3.991, Val loss 4.022\n",
      "Ep 3 (Step 017890): Train loss 3.830, Val loss 4.027\n",
      "Ep 3 (Step 017900): Train loss 3.892, Val loss 4.031\n",
      "Ep 3 (Step 017910): Train loss 4.019, Val loss 4.033\n",
      "Ep 3 (Step 017920): Train loss 4.032, Val loss 4.012\n",
      "Ep 3 (Step 017930): Train loss 3.950, Val loss 4.028\n",
      "Ep 3 (Step 017940): Train loss 3.957, Val loss 4.028\n",
      "Ep 3 (Step 017950): Train loss 3.970, Val loss 4.029\n",
      "Ep 3 (Step 017960): Train loss 3.793, Val loss 4.019\n",
      "Ep 3 (Step 017970): Train loss 4.032, Val loss 4.021\n",
      "Ep 3 (Step 017980): Train loss 3.922, Val loss 4.020\n",
      "Ep 3 (Step 017990): Train loss 3.986, Val loss 4.016\n",
      "Ep 3 (Step 018000): Train loss 4.046, Val loss 4.026\n",
      "Ep 3 (Step 018010): Train loss 4.051, Val loss 4.026\n",
      "Ep 3 (Step 018020): Train loss 4.047, Val loss 4.027\n",
      "Ep 3 (Step 018030): Train loss 4.083, Val loss 4.024\n",
      "Ep 3 (Step 018040): Train loss 4.036, Val loss 4.020\n",
      "Ep 3 (Step 018050): Train loss 4.085, Val loss 4.029\n",
      "Ep 3 (Step 018060): Train loss 3.874, Val loss 4.030\n",
      "Ep 3 (Step 018070): Train loss 3.981, Val loss 4.015\n",
      "Ep 3 (Step 018080): Train loss 4.036, Val loss 4.029\n",
      "Ep 3 (Step 018090): Train loss 4.115, Val loss 4.023\n",
      "Ep 3 (Step 018100): Train loss 3.955, Val loss 4.014\n",
      "Ep 3 (Step 018110): Train loss 4.007, Val loss 4.019\n",
      "Ep 3 (Step 018120): Train loss 4.006, Val loss 4.035\n",
      "Ep 3 (Step 018130): Train loss 3.995, Val loss 4.022\n",
      "Ep 3 (Step 018140): Train loss 4.016, Val loss 4.030\n",
      "Ep 3 (Step 018150): Train loss 4.031, Val loss 4.030\n",
      "Ep 3 (Step 018160): Train loss 4.029, Val loss 4.022\n",
      "Ep 3 (Step 018170): Train loss 3.962, Val loss 4.025\n",
      "Ep 3 (Step 018180): Train loss 3.885, Val loss 4.032\n",
      "Ep 3 (Step 018190): Train loss 3.931, Val loss 4.047\n",
      "Ep 3 (Step 018200): Train loss 4.022, Val loss 4.041\n",
      "Ep 3 (Step 018210): Train loss 3.917, Val loss 4.040\n",
      "Ep 3 (Step 018220): Train loss 3.962, Val loss 4.046\n",
      "Ep 3 (Step 018230): Train loss 3.919, Val loss 4.048\n",
      "Ep 3 (Step 018240): Train loss 4.009, Val loss 4.042\n",
      "Ep 3 (Step 018250): Train loss 3.974, Val loss 4.050\n",
      "Ep 3 (Step 018260): Train loss 4.154, Val loss 4.046\n",
      "Ep 3 (Step 018270): Train loss 3.939, Val loss 4.028\n",
      "Ep 3 (Step 018280): Train loss 3.952, Val loss 4.046\n",
      "Ep 3 (Step 018290): Train loss 3.968, Val loss 4.043\n",
      "Ep 3 (Step 018300): Train loss 3.994, Val loss 4.037\n",
      "Ep 3 (Step 018310): Train loss 3.834, Val loss 4.031\n",
      "Ep 3 (Step 018320): Train loss 4.017, Val loss 4.038\n",
      "Ep 3 (Step 018330): Train loss 3.984, Val loss 4.032\n",
      "Ep 3 (Step 018340): Train loss 3.888, Val loss 4.028\n",
      "Ep 3 (Step 018350): Train loss 4.059, Val loss 4.014\n",
      "Ep 3 (Step 018360): Train loss 4.028, Val loss 4.021\n",
      "Ep 3 (Step 018370): Train loss 4.085, Val loss 4.019\n",
      "Ep 3 (Step 018380): Train loss 3.887, Val loss 4.030\n",
      "Ep 3 (Step 018390): Train loss 3.995, Val loss 4.017\n",
      "Ep 3 (Step 018400): Train loss 3.996, Val loss 4.027\n",
      "Ep 3 (Step 018410): Train loss 3.967, Val loss 4.020\n",
      "Ep 3 (Step 018420): Train loss 4.052, Val loss 4.017\n",
      "Ep 3 (Step 018430): Train loss 4.032, Val loss 4.029\n",
      "Ep 3 (Step 018440): Train loss 3.824, Val loss 4.028\n",
      "Ep 3 (Step 018450): Train loss 3.943, Val loss 4.020\n",
      "Ep 3 (Step 018460): Train loss 3.999, Val loss 4.019\n",
      "Ep 3 (Step 018470): Train loss 4.030, Val loss 4.019\n",
      "Ep 3 (Step 018480): Train loss 3.948, Val loss 4.022\n",
      "Ep 3 (Step 018490): Train loss 3.914, Val loss 4.023\n",
      "Ep 3 (Step 018500): Train loss 3.877, Val loss 4.022\n",
      "Ep 3 (Step 018510): Train loss 3.966, Val loss 4.013\n",
      "Ep 3 (Step 018520): Train loss 3.911, Val loss 4.016\n",
      "Ep 3 (Step 018530): Train loss 3.962, Val loss 4.007\n",
      "Ep 3 (Step 018540): Train loss 4.026, Val loss 4.017\n",
      "Ep 3 (Step 018550): Train loss 3.939, Val loss 4.029\n",
      "Ep 3 (Step 018560): Train loss 4.014, Val loss 4.008\n",
      "Ep 3 (Step 018570): Train loss 3.931, Val loss 4.027\n",
      "Ep 3 (Step 018580): Train loss 4.078, Val loss 4.031\n",
      "Ep 3 (Step 018590): Train loss 4.056, Val loss 4.020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3 (Step 018600): Train loss 4.048, Val loss 4.025\n",
      "Ep 3 (Step 018610): Train loss 3.923, Val loss 4.018\n",
      "Ep 3 (Step 018620): Train loss 4.091, Val loss 4.014\n",
      "Ep 3 (Step 018630): Train loss 3.997, Val loss 4.022\n",
      "Ep 3 (Step 018640): Train loss 3.978, Val loss 4.016\n",
      "Ep 3 (Step 018650): Train loss 3.858, Val loss 4.016\n",
      "Ep 3 (Step 018660): Train loss 3.941, Val loss 4.015\n",
      "Ep 3 (Step 018670): Train loss 4.049, Val loss 4.017\n",
      "Ep 3 (Step 018680): Train loss 4.114, Val loss 4.021\n",
      "Ep 3 (Step 018690): Train loss 3.967, Val loss 4.006\n",
      "Ep 3 (Step 018700): Train loss 3.983, Val loss 4.022\n",
      "Ep 3 (Step 018710): Train loss 3.907, Val loss 4.015\n",
      "Ep 3 (Step 018720): Train loss 3.792, Val loss 4.016\n",
      "Ep 3 (Step 018730): Train loss 3.923, Val loss 4.004\n",
      "Ep 3 (Step 018740): Train loss 3.948, Val loss 4.008\n",
      "Ep 3 (Step 018750): Train loss 3.860, Val loss 4.012\n",
      "Ep 3 (Step 018760): Train loss 4.109, Val loss 4.015\n",
      "Ep 3 (Step 018770): Train loss 4.067, Val loss 4.010\n",
      "Ep 3 (Step 018780): Train loss 4.098, Val loss 4.015\n",
      "Ep 3 (Step 018790): Train loss 4.078, Val loss 4.013\n",
      "Ep 3 (Step 018800): Train loss 4.045, Val loss 4.008\n",
      "Ep 3 (Step 018810): Train loss 3.953, Val loss 4.032\n",
      "Ep 3 (Step 018820): Train loss 4.074, Val loss 4.025\n",
      "Ep 3 (Step 018830): Train loss 3.890, Val loss 4.025\n",
      "Ep 3 (Step 018840): Train loss 3.917, Val loss 4.025\n",
      "Ep 3 (Step 018850): Train loss 4.000, Val loss 4.015\n",
      "Ep 3 (Step 018860): Train loss 3.953, Val loss 4.018\n",
      "Ep 3 (Step 018870): Train loss 3.956, Val loss 4.023\n",
      "Ep 3 (Step 018880): Train loss 4.009, Val loss 4.020\n",
      "Ep 3 (Step 018890): Train loss 4.101, Val loss 4.023\n",
      "Ep 3 (Step 018900): Train loss 3.933, Val loss 4.017\n",
      "Ep 3 (Step 018910): Train loss 4.022, Val loss 4.021\n",
      "Ep 3 (Step 018920): Train loss 3.803, Val loss 4.021\n",
      "Ep 3 (Step 018930): Train loss 3.920, Val loss 4.016\n",
      "Ep 3 (Step 018940): Train loss 3.961, Val loss 4.027\n",
      "Ep 3 (Step 018950): Train loss 3.934, Val loss 4.016\n",
      "Ep 3 (Step 018960): Train loss 4.078, Val loss 4.010\n",
      "Ep 3 (Step 018970): Train loss 3.884, Val loss 4.007\n",
      "Ep 3 (Step 018980): Train loss 3.909, Val loss 4.007\n",
      "Ep 3 (Step 018990): Train loss 3.836, Val loss 4.006\n",
      "Ep 3 (Step 019000): Train loss 4.097, Val loss 4.008\n",
      "Ep 3 (Step 019010): Train loss 3.865, Val loss 4.006\n",
      "Ep 3 (Step 019020): Train loss 3.939, Val loss 4.001\n",
      "Ep 3 (Step 019030): Train loss 3.970, Val loss 4.000\n",
      "Ep 3 (Step 019040): Train loss 3.948, Val loss 4.013\n",
      "Ep 3 (Step 019050): Train loss 3.784, Val loss 4.011\n",
      "Ep 3 (Step 019060): Train loss 4.002, Val loss 4.012\n",
      "Ep 3 (Step 019070): Train loss 4.029, Val loss 3.999\n",
      "Ep 3 (Step 019080): Train loss 3.832, Val loss 4.001\n",
      "Ep 3 (Step 019090): Train loss 3.887, Val loss 4.013\n",
      "Ep 3 (Step 019100): Train loss 4.000, Val loss 4.005\n",
      "Ep 3 (Step 019110): Train loss 3.926, Val loss 4.011\n",
      "Ep 3 (Step 019120): Train loss 3.853, Val loss 4.004\n",
      "Ep 3 (Step 019130): Train loss 3.982, Val loss 4.001\n",
      "Ep 3 (Step 019140): Train loss 4.134, Val loss 3.998\n",
      "Ep 3 (Step 019150): Train loss 3.873, Val loss 3.994\n",
      "Ep 3 (Step 019160): Train loss 3.896, Val loss 4.003\n",
      "Ep 3 (Step 019170): Train loss 3.954, Val loss 3.993\n",
      "Ep 3 (Step 019180): Train loss 3.967, Val loss 3.985\n",
      "Ep 3 (Step 019190): Train loss 3.963, Val loss 3.995\n",
      "Ep 3 (Step 019200): Train loss 3.933, Val loss 3.987\n",
      "Ep 3 (Step 019210): Train loss 3.797, Val loss 3.997\n",
      "Ep 3 (Step 019220): Train loss 3.941, Val loss 3.993\n",
      "Ep 3 (Step 019230): Train loss 3.950, Val loss 4.003\n",
      "Ep 3 (Step 019240): Train loss 3.879, Val loss 4.000\n",
      "Ep 3 (Step 019250): Train loss 3.944, Val loss 4.001\n",
      "Ep 3 (Step 019260): Train loss 4.120, Val loss 4.005\n",
      "Ep 3 (Step 019270): Train loss 3.904, Val loss 3.991\n",
      "Ep 3 (Step 019280): Train loss 3.999, Val loss 3.986\n",
      "Ep 3 (Step 019290): Train loss 3.917, Val loss 4.003\n",
      "Ep 3 (Step 019300): Train loss 3.978, Val loss 3.990\n",
      "Ep 3 (Step 019310): Train loss 4.004, Val loss 3.995\n",
      "Ep 3 (Step 019320): Train loss 4.029, Val loss 3.997\n",
      "Ep 3 (Step 019330): Train loss 3.896, Val loss 3.999\n",
      "Ep 3 (Step 019340): Train loss 3.919, Val loss 4.000\n",
      "Ep 3 (Step 019350): Train loss 3.864, Val loss 3.997\n",
      "Ep 3 (Step 019360): Train loss 3.929, Val loss 4.005\n",
      "Ep 3 (Step 019370): Train loss 3.970, Val loss 4.006\n",
      "Ep 3 (Step 019380): Train loss 3.865, Val loss 4.004\n",
      "Ep 3 (Step 019390): Train loss 3.838, Val loss 3.996\n",
      "Ep 3 (Step 019400): Train loss 3.824, Val loss 3.985\n",
      "Ep 3 (Step 019410): Train loss 3.906, Val loss 3.991\n",
      "Ep 3 (Step 019420): Train loss 3.886, Val loss 4.004\n",
      "Ep 3 (Step 019430): Train loss 4.058, Val loss 3.986\n",
      "Ep 3 (Step 019440): Train loss 3.920, Val loss 3.991\n",
      "Ep 3 (Step 019450): Train loss 3.897, Val loss 3.985\n",
      "Ep 3 (Step 019460): Train loss 3.955, Val loss 3.990\n",
      "Ep 3 (Step 019470): Train loss 3.882, Val loss 3.995\n",
      "Ep 3 (Step 019480): Train loss 4.032, Val loss 4.000\n",
      "Ep 3 (Step 019490): Train loss 3.915, Val loss 4.006\n",
      "Ep 3 (Step 019500): Train loss 3.968, Val loss 4.003\n",
      "Ep 3 (Step 019510): Train loss 3.973, Val loss 3.995\n",
      "Ep 3 (Step 019520): Train loss 3.940, Val loss 3.997\n",
      "Ep 3 (Step 019530): Train loss 3.870, Val loss 3.995\n",
      "Ep 3 (Step 019540): Train loss 3.870, Val loss 3.996\n",
      "Ep 3 (Step 019550): Train loss 3.871, Val loss 3.988\n",
      "Ep 3 (Step 019560): Train loss 3.843, Val loss 3.990\n",
      "Ep 3 (Step 019570): Train loss 3.952, Val loss 3.980\n",
      "Ep 3 (Step 019580): Train loss 3.959, Val loss 3.987\n",
      "Ep 3 (Step 019590): Train loss 3.964, Val loss 3.985\n",
      "Ep 3 (Step 019600): Train loss 3.952, Val loss 3.995\n",
      "Ep 3 (Step 019610): Train loss 4.094, Val loss 3.974\n",
      "Ep 3 (Step 019620): Train loss 3.944, Val loss 3.969\n",
      "Ep 3 (Step 019630): Train loss 3.934, Val loss 3.977\n",
      "Ep 3 (Step 019640): Train loss 4.061, Val loss 3.981\n",
      "Ep 3 (Step 019650): Train loss 3.936, Val loss 3.984\n",
      "Ep 3 (Step 019660): Train loss 3.880, Val loss 3.979\n",
      "Ep 3 (Step 019670): Train loss 3.873, Val loss 3.981\n",
      "Ep 3 (Step 019680): Train loss 4.005, Val loss 3.979\n",
      "Ep 3 (Step 019690): Train loss 3.956, Val loss 3.981\n",
      "Ep 3 (Step 019700): Train loss 3.939, Val loss 3.986\n",
      "Ep 3 (Step 019710): Train loss 3.904, Val loss 3.984\n",
      "Ep 3 (Step 019720): Train loss 4.028, Val loss 3.986\n",
      "Ep 3 (Step 019730): Train loss 3.985, Val loss 3.980\n",
      "Ep 3 (Step 019740): Train loss 4.037, Val loss 3.979\n",
      "Ep 3 (Step 019750): Train loss 3.975, Val loss 3.985\n",
      "Ep 3 (Step 019760): Train loss 3.961, Val loss 3.975\n",
      "Ep 3 (Step 019770): Train loss 3.848, Val loss 3.975\n",
      "Ep 3 (Step 019780): Train loss 3.874, Val loss 3.979\n",
      "Ep 3 (Step 019790): Train loss 3.989, Val loss 3.979\n",
      "Ep 3 (Step 019800): Train loss 3.798, Val loss 3.979\n",
      "Ep 3 (Step 019810): Train loss 3.850, Val loss 3.974\n",
      "Ep 3 (Step 019820): Train loss 4.036, Val loss 3.988\n",
      "Ep 3 (Step 019830): Train loss 4.052, Val loss 3.974\n",
      "Ep 3 (Step 019840): Train loss 3.815, Val loss 3.984\n",
      "Ep 3 (Step 019850): Train loss 3.939, Val loss 3.991\n",
      "Ep 3 (Step 019860): Train loss 4.062, Val loss 3.994\n",
      "Ep 3 (Step 019870): Train loss 3.965, Val loss 3.973\n",
      "Ep 3 (Step 019880): Train loss 3.909, Val loss 3.982\n",
      "Ep 3 (Step 019890): Train loss 3.916, Val loss 3.980\n",
      "Ep 3 (Step 019900): Train loss 3.975, Val loss 3.970\n",
      "Ep 3 (Step 019910): Train loss 3.946, Val loss 3.973\n",
      "Ep 3 (Step 019920): Train loss 3.816, Val loss 3.970\n",
      "Ep 3 (Step 019930): Train loss 3.990, Val loss 3.965\n",
      "Ep 3 (Step 019940): Train loss 3.817, Val loss 3.968\n",
      "Ep 3 (Step 019950): Train loss 3.870, Val loss 3.969\n",
      "Ep 3 (Step 019960): Train loss 4.050, Val loss 3.969\n",
      "Ep 3 (Step 019970): Train loss 3.987, Val loss 3.974\n",
      "Ep 3 (Step 019980): Train loss 3.995, Val loss 3.974\n",
      "Ep 3 (Step 019990): Train loss 4.030, Val loss 3.971\n",
      "Ep 3 (Step 020000): Train loss 3.952, Val loss 3.967\n",
      "Ep 3 (Step 020010): Train loss 3.814, Val loss 3.972\n",
      "Ep 3 (Step 020020): Train loss 4.018, Val loss 3.963\n",
      "Ep 3 (Step 020030): Train loss 3.974, Val loss 3.970\n",
      "Ep 4 (Step 020040): Train loss 3.895, Val loss 3.977\n",
      "Ep 4 (Step 020050): Train loss 4.084, Val loss 3.972\n",
      "Ep 4 (Step 020060): Train loss 3.964, Val loss 3.965\n",
      "Ep 4 (Step 020070): Train loss 3.954, Val loss 3.968\n",
      "Ep 4 (Step 020080): Train loss 4.031, Val loss 3.974\n",
      "Ep 4 (Step 020090): Train loss 3.967, Val loss 3.979\n",
      "Ep 4 (Step 020100): Train loss 3.975, Val loss 3.985\n",
      "Ep 4 (Step 020110): Train loss 3.896, Val loss 3.989\n",
      "Ep 4 (Step 020120): Train loss 3.914, Val loss 3.991\n",
      "Ep 4 (Step 020130): Train loss 3.904, Val loss 3.980\n",
      "Ep 4 (Step 020140): Train loss 3.858, Val loss 3.992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 (Step 020150): Train loss 3.986, Val loss 3.978\n",
      "Ep 4 (Step 020160): Train loss 3.936, Val loss 3.981\n",
      "Ep 4 (Step 020170): Train loss 3.932, Val loss 3.986\n",
      "Ep 4 (Step 020180): Train loss 3.894, Val loss 3.985\n",
      "Ep 4 (Step 020190): Train loss 3.839, Val loss 3.972\n",
      "Ep 4 (Step 020200): Train loss 3.908, Val loss 3.976\n",
      "Ep 4 (Step 020210): Train loss 4.000, Val loss 3.973\n",
      "Ep 4 (Step 020220): Train loss 3.860, Val loss 3.977\n",
      "Ep 4 (Step 020230): Train loss 3.987, Val loss 3.971\n",
      "Ep 4 (Step 020240): Train loss 3.990, Val loss 3.981\n",
      "Ep 4 (Step 020250): Train loss 4.007, Val loss 3.978\n",
      "Ep 4 (Step 020260): Train loss 3.837, Val loss 3.984\n",
      "Ep 4 (Step 020270): Train loss 3.878, Val loss 3.977\n",
      "Ep 4 (Step 020280): Train loss 3.979, Val loss 3.977\n",
      "Ep 4 (Step 020290): Train loss 3.864, Val loss 3.970\n",
      "Ep 4 (Step 020300): Train loss 3.855, Val loss 3.969\n",
      "Ep 4 (Step 020310): Train loss 3.811, Val loss 3.955\n",
      "Ep 4 (Step 020320): Train loss 3.903, Val loss 3.954\n",
      "Ep 4 (Step 020330): Train loss 3.989, Val loss 3.961\n",
      "Ep 4 (Step 020340): Train loss 3.934, Val loss 3.960\n",
      "Ep 4 (Step 020350): Train loss 3.971, Val loss 3.961\n",
      "Ep 4 (Step 020360): Train loss 3.836, Val loss 3.965\n",
      "Ep 4 (Step 020370): Train loss 3.885, Val loss 3.955\n",
      "Ep 4 (Step 020380): Train loss 3.951, Val loss 3.959\n",
      "Ep 4 (Step 020390): Train loss 3.932, Val loss 3.956\n",
      "Ep 4 (Step 020400): Train loss 3.916, Val loss 3.957\n",
      "Ep 4 (Step 020410): Train loss 3.841, Val loss 3.946\n",
      "Ep 4 (Step 020420): Train loss 3.890, Val loss 3.951\n",
      "Ep 4 (Step 020430): Train loss 3.860, Val loss 3.951\n",
      "Ep 4 (Step 020440): Train loss 3.949, Val loss 3.949\n",
      "Ep 4 (Step 020450): Train loss 3.787, Val loss 3.950\n",
      "Ep 4 (Step 020460): Train loss 3.860, Val loss 3.946\n",
      "Ep 4 (Step 020470): Train loss 3.915, Val loss 3.957\n",
      "Ep 4 (Step 020480): Train loss 3.979, Val loss 3.943\n",
      "Ep 4 (Step 020490): Train loss 3.892, Val loss 3.964\n",
      "Ep 4 (Step 020500): Train loss 3.856, Val loss 3.960\n",
      "Ep 4 (Step 020510): Train loss 3.998, Val loss 3.949\n",
      "Ep 4 (Step 020520): Train loss 4.036, Val loss 3.960\n",
      "Ep 4 (Step 020530): Train loss 3.880, Val loss 3.957\n",
      "Ep 4 (Step 020540): Train loss 3.988, Val loss 3.954\n",
      "Ep 4 (Step 020550): Train loss 3.995, Val loss 3.960\n",
      "Ep 4 (Step 020560): Train loss 3.943, Val loss 3.971\n",
      "Ep 4 (Step 020570): Train loss 3.962, Val loss 3.963\n",
      "Ep 4 (Step 020580): Train loss 3.800, Val loss 3.964\n",
      "Ep 4 (Step 020590): Train loss 3.830, Val loss 3.956\n",
      "Ep 4 (Step 020600): Train loss 3.976, Val loss 3.961\n",
      "Ep 4 (Step 020610): Train loss 3.848, Val loss 3.939\n",
      "Ep 4 (Step 020620): Train loss 3.889, Val loss 3.955\n",
      "Ep 4 (Step 020630): Train loss 3.863, Val loss 3.962\n",
      "Ep 4 (Step 020640): Train loss 3.904, Val loss 3.966\n",
      "Ep 4 (Step 020650): Train loss 3.993, Val loss 3.963\n",
      "Ep 4 (Step 020660): Train loss 3.921, Val loss 3.961\n",
      "Ep 4 (Step 020670): Train loss 3.987, Val loss 3.952\n",
      "Ep 4 (Step 020680): Train loss 3.824, Val loss 3.963\n",
      "Ep 4 (Step 020690): Train loss 3.956, Val loss 3.957\n",
      "Ep 4 (Step 020700): Train loss 4.013, Val loss 3.966\n",
      "Ep 4 (Step 020710): Train loss 3.917, Val loss 3.954\n",
      "Ep 4 (Step 020720): Train loss 3.973, Val loss 3.958\n",
      "Ep 4 (Step 020730): Train loss 4.017, Val loss 3.963\n",
      "Ep 4 (Step 020740): Train loss 3.747, Val loss 3.961\n",
      "Ep 4 (Step 020750): Train loss 3.938, Val loss 3.959\n",
      "Ep 4 (Step 020760): Train loss 3.792, Val loss 3.966\n",
      "Ep 4 (Step 020770): Train loss 3.978, Val loss 3.968\n",
      "Ep 4 (Step 020780): Train loss 4.010, Val loss 3.964\n",
      "Ep 4 (Step 020790): Train loss 3.959, Val loss 3.963\n",
      "Ep 4 (Step 020800): Train loss 3.945, Val loss 3.959\n",
      "Ep 4 (Step 020810): Train loss 3.851, Val loss 3.960\n",
      "Ep 4 (Step 020820): Train loss 3.974, Val loss 3.964\n",
      "Ep 4 (Step 020830): Train loss 3.883, Val loss 3.959\n",
      "Ep 4 (Step 020840): Train loss 3.887, Val loss 3.961\n",
      "Ep 4 (Step 020850): Train loss 3.859, Val loss 3.954\n",
      "Ep 4 (Step 020860): Train loss 3.835, Val loss 3.963\n",
      "Ep 4 (Step 020870): Train loss 3.885, Val loss 3.961\n",
      "Ep 4 (Step 020880): Train loss 3.952, Val loss 3.960\n",
      "Ep 4 (Step 020890): Train loss 3.893, Val loss 3.954\n",
      "Ep 4 (Step 020900): Train loss 3.870, Val loss 3.950\n",
      "Ep 4 (Step 020910): Train loss 3.936, Val loss 3.959\n",
      "Ep 4 (Step 020920): Train loss 3.923, Val loss 3.960\n",
      "Ep 4 (Step 020930): Train loss 3.958, Val loss 3.961\n",
      "Ep 4 (Step 020940): Train loss 3.938, Val loss 3.973\n",
      "Ep 4 (Step 020950): Train loss 4.012, Val loss 3.976\n",
      "Ep 4 (Step 020960): Train loss 3.884, Val loss 3.962\n",
      "Ep 4 (Step 020970): Train loss 3.937, Val loss 3.979\n",
      "Ep 4 (Step 020980): Train loss 3.928, Val loss 3.975\n",
      "Ep 4 (Step 020990): Train loss 3.744, Val loss 3.964\n",
      "Ep 4 (Step 021000): Train loss 3.815, Val loss 3.972\n",
      "Ep 4 (Step 021010): Train loss 3.962, Val loss 3.969\n",
      "Ep 4 (Step 021020): Train loss 3.908, Val loss 3.978\n",
      "Ep 4 (Step 021030): Train loss 3.903, Val loss 3.968\n",
      "Ep 4 (Step 021040): Train loss 3.825, Val loss 3.958\n",
      "Ep 4 (Step 021050): Train loss 3.937, Val loss 3.959\n",
      "Ep 4 (Step 021060): Train loss 4.014, Val loss 3.960\n",
      "Ep 4 (Step 021070): Train loss 3.844, Val loss 3.966\n",
      "Ep 4 (Step 021080): Train loss 3.877, Val loss 3.970\n",
      "Ep 4 (Step 021090): Train loss 3.917, Val loss 3.977\n",
      "Ep 4 (Step 021100): Train loss 3.931, Val loss 3.982\n",
      "Ep 4 (Step 021110): Train loss 3.915, Val loss 3.969\n",
      "Ep 4 (Step 021120): Train loss 4.059, Val loss 3.978\n",
      "Ep 4 (Step 021130): Train loss 3.953, Val loss 3.981\n",
      "Ep 4 (Step 021140): Train loss 3.900, Val loss 3.977\n",
      "Ep 4 (Step 021150): Train loss 3.896, Val loss 3.978\n",
      "Ep 4 (Step 021160): Train loss 3.704, Val loss 3.972\n",
      "Ep 4 (Step 021170): Train loss 3.859, Val loss 3.980\n",
      "Ep 4 (Step 021180): Train loss 4.082, Val loss 3.982\n",
      "Ep 4 (Step 021190): Train loss 3.801, Val loss 3.974\n",
      "Ep 4 (Step 021200): Train loss 3.873, Val loss 3.969\n",
      "Ep 4 (Step 021210): Train loss 3.753, Val loss 3.976\n",
      "Ep 4 (Step 021220): Train loss 3.887, Val loss 3.967\n",
      "Ep 4 (Step 021230): Train loss 3.914, Val loss 3.967\n",
      "Ep 4 (Step 021240): Train loss 4.081, Val loss 3.960\n",
      "Ep 4 (Step 021250): Train loss 3.930, Val loss 3.962\n",
      "Ep 4 (Step 021260): Train loss 3.838, Val loss 3.963\n",
      "Ep 4 (Step 021270): Train loss 3.861, Val loss 3.961\n",
      "Ep 4 (Step 021280): Train loss 3.892, Val loss 3.967\n",
      "Ep 4 (Step 021290): Train loss 3.901, Val loss 3.959\n",
      "Ep 4 (Step 021300): Train loss 3.977, Val loss 3.951\n",
      "Ep 4 (Step 021310): Train loss 3.851, Val loss 3.954\n",
      "Ep 4 (Step 021320): Train loss 3.927, Val loss 3.962\n",
      "Ep 4 (Step 021330): Train loss 3.915, Val loss 3.971\n",
      "Ep 4 (Step 021340): Train loss 3.851, Val loss 3.958\n",
      "Ep 4 (Step 021350): Train loss 3.927, Val loss 3.954\n",
      "Ep 4 (Step 021360): Train loss 3.865, Val loss 3.956\n",
      "Ep 4 (Step 021370): Train loss 3.705, Val loss 3.958\n",
      "Ep 4 (Step 021380): Train loss 3.839, Val loss 3.961\n",
      "Ep 4 (Step 021390): Train loss 3.742, Val loss 3.962\n",
      "Ep 4 (Step 021400): Train loss 3.884, Val loss 3.955\n",
      "Ep 4 (Step 021410): Train loss 3.885, Val loss 3.961\n",
      "Ep 4 (Step 021420): Train loss 3.884, Val loss 3.965\n",
      "Ep 4 (Step 021430): Train loss 4.026, Val loss 3.960\n",
      "Ep 4 (Step 021440): Train loss 3.811, Val loss 3.961\n",
      "Ep 4 (Step 021450): Train loss 3.968, Val loss 3.964\n",
      "Ep 4 (Step 021460): Train loss 3.911, Val loss 3.957\n",
      "Ep 4 (Step 021470): Train loss 3.800, Val loss 3.964\n",
      "Ep 4 (Step 021480): Train loss 3.667, Val loss 3.949\n",
      "Ep 4 (Step 021490): Train loss 3.823, Val loss 3.960\n",
      "Ep 4 (Step 021500): Train loss 3.812, Val loss 3.963\n",
      "Ep 4 (Step 021510): Train loss 3.837, Val loss 3.964\n",
      "Ep 4 (Step 021520): Train loss 3.881, Val loss 3.944\n",
      "Ep 4 (Step 021530): Train loss 3.945, Val loss 3.960\n",
      "Ep 4 (Step 021540): Train loss 3.945, Val loss 3.950\n",
      "Ep 4 (Step 021550): Train loss 3.709, Val loss 3.955\n",
      "Ep 4 (Step 021560): Train loss 3.841, Val loss 3.966\n",
      "Ep 4 (Step 021570): Train loss 3.982, Val loss 3.963\n",
      "Ep 4 (Step 021580): Train loss 3.876, Val loss 3.967\n",
      "Ep 4 (Step 021590): Train loss 3.859, Val loss 3.965\n",
      "Ep 4 (Step 021600): Train loss 3.907, Val loss 3.951\n",
      "Ep 4 (Step 021610): Train loss 3.738, Val loss 3.951\n",
      "Ep 4 (Step 021620): Train loss 3.881, Val loss 3.956\n",
      "Ep 4 (Step 021630): Train loss 3.807, Val loss 3.960\n",
      "Ep 4 (Step 021640): Train loss 3.875, Val loss 3.953\n",
      "Ep 4 (Step 021650): Train loss 3.781, Val loss 3.960\n",
      "Ep 4 (Step 021660): Train loss 3.700, Val loss 3.955\n",
      "Ep 4 (Step 021670): Train loss 3.852, Val loss 3.955\n",
      "Ep 4 (Step 021680): Train loss 3.918, Val loss 3.956\n",
      "Ep 4 (Step 021690): Train loss 4.042, Val loss 3.952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 (Step 021700): Train loss 3.848, Val loss 3.957\n",
      "Ep 4 (Step 021710): Train loss 3.891, Val loss 3.957\n",
      "Ep 4 (Step 021720): Train loss 3.813, Val loss 3.962\n",
      "Ep 4 (Step 021730): Train loss 3.809, Val loss 3.967\n",
      "Ep 4 (Step 021740): Train loss 3.931, Val loss 3.967\n",
      "Ep 4 (Step 021750): Train loss 3.872, Val loss 3.961\n",
      "Ep 4 (Step 021760): Train loss 3.929, Val loss 3.966\n",
      "Ep 4 (Step 021770): Train loss 3.898, Val loss 3.966\n",
      "Ep 4 (Step 021780): Train loss 3.908, Val loss 3.959\n",
      "Ep 4 (Step 021790): Train loss 3.823, Val loss 3.955\n",
      "Ep 4 (Step 021800): Train loss 3.847, Val loss 3.964\n",
      "Ep 4 (Step 021810): Train loss 3.909, Val loss 3.965\n",
      "Ep 4 (Step 021820): Train loss 3.886, Val loss 3.961\n",
      "Ep 4 (Step 021830): Train loss 3.848, Val loss 3.969\n",
      "Ep 4 (Step 021840): Train loss 3.799, Val loss 3.976\n",
      "Ep 4 (Step 021850): Train loss 3.883, Val loss 3.976\n",
      "Ep 4 (Step 021860): Train loss 3.902, Val loss 3.966\n",
      "Ep 4 (Step 021870): Train loss 3.880, Val loss 3.966\n",
      "Ep 4 (Step 021880): Train loss 3.789, Val loss 3.962\n",
      "Ep 4 (Step 021890): Train loss 3.994, Val loss 3.966\n",
      "Ep 4 (Step 021900): Train loss 3.874, Val loss 3.958\n",
      "Ep 4 (Step 021910): Train loss 3.781, Val loss 3.953\n",
      "Ep 4 (Step 021920): Train loss 3.866, Val loss 3.961\n",
      "Ep 4 (Step 021930): Train loss 3.753, Val loss 3.963\n",
      "Ep 4 (Step 021940): Train loss 3.900, Val loss 3.963\n",
      "Ep 4 (Step 021950): Train loss 3.896, Val loss 3.965\n",
      "Ep 4 (Step 021960): Train loss 3.787, Val loss 3.964\n",
      "Ep 4 (Step 021970): Train loss 3.872, Val loss 3.964\n",
      "Ep 4 (Step 021980): Train loss 3.849, Val loss 3.959\n",
      "Ep 4 (Step 021990): Train loss 3.866, Val loss 3.956\n",
      "Ep 4 (Step 022000): Train loss 3.824, Val loss 3.957\n",
      "Ep 4 (Step 022010): Train loss 3.882, Val loss 3.956\n",
      "Ep 4 (Step 022020): Train loss 3.807, Val loss 3.960\n",
      "Ep 4 (Step 022030): Train loss 3.918, Val loss 3.970\n",
      "Ep 4 (Step 022040): Train loss 3.732, Val loss 3.959\n",
      "Ep 4 (Step 022050): Train loss 4.054, Val loss 3.960\n",
      "Ep 4 (Step 022060): Train loss 3.844, Val loss 3.963\n",
      "Ep 4 (Step 022070): Train loss 3.768, Val loss 3.963\n",
      "Ep 4 (Step 022080): Train loss 3.798, Val loss 3.959\n",
      "Ep 4 (Step 022090): Train loss 3.857, Val loss 3.963\n",
      "Ep 4 (Step 022100): Train loss 3.859, Val loss 3.953\n",
      "Ep 4 (Step 022110): Train loss 3.947, Val loss 3.960\n",
      "Ep 4 (Step 022120): Train loss 3.794, Val loss 3.954\n",
      "Ep 4 (Step 022130): Train loss 3.855, Val loss 3.962\n",
      "Ep 4 (Step 022140): Train loss 3.873, Val loss 3.954\n",
      "Ep 4 (Step 022150): Train loss 3.823, Val loss 3.950\n",
      "Ep 4 (Step 022160): Train loss 3.874, Val loss 3.952\n",
      "Ep 4 (Step 022170): Train loss 3.897, Val loss 3.947\n",
      "Ep 4 (Step 022180): Train loss 3.782, Val loss 3.949\n",
      "Ep 4 (Step 022190): Train loss 3.762, Val loss 3.940\n",
      "Ep 4 (Step 022200): Train loss 3.830, Val loss 3.945\n",
      "Ep 4 (Step 022210): Train loss 3.826, Val loss 3.951\n",
      "Ep 4 (Step 022220): Train loss 3.929, Val loss 3.943\n",
      "Ep 4 (Step 022230): Train loss 4.082, Val loss 3.963\n",
      "Ep 4 (Step 022240): Train loss 3.938, Val loss 3.960\n",
      "Ep 4 (Step 022250): Train loss 3.892, Val loss 3.953\n",
      "Ep 4 (Step 022260): Train loss 3.856, Val loss 3.942\n",
      "Ep 4 (Step 022270): Train loss 3.829, Val loss 3.948\n",
      "Ep 4 (Step 022280): Train loss 3.791, Val loss 3.943\n",
      "Ep 4 (Step 022290): Train loss 3.761, Val loss 3.939\n",
      "Ep 4 (Step 022300): Train loss 3.787, Val loss 3.948\n",
      "Ep 4 (Step 022310): Train loss 3.828, Val loss 3.947\n",
      "Ep 4 (Step 022320): Train loss 3.877, Val loss 3.950\n",
      "Ep 4 (Step 022330): Train loss 3.987, Val loss 3.950\n",
      "Ep 4 (Step 022340): Train loss 3.880, Val loss 3.952\n",
      "Ep 4 (Step 022350): Train loss 3.910, Val loss 3.951\n",
      "Ep 4 (Step 022360): Train loss 3.887, Val loss 3.951\n",
      "Ep 4 (Step 022370): Train loss 3.795, Val loss 3.958\n",
      "Ep 4 (Step 022380): Train loss 3.850, Val loss 3.954\n",
      "Ep 4 (Step 022390): Train loss 3.938, Val loss 3.952\n",
      "Ep 4 (Step 022400): Train loss 3.909, Val loss 3.951\n",
      "Ep 4 (Step 022410): Train loss 3.848, Val loss 3.961\n",
      "Ep 4 (Step 022420): Train loss 3.747, Val loss 3.956\n",
      "Ep 4 (Step 022430): Train loss 3.902, Val loss 3.956\n",
      "Ep 4 (Step 022440): Train loss 3.902, Val loss 3.967\n",
      "Ep 4 (Step 022450): Train loss 3.871, Val loss 3.957\n",
      "Ep 4 (Step 022460): Train loss 3.868, Val loss 3.959\n",
      "Ep 4 (Step 022470): Train loss 3.851, Val loss 3.960\n",
      "Ep 4 (Step 022480): Train loss 3.855, Val loss 3.955\n",
      "Ep 4 (Step 022490): Train loss 3.802, Val loss 3.946\n",
      "Ep 4 (Step 022500): Train loss 3.853, Val loss 3.953\n",
      "Ep 4 (Step 022510): Train loss 3.909, Val loss 3.948\n",
      "Ep 4 (Step 022520): Train loss 3.847, Val loss 3.951\n",
      "Ep 4 (Step 022530): Train loss 3.872, Val loss 3.942\n",
      "Ep 4 (Step 022540): Train loss 3.850, Val loss 3.952\n",
      "Ep 4 (Step 022550): Train loss 3.767, Val loss 3.943\n",
      "Ep 4 (Step 022560): Train loss 3.821, Val loss 3.935\n",
      "Ep 4 (Step 022570): Train loss 3.800, Val loss 3.936\n",
      "Ep 4 (Step 022580): Train loss 3.868, Val loss 3.936\n",
      "Ep 4 (Step 022590): Train loss 3.788, Val loss 3.931\n",
      "Ep 4 (Step 022600): Train loss 3.875, Val loss 3.939\n",
      "Ep 4 (Step 022610): Train loss 3.761, Val loss 3.934\n",
      "Ep 4 (Step 022620): Train loss 3.889, Val loss 3.945\n",
      "Ep 4 (Step 022630): Train loss 3.844, Val loss 3.934\n",
      "Ep 4 (Step 022640): Train loss 3.960, Val loss 3.934\n",
      "Ep 4 (Step 022650): Train loss 3.818, Val loss 3.937\n",
      "Ep 4 (Step 022660): Train loss 3.822, Val loss 3.942\n",
      "Ep 4 (Step 022670): Train loss 3.765, Val loss 3.945\n",
      "Ep 4 (Step 022680): Train loss 3.914, Val loss 3.944\n",
      "Ep 4 (Step 022690): Train loss 3.901, Val loss 3.937\n",
      "Ep 4 (Step 022700): Train loss 3.809, Val loss 3.949\n",
      "Ep 4 (Step 022710): Train loss 3.899, Val loss 3.939\n",
      "Ep 4 (Step 022720): Train loss 3.797, Val loss 3.934\n",
      "Ep 4 (Step 022730): Train loss 3.821, Val loss 3.945\n",
      "Ep 4 (Step 022740): Train loss 3.769, Val loss 3.929\n",
      "Ep 4 (Step 022750): Train loss 3.887, Val loss 3.936\n",
      "Ep 4 (Step 022760): Train loss 3.899, Val loss 3.943\n",
      "Ep 4 (Step 022770): Train loss 3.889, Val loss 3.948\n",
      "Ep 4 (Step 022780): Train loss 3.896, Val loss 3.941\n",
      "Ep 4 (Step 022790): Train loss 3.842, Val loss 3.957\n",
      "Ep 4 (Step 022800): Train loss 3.856, Val loss 3.943\n",
      "Ep 4 (Step 022810): Train loss 3.704, Val loss 3.945\n",
      "Ep 4 (Step 022820): Train loss 3.862, Val loss 3.939\n",
      "Ep 4 (Step 022830): Train loss 3.739, Val loss 3.938\n",
      "Ep 4 (Step 022840): Train loss 3.794, Val loss 3.937\n",
      "Ep 4 (Step 022850): Train loss 3.888, Val loss 3.943\n",
      "Ep 4 (Step 022860): Train loss 3.783, Val loss 3.945\n",
      "Ep 4 (Step 022870): Train loss 3.900, Val loss 3.940\n",
      "Ep 4 (Step 022880): Train loss 3.806, Val loss 3.934\n",
      "Ep 4 (Step 022890): Train loss 3.866, Val loss 3.942\n",
      "Ep 4 (Step 022900): Train loss 3.874, Val loss 3.933\n",
      "Ep 4 (Step 022910): Train loss 3.791, Val loss 3.943\n",
      "Ep 4 (Step 022920): Train loss 3.802, Val loss 3.938\n",
      "Ep 4 (Step 022930): Train loss 3.695, Val loss 3.942\n",
      "Ep 4 (Step 022940): Train loss 3.935, Val loss 3.943\n",
      "Ep 4 (Step 022950): Train loss 3.753, Val loss 3.943\n",
      "Ep 4 (Step 022960): Train loss 3.860, Val loss 3.944\n",
      "Ep 4 (Step 022970): Train loss 3.751, Val loss 3.944\n",
      "Ep 4 (Step 022980): Train loss 3.937, Val loss 3.941\n",
      "Ep 4 (Step 022990): Train loss 3.731, Val loss 3.932\n",
      "Ep 4 (Step 023000): Train loss 3.817, Val loss 3.943\n",
      "Ep 4 (Step 023010): Train loss 3.839, Val loss 3.939\n",
      "Ep 4 (Step 023020): Train loss 3.916, Val loss 3.942\n",
      "Ep 4 (Step 023030): Train loss 3.839, Val loss 3.947\n",
      "Ep 4 (Step 023040): Train loss 3.877, Val loss 3.945\n",
      "Ep 4 (Step 023050): Train loss 3.937, Val loss 3.950\n",
      "Ep 4 (Step 023060): Train loss 3.823, Val loss 3.945\n",
      "Ep 4 (Step 023070): Train loss 3.648, Val loss 3.939\n",
      "Ep 4 (Step 023080): Train loss 3.791, Val loss 3.935\n",
      "Ep 4 (Step 023090): Train loss 3.970, Val loss 3.933\n",
      "Ep 4 (Step 023100): Train loss 3.945, Val loss 3.940\n",
      "Ep 4 (Step 023110): Train loss 4.029, Val loss 3.934\n",
      "Ep 4 (Step 023120): Train loss 3.835, Val loss 3.938\n",
      "Ep 4 (Step 023130): Train loss 3.825, Val loss 3.937\n",
      "Ep 4 (Step 023140): Train loss 3.813, Val loss 3.940\n",
      "Ep 4 (Step 023150): Train loss 3.847, Val loss 3.935\n",
      "Ep 4 (Step 023160): Train loss 3.817, Val loss 3.934\n",
      "Ep 4 (Step 023170): Train loss 3.849, Val loss 3.939\n",
      "Ep 4 (Step 023180): Train loss 3.867, Val loss 3.940\n",
      "Ep 4 (Step 023190): Train loss 3.761, Val loss 3.943\n",
      "Ep 4 (Step 023200): Train loss 3.846, Val loss 3.932\n",
      "Ep 4 (Step 023210): Train loss 3.901, Val loss 3.939\n",
      "Ep 4 (Step 023220): Train loss 3.870, Val loss 3.925\n",
      "Ep 4 (Step 023230): Train loss 3.733, Val loss 3.924\n",
      "Ep 4 (Step 023240): Train loss 4.025, Val loss 3.932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 (Step 023250): Train loss 3.929, Val loss 3.921\n",
      "Ep 4 (Step 023260): Train loss 3.903, Val loss 3.921\n",
      "Ep 4 (Step 023270): Train loss 3.682, Val loss 3.921\n",
      "Ep 4 (Step 023280): Train loss 3.903, Val loss 3.932\n",
      "Ep 4 (Step 023290): Train loss 3.768, Val loss 3.931\n",
      "Ep 4 (Step 023300): Train loss 3.971, Val loss 3.935\n",
      "Ep 4 (Step 023310): Train loss 3.823, Val loss 3.935\n",
      "Ep 4 (Step 023320): Train loss 3.762, Val loss 3.932\n",
      "Ep 4 (Step 023330): Train loss 3.794, Val loss 3.931\n",
      "Ep 4 (Step 023340): Train loss 3.735, Val loss 3.928\n",
      "Ep 4 (Step 023350): Train loss 3.749, Val loss 3.939\n",
      "Ep 4 (Step 023360): Train loss 3.778, Val loss 3.939\n",
      "Ep 4 (Step 023370): Train loss 3.801, Val loss 3.933\n",
      "Ep 4 (Step 023380): Train loss 3.801, Val loss 3.937\n",
      "Ep 4 (Step 023390): Train loss 3.887, Val loss 3.940\n",
      "Ep 4 (Step 023400): Train loss 3.908, Val loss 3.929\n",
      "Ep 4 (Step 023410): Train loss 3.881, Val loss 3.928\n",
      "Ep 4 (Step 023420): Train loss 3.805, Val loss 3.930\n",
      "Ep 4 (Step 023430): Train loss 3.820, Val loss 3.930\n",
      "Ep 4 (Step 023440): Train loss 3.878, Val loss 3.927\n",
      "Ep 4 (Step 023450): Train loss 3.778, Val loss 3.924\n",
      "Ep 4 (Step 023460): Train loss 3.789, Val loss 3.931\n",
      "Ep 4 (Step 023470): Train loss 3.815, Val loss 3.935\n",
      "Ep 4 (Step 023480): Train loss 3.868, Val loss 3.930\n",
      "Ep 4 (Step 023490): Train loss 3.607, Val loss 3.933\n",
      "Ep 4 (Step 023500): Train loss 3.746, Val loss 3.925\n",
      "Ep 4 (Step 023510): Train loss 3.874, Val loss 3.919\n",
      "Ep 4 (Step 023520): Train loss 3.824, Val loss 3.925\n",
      "Ep 4 (Step 023530): Train loss 3.719, Val loss 3.920\n",
      "Ep 4 (Step 023540): Train loss 3.766, Val loss 3.919\n",
      "Ep 4 (Step 023550): Train loss 3.827, Val loss 3.926\n",
      "Ep 4 (Step 023560): Train loss 3.813, Val loss 3.926\n",
      "Ep 4 (Step 023570): Train loss 3.852, Val loss 3.925\n",
      "Ep 4 (Step 023580): Train loss 3.813, Val loss 3.922\n",
      "Ep 4 (Step 023590): Train loss 3.692, Val loss 3.926\n",
      "Ep 4 (Step 023600): Train loss 3.824, Val loss 3.922\n",
      "Ep 4 (Step 023610): Train loss 3.793, Val loss 3.922\n",
      "Ep 4 (Step 023620): Train loss 3.734, Val loss 3.931\n",
      "Ep 4 (Step 023630): Train loss 3.743, Val loss 3.927\n",
      "Ep 4 (Step 023640): Train loss 3.766, Val loss 3.935\n",
      "Ep 4 (Step 023650): Train loss 3.876, Val loss 3.927\n",
      "Ep 4 (Step 023660): Train loss 3.737, Val loss 3.935\n",
      "Ep 4 (Step 023670): Train loss 3.771, Val loss 3.932\n",
      "Ep 4 (Step 023680): Train loss 3.728, Val loss 3.925\n",
      "Ep 4 (Step 023690): Train loss 3.729, Val loss 3.929\n",
      "Ep 4 (Step 023700): Train loss 3.778, Val loss 3.924\n",
      "Ep 4 (Step 023710): Train loss 3.836, Val loss 3.931\n",
      "Ep 4 (Step 023720): Train loss 3.814, Val loss 3.924\n",
      "Ep 4 (Step 023730): Train loss 3.780, Val loss 3.921\n",
      "Ep 4 (Step 023740): Train loss 3.792, Val loss 3.924\n",
      "Ep 4 (Step 023750): Train loss 3.913, Val loss 3.920\n",
      "Ep 4 (Step 023760): Train loss 3.822, Val loss 3.926\n",
      "Ep 4 (Step 023770): Train loss 3.896, Val loss 3.929\n",
      "Ep 4 (Step 023780): Train loss 3.769, Val loss 3.923\n",
      "Ep 4 (Step 023790): Train loss 3.794, Val loss 3.926\n",
      "Ep 4 (Step 023800): Train loss 3.812, Val loss 3.927\n",
      "Ep 4 (Step 023810): Train loss 3.701, Val loss 3.923\n",
      "Ep 4 (Step 023820): Train loss 3.879, Val loss 3.918\n",
      "Ep 4 (Step 023830): Train loss 3.912, Val loss 3.918\n",
      "Ep 4 (Step 023840): Train loss 3.990, Val loss 3.922\n",
      "Ep 4 (Step 023850): Train loss 3.793, Val loss 3.927\n",
      "Ep 4 (Step 023860): Train loss 3.900, Val loss 3.922\n",
      "Ep 4 (Step 023870): Train loss 3.898, Val loss 3.924\n",
      "Ep 4 (Step 023880): Train loss 3.866, Val loss 3.924\n",
      "Ep 4 (Step 023890): Train loss 3.746, Val loss 3.924\n",
      "Ep 4 (Step 023900): Train loss 3.754, Val loss 3.928\n",
      "Ep 4 (Step 023910): Train loss 3.828, Val loss 3.924\n",
      "Ep 4 (Step 023920): Train loss 3.747, Val loss 3.929\n",
      "Ep 4 (Step 023930): Train loss 3.854, Val loss 3.918\n",
      "Ep 4 (Step 023940): Train loss 3.892, Val loss 3.915\n",
      "Ep 4 (Step 023950): Train loss 3.697, Val loss 3.918\n",
      "Ep 4 (Step 023960): Train loss 3.967, Val loss 3.919\n",
      "Ep 4 (Step 023970): Train loss 3.894, Val loss 3.915\n",
      "Ep 4 (Step 023980): Train loss 4.006, Val loss 3.919\n",
      "Ep 4 (Step 023990): Train loss 3.715, Val loss 3.921\n",
      "Ep 4 (Step 024000): Train loss 3.907, Val loss 3.924\n",
      "Ep 4 (Step 024010): Train loss 3.698, Val loss 3.914\n",
      "Ep 4 (Step 024020): Train loss 3.849, Val loss 3.914\n",
      "Ep 4 (Step 024030): Train loss 4.023, Val loss 3.923\n",
      "Ep 4 (Step 024040): Train loss 3.809, Val loss 3.924\n",
      "Ep 4 (Step 024050): Train loss 3.743, Val loss 3.919\n",
      "Ep 4 (Step 024060): Train loss 3.826, Val loss 3.923\n",
      "Ep 4 (Step 024070): Train loss 3.791, Val loss 3.927\n",
      "Ep 4 (Step 024080): Train loss 3.775, Val loss 3.922\n",
      "Ep 4 (Step 024090): Train loss 3.731, Val loss 3.919\n",
      "Ep 4 (Step 024100): Train loss 3.828, Val loss 3.922\n",
      "Ep 4 (Step 024110): Train loss 3.887, Val loss 3.914\n",
      "Ep 4 (Step 024120): Train loss 3.781, Val loss 3.917\n",
      "Ep 4 (Step 024130): Train loss 3.748, Val loss 3.915\n",
      "Ep 4 (Step 024140): Train loss 3.861, Val loss 3.918\n",
      "Ep 4 (Step 024150): Train loss 3.702, Val loss 3.915\n",
      "Ep 4 (Step 024160): Train loss 3.857, Val loss 3.915\n",
      "Ep 4 (Step 024170): Train loss 3.791, Val loss 3.922\n",
      "Ep 4 (Step 024180): Train loss 3.934, Val loss 3.916\n",
      "Ep 4 (Step 024190): Train loss 3.778, Val loss 3.910\n",
      "Ep 4 (Step 024200): Train loss 3.821, Val loss 3.916\n",
      "Ep 4 (Step 024210): Train loss 3.746, Val loss 3.914\n",
      "Ep 4 (Step 024220): Train loss 3.848, Val loss 3.916\n",
      "Ep 4 (Step 024230): Train loss 3.749, Val loss 3.914\n",
      "Ep 4 (Step 024240): Train loss 3.819, Val loss 3.906\n",
      "Ep 4 (Step 024250): Train loss 3.876, Val loss 3.915\n",
      "Ep 4 (Step 024260): Train loss 3.875, Val loss 3.915\n",
      "Ep 4 (Step 024270): Train loss 3.821, Val loss 3.910\n",
      "Ep 4 (Step 024280): Train loss 3.837, Val loss 3.919\n",
      "Ep 4 (Step 024290): Train loss 3.767, Val loss 3.916\n",
      "Ep 4 (Step 024300): Train loss 3.711, Val loss 3.909\n",
      "Ep 4 (Step 024310): Train loss 3.702, Val loss 3.913\n",
      "Ep 4 (Step 024320): Train loss 3.820, Val loss 3.915\n",
      "Ep 4 (Step 024330): Train loss 3.737, Val loss 3.917\n",
      "Ep 4 (Step 024340): Train loss 3.980, Val loss 3.927\n",
      "Ep 4 (Step 024350): Train loss 3.943, Val loss 3.915\n",
      "Ep 4 (Step 024360): Train loss 3.765, Val loss 3.915\n",
      "Ep 4 (Step 024370): Train loss 3.783, Val loss 3.917\n",
      "Ep 4 (Step 024380): Train loss 3.738, Val loss 3.918\n",
      "Ep 4 (Step 024390): Train loss 3.858, Val loss 3.920\n",
      "Ep 4 (Step 024400): Train loss 3.585, Val loss 3.925\n",
      "Ep 4 (Step 024410): Train loss 3.816, Val loss 3.923\n",
      "Ep 4 (Step 024420): Train loss 3.699, Val loss 3.917\n",
      "Ep 4 (Step 024430): Train loss 3.780, Val loss 3.914\n",
      "Ep 4 (Step 024440): Train loss 3.830, Val loss 3.922\n",
      "Ep 4 (Step 024450): Train loss 3.863, Val loss 3.917\n",
      "Ep 4 (Step 024460): Train loss 3.827, Val loss 3.911\n",
      "Ep 4 (Step 024470): Train loss 3.843, Val loss 3.914\n",
      "Ep 4 (Step 024480): Train loss 3.790, Val loss 3.913\n",
      "Ep 4 (Step 024490): Train loss 3.802, Val loss 3.906\n",
      "Ep 4 (Step 024500): Train loss 3.862, Val loss 3.916\n",
      "Ep 4 (Step 024510): Train loss 3.823, Val loss 3.921\n",
      "Ep 4 (Step 024520): Train loss 3.764, Val loss 3.913\n",
      "Ep 4 (Step 024530): Train loss 3.846, Val loss 3.911\n",
      "Ep 4 (Step 024540): Train loss 3.885, Val loss 3.910\n",
      "Ep 4 (Step 024550): Train loss 3.869, Val loss 3.914\n",
      "Ep 4 (Step 024560): Train loss 3.778, Val loss 3.903\n",
      "Ep 4 (Step 024570): Train loss 3.790, Val loss 3.904\n",
      "Ep 4 (Step 024580): Train loss 3.743, Val loss 3.914\n",
      "Ep 4 (Step 024590): Train loss 3.864, Val loss 3.913\n",
      "Ep 4 (Step 024600): Train loss 3.835, Val loss 3.907\n",
      "Ep 4 (Step 024610): Train loss 3.663, Val loss 3.911\n",
      "Ep 4 (Step 024620): Train loss 3.725, Val loss 3.909\n",
      "Ep 4 (Step 024630): Train loss 3.816, Val loss 3.908\n",
      "Ep 4 (Step 024640): Train loss 3.706, Val loss 3.904\n",
      "Ep 4 (Step 024650): Train loss 3.825, Val loss 3.910\n",
      "Ep 4 (Step 024660): Train loss 3.835, Val loss 3.917\n",
      "Ep 4 (Step 024670): Train loss 3.793, Val loss 3.918\n",
      "Ep 4 (Step 024680): Train loss 3.782, Val loss 3.913\n",
      "Ep 4 (Step 024690): Train loss 3.754, Val loss 3.907\n",
      "Ep 4 (Step 024700): Train loss 3.858, Val loss 3.904\n",
      "Ep 4 (Step 024710): Train loss 3.715, Val loss 3.922\n",
      "Ep 4 (Step 024720): Train loss 3.906, Val loss 3.917\n",
      "Ep 4 (Step 024730): Train loss 3.809, Val loss 3.915\n",
      "Ep 4 (Step 024740): Train loss 3.785, Val loss 3.923\n",
      "Ep 4 (Step 024750): Train loss 3.744, Val loss 3.923\n",
      "Ep 4 (Step 024760): Train loss 3.849, Val loss 3.918\n",
      "Ep 4 (Step 024770): Train loss 3.710, Val loss 3.917\n",
      "Ep 4 (Step 024780): Train loss 3.674, Val loss 3.919\n",
      "Ep 4 (Step 024790): Train loss 3.684, Val loss 3.921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 (Step 024800): Train loss 4.031, Val loss 3.925\n",
      "Ep 4 (Step 024810): Train loss 3.900, Val loss 3.923\n",
      "Ep 4 (Step 024820): Train loss 3.782, Val loss 3.920\n",
      "Ep 4 (Step 024830): Train loss 3.822, Val loss 3.928\n",
      "Ep 4 (Step 024840): Train loss 3.759, Val loss 3.921\n",
      "Ep 4 (Step 024850): Train loss 3.945, Val loss 3.916\n",
      "Ep 4 (Step 024860): Train loss 3.761, Val loss 3.922\n",
      "Ep 4 (Step 024870): Train loss 3.710, Val loss 3.927\n",
      "Ep 4 (Step 024880): Train loss 3.852, Val loss 3.925\n",
      "Ep 4 (Step 024890): Train loss 3.712, Val loss 3.924\n",
      "Ep 4 (Step 024900): Train loss 3.690, Val loss 3.934\n",
      "Ep 4 (Step 024910): Train loss 3.745, Val loss 3.926\n",
      "Ep 4 (Step 024920): Train loss 3.784, Val loss 3.934\n",
      "Ep 4 (Step 024930): Train loss 3.842, Val loss 3.929\n",
      "Ep 4 (Step 024940): Train loss 3.712, Val loss 3.927\n",
      "Ep 4 (Step 024950): Train loss 3.742, Val loss 3.921\n",
      "Ep 4 (Step 024960): Train loss 3.840, Val loss 3.917\n",
      "Ep 4 (Step 024970): Train loss 3.723, Val loss 3.915\n",
      "Ep 4 (Step 024980): Train loss 3.838, Val loss 3.916\n",
      "Ep 4 (Step 024990): Train loss 3.775, Val loss 3.917\n",
      "Ep 4 (Step 025000): Train loss 3.950, Val loss 3.916\n",
      "Ep 4 (Step 025010): Train loss 3.802, Val loss 3.912\n",
      "Ep 4 (Step 025020): Train loss 3.730, Val loss 3.908\n",
      "Ep 4 (Step 025030): Train loss 3.795, Val loss 3.920\n",
      "Ep 4 (Step 025040): Train loss 3.743, Val loss 3.913\n",
      "Ep 4 (Step 025050): Train loss 3.805, Val loss 3.914\n",
      "Ep 4 (Step 025060): Train loss 3.794, Val loss 3.918\n",
      "Ep 4 (Step 025070): Train loss 3.828, Val loss 3.909\n",
      "Ep 4 (Step 025080): Train loss 3.741, Val loss 3.909\n",
      "Ep 4 (Step 025090): Train loss 3.810, Val loss 3.908\n",
      "Ep 4 (Step 025100): Train loss 3.803, Val loss 3.910\n",
      "Ep 4 (Step 025110): Train loss 3.890, Val loss 3.911\n",
      "Ep 4 (Step 025120): Train loss 3.772, Val loss 3.919\n",
      "Ep 4 (Step 025130): Train loss 3.768, Val loss 3.916\n",
      "Ep 4 (Step 025140): Train loss 3.762, Val loss 3.913\n",
      "Ep 4 (Step 025150): Train loss 3.743, Val loss 3.913\n",
      "Ep 4 (Step 025160): Train loss 3.672, Val loss 3.914\n",
      "Ep 4 (Step 025170): Train loss 3.817, Val loss 3.916\n",
      "Ep 4 (Step 025180): Train loss 3.740, Val loss 3.910\n",
      "Ep 4 (Step 025190): Train loss 3.819, Val loss 3.907\n",
      "Ep 4 (Step 025200): Train loss 3.802, Val loss 3.919\n",
      "Ep 4 (Step 025210): Train loss 3.732, Val loss 3.916\n",
      "Ep 4 (Step 025220): Train loss 3.876, Val loss 3.918\n",
      "Ep 4 (Step 025230): Train loss 3.768, Val loss 3.921\n",
      "Ep 4 (Step 025240): Train loss 3.707, Val loss 3.919\n",
      "Ep 4 (Step 025250): Train loss 3.611, Val loss 3.902\n",
      "Ep 4 (Step 025260): Train loss 3.793, Val loss 3.906\n",
      "Ep 4 (Step 025270): Train loss 3.782, Val loss 3.917\n",
      "Ep 4 (Step 025280): Train loss 3.756, Val loss 3.911\n",
      "Ep 4 (Step 025290): Train loss 3.690, Val loss 3.910\n",
      "Ep 4 (Step 025300): Train loss 3.754, Val loss 3.915\n",
      "Ep 4 (Step 025310): Train loss 3.783, Val loss 3.914\n",
      "Ep 4 (Step 025320): Train loss 3.943, Val loss 3.910\n",
      "Ep 4 (Step 025330): Train loss 3.839, Val loss 3.916\n",
      "Ep 4 (Step 025340): Train loss 3.580, Val loss 3.917\n",
      "Ep 4 (Step 025350): Train loss 3.823, Val loss 3.908\n",
      "Ep 4 (Step 025360): Train loss 3.832, Val loss 3.915\n",
      "Ep 4 (Step 025370): Train loss 3.788, Val loss 3.910\n",
      "Ep 4 (Step 025380): Train loss 3.787, Val loss 3.909\n",
      "Ep 4 (Step 025390): Train loss 3.817, Val loss 3.920\n",
      "Ep 4 (Step 025400): Train loss 3.700, Val loss 3.917\n",
      "Ep 4 (Step 025410): Train loss 3.823, Val loss 3.910\n",
      "Ep 4 (Step 025420): Train loss 3.623, Val loss 3.915\n",
      "Ep 4 (Step 025430): Train loss 3.816, Val loss 3.908\n",
      "Ep 4 (Step 025440): Train loss 3.745, Val loss 3.908\n",
      "Ep 4 (Step 025450): Train loss 3.807, Val loss 3.909\n",
      "Ep 4 (Step 025460): Train loss 3.858, Val loss 3.910\n",
      "Ep 4 (Step 025470): Train loss 3.703, Val loss 3.912\n",
      "Ep 4 (Step 025480): Train loss 3.868, Val loss 3.904\n",
      "Ep 4 (Step 025490): Train loss 3.691, Val loss 3.910\n",
      "Ep 4 (Step 025500): Train loss 3.726, Val loss 3.917\n",
      "Ep 4 (Step 025510): Train loss 3.790, Val loss 3.920\n",
      "Ep 4 (Step 025520): Train loss 3.839, Val loss 3.923\n",
      "Ep 4 (Step 025530): Train loss 3.837, Val loss 3.913\n",
      "Ep 4 (Step 025540): Train loss 3.714, Val loss 3.914\n",
      "Ep 4 (Step 025550): Train loss 3.640, Val loss 3.914\n",
      "Ep 4 (Step 025560): Train loss 3.829, Val loss 3.912\n",
      "Ep 4 (Step 025570): Train loss 3.747, Val loss 3.919\n",
      "Ep 4 (Step 025580): Train loss 3.701, Val loss 3.920\n",
      "Ep 4 (Step 025590): Train loss 3.731, Val loss 3.920\n",
      "Ep 4 (Step 025600): Train loss 3.789, Val loss 3.917\n",
      "Ep 4 (Step 025610): Train loss 3.769, Val loss 3.913\n",
      "Ep 4 (Step 025620): Train loss 3.720, Val loss 3.923\n",
      "Ep 4 (Step 025630): Train loss 3.907, Val loss 3.915\n",
      "Ep 4 (Step 025640): Train loss 3.698, Val loss 3.913\n",
      "Ep 4 (Step 025650): Train loss 3.773, Val loss 3.909\n",
      "Ep 4 (Step 025660): Train loss 3.689, Val loss 3.911\n",
      "Ep 4 (Step 025670): Train loss 3.680, Val loss 3.911\n",
      "Ep 4 (Step 025680): Train loss 3.704, Val loss 3.912\n",
      "Ep 4 (Step 025690): Train loss 3.732, Val loss 3.927\n",
      "Ep 4 (Step 025700): Train loss 3.836, Val loss 3.919\n",
      "Ep 4 (Step 025710): Train loss 3.704, Val loss 3.916\n",
      "Ep 4 (Step 025720): Train loss 3.731, Val loss 3.921\n",
      "Ep 4 (Step 025730): Train loss 3.884, Val loss 3.919\n",
      "Ep 4 (Step 025740): Train loss 3.818, Val loss 3.917\n",
      "Ep 4 (Step 025750): Train loss 3.853, Val loss 3.918\n",
      "Ep 4 (Step 025760): Train loss 3.741, Val loss 3.917\n",
      "Ep 4 (Step 025770): Train loss 3.707, Val loss 3.919\n",
      "Ep 4 (Step 025780): Train loss 3.728, Val loss 3.917\n",
      "Ep 4 (Step 025790): Train loss 3.781, Val loss 3.921\n",
      "Ep 4 (Step 025800): Train loss 3.829, Val loss 3.916\n",
      "Ep 4 (Step 025810): Train loss 3.793, Val loss 3.918\n",
      "Ep 4 (Step 025820): Train loss 3.845, Val loss 3.919\n",
      "Ep 4 (Step 025830): Train loss 3.766, Val loss 3.914\n",
      "Ep 4 (Step 025840): Train loss 3.788, Val loss 3.919\n",
      "Ep 4 (Step 025850): Train loss 3.768, Val loss 3.921\n",
      "Ep 4 (Step 025860): Train loss 3.803, Val loss 3.915\n",
      "Ep 4 (Step 025870): Train loss 3.697, Val loss 3.920\n",
      "Ep 4 (Step 025880): Train loss 3.715, Val loss 3.922\n",
      "Ep 4 (Step 025890): Train loss 3.732, Val loss 3.914\n",
      "Ep 4 (Step 025900): Train loss 3.792, Val loss 3.913\n",
      "Ep 4 (Step 025910): Train loss 3.747, Val loss 3.910\n",
      "Ep 4 (Step 025920): Train loss 3.758, Val loss 3.914\n",
      "Ep 4 (Step 025930): Train loss 3.742, Val loss 3.918\n",
      "Ep 4 (Step 025940): Train loss 3.847, Val loss 3.910\n",
      "Ep 4 (Step 025950): Train loss 3.781, Val loss 3.907\n",
      "Ep 4 (Step 025960): Train loss 3.736, Val loss 3.912\n",
      "Ep 4 (Step 025970): Train loss 3.878, Val loss 3.913\n",
      "Ep 4 (Step 025980): Train loss 3.911, Val loss 3.915\n",
      "Ep 4 (Step 025990): Train loss 3.832, Val loss 3.918\n",
      "Ep 4 (Step 026000): Train loss 3.778, Val loss 3.920\n",
      "Ep 4 (Step 026010): Train loss 3.725, Val loss 3.928\n",
      "Ep 4 (Step 026020): Train loss 3.814, Val loss 3.918\n",
      "Ep 4 (Step 026030): Train loss 3.812, Val loss 3.921\n",
      "Ep 4 (Step 026040): Train loss 3.817, Val loss 3.923\n",
      "Ep 4 (Step 026050): Train loss 3.670, Val loss 3.928\n",
      "Ep 4 (Step 026060): Train loss 3.792, Val loss 3.932\n",
      "Ep 4 (Step 026070): Train loss 3.675, Val loss 3.931\n",
      "Ep 4 (Step 026080): Train loss 3.651, Val loss 3.932\n",
      "Ep 4 (Step 026090): Train loss 3.726, Val loss 3.921\n",
      "Ep 4 (Step 026100): Train loss 3.649, Val loss 3.921\n",
      "Ep 4 (Step 026110): Train loss 3.684, Val loss 3.914\n",
      "Ep 4 (Step 026120): Train loss 3.795, Val loss 3.918\n",
      "Ep 4 (Step 026130): Train loss 3.803, Val loss 3.928\n",
      "Ep 4 (Step 026140): Train loss 3.761, Val loss 3.935\n",
      "Ep 4 (Step 026150): Train loss 3.857, Val loss 3.928\n",
      "Ep 4 (Step 026160): Train loss 3.760, Val loss 3.922\n",
      "Ep 4 (Step 026170): Train loss 3.832, Val loss 3.919\n",
      "Ep 4 (Step 026180): Train loss 3.746, Val loss 3.925\n",
      "Ep 4 (Step 026190): Train loss 3.668, Val loss 3.922\n",
      "Ep 4 (Step 026200): Train loss 3.888, Val loss 3.927\n",
      "Ep 4 (Step 026210): Train loss 3.757, Val loss 3.923\n",
      "Ep 4 (Step 026220): Train loss 3.698, Val loss 3.923\n",
      "Ep 4 (Step 026230): Train loss 3.773, Val loss 3.915\n",
      "Ep 4 (Step 026240): Train loss 3.747, Val loss 3.914\n",
      "Ep 4 (Step 026250): Train loss 3.674, Val loss 3.914\n",
      "Ep 4 (Step 026260): Train loss 3.879, Val loss 3.913\n",
      "Ep 4 (Step 026270): Train loss 3.778, Val loss 3.917\n",
      "Ep 4 (Step 026280): Train loss 3.722, Val loss 3.913\n",
      "Ep 4 (Step 026290): Train loss 3.802, Val loss 3.910\n",
      "Ep 4 (Step 026300): Train loss 3.697, Val loss 3.911\n",
      "Ep 4 (Step 026310): Train loss 3.852, Val loss 3.913\n",
      "Ep 4 (Step 026320): Train loss 3.666, Val loss 3.907\n",
      "Ep 4 (Step 026330): Train loss 3.751, Val loss 3.914\n",
      "Ep 4 (Step 026340): Train loss 3.846, Val loss 3.917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 (Step 026350): Train loss 3.705, Val loss 3.917\n",
      "Ep 4 (Step 026360): Train loss 3.728, Val loss 3.907\n",
      "Ep 4 (Step 026370): Train loss 3.732, Val loss 3.913\n",
      "Ep 4 (Step 026380): Train loss 3.723, Val loss 3.914\n",
      "Ep 4 (Step 026390): Train loss 3.868, Val loss 3.907\n",
      "Ep 4 (Step 026400): Train loss 3.890, Val loss 3.909\n",
      "Ep 4 (Step 026410): Train loss 3.805, Val loss 3.909\n",
      "Ep 4 (Step 026420): Train loss 3.837, Val loss 3.910\n",
      "Ep 4 (Step 026430): Train loss 3.863, Val loss 3.906\n",
      "Ep 4 (Step 026440): Train loss 3.589, Val loss 3.904\n",
      "Ep 4 (Step 026450): Train loss 3.697, Val loss 3.908\n",
      "Ep 4 (Step 026460): Train loss 3.717, Val loss 3.912\n",
      "Ep 4 (Step 026470): Train loss 3.835, Val loss 3.918\n",
      "Ep 4 (Step 026480): Train loss 3.915, Val loss 3.913\n",
      "Ep 4 (Step 026490): Train loss 3.755, Val loss 3.910\n",
      "Ep 4 (Step 026500): Train loss 3.844, Val loss 3.912\n",
      "Ep 4 (Step 026510): Train loss 3.772, Val loss 3.914\n",
      "Ep 4 (Step 026520): Train loss 3.829, Val loss 3.915\n",
      "Ep 4 (Step 026530): Train loss 3.733, Val loss 3.909\n",
      "Ep 4 (Step 026540): Train loss 3.663, Val loss 3.901\n",
      "Ep 4 (Step 026550): Train loss 3.861, Val loss 3.907\n",
      "Ep 4 (Step 026560): Train loss 3.866, Val loss 3.908\n",
      "Ep 4 (Step 026570): Train loss 3.807, Val loss 3.911\n",
      "Ep 4 (Step 026580): Train loss 3.728, Val loss 3.914\n",
      "Ep 4 (Step 026590): Train loss 3.888, Val loss 3.905\n",
      "Ep 4 (Step 026600): Train loss 3.762, Val loss 3.903\n",
      "Ep 4 (Step 026610): Train loss 3.808, Val loss 3.903\n",
      "Ep 4 (Step 026620): Train loss 3.740, Val loss 3.903\n",
      "Ep 4 (Step 026630): Train loss 3.805, Val loss 3.905\n",
      "Ep 4 (Step 026640): Train loss 3.652, Val loss 3.906\n",
      "Ep 4 (Step 026650): Train loss 3.839, Val loss 3.906\n",
      "Ep 4 (Step 026660): Train loss 3.656, Val loss 3.909\n",
      "Ep 4 (Step 026670): Train loss 3.790, Val loss 3.910\n",
      "Ep 4 (Step 026680): Train loss 3.656, Val loss 3.904\n",
      "Ep 4 (Step 026690): Train loss 3.820, Val loss 3.905\n",
      "Ep 4 (Step 026700): Train loss 3.815, Val loss 3.905\n",
      "Ep 4 (Step 026710): Train loss 3.823, Val loss 3.899\n",
      "Ep 5 (Step 026720): Train loss 3.838, Val loss 3.902\n",
      "Ep 5 (Step 026730): Train loss 3.708, Val loss 3.913\n",
      "Ep 5 (Step 026740): Train loss 3.665, Val loss 3.913\n",
      "Ep 5 (Step 026750): Train loss 3.731, Val loss 3.914\n",
      "Ep 5 (Step 026760): Train loss 3.706, Val loss 3.910\n",
      "Ep 5 (Step 026770): Train loss 3.798, Val loss 3.911\n",
      "Ep 5 (Step 026780): Train loss 3.694, Val loss 3.906\n",
      "Ep 5 (Step 026790): Train loss 3.723, Val loss 3.916\n",
      "Ep 5 (Step 026800): Train loss 3.739, Val loss 3.917\n",
      "Ep 5 (Step 026810): Train loss 3.736, Val loss 3.912\n",
      "Ep 5 (Step 026820): Train loss 3.761, Val loss 3.921\n",
      "Ep 5 (Step 026830): Train loss 3.776, Val loss 3.916\n",
      "Ep 5 (Step 026840): Train loss 3.844, Val loss 3.916\n",
      "Ep 5 (Step 026850): Train loss 3.764, Val loss 3.925\n",
      "Ep 5 (Step 026860): Train loss 3.653, Val loss 3.920\n",
      "Ep 5 (Step 026870): Train loss 3.768, Val loss 3.917\n",
      "Ep 5 (Step 026880): Train loss 3.736, Val loss 3.928\n",
      "Ep 5 (Step 026890): Train loss 3.819, Val loss 3.926\n",
      "Ep 5 (Step 026900): Train loss 3.820, Val loss 3.919\n",
      "Ep 5 (Step 026910): Train loss 3.658, Val loss 3.917\n",
      "Ep 5 (Step 026920): Train loss 3.669, Val loss 3.923\n",
      "Ep 5 (Step 026930): Train loss 3.740, Val loss 3.922\n",
      "Ep 5 (Step 026940): Train loss 3.696, Val loss 3.925\n",
      "Ep 5 (Step 026950): Train loss 3.603, Val loss 3.922\n",
      "Ep 5 (Step 026960): Train loss 3.775, Val loss 3.918\n",
      "Ep 5 (Step 026970): Train loss 3.675, Val loss 3.921\n",
      "Ep 5 (Step 026980): Train loss 3.689, Val loss 3.927\n",
      "Ep 5 (Step 026990): Train loss 3.666, Val loss 3.926\n",
      "Ep 5 (Step 027000): Train loss 3.775, Val loss 3.928\n",
      "Ep 5 (Step 027010): Train loss 3.804, Val loss 3.926\n",
      "Ep 5 (Step 027020): Train loss 3.787, Val loss 3.920\n",
      "Ep 5 (Step 027030): Train loss 3.699, Val loss 3.921\n",
      "Ep 5 (Step 027040): Train loss 3.749, Val loss 3.924\n",
      "Ep 5 (Step 027050): Train loss 3.752, Val loss 3.923\n",
      "Ep 5 (Step 027060): Train loss 3.745, Val loss 3.923\n",
      "Ep 5 (Step 027070): Train loss 3.817, Val loss 3.924\n",
      "Ep 5 (Step 027080): Train loss 3.722, Val loss 3.919\n",
      "Ep 5 (Step 027090): Train loss 3.740, Val loss 3.915\n",
      "Ep 5 (Step 027100): Train loss 3.787, Val loss 3.922\n",
      "Ep 5 (Step 027110): Train loss 3.663, Val loss 3.925\n",
      "Ep 5 (Step 027120): Train loss 3.697, Val loss 3.922\n",
      "Ep 5 (Step 027130): Train loss 3.705, Val loss 3.925\n",
      "Ep 5 (Step 027140): Train loss 3.682, Val loss 3.914\n",
      "Ep 5 (Step 027150): Train loss 3.683, Val loss 3.916\n",
      "Ep 5 (Step 027160): Train loss 3.709, Val loss 3.922\n",
      "Ep 5 (Step 027170): Train loss 3.763, Val loss 3.920\n",
      "Ep 5 (Step 027180): Train loss 3.677, Val loss 3.919\n",
      "Ep 5 (Step 027190): Train loss 3.884, Val loss 3.918\n",
      "Ep 5 (Step 027200): Train loss 3.623, Val loss 3.921\n",
      "Ep 5 (Step 027210): Train loss 3.743, Val loss 3.921\n",
      "Ep 5 (Step 027220): Train loss 3.738, Val loss 3.927\n",
      "Ep 5 (Step 027230): Train loss 3.602, Val loss 3.921\n",
      "Ep 5 (Step 027240): Train loss 3.730, Val loss 3.921\n",
      "Ep 5 (Step 027250): Train loss 3.719, Val loss 3.924\n",
      "Ep 5 (Step 027260): Train loss 3.681, Val loss 3.928\n",
      "Ep 5 (Step 027270): Train loss 3.819, Val loss 3.923\n",
      "Ep 5 (Step 027280): Train loss 3.863, Val loss 3.925\n",
      "Ep 5 (Step 027290): Train loss 3.596, Val loss 3.930\n",
      "Ep 5 (Step 027300): Train loss 3.692, Val loss 3.919\n",
      "Ep 5 (Step 027310): Train loss 3.705, Val loss 3.922\n",
      "Ep 5 (Step 027320): Train loss 3.815, Val loss 3.925\n",
      "Ep 5 (Step 027330): Train loss 3.816, Val loss 3.922\n",
      "Ep 5 (Step 027340): Train loss 3.765, Val loss 3.922\n",
      "Ep 5 (Step 027350): Train loss 3.737, Val loss 3.923\n",
      "Ep 5 (Step 027360): Train loss 3.664, Val loss 3.921\n",
      "Ep 5 (Step 027370): Train loss 3.783, Val loss 3.922\n",
      "Ep 5 (Step 027380): Train loss 3.772, Val loss 3.927\n",
      "Ep 5 (Step 027390): Train loss 3.563, Val loss 3.921\n",
      "Ep 5 (Step 027400): Train loss 3.800, Val loss 3.919\n",
      "Ep 5 (Step 027410): Train loss 3.855, Val loss 3.926\n",
      "Ep 5 (Step 027420): Train loss 3.711, Val loss 3.928\n",
      "Ep 5 (Step 027430): Train loss 3.660, Val loss 3.929\n",
      "Ep 5 (Step 027440): Train loss 3.635, Val loss 3.923\n",
      "Ep 5 (Step 027450): Train loss 3.758, Val loss 3.922\n",
      "Ep 5 (Step 027460): Train loss 3.757, Val loss 3.924\n",
      "Ep 5 (Step 027470): Train loss 3.813, Val loss 3.916\n",
      "Ep 5 (Step 027480): Train loss 3.743, Val loss 3.915\n",
      "Ep 5 (Step 027490): Train loss 3.764, Val loss 3.915\n",
      "Ep 5 (Step 027500): Train loss 3.577, Val loss 3.916\n",
      "Ep 5 (Step 027510): Train loss 3.807, Val loss 3.917\n",
      "Ep 5 (Step 027520): Train loss 3.710, Val loss 3.912\n",
      "Ep 5 (Step 027530): Train loss 3.723, Val loss 3.915\n",
      "Ep 5 (Step 027540): Train loss 3.693, Val loss 3.916\n",
      "Ep 5 (Step 027550): Train loss 3.738, Val loss 3.908\n",
      "Ep 5 (Step 027560): Train loss 3.787, Val loss 3.910\n",
      "Ep 5 (Step 027570): Train loss 3.612, Val loss 3.918\n",
      "Ep 5 (Step 027580): Train loss 3.629, Val loss 3.914\n",
      "Ep 5 (Step 027590): Train loss 3.809, Val loss 3.909\n",
      "Ep 5 (Step 027600): Train loss 3.737, Val loss 3.913\n",
      "Ep 5 (Step 027610): Train loss 3.729, Val loss 3.909\n",
      "Ep 5 (Step 027620): Train loss 3.816, Val loss 3.906\n",
      "Ep 5 (Step 027630): Train loss 3.669, Val loss 3.906\n",
      "Ep 5 (Step 027640): Train loss 3.791, Val loss 3.908\n",
      "Ep 5 (Step 027650): Train loss 3.711, Val loss 3.907\n",
      "Ep 5 (Step 027660): Train loss 3.884, Val loss 3.911\n",
      "Ep 5 (Step 027670): Train loss 3.602, Val loss 3.913\n",
      "Ep 5 (Step 027680): Train loss 3.752, Val loss 3.916\n",
      "Ep 5 (Step 027690): Train loss 3.755, Val loss 3.912\n",
      "Ep 5 (Step 027700): Train loss 3.780, Val loss 3.908\n",
      "Ep 5 (Step 027710): Train loss 3.850, Val loss 3.914\n",
      "Ep 5 (Step 027720): Train loss 3.714, Val loss 3.915\n",
      "Ep 5 (Step 027730): Train loss 3.765, Val loss 3.912\n",
      "Ep 5 (Step 027740): Train loss 3.699, Val loss 3.909\n",
      "Ep 5 (Step 027750): Train loss 3.684, Val loss 3.905\n",
      "Ep 5 (Step 027760): Train loss 3.799, Val loss 3.905\n",
      "Ep 5 (Step 027770): Train loss 3.778, Val loss 3.904\n",
      "Ep 5 (Step 027780): Train loss 3.787, Val loss 3.910\n",
      "Ep 5 (Step 027790): Train loss 3.718, Val loss 3.909\n",
      "Ep 5 (Step 027800): Train loss 3.700, Val loss 3.912\n",
      "Ep 5 (Step 027810): Train loss 3.590, Val loss 3.907\n",
      "Ep 5 (Step 027820): Train loss 3.737, Val loss 3.913\n",
      "Ep 5 (Step 027830): Train loss 3.775, Val loss 3.911\n",
      "Ep 5 (Step 027840): Train loss 3.623, Val loss 3.905\n",
      "Ep 5 (Step 027850): Train loss 3.693, Val loss 3.905\n",
      "Ep 5 (Step 027860): Train loss 3.829, Val loss 3.905\n",
      "Ep 5 (Step 027870): Train loss 3.720, Val loss 3.906\n",
      "Ep 5 (Step 027880): Train loss 3.709, Val loss 3.902\n",
      "Ep 5 (Step 027890): Train loss 3.739, Val loss 3.903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5 (Step 027900): Train loss 3.928, Val loss 3.905\n",
      "Ep 5 (Step 027910): Train loss 3.655, Val loss 3.905\n",
      "Ep 5 (Step 027920): Train loss 3.776, Val loss 3.905\n",
      "Ep 5 (Step 027930): Train loss 3.738, Val loss 3.904\n",
      "Ep 5 (Step 027940): Train loss 3.805, Val loss 3.918\n",
      "Ep 5 (Step 027950): Train loss 3.818, Val loss 3.914\n",
      "Ep 5 (Step 027960): Train loss 3.810, Val loss 3.907\n",
      "Ep 5 (Step 027970): Train loss 3.740, Val loss 3.913\n",
      "Ep 5 (Step 027980): Train loss 3.746, Val loss 3.908\n",
      "Ep 5 (Step 027990): Train loss 3.796, Val loss 3.909\n",
      "Ep 5 (Step 028000): Train loss 3.662, Val loss 3.907\n",
      "Ep 5 (Step 028010): Train loss 3.873, Val loss 3.909\n",
      "Ep 5 (Step 028020): Train loss 3.713, Val loss 3.910\n",
      "Ep 5 (Step 028030): Train loss 3.744, Val loss 3.904\n",
      "Ep 5 (Step 028040): Train loss 3.795, Val loss 3.903\n",
      "Ep 5 (Step 028050): Train loss 3.810, Val loss 3.906\n",
      "Ep 5 (Step 028060): Train loss 3.709, Val loss 3.900\n",
      "Ep 5 (Step 028070): Train loss 3.786, Val loss 3.897\n",
      "Ep 5 (Step 028080): Train loss 3.850, Val loss 3.901\n",
      "Ep 5 (Step 028090): Train loss 3.750, Val loss 3.904\n",
      "Ep 5 (Step 028100): Train loss 3.732, Val loss 3.902\n",
      "Ep 5 (Step 028110): Train loss 3.818, Val loss 3.902\n",
      "Ep 5 (Step 028120): Train loss 3.636, Val loss 3.902\n",
      "Ep 5 (Step 028130): Train loss 3.691, Val loss 3.903\n",
      "Ep 5 (Step 028140): Train loss 3.783, Val loss 3.898\n",
      "Ep 5 (Step 028150): Train loss 3.818, Val loss 3.897\n",
      "Ep 5 (Step 028160): Train loss 3.567, Val loss 3.902\n",
      "Ep 5 (Step 028170): Train loss 3.583, Val loss 3.901\n",
      "Ep 5 (Step 028180): Train loss 3.602, Val loss 3.896\n",
      "Ep 5 (Step 028190): Train loss 3.645, Val loss 3.895\n",
      "Ep 5 (Step 028200): Train loss 3.735, Val loss 3.898\n",
      "Ep 5 (Step 028210): Train loss 3.844, Val loss 3.895\n",
      "Ep 5 (Step 028220): Train loss 3.696, Val loss 3.893\n",
      "Ep 5 (Step 028230): Train loss 3.869, Val loss 3.900\n",
      "Ep 5 (Step 028240): Train loss 3.754, Val loss 3.898\n",
      "Ep 5 (Step 028250): Train loss 3.711, Val loss 3.897\n",
      "Ep 5 (Step 028260): Train loss 3.755, Val loss 3.899\n",
      "Ep 5 (Step 028270): Train loss 3.680, Val loss 3.898\n",
      "Ep 5 (Step 028280): Train loss 3.781, Val loss 3.899\n",
      "Ep 5 (Step 028290): Train loss 3.684, Val loss 3.902\n",
      "Ep 5 (Step 028300): Train loss 3.836, Val loss 3.905\n",
      "Ep 5 (Step 028310): Train loss 3.642, Val loss 3.896\n",
      "Ep 5 (Step 028320): Train loss 3.664, Val loss 3.893\n",
      "Ep 5 (Step 028330): Train loss 3.573, Val loss 3.900\n",
      "Ep 5 (Step 028340): Train loss 3.600, Val loss 3.901\n",
      "Ep 5 (Step 028350): Train loss 3.626, Val loss 3.901\n",
      "Ep 5 (Step 028360): Train loss 3.721, Val loss 3.897\n",
      "Ep 5 (Step 028370): Train loss 3.661, Val loss 3.892\n",
      "Ep 5 (Step 028380): Train loss 3.903, Val loss 3.900\n",
      "Ep 5 (Step 028390): Train loss 3.684, Val loss 3.898\n",
      "Ep 5 (Step 028400): Train loss 3.620, Val loss 3.899\n",
      "Ep 5 (Step 028410): Train loss 3.693, Val loss 3.904\n",
      "Ep 5 (Step 028420): Train loss 3.812, Val loss 3.903\n",
      "Ep 5 (Step 028430): Train loss 3.769, Val loss 3.902\n",
      "Ep 5 (Step 028440): Train loss 3.735, Val loss 3.900\n",
      "Ep 5 (Step 028450): Train loss 3.587, Val loss 3.901\n",
      "Ep 5 (Step 028460): Train loss 3.669, Val loss 3.899\n",
      "Ep 5 (Step 028470): Train loss 3.818, Val loss 3.903\n",
      "Ep 5 (Step 028480): Train loss 3.817, Val loss 3.904\n",
      "Ep 5 (Step 028490): Train loss 3.904, Val loss 3.902\n",
      "Ep 5 (Step 028500): Train loss 3.521, Val loss 3.900\n",
      "Ep 5 (Step 028510): Train loss 3.667, Val loss 3.902\n",
      "Ep 5 (Step 028520): Train loss 3.727, Val loss 3.904\n",
      "Ep 5 (Step 028530): Train loss 3.724, Val loss 3.901\n",
      "Ep 5 (Step 028540): Train loss 3.775, Val loss 3.901\n",
      "Ep 5 (Step 028550): Train loss 3.779, Val loss 3.897\n",
      "Ep 5 (Step 028560): Train loss 3.702, Val loss 3.897\n",
      "Ep 5 (Step 028570): Train loss 3.577, Val loss 3.895\n",
      "Ep 5 (Step 028580): Train loss 3.728, Val loss 3.894\n",
      "Ep 5 (Step 028590): Train loss 3.747, Val loss 3.893\n",
      "Ep 5 (Step 028600): Train loss 3.811, Val loss 3.893\n",
      "Ep 5 (Step 028610): Train loss 3.710, Val loss 3.897\n",
      "Ep 5 (Step 028620): Train loss 3.776, Val loss 3.900\n",
      "Ep 5 (Step 028630): Train loss 3.779, Val loss 3.901\n",
      "Ep 5 (Step 028640): Train loss 3.827, Val loss 3.897\n",
      "Ep 5 (Step 028650): Train loss 3.743, Val loss 3.893\n",
      "Ep 5 (Step 028660): Train loss 3.677, Val loss 3.897\n",
      "Ep 5 (Step 028670): Train loss 3.670, Val loss 3.891\n",
      "Ep 5 (Step 028680): Train loss 3.751, Val loss 3.889\n",
      "Ep 5 (Step 028690): Train loss 3.744, Val loss 3.893\n",
      "Ep 5 (Step 028700): Train loss 3.746, Val loss 3.897\n",
      "Ep 5 (Step 028710): Train loss 3.816, Val loss 3.901\n",
      "Ep 5 (Step 028720): Train loss 3.810, Val loss 3.896\n",
      "Ep 5 (Step 028730): Train loss 3.712, Val loss 3.892\n",
      "Ep 5 (Step 028740): Train loss 3.667, Val loss 3.890\n",
      "Ep 5 (Step 028750): Train loss 3.744, Val loss 3.894\n",
      "Ep 5 (Step 028760): Train loss 3.599, Val loss 3.900\n",
      "Ep 5 (Step 028770): Train loss 3.721, Val loss 3.898\n",
      "Ep 5 (Step 028780): Train loss 3.719, Val loss 3.889\n",
      "Ep 5 (Step 028790): Train loss 3.790, Val loss 3.895\n",
      "Ep 5 (Step 028800): Train loss 3.556, Val loss 3.899\n",
      "Ep 5 (Step 028810): Train loss 3.843, Val loss 3.899\n",
      "Ep 5 (Step 028820): Train loss 3.728, Val loss 3.894\n",
      "Ep 5 (Step 028830): Train loss 3.586, Val loss 3.893\n",
      "Ep 5 (Step 028840): Train loss 3.701, Val loss 3.894\n",
      "Ep 5 (Step 028850): Train loss 3.709, Val loss 3.894\n",
      "Ep 5 (Step 028860): Train loss 3.784, Val loss 3.896\n",
      "Ep 5 (Step 028870): Train loss 3.727, Val loss 3.895\n",
      "Ep 5 (Step 028880): Train loss 3.664, Val loss 3.896\n",
      "Ep 5 (Step 028890): Train loss 3.753, Val loss 3.899\n",
      "Ep 5 (Step 028900): Train loss 3.699, Val loss 3.900\n",
      "Ep 5 (Step 028910): Train loss 3.706, Val loss 3.902\n",
      "Ep 5 (Step 028920): Train loss 3.679, Val loss 3.896\n",
      "Ep 5 (Step 028930): Train loss 3.786, Val loss 3.897\n",
      "Ep 5 (Step 028940): Train loss 3.756, Val loss 3.895\n",
      "Ep 5 (Step 028950): Train loss 3.717, Val loss 3.896\n",
      "Ep 5 (Step 028960): Train loss 3.643, Val loss 3.898\n",
      "Ep 5 (Step 028970): Train loss 3.725, Val loss 3.900\n",
      "Ep 5 (Step 028980): Train loss 3.686, Val loss 3.892\n",
      "Ep 5 (Step 028990): Train loss 3.822, Val loss 3.890\n",
      "Ep 5 (Step 029000): Train loss 3.580, Val loss 3.893\n",
      "Ep 5 (Step 029010): Train loss 3.739, Val loss 3.897\n",
      "Ep 5 (Step 029020): Train loss 3.755, Val loss 3.895\n",
      "Ep 5 (Step 029030): Train loss 3.605, Val loss 3.889\n",
      "Ep 5 (Step 029040): Train loss 3.813, Val loss 3.892\n",
      "Ep 5 (Step 029050): Train loss 3.641, Val loss 3.894\n",
      "Ep 5 (Step 029060): Train loss 3.777, Val loss 3.893\n",
      "Ep 5 (Step 029070): Train loss 3.753, Val loss 3.894\n",
      "Ep 5 (Step 029080): Train loss 3.704, Val loss 3.894\n",
      "Ep 5 (Step 029090): Train loss 3.621, Val loss 3.890\n",
      "Ep 5 (Step 029100): Train loss 3.614, Val loss 3.892\n",
      "Ep 5 (Step 029110): Train loss 3.751, Val loss 3.895\n",
      "Ep 5 (Step 029120): Train loss 3.571, Val loss 3.898\n",
      "Ep 5 (Step 029130): Train loss 3.842, Val loss 3.898\n",
      "Ep 5 (Step 029140): Train loss 3.677, Val loss 3.896\n",
      "Ep 5 (Step 029150): Train loss 3.691, Val loss 3.896\n",
      "Ep 5 (Step 029160): Train loss 3.735, Val loss 3.896\n",
      "Ep 5 (Step 029170): Train loss 3.652, Val loss 3.894\n",
      "Ep 5 (Step 029180): Train loss 3.681, Val loss 3.895\n",
      "Ep 5 (Step 029190): Train loss 3.848, Val loss 3.895\n",
      "Ep 5 (Step 029200): Train loss 3.702, Val loss 3.895\n",
      "Ep 5 (Step 029210): Train loss 3.738, Val loss 3.894\n",
      "Ep 5 (Step 029220): Train loss 3.647, Val loss 3.895\n",
      "Ep 5 (Step 029230): Train loss 3.779, Val loss 3.891\n",
      "Ep 5 (Step 029240): Train loss 3.705, Val loss 3.889\n",
      "Ep 5 (Step 029250): Train loss 3.773, Val loss 3.889\n",
      "Ep 5 (Step 029260): Train loss 3.704, Val loss 3.891\n",
      "Ep 5 (Step 029270): Train loss 3.701, Val loss 3.892\n",
      "Ep 5 (Step 029280): Train loss 3.731, Val loss 3.896\n",
      "Ep 5 (Step 029290): Train loss 3.651, Val loss 3.900\n",
      "Ep 5 (Step 029300): Train loss 3.623, Val loss 3.898\n",
      "Ep 5 (Step 029310): Train loss 3.747, Val loss 3.896\n",
      "Ep 5 (Step 029320): Train loss 3.690, Val loss 3.898\n",
      "Ep 5 (Step 029330): Train loss 3.674, Val loss 3.896\n",
      "Ep 5 (Step 029340): Train loss 3.707, Val loss 3.893\n",
      "Ep 5 (Step 029350): Train loss 3.691, Val loss 3.894\n",
      "Ep 5 (Step 029360): Train loss 3.663, Val loss 3.898\n",
      "Ep 5 (Step 029370): Train loss 3.675, Val loss 3.892\n",
      "Ep 5 (Step 029380): Train loss 3.801, Val loss 3.890\n",
      "Ep 5 (Step 029390): Train loss 3.608, Val loss 3.889\n",
      "Ep 5 (Step 029400): Train loss 3.537, Val loss 3.891\n",
      "Ep 5 (Step 029410): Train loss 3.772, Val loss 3.890\n",
      "Ep 5 (Step 029420): Train loss 3.788, Val loss 3.891\n",
      "Ep 5 (Step 029430): Train loss 3.652, Val loss 3.893\n",
      "Ep 5 (Step 029440): Train loss 3.806, Val loss 3.890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5 (Step 029450): Train loss 3.616, Val loss 3.890\n",
      "Ep 5 (Step 029460): Train loss 3.717, Val loss 3.891\n",
      "Ep 5 (Step 029470): Train loss 3.721, Val loss 3.893\n",
      "Ep 5 (Step 029480): Train loss 3.719, Val loss 3.895\n",
      "Ep 5 (Step 029490): Train loss 3.702, Val loss 3.889\n",
      "Ep 5 (Step 029500): Train loss 3.652, Val loss 3.880\n",
      "Ep 5 (Step 029510): Train loss 3.673, Val loss 3.884\n",
      "Ep 5 (Step 029520): Train loss 3.685, Val loss 3.884\n",
      "Ep 5 (Step 029530): Train loss 3.680, Val loss 3.882\n",
      "Ep 5 (Step 029540): Train loss 3.712, Val loss 3.883\n",
      "Ep 5 (Step 029550): Train loss 3.775, Val loss 3.886\n",
      "Ep 5 (Step 029560): Train loss 3.762, Val loss 3.887\n",
      "Ep 5 (Step 029570): Train loss 3.652, Val loss 3.890\n",
      "Ep 5 (Step 029580): Train loss 3.692, Val loss 3.896\n",
      "Ep 5 (Step 029590): Train loss 3.817, Val loss 3.897\n",
      "Ep 5 (Step 029600): Train loss 3.655, Val loss 3.891\n",
      "Ep 5 (Step 029610): Train loss 3.656, Val loss 3.892\n",
      "Ep 5 (Step 029620): Train loss 3.681, Val loss 3.892\n",
      "Ep 5 (Step 029630): Train loss 3.568, Val loss 3.893\n",
      "Ep 5 (Step 029640): Train loss 3.692, Val loss 3.892\n",
      "Ep 5 (Step 029650): Train loss 3.678, Val loss 3.891\n",
      "Ep 5 (Step 029660): Train loss 3.840, Val loss 3.892\n",
      "Ep 5 (Step 029670): Train loss 3.636, Val loss 3.888\n",
      "Ep 5 (Step 029680): Train loss 3.812, Val loss 3.891\n",
      "Ep 5 (Step 029690): Train loss 3.707, Val loss 3.893\n",
      "Ep 5 (Step 029700): Train loss 3.807, Val loss 3.886\n",
      "Ep 5 (Step 029710): Train loss 3.644, Val loss 3.891\n",
      "Ep 5 (Step 029720): Train loss 3.672, Val loss 3.891\n",
      "Ep 5 (Step 029730): Train loss 3.698, Val loss 3.890\n",
      "Ep 5 (Step 029740): Train loss 3.579, Val loss 3.896\n",
      "Ep 5 (Step 029750): Train loss 3.706, Val loss 3.898\n",
      "Ep 5 (Step 029760): Train loss 3.615, Val loss 3.894\n",
      "Ep 5 (Step 029770): Train loss 3.708, Val loss 3.896\n",
      "Ep 5 (Step 029780): Train loss 3.695, Val loss 3.897\n",
      "Ep 5 (Step 029790): Train loss 3.608, Val loss 3.892\n",
      "Ep 5 (Step 029800): Train loss 3.715, Val loss 3.890\n",
      "Ep 5 (Step 029810): Train loss 3.670, Val loss 3.888\n",
      "Ep 5 (Step 029820): Train loss 3.768, Val loss 3.889\n",
      "Ep 5 (Step 029830): Train loss 3.667, Val loss 3.892\n",
      "Ep 5 (Step 029840): Train loss 3.665, Val loss 3.888\n",
      "Ep 5 (Step 029850): Train loss 3.525, Val loss 3.889\n",
      "Ep 5 (Step 029860): Train loss 3.672, Val loss 3.892\n",
      "Ep 5 (Step 029870): Train loss 3.733, Val loss 3.890\n",
      "Ep 5 (Step 029880): Train loss 3.714, Val loss 3.885\n",
      "Ep 5 (Step 029890): Train loss 3.601, Val loss 3.885\n",
      "Ep 5 (Step 029900): Train loss 3.655, Val loss 3.886\n",
      "Ep 5 (Step 029910): Train loss 3.580, Val loss 3.886\n",
      "Ep 5 (Step 029920): Train loss 3.697, Val loss 3.887\n",
      "Ep 5 (Step 029930): Train loss 3.662, Val loss 3.887\n",
      "Ep 5 (Step 029940): Train loss 3.698, Val loss 3.887\n",
      "Ep 5 (Step 029950): Train loss 3.649, Val loss 3.888\n",
      "Ep 5 (Step 029960): Train loss 3.677, Val loss 3.888\n",
      "Ep 5 (Step 029970): Train loss 3.552, Val loss 3.893\n",
      "Ep 5 (Step 029980): Train loss 3.621, Val loss 3.889\n",
      "Ep 5 (Step 029990): Train loss 3.663, Val loss 3.882\n",
      "Ep 5 (Step 030000): Train loss 3.615, Val loss 3.884\n",
      "Ep 5 (Step 030010): Train loss 3.685, Val loss 3.888\n",
      "Ep 5 (Step 030020): Train loss 3.665, Val loss 3.886\n",
      "Ep 5 (Step 030030): Train loss 3.704, Val loss 3.885\n",
      "Ep 5 (Step 030040): Train loss 3.718, Val loss 3.886\n",
      "Ep 5 (Step 030050): Train loss 3.622, Val loss 3.889\n",
      "Ep 5 (Step 030060): Train loss 3.725, Val loss 3.891\n",
      "Ep 5 (Step 030070): Train loss 3.728, Val loss 3.887\n",
      "Ep 5 (Step 030080): Train loss 3.600, Val loss 3.882\n",
      "Ep 5 (Step 030090): Train loss 3.671, Val loss 3.882\n",
      "Ep 5 (Step 030100): Train loss 3.630, Val loss 3.875\n",
      "Ep 5 (Step 030110): Train loss 3.768, Val loss 3.876\n",
      "Ep 5 (Step 030120): Train loss 3.667, Val loss 3.881\n",
      "Ep 5 (Step 030130): Train loss 3.757, Val loss 3.879\n",
      "Ep 5 (Step 030140): Train loss 3.698, Val loss 3.878\n",
      "Ep 5 (Step 030150): Train loss 3.678, Val loss 3.879\n",
      "Ep 5 (Step 030160): Train loss 3.715, Val loss 3.880\n",
      "Ep 5 (Step 030170): Train loss 3.686, Val loss 3.875\n",
      "Ep 5 (Step 030180): Train loss 3.671, Val loss 3.875\n",
      "Ep 5 (Step 030190): Train loss 3.822, Val loss 3.881\n",
      "Ep 5 (Step 030200): Train loss 3.739, Val loss 3.880\n",
      "Ep 5 (Step 030210): Train loss 3.729, Val loss 3.885\n",
      "Ep 5 (Step 030220): Train loss 3.744, Val loss 3.885\n",
      "Ep 5 (Step 030230): Train loss 3.737, Val loss 3.881\n",
      "Ep 5 (Step 030240): Train loss 3.733, Val loss 3.876\n",
      "Ep 5 (Step 030250): Train loss 3.874, Val loss 3.880\n",
      "Ep 5 (Step 030260): Train loss 3.745, Val loss 3.880\n",
      "Ep 5 (Step 030270): Train loss 3.655, Val loss 3.878\n",
      "Ep 5 (Step 030280): Train loss 3.601, Val loss 3.874\n",
      "Ep 5 (Step 030290): Train loss 3.827, Val loss 3.879\n",
      "Ep 5 (Step 030300): Train loss 3.710, Val loss 3.877\n",
      "Ep 5 (Step 030310): Train loss 3.801, Val loss 3.878\n",
      "Ep 5 (Step 030320): Train loss 3.610, Val loss 3.877\n",
      "Ep 5 (Step 030330): Train loss 3.743, Val loss 3.882\n",
      "Ep 5 (Step 030340): Train loss 3.638, Val loss 3.884\n",
      "Ep 5 (Step 030350): Train loss 3.682, Val loss 3.880\n",
      "Ep 5 (Step 030360): Train loss 3.794, Val loss 3.876\n",
      "Ep 5 (Step 030370): Train loss 3.679, Val loss 3.878\n",
      "Ep 5 (Step 030380): Train loss 3.570, Val loss 3.881\n",
      "Ep 5 (Step 030390): Train loss 3.549, Val loss 3.878\n",
      "Ep 5 (Step 030400): Train loss 3.737, Val loss 3.876\n",
      "Ep 5 (Step 030410): Train loss 3.664, Val loss 3.879\n",
      "Ep 5 (Step 030420): Train loss 3.724, Val loss 3.881\n",
      "Ep 5 (Step 030430): Train loss 3.761, Val loss 3.877\n",
      "Ep 5 (Step 030440): Train loss 3.619, Val loss 3.877\n",
      "Ep 5 (Step 030450): Train loss 3.668, Val loss 3.879\n",
      "Ep 5 (Step 030460): Train loss 3.748, Val loss 3.878\n",
      "Ep 5 (Step 030470): Train loss 3.839, Val loss 3.877\n",
      "Ep 5 (Step 030480): Train loss 3.629, Val loss 3.877\n",
      "Ep 5 (Step 030490): Train loss 3.753, Val loss 3.877\n",
      "Ep 5 (Step 030500): Train loss 3.610, Val loss 3.872\n",
      "Ep 5 (Step 030510): Train loss 3.586, Val loss 3.874\n",
      "Ep 5 (Step 030520): Train loss 3.818, Val loss 3.877\n",
      "Ep 5 (Step 030530): Train loss 3.716, Val loss 3.876\n",
      "Ep 5 (Step 030540): Train loss 3.744, Val loss 3.875\n",
      "Ep 5 (Step 030550): Train loss 3.698, Val loss 3.877\n",
      "Ep 5 (Step 030560): Train loss 3.639, Val loss 3.882\n",
      "Ep 5 (Step 030570): Train loss 3.647, Val loss 3.878\n",
      "Ep 5 (Step 030580): Train loss 3.671, Val loss 3.873\n",
      "Ep 5 (Step 030590): Train loss 3.695, Val loss 3.873\n",
      "Ep 5 (Step 030600): Train loss 3.675, Val loss 3.878\n",
      "Ep 5 (Step 030610): Train loss 3.609, Val loss 3.873\n",
      "Ep 5 (Step 030620): Train loss 3.742, Val loss 3.871\n",
      "Ep 5 (Step 030630): Train loss 3.572, Val loss 3.875\n",
      "Ep 5 (Step 030640): Train loss 3.721, Val loss 3.878\n",
      "Ep 5 (Step 030650): Train loss 3.668, Val loss 3.876\n",
      "Ep 5 (Step 030660): Train loss 3.689, Val loss 3.874\n",
      "Ep 5 (Step 030670): Train loss 3.553, Val loss 3.876\n",
      "Ep 5 (Step 030680): Train loss 3.622, Val loss 3.876\n",
      "Ep 5 (Step 030690): Train loss 3.664, Val loss 3.877\n",
      "Ep 5 (Step 030700): Train loss 3.603, Val loss 3.875\n",
      "Ep 5 (Step 030710): Train loss 3.607, Val loss 3.878\n",
      "Ep 5 (Step 030720): Train loss 3.696, Val loss 3.880\n",
      "Ep 5 (Step 030730): Train loss 3.751, Val loss 3.880\n",
      "Ep 5 (Step 030740): Train loss 3.688, Val loss 3.875\n",
      "Ep 5 (Step 030750): Train loss 3.693, Val loss 3.875\n",
      "Ep 5 (Step 030760): Train loss 3.794, Val loss 3.878\n",
      "Ep 5 (Step 030770): Train loss 3.577, Val loss 3.878\n",
      "Ep 5 (Step 030780): Train loss 3.768, Val loss 3.877\n",
      "Ep 5 (Step 030790): Train loss 3.646, Val loss 3.872\n",
      "Ep 5 (Step 030800): Train loss 3.699, Val loss 3.869\n",
      "Ep 5 (Step 030810): Train loss 3.550, Val loss 3.874\n",
      "Ep 5 (Step 030820): Train loss 3.760, Val loss 3.876\n",
      "Ep 5 (Step 030830): Train loss 3.613, Val loss 3.876\n",
      "Ep 5 (Step 030840): Train loss 3.644, Val loss 3.876\n",
      "Ep 5 (Step 030850): Train loss 3.697, Val loss 3.871\n",
      "Ep 5 (Step 030860): Train loss 3.771, Val loss 3.872\n",
      "Ep 5 (Step 030870): Train loss 3.680, Val loss 3.874\n",
      "Ep 5 (Step 030880): Train loss 3.653, Val loss 3.874\n",
      "Ep 5 (Step 030890): Train loss 3.788, Val loss 3.873\n",
      "Ep 5 (Step 030900): Train loss 3.771, Val loss 3.870\n",
      "Ep 5 (Step 030910): Train loss 3.739, Val loss 3.874\n",
      "Ep 5 (Step 030920): Train loss 3.694, Val loss 3.875\n",
      "Ep 5 (Step 030930): Train loss 3.770, Val loss 3.876\n",
      "Ep 5 (Step 030940): Train loss 3.804, Val loss 3.875\n",
      "Ep 5 (Step 030950): Train loss 3.661, Val loss 3.870\n",
      "Ep 5 (Step 030960): Train loss 3.698, Val loss 3.871\n",
      "Ep 5 (Step 030970): Train loss 3.568, Val loss 3.873\n",
      "Ep 5 (Step 030980): Train loss 3.724, Val loss 3.873\n",
      "Ep 5 (Step 030990): Train loss 3.738, Val loss 3.872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5 (Step 031000): Train loss 3.712, Val loss 3.872\n",
      "Ep 5 (Step 031010): Train loss 3.504, Val loss 3.872\n",
      "Ep 5 (Step 031020): Train loss 3.551, Val loss 3.870\n",
      "Ep 5 (Step 031030): Train loss 3.552, Val loss 3.869\n",
      "Ep 5 (Step 031040): Train loss 3.760, Val loss 3.867\n",
      "Ep 5 (Step 031050): Train loss 3.618, Val loss 3.870\n",
      "Ep 5 (Step 031060): Train loss 3.698, Val loss 3.872\n",
      "Ep 5 (Step 031070): Train loss 3.648, Val loss 3.875\n",
      "Ep 5 (Step 031080): Train loss 3.771, Val loss 3.872\n",
      "Ep 5 (Step 031090): Train loss 3.627, Val loss 3.869\n",
      "Ep 5 (Step 031100): Train loss 3.665, Val loss 3.868\n",
      "Ep 5 (Step 031110): Train loss 3.713, Val loss 3.869\n",
      "Ep 5 (Step 031120): Train loss 3.662, Val loss 3.873\n",
      "Ep 5 (Step 031130): Train loss 3.710, Val loss 3.870\n",
      "Ep 5 (Step 031140): Train loss 3.656, Val loss 3.872\n",
      "Ep 5 (Step 031150): Train loss 3.667, Val loss 3.875\n",
      "Ep 5 (Step 031160): Train loss 3.580, Val loss 3.872\n",
      "Ep 5 (Step 031170): Train loss 3.687, Val loss 3.872\n",
      "Ep 5 (Step 031180): Train loss 3.713, Val loss 3.869\n",
      "Ep 5 (Step 031190): Train loss 3.619, Val loss 3.870\n",
      "Ep 5 (Step 031200): Train loss 3.666, Val loss 3.876\n",
      "Ep 5 (Step 031210): Train loss 3.738, Val loss 3.877\n",
      "Ep 5 (Step 031220): Train loss 3.766, Val loss 3.873\n",
      "Ep 5 (Step 031230): Train loss 3.745, Val loss 3.870\n",
      "Ep 5 (Step 031240): Train loss 3.626, Val loss 3.875\n",
      "Ep 5 (Step 031250): Train loss 3.688, Val loss 3.876\n",
      "Ep 5 (Step 031260): Train loss 3.698, Val loss 3.876\n",
      "Ep 5 (Step 031270): Train loss 3.700, Val loss 3.877\n",
      "Ep 5 (Step 031280): Train loss 3.619, Val loss 3.873\n",
      "Ep 5 (Step 031290): Train loss 3.663, Val loss 3.871\n",
      "Ep 5 (Step 031300): Train loss 3.641, Val loss 3.876\n",
      "Ep 5 (Step 031310): Train loss 3.765, Val loss 3.879\n",
      "Ep 5 (Step 031320): Train loss 3.661, Val loss 3.874\n",
      "Ep 5 (Step 031330): Train loss 3.653, Val loss 3.872\n",
      "Ep 5 (Step 031340): Train loss 3.604, Val loss 3.873\n",
      "Ep 5 (Step 031350): Train loss 3.567, Val loss 3.872\n",
      "Ep 5 (Step 031360): Train loss 3.763, Val loss 3.871\n",
      "Ep 5 (Step 031370): Train loss 3.808, Val loss 3.872\n",
      "Ep 5 (Step 031380): Train loss 3.669, Val loss 3.871\n",
      "Ep 5 (Step 031390): Train loss 3.641, Val loss 3.874\n",
      "Ep 5 (Step 031400): Train loss 3.631, Val loss 3.877\n",
      "Ep 5 (Step 031410): Train loss 3.674, Val loss 3.876\n",
      "Ep 5 (Step 031420): Train loss 3.641, Val loss 3.874\n",
      "Ep 5 (Step 031430): Train loss 3.714, Val loss 3.875\n",
      "Ep 5 (Step 031440): Train loss 3.633, Val loss 3.875\n",
      "Ep 5 (Step 031450): Train loss 3.597, Val loss 3.873\n",
      "Ep 5 (Step 031460): Train loss 3.655, Val loss 3.876\n",
      "Ep 5 (Step 031470): Train loss 3.700, Val loss 3.877\n",
      "Ep 5 (Step 031480): Train loss 3.597, Val loss 3.876\n",
      "Ep 5 (Step 031490): Train loss 3.782, Val loss 3.874\n",
      "Ep 5 (Step 031500): Train loss 3.666, Val loss 3.872\n",
      "Ep 5 (Step 031510): Train loss 3.638, Val loss 3.876\n",
      "Ep 5 (Step 031520): Train loss 3.650, Val loss 3.880\n",
      "Ep 5 (Step 031530): Train loss 3.692, Val loss 3.875\n",
      "Ep 5 (Step 031540): Train loss 3.662, Val loss 3.870\n",
      "Ep 5 (Step 031550): Train loss 3.704, Val loss 3.870\n",
      "Ep 5 (Step 031560): Train loss 3.750, Val loss 3.874\n",
      "Ep 5 (Step 031570): Train loss 3.605, Val loss 3.876\n",
      "Ep 5 (Step 031580): Train loss 3.718, Val loss 3.870\n",
      "Ep 5 (Step 031590): Train loss 3.683, Val loss 3.867\n",
      "Ep 5 (Step 031600): Train loss 3.657, Val loss 3.871\n",
      "Ep 5 (Step 031610): Train loss 3.657, Val loss 3.875\n",
      "Ep 5 (Step 031620): Train loss 3.547, Val loss 3.875\n",
      "Ep 5 (Step 031630): Train loss 3.760, Val loss 3.875\n",
      "Ep 5 (Step 031640): Train loss 3.770, Val loss 3.872\n",
      "Ep 5 (Step 031650): Train loss 3.726, Val loss 3.871\n",
      "Ep 5 (Step 031660): Train loss 3.716, Val loss 3.872\n",
      "Ep 5 (Step 031670): Train loss 3.651, Val loss 3.873\n",
      "Ep 5 (Step 031680): Train loss 3.684, Val loss 3.876\n",
      "Ep 5 (Step 031690): Train loss 3.663, Val loss 3.873\n",
      "Ep 5 (Step 031700): Train loss 3.704, Val loss 3.868\n",
      "Ep 5 (Step 031710): Train loss 3.811, Val loss 3.867\n",
      "Ep 5 (Step 031720): Train loss 3.684, Val loss 3.870\n",
      "Ep 5 (Step 031730): Train loss 3.671, Val loss 3.874\n",
      "Ep 5 (Step 031740): Train loss 3.726, Val loss 3.877\n",
      "Ep 5 (Step 031750): Train loss 3.665, Val loss 3.874\n",
      "Ep 5 (Step 031760): Train loss 3.562, Val loss 3.873\n",
      "Ep 5 (Step 031770): Train loss 3.648, Val loss 3.875\n",
      "Ep 5 (Step 031780): Train loss 3.773, Val loss 3.877\n",
      "Ep 5 (Step 031790): Train loss 3.562, Val loss 3.877\n",
      "Ep 5 (Step 031800): Train loss 3.692, Val loss 3.876\n",
      "Ep 5 (Step 031810): Train loss 3.603, Val loss 3.877\n",
      "Ep 5 (Step 031820): Train loss 3.678, Val loss 3.876\n",
      "Ep 5 (Step 031830): Train loss 3.654, Val loss 3.875\n",
      "Ep 5 (Step 031840): Train loss 3.681, Val loss 3.875\n",
      "Ep 5 (Step 031850): Train loss 3.712, Val loss 3.872\n",
      "Ep 5 (Step 031860): Train loss 3.739, Val loss 3.872\n",
      "Ep 5 (Step 031870): Train loss 3.585, Val loss 3.876\n",
      "Ep 5 (Step 031880): Train loss 3.529, Val loss 3.876\n",
      "Ep 5 (Step 031890): Train loss 3.763, Val loss 3.874\n",
      "Ep 5 (Step 031900): Train loss 3.609, Val loss 3.872\n",
      "Ep 5 (Step 031910): Train loss 3.626, Val loss 3.875\n",
      "Ep 5 (Step 031920): Train loss 3.623, Val loss 3.875\n",
      "Ep 5 (Step 031930): Train loss 3.650, Val loss 3.877\n",
      "Ep 5 (Step 031940): Train loss 3.545, Val loss 3.875\n",
      "Ep 5 (Step 031950): Train loss 3.641, Val loss 3.876\n",
      "Ep 5 (Step 031960): Train loss 3.741, Val loss 3.875\n",
      "Ep 5 (Step 031970): Train loss 3.778, Val loss 3.875\n",
      "Ep 5 (Step 031980): Train loss 3.678, Val loss 3.878\n",
      "Ep 5 (Step 031990): Train loss 3.601, Val loss 3.880\n",
      "Ep 5 (Step 032000): Train loss 3.618, Val loss 3.879\n",
      "Ep 5 (Step 032010): Train loss 3.741, Val loss 3.879\n",
      "Ep 5 (Step 032020): Train loss 3.547, Val loss 3.878\n",
      "Ep 5 (Step 032030): Train loss 3.663, Val loss 3.878\n",
      "Ep 5 (Step 032040): Train loss 3.705, Val loss 3.880\n",
      "Ep 5 (Step 032050): Train loss 3.705, Val loss 3.881\n",
      "Ep 5 (Step 032060): Train loss 3.741, Val loss 3.878\n",
      "Ep 5 (Step 032070): Train loss 3.682, Val loss 3.875\n",
      "Ep 5 (Step 032080): Train loss 3.685, Val loss 3.874\n",
      "Ep 5 (Step 032090): Train loss 3.754, Val loss 3.874\n",
      "Ep 5 (Step 032100): Train loss 3.688, Val loss 3.878\n",
      "Ep 5 (Step 032110): Train loss 3.728, Val loss 3.877\n",
      "Ep 5 (Step 032120): Train loss 3.625, Val loss 3.875\n",
      "Ep 5 (Step 032130): Train loss 3.602, Val loss 3.874\n",
      "Ep 5 (Step 032140): Train loss 3.601, Val loss 3.875\n",
      "Ep 5 (Step 032150): Train loss 3.603, Val loss 3.878\n",
      "Ep 5 (Step 032160): Train loss 3.624, Val loss 3.876\n",
      "Ep 5 (Step 032170): Train loss 3.656, Val loss 3.874\n",
      "Ep 5 (Step 032180): Train loss 3.732, Val loss 3.874\n",
      "Ep 5 (Step 032190): Train loss 3.700, Val loss 3.872\n",
      "Ep 5 (Step 032200): Train loss 3.632, Val loss 3.873\n",
      "Ep 5 (Step 032210): Train loss 3.608, Val loss 3.872\n",
      "Ep 5 (Step 032220): Train loss 3.654, Val loss 3.874\n",
      "Ep 5 (Step 032230): Train loss 3.705, Val loss 3.876\n",
      "Ep 5 (Step 032240): Train loss 3.637, Val loss 3.877\n",
      "Ep 5 (Step 032250): Train loss 3.656, Val loss 3.877\n",
      "Ep 5 (Step 032260): Train loss 3.645, Val loss 3.877\n",
      "Ep 5 (Step 032270): Train loss 3.679, Val loss 3.875\n",
      "Ep 5 (Step 032280): Train loss 3.665, Val loss 3.874\n",
      "Ep 5 (Step 032290): Train loss 3.730, Val loss 3.874\n",
      "Ep 5 (Step 032300): Train loss 3.601, Val loss 3.874\n",
      "Ep 5 (Step 032310): Train loss 3.673, Val loss 3.873\n",
      "Ep 5 (Step 032320): Train loss 3.725, Val loss 3.874\n",
      "Ep 5 (Step 032330): Train loss 3.727, Val loss 3.875\n",
      "Ep 5 (Step 032340): Train loss 3.600, Val loss 3.874\n",
      "Ep 5 (Step 032350): Train loss 3.639, Val loss 3.875\n",
      "Ep 5 (Step 032360): Train loss 3.509, Val loss 3.874\n",
      "Ep 5 (Step 032370): Train loss 3.639, Val loss 3.873\n",
      "Ep 5 (Step 032380): Train loss 3.587, Val loss 3.872\n",
      "Ep 5 (Step 032390): Train loss 3.692, Val loss 3.871\n",
      "Ep 5 (Step 032400): Train loss 3.719, Val loss 3.872\n",
      "Ep 5 (Step 032410): Train loss 3.691, Val loss 3.872\n",
      "Ep 5 (Step 032420): Train loss 3.613, Val loss 3.869\n",
      "Ep 5 (Step 032430): Train loss 3.621, Val loss 3.867\n",
      "Ep 5 (Step 032440): Train loss 3.757, Val loss 3.869\n",
      "Ep 5 (Step 032450): Train loss 3.764, Val loss 3.869\n",
      "Ep 5 (Step 032460): Train loss 3.715, Val loss 3.871\n",
      "Ep 5 (Step 032470): Train loss 3.745, Val loss 3.872\n",
      "Ep 5 (Step 032480): Train loss 3.587, Val loss 3.875\n",
      "Ep 5 (Step 032490): Train loss 3.531, Val loss 3.878\n",
      "Ep 5 (Step 032500): Train loss 3.739, Val loss 3.879\n",
      "Ep 5 (Step 032510): Train loss 3.758, Val loss 3.876\n",
      "Ep 5 (Step 032520): Train loss 3.607, Val loss 3.874\n",
      "Ep 5 (Step 032530): Train loss 3.535, Val loss 3.874\n",
      "Ep 5 (Step 032540): Train loss 3.772, Val loss 3.871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5 (Step 032550): Train loss 3.637, Val loss 3.871\n",
      "Ep 5 (Step 032560): Train loss 3.665, Val loss 3.872\n",
      "Ep 5 (Step 032570): Train loss 3.675, Val loss 3.872\n",
      "Ep 5 (Step 032580): Train loss 3.688, Val loss 3.872\n",
      "Ep 5 (Step 032590): Train loss 3.642, Val loss 3.873\n",
      "Ep 5 (Step 032600): Train loss 3.631, Val loss 3.874\n",
      "Ep 5 (Step 032610): Train loss 3.689, Val loss 3.877\n",
      "Ep 5 (Step 032620): Train loss 3.694, Val loss 3.873\n",
      "Ep 5 (Step 032630): Train loss 3.709, Val loss 3.874\n",
      "Ep 5 (Step 032640): Train loss 3.639, Val loss 3.876\n",
      "Ep 5 (Step 032650): Train loss 3.614, Val loss 3.877\n",
      "Ep 5 (Step 032660): Train loss 3.679, Val loss 3.877\n",
      "Ep 5 (Step 032670): Train loss 3.693, Val loss 3.875\n",
      "Ep 5 (Step 032680): Train loss 3.670, Val loss 3.872\n",
      "Ep 5 (Step 032690): Train loss 3.692, Val loss 3.872\n",
      "Ep 5 (Step 032700): Train loss 3.607, Val loss 3.873\n",
      "Ep 5 (Step 032710): Train loss 3.682, Val loss 3.872\n",
      "Ep 5 (Step 032720): Train loss 3.616, Val loss 3.873\n",
      "Ep 5 (Step 032730): Train loss 3.776, Val loss 3.873\n",
      "Ep 5 (Step 032740): Train loss 3.652, Val loss 3.871\n",
      "Ep 5 (Step 032750): Train loss 3.605, Val loss 3.871\n",
      "Ep 5 (Step 032760): Train loss 3.689, Val loss 3.874\n",
      "Ep 5 (Step 032770): Train loss 3.656, Val loss 3.873\n",
      "Ep 5 (Step 032780): Train loss 3.596, Val loss 3.873\n",
      "Ep 5 (Step 032790): Train loss 3.630, Val loss 3.873\n",
      "Ep 5 (Step 032800): Train loss 3.618, Val loss 3.869\n",
      "Ep 5 (Step 032810): Train loss 3.605, Val loss 3.868\n",
      "Ep 5 (Step 032820): Train loss 3.601, Val loss 3.870\n",
      "Ep 5 (Step 032830): Train loss 3.790, Val loss 3.872\n",
      "Ep 5 (Step 032840): Train loss 3.569, Val loss 3.872\n",
      "Ep 5 (Step 032850): Train loss 3.648, Val loss 3.867\n",
      "Ep 5 (Step 032860): Train loss 3.681, Val loss 3.868\n",
      "Ep 5 (Step 032870): Train loss 3.684, Val loss 3.870\n",
      "Ep 5 (Step 032880): Train loss 3.637, Val loss 3.872\n",
      "Ep 5 (Step 032890): Train loss 3.698, Val loss 3.872\n",
      "Ep 5 (Step 032900): Train loss 3.767, Val loss 3.869\n",
      "Ep 5 (Step 032910): Train loss 3.663, Val loss 3.869\n",
      "Ep 5 (Step 032920): Train loss 3.661, Val loss 3.874\n",
      "Ep 5 (Step 032930): Train loss 3.554, Val loss 3.874\n",
      "Ep 5 (Step 032940): Train loss 3.729, Val loss 3.873\n",
      "Ep 5 (Step 032950): Train loss 3.597, Val loss 3.872\n",
      "Ep 5 (Step 032960): Train loss 3.640, Val loss 3.871\n",
      "Ep 5 (Step 032970): Train loss 3.564, Val loss 3.872\n",
      "Ep 5 (Step 032980): Train loss 3.838, Val loss 3.873\n",
      "Ep 5 (Step 032990): Train loss 3.665, Val loss 3.871\n",
      "Ep 5 (Step 033000): Train loss 3.565, Val loss 3.866\n",
      "Ep 5 (Step 033010): Train loss 3.654, Val loss 3.865\n",
      "Ep 5 (Step 033020): Train loss 3.603, Val loss 3.866\n",
      "Ep 5 (Step 033030): Train loss 3.586, Val loss 3.868\n",
      "Ep 5 (Step 033040): Train loss 3.612, Val loss 3.869\n",
      "Ep 5 (Step 033050): Train loss 3.783, Val loss 3.869\n",
      "Ep 5 (Step 033060): Train loss 3.651, Val loss 3.867\n",
      "Ep 5 (Step 033070): Train loss 3.651, Val loss 3.866\n",
      "Ep 5 (Step 033080): Train loss 3.708, Val loss 3.867\n",
      "Ep 5 (Step 033090): Train loss 3.604, Val loss 3.868\n",
      "Ep 5 (Step 033100): Train loss 3.655, Val loss 3.868\n",
      "Ep 5 (Step 033110): Train loss 3.658, Val loss 3.867\n",
      "Ep 5 (Step 033120): Train loss 3.562, Val loss 3.870\n",
      "Ep 5 (Step 033130): Train loss 3.592, Val loss 3.871\n",
      "Ep 5 (Step 033140): Train loss 3.631, Val loss 3.871\n",
      "Ep 5 (Step 033150): Train loss 3.712, Val loss 3.871\n",
      "Ep 5 (Step 033160): Train loss 3.675, Val loss 3.870\n",
      "Ep 5 (Step 033170): Train loss 3.715, Val loss 3.871\n",
      "Ep 5 (Step 033180): Train loss 3.631, Val loss 3.873\n",
      "Ep 5 (Step 033190): Train loss 3.686, Val loss 3.872\n",
      "Ep 5 (Step 033200): Train loss 3.655, Val loss 3.876\n",
      "Ep 5 (Step 033210): Train loss 3.737, Val loss 3.875\n",
      "Ep 5 (Step 033220): Train loss 3.557, Val loss 3.873\n",
      "Ep 5 (Step 033230): Train loss 3.534, Val loss 3.871\n",
      "Ep 5 (Step 033240): Train loss 3.601, Val loss 3.869\n",
      "Ep 5 (Step 033250): Train loss 3.612, Val loss 3.870\n",
      "Ep 5 (Step 033260): Train loss 3.587, Val loss 3.871\n",
      "Ep 5 (Step 033270): Train loss 3.690, Val loss 3.873\n",
      "Ep 5 (Step 033280): Train loss 3.610, Val loss 3.872\n",
      "Ep 5 (Step 033290): Train loss 3.613, Val loss 3.870\n",
      "Ep 5 (Step 033300): Train loss 3.708, Val loss 3.871\n",
      "Ep 5 (Step 033310): Train loss 3.626, Val loss 3.872\n",
      "Ep 5 (Step 033320): Train loss 3.628, Val loss 3.871\n",
      "Ep 5 (Step 033330): Train loss 3.689, Val loss 3.871\n",
      "Ep 5 (Step 033340): Train loss 3.647, Val loss 3.873\n",
      "Ep 5 (Step 033350): Train loss 3.743, Val loss 3.871\n",
      "Ep 5 (Step 033360): Train loss 3.719, Val loss 3.870\n",
      "Ep 5 (Step 033370): Train loss 3.612, Val loss 3.870\n",
      "Ep 5 (Step 033380): Train loss 3.647, Val loss 3.872\n",
      "Ep 6 (Step 033390): Train loss 3.611, Val loss 3.873\n",
      "Ep 6 (Step 033400): Train loss 3.710, Val loss 3.872\n",
      "Ep 6 (Step 033410): Train loss 3.641, Val loss 3.872\n",
      "Ep 6 (Step 033420): Train loss 3.765, Val loss 3.871\n",
      "Ep 6 (Step 033430): Train loss 3.713, Val loss 3.870\n",
      "Ep 6 (Step 033440): Train loss 3.680, Val loss 3.868\n",
      "Ep 6 (Step 033450): Train loss 3.638, Val loss 3.869\n",
      "Ep 6 (Step 033460): Train loss 3.631, Val loss 3.871\n",
      "Ep 6 (Step 033470): Train loss 3.627, Val loss 3.871\n",
      "Ep 6 (Step 033480): Train loss 3.624, Val loss 3.872\n",
      "Ep 6 (Step 033490): Train loss 3.790, Val loss 3.871\n",
      "Ep 6 (Step 033500): Train loss 3.724, Val loss 3.870\n",
      "Ep 6 (Step 033510): Train loss 3.734, Val loss 3.871\n",
      "Ep 6 (Step 033520): Train loss 3.564, Val loss 3.872\n",
      "Ep 6 (Step 033530): Train loss 3.735, Val loss 3.870\n",
      "Ep 6 (Step 033540): Train loss 3.724, Val loss 3.869\n",
      "Ep 6 (Step 033550): Train loss 3.559, Val loss 3.868\n",
      "Ep 6 (Step 033560): Train loss 3.544, Val loss 3.867\n",
      "Ep 6 (Step 033570): Train loss 3.678, Val loss 3.869\n",
      "Ep 6 (Step 033580): Train loss 3.632, Val loss 3.871\n",
      "Ep 6 (Step 033590): Train loss 3.606, Val loss 3.873\n",
      "Ep 6 (Step 033600): Train loss 3.672, Val loss 3.872\n",
      "Ep 6 (Step 033610): Train loss 3.572, Val loss 3.871\n",
      "Ep 6 (Step 033620): Train loss 3.682, Val loss 3.869\n",
      "Ep 6 (Step 033630): Train loss 3.585, Val loss 3.869\n",
      "Ep 6 (Step 033640): Train loss 3.682, Val loss 3.868\n",
      "Ep 6 (Step 033650): Train loss 3.643, Val loss 3.865\n",
      "Ep 6 (Step 033660): Train loss 3.682, Val loss 3.864\n",
      "Ep 6 (Step 033670): Train loss 3.659, Val loss 3.867\n",
      "Ep 6 (Step 033680): Train loss 3.673, Val loss 3.870\n",
      "Ep 6 (Step 033690): Train loss 3.666, Val loss 3.869\n",
      "Ep 6 (Step 033700): Train loss 3.598, Val loss 3.870\n",
      "Ep 6 (Step 033710): Train loss 3.689, Val loss 3.870\n",
      "Ep 6 (Step 033720): Train loss 3.727, Val loss 3.872\n",
      "Ep 6 (Step 033730): Train loss 3.509, Val loss 3.871\n",
      "Ep 6 (Step 033740): Train loss 3.639, Val loss 3.872\n",
      "Ep 6 (Step 033750): Train loss 3.715, Val loss 3.870\n",
      "Ep 6 (Step 033760): Train loss 3.626, Val loss 3.868\n",
      "Ep 6 (Step 033770): Train loss 3.599, Val loss 3.868\n",
      "Ep 6 (Step 033780): Train loss 3.623, Val loss 3.868\n",
      "Ep 6 (Step 033790): Train loss 3.498, Val loss 3.867\n",
      "Ep 6 (Step 033800): Train loss 3.696, Val loss 3.867\n",
      "Ep 6 (Step 033810): Train loss 3.631, Val loss 3.868\n",
      "Ep 6 (Step 033820): Train loss 3.801, Val loss 3.868\n",
      "Ep 6 (Step 033830): Train loss 3.706, Val loss 3.869\n",
      "Ep 6 (Step 033840): Train loss 3.629, Val loss 3.871\n",
      "Ep 6 (Step 033850): Train loss 3.690, Val loss 3.872\n",
      "Ep 6 (Step 033860): Train loss 3.497, Val loss 3.872\n",
      "Ep 6 (Step 033870): Train loss 3.620, Val loss 3.873\n",
      "Ep 6 (Step 033880): Train loss 3.662, Val loss 3.872\n",
      "Ep 6 (Step 033890): Train loss 3.604, Val loss 3.870\n",
      "Ep 6 (Step 033900): Train loss 3.733, Val loss 3.870\n",
      "Ep 6 (Step 033910): Train loss 3.718, Val loss 3.870\n",
      "Ep 6 (Step 033920): Train loss 3.654, Val loss 3.868\n",
      "Ep 6 (Step 033930): Train loss 3.747, Val loss 3.866\n",
      "Ep 6 (Step 033940): Train loss 3.615, Val loss 3.865\n",
      "Ep 6 (Step 033950): Train loss 3.715, Val loss 3.867\n",
      "Ep 6 (Step 033960): Train loss 3.584, Val loss 3.870\n",
      "Ep 6 (Step 033970): Train loss 3.603, Val loss 3.871\n",
      "Ep 6 (Step 033980): Train loss 3.669, Val loss 3.870\n",
      "Ep 6 (Step 033990): Train loss 3.693, Val loss 3.868\n",
      "Ep 6 (Step 034000): Train loss 3.572, Val loss 3.869\n",
      "Ep 6 (Step 034010): Train loss 3.614, Val loss 3.869\n",
      "Ep 6 (Step 034020): Train loss 3.564, Val loss 3.869\n",
      "Ep 6 (Step 034030): Train loss 3.665, Val loss 3.870\n",
      "Ep 6 (Step 034040): Train loss 3.557, Val loss 3.870\n",
      "Ep 6 (Step 034050): Train loss 3.557, Val loss 3.869\n",
      "Ep 6 (Step 034060): Train loss 3.592, Val loss 3.870\n",
      "Ep 6 (Step 034070): Train loss 3.520, Val loss 3.871\n",
      "Ep 6 (Step 034080): Train loss 3.692, Val loss 3.870\n",
      "Ep 6 (Step 034090): Train loss 3.660, Val loss 3.871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6 (Step 034100): Train loss 3.669, Val loss 3.871\n",
      "Ep 6 (Step 034110): Train loss 3.595, Val loss 3.868\n",
      "Ep 6 (Step 034120): Train loss 3.626, Val loss 3.867\n",
      "Ep 6 (Step 034130): Train loss 3.665, Val loss 3.870\n",
      "Ep 6 (Step 034140): Train loss 3.639, Val loss 3.871\n",
      "Ep 6 (Step 034150): Train loss 3.654, Val loss 3.871\n",
      "Ep 6 (Step 034160): Train loss 3.564, Val loss 3.870\n",
      "Ep 6 (Step 034170): Train loss 3.617, Val loss 3.870\n",
      "Ep 6 (Step 034180): Train loss 3.644, Val loss 3.870\n",
      "Ep 6 (Step 034190): Train loss 3.600, Val loss 3.870\n",
      "Ep 6 (Step 034200): Train loss 3.729, Val loss 3.870\n",
      "Ep 6 (Step 034210): Train loss 3.612, Val loss 3.870\n",
      "Ep 6 (Step 034220): Train loss 3.641, Val loss 3.870\n",
      "Ep 6 (Step 034230): Train loss 3.608, Val loss 3.869\n",
      "Ep 6 (Step 034240): Train loss 3.668, Val loss 3.871\n",
      "Ep 6 (Step 034250): Train loss 3.630, Val loss 3.871\n",
      "Ep 6 (Step 034260): Train loss 3.551, Val loss 3.871\n",
      "Ep 6 (Step 034270): Train loss 3.556, Val loss 3.870\n",
      "Ep 6 (Step 034280): Train loss 3.679, Val loss 3.868\n",
      "Ep 6 (Step 034290): Train loss 3.617, Val loss 3.867\n",
      "Ep 6 (Step 034300): Train loss 3.563, Val loss 3.866\n",
      "Ep 6 (Step 034310): Train loss 3.653, Val loss 3.867\n",
      "Ep 6 (Step 034320): Train loss 3.636, Val loss 3.868\n",
      "Ep 6 (Step 034330): Train loss 3.664, Val loss 3.869\n",
      "Ep 6 (Step 034340): Train loss 3.707, Val loss 3.868\n",
      "Ep 6 (Step 034350): Train loss 3.779, Val loss 3.869\n",
      "Ep 6 (Step 034360): Train loss 3.753, Val loss 3.868\n",
      "Ep 6 (Step 034370): Train loss 3.713, Val loss 3.868\n",
      "Ep 6 (Step 034380): Train loss 3.501, Val loss 3.867\n",
      "Ep 6 (Step 034390): Train loss 3.566, Val loss 3.868\n",
      "Ep 6 (Step 034400): Train loss 3.651, Val loss 3.868\n",
      "Ep 6 (Step 034410): Train loss 3.641, Val loss 3.869\n",
      "Ep 6 (Step 034420): Train loss 3.686, Val loss 3.869\n",
      "Ep 6 (Step 034430): Train loss 3.683, Val loss 3.869\n",
      "Ep 6 (Step 034440): Train loss 3.610, Val loss 3.868\n",
      "Ep 6 (Step 034450): Train loss 3.723, Val loss 3.868\n",
      "Ep 6 (Step 034460): Train loss 3.648, Val loss 3.869\n",
      "Ep 6 (Step 034470): Train loss 3.659, Val loss 3.871\n",
      "Ep 6 (Step 034480): Train loss 3.680, Val loss 3.872\n",
      "Ep 6 (Step 034490): Train loss 3.523, Val loss 3.873\n",
      "Ep 6 (Step 034500): Train loss 3.708, Val loss 3.872\n",
      "Ep 6 (Step 034510): Train loss 3.768, Val loss 3.871\n",
      "Ep 6 (Step 034520): Train loss 3.710, Val loss 3.871\n",
      "Ep 6 (Step 034530): Train loss 3.674, Val loss 3.871\n",
      "Ep 6 (Step 034540): Train loss 3.622, Val loss 3.871\n",
      "Ep 6 (Step 034550): Train loss 3.625, Val loss 3.872\n",
      "Ep 6 (Step 034560): Train loss 3.649, Val loss 3.873\n",
      "Ep 6 (Step 034570): Train loss 3.589, Val loss 3.872\n",
      "Ep 6 (Step 034580): Train loss 3.532, Val loss 3.871\n",
      "Ep 6 (Step 034590): Train loss 3.637, Val loss 3.870\n",
      "Ep 6 (Step 034600): Train loss 3.748, Val loss 3.869\n",
      "Ep 6 (Step 034610): Train loss 3.591, Val loss 3.871\n",
      "Ep 6 (Step 034620): Train loss 3.677, Val loss 3.872\n",
      "Ep 6 (Step 034630): Train loss 3.553, Val loss 3.871\n",
      "Ep 6 (Step 034640): Train loss 3.640, Val loss 3.870\n",
      "Ep 6 (Step 034650): Train loss 3.664, Val loss 3.870\n",
      "Ep 6 (Step 034660): Train loss 3.748, Val loss 3.870\n",
      "Ep 6 (Step 034670): Train loss 3.539, Val loss 3.870\n",
      "Ep 6 (Step 034680): Train loss 3.671, Val loss 3.870\n",
      "Ep 6 (Step 034690): Train loss 3.613, Val loss 3.870\n",
      "Ep 6 (Step 034700): Train loss 3.573, Val loss 3.871\n",
      "Ep 6 (Step 034710): Train loss 3.629, Val loss 3.871\n",
      "Ep 6 (Step 034720): Train loss 3.627, Val loss 3.869\n",
      "Ep 6 (Step 034730): Train loss 3.677, Val loss 3.868\n",
      "Ep 6 (Step 034740): Train loss 3.479, Val loss 3.867\n",
      "Ep 6 (Step 034750): Train loss 3.723, Val loss 3.867\n",
      "Ep 6 (Step 034760): Train loss 3.515, Val loss 3.867\n",
      "Ep 6 (Step 034770): Train loss 3.676, Val loss 3.869\n",
      "Ep 6 (Step 034780): Train loss 3.613, Val loss 3.871\n",
      "Ep 6 (Step 034790): Train loss 3.585, Val loss 3.871\n",
      "Ep 6 (Step 034800): Train loss 3.682, Val loss 3.871\n",
      "Ep 6 (Step 034810): Train loss 3.628, Val loss 3.869\n",
      "Ep 6 (Step 034820): Train loss 3.646, Val loss 3.868\n",
      "Ep 6 (Step 034830): Train loss 3.666, Val loss 3.867\n",
      "Ep 6 (Step 034840): Train loss 3.728, Val loss 3.869\n",
      "Ep 6 (Step 034850): Train loss 3.735, Val loss 3.871\n",
      "Ep 6 (Step 034860): Train loss 3.687, Val loss 3.871\n",
      "Ep 6 (Step 034870): Train loss 3.672, Val loss 3.871\n",
      "Ep 6 (Step 034880): Train loss 3.625, Val loss 3.869\n",
      "Ep 6 (Step 034890): Train loss 3.597, Val loss 3.867\n",
      "Ep 6 (Step 034900): Train loss 3.744, Val loss 3.867\n",
      "Ep 6 (Step 034910): Train loss 3.596, Val loss 3.869\n",
      "Ep 6 (Step 034920): Train loss 3.793, Val loss 3.870\n",
      "Ep 6 (Step 034930): Train loss 3.771, Val loss 3.871\n",
      "Ep 6 (Step 034940): Train loss 3.556, Val loss 3.870\n",
      "Ep 6 (Step 034950): Train loss 3.649, Val loss 3.869\n",
      "Ep 6 (Step 034960): Train loss 3.676, Val loss 3.869\n",
      "Ep 6 (Step 034970): Train loss 3.561, Val loss 3.868\n",
      "Ep 6 (Step 034980): Train loss 3.617, Val loss 3.868\n",
      "Ep 6 (Step 034990): Train loss 3.716, Val loss 3.868\n",
      "Ep 6 (Step 035000): Train loss 3.795, Val loss 3.867\n",
      "Ep 6 (Step 035010): Train loss 3.607, Val loss 3.866\n",
      "Ep 6 (Step 035020): Train loss 3.805, Val loss 3.865\n",
      "Ep 6 (Step 035030): Train loss 3.637, Val loss 3.865\n",
      "Ep 6 (Step 035040): Train loss 3.661, Val loss 3.866\n",
      "Ep 6 (Step 035050): Train loss 3.690, Val loss 3.865\n",
      "Ep 6 (Step 035060): Train loss 3.714, Val loss 3.865\n",
      "Ep 6 (Step 035070): Train loss 3.539, Val loss 3.866\n",
      "Ep 6 (Step 035080): Train loss 3.607, Val loss 3.867\n",
      "Ep 6 (Step 035090): Train loss 3.747, Val loss 3.865\n",
      "Ep 6 (Step 035100): Train loss 3.631, Val loss 3.863\n",
      "Ep 6 (Step 035110): Train loss 3.781, Val loss 3.864\n",
      "Ep 6 (Step 035120): Train loss 3.609, Val loss 3.865\n",
      "Ep 6 (Step 035130): Train loss 3.667, Val loss 3.866\n",
      "Ep 6 (Step 035140): Train loss 3.653, Val loss 3.865\n",
      "Ep 6 (Step 035150): Train loss 3.586, Val loss 3.866\n",
      "Ep 6 (Step 035160): Train loss 3.586, Val loss 3.866\n",
      "Ep 6 (Step 035170): Train loss 3.564, Val loss 3.866\n",
      "Ep 6 (Step 035180): Train loss 3.490, Val loss 3.865\n",
      "Ep 6 (Step 035190): Train loss 3.518, Val loss 3.865\n",
      "Ep 6 (Step 035200): Train loss 3.641, Val loss 3.867\n",
      "Ep 6 (Step 035210): Train loss 3.724, Val loss 3.866\n",
      "Ep 6 (Step 035220): Train loss 3.641, Val loss 3.866\n",
      "Ep 6 (Step 035230): Train loss 3.588, Val loss 3.864\n",
      "Ep 6 (Step 035240): Train loss 3.616, Val loss 3.864\n",
      "Ep 6 (Step 035250): Train loss 3.574, Val loss 3.865\n",
      "Ep 6 (Step 035260): Train loss 3.606, Val loss 3.864\n",
      "Ep 6 (Step 035270): Train loss 3.716, Val loss 3.864\n",
      "Ep 6 (Step 035280): Train loss 3.696, Val loss 3.865\n",
      "Ep 6 (Step 035290): Train loss 3.665, Val loss 3.865\n",
      "Ep 6 (Step 035300): Train loss 3.690, Val loss 3.865\n",
      "Ep 6 (Step 035310): Train loss 3.609, Val loss 3.865\n",
      "Ep 6 (Step 035320): Train loss 3.723, Val loss 3.865\n",
      "Ep 6 (Step 035330): Train loss 3.757, Val loss 3.865\n",
      "Ep 6 (Step 035340): Train loss 3.611, Val loss 3.865\n",
      "Ep 6 (Step 035350): Train loss 3.657, Val loss 3.864\n",
      "Ep 6 (Step 035360): Train loss 3.615, Val loss 3.863\n",
      "Ep 6 (Step 035370): Train loss 3.619, Val loss 3.863\n",
      "Ep 6 (Step 035380): Train loss 3.639, Val loss 3.863\n",
      "Ep 6 (Step 035390): Train loss 3.689, Val loss 3.864\n",
      "Ep 6 (Step 035400): Train loss 3.614, Val loss 3.864\n",
      "Ep 6 (Step 035410): Train loss 3.708, Val loss 3.864\n",
      "Ep 6 (Step 035420): Train loss 3.729, Val loss 3.865\n",
      "Ep 6 (Step 035430): Train loss 3.561, Val loss 3.866\n",
      "Ep 6 (Step 035440): Train loss 3.631, Val loss 3.867\n",
      "Ep 6 (Step 035450): Train loss 3.625, Val loss 3.866\n",
      "Ep 6 (Step 035460): Train loss 3.646, Val loss 3.865\n",
      "Ep 6 (Step 035470): Train loss 3.718, Val loss 3.865\n",
      "Ep 6 (Step 035480): Train loss 3.694, Val loss 3.866\n",
      "Ep 6 (Step 035490): Train loss 3.607, Val loss 3.865\n",
      "Ep 6 (Step 035500): Train loss 3.659, Val loss 3.865\n",
      "Ep 6 (Step 035510): Train loss 3.609, Val loss 3.865\n",
      "Ep 6 (Step 035520): Train loss 3.649, Val loss 3.864\n",
      "Ep 6 (Step 035530): Train loss 3.708, Val loss 3.865\n",
      "Ep 6 (Step 035540): Train loss 3.657, Val loss 3.866\n",
      "Ep 6 (Step 035550): Train loss 3.577, Val loss 3.868\n",
      "Ep 6 (Step 035560): Train loss 3.695, Val loss 3.867\n",
      "Ep 6 (Step 035570): Train loss 3.650, Val loss 3.865\n",
      "Ep 6 (Step 035580): Train loss 3.626, Val loss 3.864\n",
      "Ep 6 (Step 035590): Train loss 3.687, Val loss 3.865\n",
      "Ep 6 (Step 035600): Train loss 3.648, Val loss 3.865\n",
      "Ep 6 (Step 035610): Train loss 3.671, Val loss 3.864\n",
      "Ep 6 (Step 035620): Train loss 3.513, Val loss 3.864\n",
      "Ep 6 (Step 035630): Train loss 3.664, Val loss 3.865\n",
      "Ep 6 (Step 035640): Train loss 3.524, Val loss 3.866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6 (Step 035650): Train loss 3.620, Val loss 3.865\n",
      "Ep 6 (Step 035660): Train loss 3.591, Val loss 3.864\n",
      "Ep 6 (Step 035670): Train loss 3.582, Val loss 3.864\n",
      "Ep 6 (Step 035680): Train loss 3.576, Val loss 3.864\n",
      "Ep 6 (Step 035690): Train loss 3.463, Val loss 3.865\n",
      "Ep 6 (Step 035700): Train loss 3.649, Val loss 3.866\n",
      "Ep 6 (Step 035710): Train loss 3.674, Val loss 3.867\n",
      "Ep 6 (Step 035720): Train loss 3.478, Val loss 3.868\n",
      "Ep 6 (Step 035730): Train loss 3.679, Val loss 3.867\n",
      "Ep 6 (Step 035740): Train loss 3.590, Val loss 3.867\n",
      "Ep 6 (Step 035750): Train loss 3.683, Val loss 3.867\n",
      "Ep 6 (Step 035760): Train loss 3.601, Val loss 3.866\n",
      "Ep 6 (Step 035770): Train loss 3.635, Val loss 3.866\n",
      "Ep 6 (Step 035780): Train loss 3.536, Val loss 3.867\n",
      "Ep 6 (Step 035790): Train loss 3.666, Val loss 3.867\n",
      "Ep 6 (Step 035800): Train loss 3.722, Val loss 3.867\n",
      "Ep 6 (Step 035810): Train loss 3.550, Val loss 3.868\n",
      "Ep 6 (Step 035820): Train loss 3.484, Val loss 3.868\n",
      "Ep 6 (Step 035830): Train loss 3.627, Val loss 3.868\n",
      "Ep 6 (Step 035840): Train loss 3.564, Val loss 3.867\n",
      "Ep 6 (Step 035850): Train loss 3.618, Val loss 3.866\n",
      "Ep 6 (Step 035860): Train loss 3.481, Val loss 3.866\n",
      "Ep 6 (Step 035870): Train loss 3.695, Val loss 3.866\n",
      "Ep 6 (Step 035880): Train loss 3.611, Val loss 3.866\n",
      "Ep 6 (Step 035890): Train loss 3.578, Val loss 3.864\n",
      "Ep 6 (Step 035900): Train loss 3.625, Val loss 3.865\n",
      "Ep 6 (Step 035910): Train loss 3.772, Val loss 3.866\n",
      "Ep 6 (Step 035920): Train loss 3.640, Val loss 3.866\n",
      "Ep 6 (Step 035930): Train loss 3.517, Val loss 3.867\n",
      "Ep 6 (Step 035940): Train loss 3.592, Val loss 3.867\n",
      "Ep 6 (Step 035950): Train loss 3.738, Val loss 3.868\n",
      "Ep 6 (Step 035960): Train loss 3.709, Val loss 3.868\n",
      "Ep 6 (Step 035970): Train loss 3.682, Val loss 3.867\n",
      "Ep 6 (Step 035980): Train loss 3.641, Val loss 3.867\n",
      "Ep 6 (Step 035990): Train loss 3.601, Val loss 3.867\n",
      "Ep 6 (Step 036000): Train loss 3.653, Val loss 3.867\n",
      "Ep 6 (Step 036010): Train loss 3.500, Val loss 3.866\n",
      "Ep 6 (Step 036020): Train loss 3.744, Val loss 3.866\n",
      "Ep 6 (Step 036030): Train loss 3.582, Val loss 3.866\n",
      "Ep 6 (Step 036040): Train loss 3.505, Val loss 3.866\n",
      "Ep 6 (Step 036050): Train loss 3.775, Val loss 3.866\n",
      "Ep 6 (Step 036060): Train loss 3.598, Val loss 3.866\n",
      "Ep 6 (Step 036070): Train loss 3.538, Val loss 3.867\n",
      "Ep 6 (Step 036080): Train loss 3.554, Val loss 3.867\n",
      "Ep 6 (Step 036090): Train loss 3.543, Val loss 3.867\n",
      "Ep 6 (Step 036100): Train loss 3.585, Val loss 3.867\n",
      "Ep 6 (Step 036110): Train loss 3.713, Val loss 3.867\n",
      "Ep 6 (Step 036120): Train loss 3.797, Val loss 3.867\n",
      "Ep 6 (Step 036130): Train loss 3.654, Val loss 3.868\n",
      "Ep 6 (Step 036140): Train loss 3.438, Val loss 3.868\n",
      "Ep 6 (Step 036150): Train loss 3.601, Val loss 3.868\n",
      "Ep 6 (Step 036160): Train loss 3.710, Val loss 3.868\n",
      "Ep 6 (Step 036170): Train loss 3.695, Val loss 3.867\n",
      "Ep 6 (Step 036180): Train loss 3.689, Val loss 3.865\n",
      "Ep 6 (Step 036190): Train loss 3.600, Val loss 3.865\n",
      "Ep 6 (Step 036200): Train loss 3.658, Val loss 3.864\n",
      "Ep 6 (Step 036210): Train loss 3.603, Val loss 3.865\n",
      "Ep 6 (Step 036220): Train loss 3.713, Val loss 3.865\n",
      "Ep 6 (Step 036230): Train loss 3.646, Val loss 3.864\n",
      "Ep 6 (Step 036240): Train loss 3.688, Val loss 3.864\n",
      "Ep 6 (Step 036250): Train loss 3.666, Val loss 3.864\n",
      "Ep 6 (Step 036260): Train loss 3.555, Val loss 3.865\n",
      "Ep 6 (Step 036270): Train loss 3.639, Val loss 3.866\n",
      "Ep 6 (Step 036280): Train loss 3.756, Val loss 3.866\n",
      "Ep 6 (Step 036290): Train loss 3.706, Val loss 3.866\n",
      "Ep 6 (Step 036300): Train loss 3.507, Val loss 3.866\n",
      "Ep 6 (Step 036310): Train loss 3.651, Val loss 3.866\n",
      "Ep 6 (Step 036320): Train loss 3.608, Val loss 3.866\n",
      "Ep 6 (Step 036330): Train loss 3.636, Val loss 3.865\n",
      "Ep 6 (Step 036340): Train loss 3.540, Val loss 3.864\n",
      "Ep 6 (Step 036350): Train loss 3.610, Val loss 3.864\n",
      "Ep 6 (Step 036360): Train loss 3.702, Val loss 3.863\n",
      "Ep 6 (Step 036370): Train loss 3.551, Val loss 3.864\n",
      "Ep 6 (Step 036380): Train loss 3.590, Val loss 3.864\n",
      "Ep 6 (Step 036390): Train loss 3.573, Val loss 3.865\n",
      "Ep 6 (Step 036400): Train loss 3.496, Val loss 3.865\n",
      "Ep 6 (Step 036410): Train loss 3.613, Val loss 3.865\n",
      "Ep 6 (Step 036420): Train loss 3.624, Val loss 3.865\n",
      "Ep 6 (Step 036430): Train loss 3.746, Val loss 3.866\n",
      "Ep 6 (Step 036440): Train loss 3.642, Val loss 3.865\n",
      "Ep 6 (Step 036450): Train loss 3.733, Val loss 3.865\n",
      "Ep 6 (Step 036460): Train loss 3.577, Val loss 3.865\n",
      "Ep 6 (Step 036470): Train loss 3.699, Val loss 3.864\n",
      "Ep 6 (Step 036480): Train loss 3.683, Val loss 3.865\n",
      "Ep 6 (Step 036490): Train loss 3.651, Val loss 3.864\n",
      "Ep 6 (Step 036500): Train loss 3.563, Val loss 3.864\n",
      "Ep 6 (Step 036510): Train loss 3.589, Val loss 3.865\n",
      "Ep 6 (Step 036520): Train loss 3.472, Val loss 3.864\n",
      "Ep 6 (Step 036530): Train loss 3.750, Val loss 3.864\n",
      "Ep 6 (Step 036540): Train loss 3.696, Val loss 3.863\n",
      "Ep 6 (Step 036550): Train loss 3.539, Val loss 3.864\n",
      "Ep 6 (Step 036560): Train loss 3.453, Val loss 3.864\n",
      "Ep 6 (Step 036570): Train loss 3.677, Val loss 3.864\n",
      "Ep 6 (Step 036580): Train loss 3.661, Val loss 3.864\n",
      "Ep 6 (Step 036590): Train loss 3.715, Val loss 3.864\n",
      "Ep 6 (Step 036600): Train loss 3.720, Val loss 3.865\n",
      "Ep 6 (Step 036610): Train loss 3.661, Val loss 3.865\n",
      "Ep 6 (Step 036620): Train loss 3.677, Val loss 3.865\n",
      "Ep 6 (Step 036630): Train loss 3.597, Val loss 3.865\n",
      "Ep 6 (Step 036640): Train loss 3.557, Val loss 3.866\n",
      "Ep 6 (Step 036650): Train loss 3.597, Val loss 3.866\n",
      "Ep 6 (Step 036660): Train loss 3.593, Val loss 3.866\n",
      "Ep 6 (Step 036670): Train loss 3.608, Val loss 3.865\n",
      "Ep 6 (Step 036680): Train loss 3.643, Val loss 3.866\n",
      "Ep 6 (Step 036690): Train loss 3.509, Val loss 3.866\n",
      "Ep 6 (Step 036700): Train loss 3.509, Val loss 3.866\n",
      "Ep 6 (Step 036710): Train loss 3.588, Val loss 3.865\n",
      "Ep 6 (Step 036720): Train loss 3.669, Val loss 3.864\n",
      "Ep 6 (Step 036730): Train loss 3.583, Val loss 3.864\n",
      "Ep 6 (Step 036740): Train loss 3.677, Val loss 3.864\n",
      "Ep 6 (Step 036750): Train loss 3.635, Val loss 3.864\n",
      "Ep 6 (Step 036760): Train loss 3.772, Val loss 3.865\n",
      "Ep 6 (Step 036770): Train loss 3.606, Val loss 3.865\n",
      "Ep 6 (Step 036780): Train loss 3.652, Val loss 3.866\n",
      "Ep 6 (Step 036790): Train loss 3.639, Val loss 3.865\n",
      "Ep 6 (Step 036800): Train loss 3.643, Val loss 3.865\n",
      "Ep 6 (Step 036810): Train loss 3.633, Val loss 3.865\n",
      "Ep 6 (Step 036820): Train loss 3.656, Val loss 3.865\n",
      "Ep 6 (Step 036830): Train loss 3.650, Val loss 3.864\n",
      "Ep 6 (Step 036840): Train loss 3.501, Val loss 3.865\n",
      "Ep 6 (Step 036850): Train loss 3.564, Val loss 3.865\n",
      "Ep 6 (Step 036860): Train loss 3.691, Val loss 3.865\n",
      "Ep 6 (Step 036870): Train loss 3.589, Val loss 3.865\n",
      "Ep 6 (Step 036880): Train loss 3.608, Val loss 3.864\n",
      "Ep 6 (Step 036890): Train loss 3.563, Val loss 3.864\n",
      "Ep 6 (Step 036900): Train loss 3.617, Val loss 3.864\n",
      "Ep 6 (Step 036910): Train loss 3.813, Val loss 3.864\n",
      "Ep 6 (Step 036920): Train loss 3.506, Val loss 3.864\n",
      "Ep 6 (Step 036930): Train loss 3.663, Val loss 3.863\n",
      "Ep 6 (Step 036940): Train loss 3.563, Val loss 3.863\n",
      "Ep 6 (Step 036950): Train loss 3.617, Val loss 3.863\n",
      "Ep 6 (Step 036960): Train loss 3.609, Val loss 3.863\n",
      "Ep 6 (Step 036970): Train loss 3.634, Val loss 3.863\n",
      "Ep 6 (Step 036980): Train loss 3.591, Val loss 3.863\n",
      "Ep 6 (Step 036990): Train loss 3.736, Val loss 3.863\n",
      "Ep 6 (Step 037000): Train loss 3.631, Val loss 3.864\n",
      "Ep 6 (Step 037010): Train loss 3.691, Val loss 3.864\n",
      "Ep 6 (Step 037020): Train loss 3.618, Val loss 3.863\n",
      "Ep 6 (Step 037030): Train loss 3.641, Val loss 3.864\n",
      "Ep 6 (Step 037040): Train loss 3.665, Val loss 3.864\n",
      "Ep 6 (Step 037050): Train loss 3.608, Val loss 3.865\n",
      "Ep 6 (Step 037060): Train loss 3.548, Val loss 3.865\n",
      "Ep 6 (Step 037070): Train loss 3.618, Val loss 3.865\n",
      "Ep 6 (Step 037080): Train loss 3.561, Val loss 3.866\n",
      "Ep 6 (Step 037090): Train loss 3.732, Val loss 3.865\n",
      "Ep 6 (Step 037100): Train loss 3.668, Val loss 3.865\n",
      "Ep 6 (Step 037110): Train loss 3.603, Val loss 3.865\n",
      "Ep 6 (Step 037120): Train loss 3.597, Val loss 3.865\n",
      "Ep 6 (Step 037130): Train loss 3.542, Val loss 3.864\n",
      "Ep 6 (Step 037140): Train loss 3.790, Val loss 3.864\n",
      "Ep 6 (Step 037150): Train loss 3.680, Val loss 3.864\n",
      "Ep 6 (Step 037160): Train loss 3.621, Val loss 3.864\n",
      "Ep 6 (Step 037170): Train loss 3.584, Val loss 3.864\n",
      "Ep 6 (Step 037180): Train loss 3.528, Val loss 3.864\n",
      "Ep 6 (Step 037190): Train loss 3.693, Val loss 3.864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6 (Step 037200): Train loss 3.622, Val loss 3.864\n",
      "Ep 6 (Step 037210): Train loss 3.589, Val loss 3.865\n",
      "Ep 6 (Step 037220): Train loss 3.700, Val loss 3.864\n",
      "Ep 6 (Step 037230): Train loss 3.618, Val loss 3.863\n",
      "Ep 6 (Step 037240): Train loss 3.625, Val loss 3.863\n",
      "Ep 6 (Step 037250): Train loss 3.730, Val loss 3.863\n",
      "Ep 6 (Step 037260): Train loss 3.714, Val loss 3.863\n",
      "Ep 6 (Step 037270): Train loss 3.701, Val loss 3.863\n",
      "Ep 6 (Step 037280): Train loss 3.563, Val loss 3.862\n",
      "Ep 6 (Step 037290): Train loss 3.666, Val loss 3.863\n",
      "Ep 6 (Step 037300): Train loss 3.702, Val loss 3.863\n",
      "Ep 6 (Step 037310): Train loss 3.689, Val loss 3.863\n",
      "Ep 6 (Step 037320): Train loss 3.715, Val loss 3.863\n",
      "Ep 6 (Step 037330): Train loss 3.733, Val loss 3.863\n",
      "Ep 6 (Step 037340): Train loss 3.591, Val loss 3.863\n",
      "Ep 6 (Step 037350): Train loss 3.601, Val loss 3.863\n",
      "Ep 6 (Step 037360): Train loss 3.564, Val loss 3.863\n",
      "Ep 6 (Step 037370): Train loss 3.602, Val loss 3.863\n",
      "Ep 6 (Step 037380): Train loss 3.659, Val loss 3.863\n",
      "Ep 6 (Step 037390): Train loss 3.697, Val loss 3.864\n",
      "Ep 6 (Step 037400): Train loss 3.665, Val loss 3.864\n",
      "Ep 6 (Step 037410): Train loss 3.614, Val loss 3.863\n",
      "Ep 6 (Step 037420): Train loss 3.672, Val loss 3.863\n",
      "Ep 6 (Step 037430): Train loss 3.603, Val loss 3.863\n",
      "Ep 6 (Step 037440): Train loss 3.629, Val loss 3.863\n",
      "Ep 6 (Step 037450): Train loss 3.682, Val loss 3.863\n",
      "Ep 6 (Step 037460): Train loss 3.729, Val loss 3.863\n",
      "Ep 6 (Step 037470): Train loss 3.630, Val loss 3.863\n",
      "Ep 6 (Step 037480): Train loss 3.638, Val loss 3.863\n",
      "Ep 6 (Step 037490): Train loss 3.770, Val loss 3.863\n",
      "Ep 6 (Step 037500): Train loss 3.633, Val loss 3.863\n",
      "Ep 6 (Step 037510): Train loss 3.676, Val loss 3.863\n",
      "Ep 6 (Step 037520): Train loss 3.555, Val loss 3.863\n",
      "Ep 6 (Step 037530): Train loss 3.759, Val loss 3.863\n",
      "Ep 6 (Step 037540): Train loss 3.640, Val loss 3.863\n",
      "Ep 6 (Step 037550): Train loss 3.611, Val loss 3.863\n",
      "Ep 6 (Step 037560): Train loss 3.602, Val loss 3.863\n",
      "Ep 6 (Step 037570): Train loss 3.690, Val loss 3.863\n",
      "Ep 6 (Step 037580): Train loss 3.712, Val loss 3.863\n",
      "Ep 6 (Step 037590): Train loss 3.555, Val loss 3.863\n",
      "Ep 6 (Step 037600): Train loss 3.719, Val loss 3.863\n",
      "Ep 6 (Step 037610): Train loss 3.773, Val loss 3.863\n",
      "Ep 6 (Step 037620): Train loss 3.792, Val loss 3.863\n",
      "Ep 6 (Step 037630): Train loss 3.538, Val loss 3.863\n",
      "Ep 6 (Step 037640): Train loss 3.643, Val loss 3.863\n",
      "Ep 6 (Step 037650): Train loss 3.653, Val loss 3.863\n",
      "Ep 6 (Step 037660): Train loss 3.690, Val loss 3.863\n",
      "Ep 6 (Step 037670): Train loss 3.659, Val loss 3.863\n",
      "Ep 6 (Step 037680): Train loss 3.683, Val loss 3.862\n",
      "Ep 6 (Step 037690): Train loss 3.545, Val loss 3.863\n",
      "Ep 6 (Step 037700): Train loss 3.693, Val loss 3.862\n",
      "Ep 6 (Step 037710): Train loss 3.644, Val loss 3.863\n",
      "Ep 6 (Step 037720): Train loss 3.646, Val loss 3.863\n",
      "Ep 6 (Step 037730): Train loss 3.624, Val loss 3.863\n",
      "Ep 6 (Step 037740): Train loss 3.601, Val loss 3.862\n",
      "Ep 6 (Step 037750): Train loss 3.592, Val loss 3.862\n",
      "Ep 6 (Step 037760): Train loss 3.718, Val loss 3.862\n",
      "Ep 6 (Step 037770): Train loss 3.606, Val loss 3.862\n",
      "Ep 6 (Step 037780): Train loss 3.648, Val loss 3.862\n",
      "Ep 6 (Step 037790): Train loss 3.601, Val loss 3.862\n",
      "Ep 6 (Step 037800): Train loss 3.711, Val loss 3.862\n",
      "Ep 6 (Step 037810): Train loss 3.656, Val loss 3.862\n",
      "Ep 6 (Step 037820): Train loss 3.585, Val loss 3.862\n",
      "Ep 6 (Step 037830): Train loss 3.791, Val loss 3.862\n",
      "Ep 6 (Step 037840): Train loss 3.666, Val loss 3.862\n",
      "Ep 6 (Step 037850): Train loss 3.655, Val loss 3.862\n",
      "Ep 6 (Step 037860): Train loss 3.567, Val loss 3.862\n",
      "Ep 6 (Step 037870): Train loss 3.807, Val loss 3.863\n",
      "Ep 6 (Step 037880): Train loss 3.714, Val loss 3.863\n",
      "Ep 6 (Step 037890): Train loss 3.619, Val loss 3.863\n",
      "Ep 6 (Step 037900): Train loss 3.568, Val loss 3.863\n",
      "Ep 6 (Step 037910): Train loss 3.684, Val loss 3.863\n",
      "Ep 6 (Step 037920): Train loss 3.694, Val loss 3.863\n",
      "Ep 6 (Step 037930): Train loss 3.517, Val loss 3.863\n",
      "Ep 6 (Step 037940): Train loss 3.726, Val loss 3.863\n",
      "Ep 6 (Step 037950): Train loss 3.587, Val loss 3.863\n",
      "Ep 6 (Step 037960): Train loss 3.504, Val loss 3.862\n",
      "Ep 6 (Step 037970): Train loss 3.571, Val loss 3.862\n",
      "Ep 6 (Step 037980): Train loss 3.570, Val loss 3.863\n",
      "Ep 6 (Step 037990): Train loss 3.806, Val loss 3.863\n",
      "Ep 6 (Step 038000): Train loss 3.701, Val loss 3.863\n",
      "Ep 6 (Step 038010): Train loss 3.730, Val loss 3.863\n",
      "Ep 6 (Step 038020): Train loss 3.679, Val loss 3.863\n",
      "Ep 6 (Step 038030): Train loss 3.597, Val loss 3.863\n",
      "Ep 6 (Step 038040): Train loss 3.653, Val loss 3.863\n",
      "Ep 6 (Step 038050): Train loss 3.501, Val loss 3.863\n",
      "Ep 6 (Step 038060): Train loss 3.641, Val loss 3.863\n",
      "Ep 6 (Step 038070): Train loss 3.703, Val loss 3.863\n",
      "Ep 6 (Step 038080): Train loss 3.729, Val loss 3.863\n",
      "Ep 6 (Step 038090): Train loss 3.695, Val loss 3.863\n",
      "Ep 6 (Step 038100): Train loss 3.685, Val loss 3.863\n",
      "Ep 6 (Step 038110): Train loss 3.699, Val loss 3.863\n",
      "Ep 6 (Step 038120): Train loss 3.611, Val loss 3.863\n",
      "Ep 6 (Step 038130): Train loss 3.652, Val loss 3.863\n",
      "Ep 6 (Step 038140): Train loss 3.691, Val loss 3.863\n",
      "Ep 6 (Step 038150): Train loss 3.626, Val loss 3.863\n",
      "Ep 6 (Step 038160): Train loss 3.583, Val loss 3.863\n",
      "Ep 6 (Step 038170): Train loss 3.662, Val loss 3.863\n",
      "Ep 6 (Step 038180): Train loss 3.592, Val loss 3.863\n",
      "Ep 6 (Step 038190): Train loss 3.689, Val loss 3.863\n",
      "Ep 6 (Step 038200): Train loss 3.798, Val loss 3.862\n",
      "Ep 6 (Step 038210): Train loss 3.657, Val loss 3.862\n",
      "Ep 6 (Step 038220): Train loss 3.576, Val loss 3.862\n",
      "Ep 6 (Step 038230): Train loss 3.629, Val loss 3.862\n",
      "Ep 6 (Step 038240): Train loss 3.616, Val loss 3.862\n",
      "Ep 6 (Step 038250): Train loss 3.638, Val loss 3.862\n",
      "Ep 6 (Step 038260): Train loss 3.530, Val loss 3.862\n",
      "Ep 6 (Step 038270): Train loss 3.544, Val loss 3.862\n",
      "Ep 6 (Step 038280): Train loss 3.668, Val loss 3.862\n",
      "Ep 6 (Step 038290): Train loss 3.831, Val loss 3.862\n",
      "Ep 6 (Step 038300): Train loss 3.636, Val loss 3.862\n",
      "Ep 6 (Step 038310): Train loss 3.656, Val loss 3.862\n",
      "Ep 6 (Step 038320): Train loss 3.620, Val loss 3.861\n",
      "Ep 6 (Step 038330): Train loss 3.673, Val loss 3.861\n",
      "Ep 6 (Step 038340): Train loss 3.680, Val loss 3.861\n",
      "Ep 6 (Step 038350): Train loss 3.732, Val loss 3.861\n",
      "Ep 6 (Step 038360): Train loss 3.663, Val loss 3.861\n",
      "Ep 6 (Step 038370): Train loss 3.657, Val loss 3.861\n",
      "Ep 6 (Step 038380): Train loss 3.598, Val loss 3.861\n",
      "Ep 6 (Step 038390): Train loss 3.571, Val loss 3.861\n",
      "Ep 6 (Step 038400): Train loss 3.591, Val loss 3.861\n",
      "Ep 6 (Step 038410): Train loss 3.594, Val loss 3.861\n",
      "Ep 6 (Step 038420): Train loss 3.744, Val loss 3.862\n",
      "Ep 6 (Step 038430): Train loss 3.745, Val loss 3.862\n",
      "Ep 6 (Step 038440): Train loss 3.538, Val loss 3.862\n",
      "Ep 6 (Step 038450): Train loss 3.614, Val loss 3.862\n",
      "Ep 6 (Step 038460): Train loss 3.601, Val loss 3.862\n",
      "Ep 6 (Step 038470): Train loss 3.676, Val loss 3.862\n",
      "Ep 6 (Step 038480): Train loss 3.631, Val loss 3.862\n",
      "Ep 6 (Step 038490): Train loss 3.614, Val loss 3.862\n",
      "Ep 6 (Step 038500): Train loss 3.618, Val loss 3.862\n",
      "Ep 6 (Step 038510): Train loss 3.662, Val loss 3.862\n",
      "Ep 6 (Step 038520): Train loss 3.641, Val loss 3.862\n",
      "Ep 6 (Step 038530): Train loss 3.716, Val loss 3.862\n",
      "Ep 6 (Step 038540): Train loss 3.571, Val loss 3.862\n",
      "Ep 6 (Step 038550): Train loss 3.631, Val loss 3.862\n",
      "Ep 6 (Step 038560): Train loss 3.629, Val loss 3.862\n",
      "Ep 6 (Step 038570): Train loss 3.677, Val loss 3.862\n",
      "Ep 6 (Step 038580): Train loss 3.640, Val loss 3.862\n",
      "Ep 6 (Step 038590): Train loss 3.594, Val loss 3.862\n",
      "Ep 6 (Step 038600): Train loss 3.669, Val loss 3.862\n",
      "Ep 6 (Step 038610): Train loss 3.525, Val loss 3.862\n",
      "Ep 6 (Step 038620): Train loss 3.768, Val loss 3.862\n",
      "Ep 6 (Step 038630): Train loss 3.654, Val loss 3.862\n",
      "Ep 6 (Step 038640): Train loss 3.444, Val loss 3.862\n",
      "Ep 6 (Step 038650): Train loss 3.694, Val loss 3.862\n",
      "Ep 6 (Step 038660): Train loss 3.780, Val loss 3.862\n",
      "Ep 6 (Step 038670): Train loss 3.557, Val loss 3.862\n",
      "Ep 6 (Step 038680): Train loss 3.609, Val loss 3.862\n",
      "Ep 6 (Step 038690): Train loss 3.676, Val loss 3.862\n",
      "Ep 6 (Step 038700): Train loss 3.642, Val loss 3.862\n",
      "Ep 6 (Step 038710): Train loss 3.632, Val loss 3.862\n",
      "Ep 6 (Step 038720): Train loss 3.615, Val loss 3.862\n",
      "Ep 6 (Step 038730): Train loss 3.708, Val loss 3.862\n",
      "Ep 6 (Step 038740): Train loss 3.758, Val loss 3.862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6 (Step 038750): Train loss 3.610, Val loss 3.862\n",
      "Ep 6 (Step 038760): Train loss 3.723, Val loss 3.862\n",
      "Ep 6 (Step 038770): Train loss 3.626, Val loss 3.862\n",
      "Ep 6 (Step 038780): Train loss 3.593, Val loss 3.862\n",
      "Ep 6 (Step 038790): Train loss 3.625, Val loss 3.862\n",
      "Ep 6 (Step 038800): Train loss 3.631, Val loss 3.862\n",
      "Ep 6 (Step 038810): Train loss 3.575, Val loss 3.862\n",
      "Ep 6 (Step 038820): Train loss 3.710, Val loss 3.862\n",
      "Ep 6 (Step 038830): Train loss 3.696, Val loss 3.862\n",
      "Ep 6 (Step 038840): Train loss 3.680, Val loss 3.862\n",
      "Ep 6 (Step 038850): Train loss 3.729, Val loss 3.862\n",
      "Ep 6 (Step 038860): Train loss 3.526, Val loss 3.862\n",
      "Ep 6 (Step 038870): Train loss 3.595, Val loss 3.862\n",
      "Ep 6 (Step 038880): Train loss 3.568, Val loss 3.862\n",
      "Ep 6 (Step 038890): Train loss 3.481, Val loss 3.862\n",
      "Ep 6 (Step 038900): Train loss 3.660, Val loss 3.862\n",
      "Ep 6 (Step 038910): Train loss 3.585, Val loss 3.862\n",
      "Ep 6 (Step 038920): Train loss 3.581, Val loss 3.862\n",
      "Ep 6 (Step 038930): Train loss 3.747, Val loss 3.862\n",
      "Ep 6 (Step 038940): Train loss 3.645, Val loss 3.862\n",
      "Ep 6 (Step 038950): Train loss 3.685, Val loss 3.862\n",
      "Ep 6 (Step 038960): Train loss 3.560, Val loss 3.862\n",
      "Ep 6 (Step 038970): Train loss 3.627, Val loss 3.862\n",
      "Ep 6 (Step 038980): Train loss 3.504, Val loss 3.862\n",
      "Ep 6 (Step 038990): Train loss 3.582, Val loss 3.862\n",
      "Ep 6 (Step 039000): Train loss 3.638, Val loss 3.862\n",
      "Ep 6 (Step 039010): Train loss 3.705, Val loss 3.862\n",
      "Ep 6 (Step 039020): Train loss 3.688, Val loss 3.862\n",
      "Ep 6 (Step 039030): Train loss 3.648, Val loss 3.862\n",
      "Ep 6 (Step 039040): Train loss 3.560, Val loss 3.862\n",
      "Ep 6 (Step 039050): Train loss 3.501, Val loss 3.862\n",
      "Ep 6 (Step 039060): Train loss 3.635, Val loss 3.862\n",
      "Ep 6 (Step 039070): Train loss 3.587, Val loss 3.862\n",
      "Ep 6 (Step 039080): Train loss 3.539, Val loss 3.862\n",
      "Ep 6 (Step 039090): Train loss 3.712, Val loss 3.862\n",
      "Ep 6 (Step 039100): Train loss 3.487, Val loss 3.862\n",
      "Ep 6 (Step 039110): Train loss 3.600, Val loss 3.862\n",
      "Ep 6 (Step 039120): Train loss 3.656, Val loss 3.862\n",
      "Ep 6 (Step 039130): Train loss 3.639, Val loss 3.862\n",
      "Ep 6 (Step 039140): Train loss 3.635, Val loss 3.862\n",
      "Ep 6 (Step 039150): Train loss 3.662, Val loss 3.862\n",
      "Ep 6 (Step 039160): Train loss 3.592, Val loss 3.862\n",
      "Ep 6 (Step 039170): Train loss 3.658, Val loss 3.862\n",
      "Ep 6 (Step 039180): Train loss 3.679, Val loss 3.862\n",
      "Ep 6 (Step 039190): Train loss 3.672, Val loss 3.862\n",
      "Ep 6 (Step 039200): Train loss 3.545, Val loss 3.862\n",
      "Ep 6 (Step 039210): Train loss 3.577, Val loss 3.862\n",
      "Ep 6 (Step 039220): Train loss 3.704, Val loss 3.862\n",
      "Ep 6 (Step 039230): Train loss 3.572, Val loss 3.862\n",
      "Ep 6 (Step 039240): Train loss 3.701, Val loss 3.862\n",
      "Ep 6 (Step 039250): Train loss 3.592, Val loss 3.862\n",
      "Ep 6 (Step 039260): Train loss 3.571, Val loss 3.862\n",
      "Ep 6 (Step 039270): Train loss 3.518, Val loss 3.862\n",
      "Ep 6 (Step 039280): Train loss 3.700, Val loss 3.862\n",
      "Ep 6 (Step 039290): Train loss 3.536, Val loss 3.862\n",
      "Ep 6 (Step 039300): Train loss 3.653, Val loss 3.862\n",
      "Ep 6 (Step 039310): Train loss 3.622, Val loss 3.862\n",
      "Ep 6 (Step 039320): Train loss 3.607, Val loss 3.862\n",
      "Ep 6 (Step 039330): Train loss 3.712, Val loss 3.862\n",
      "Ep 6 (Step 039340): Train loss 3.537, Val loss 3.862\n",
      "Ep 6 (Step 039350): Train loss 3.691, Val loss 3.862\n",
      "Ep 6 (Step 039360): Train loss 3.629, Val loss 3.862\n",
      "Ep 6 (Step 039370): Train loss 3.607, Val loss 3.862\n",
      "Ep 6 (Step 039380): Train loss 3.498, Val loss 3.862\n",
      "Ep 6 (Step 039390): Train loss 3.589, Val loss 3.862\n",
      "Ep 6 (Step 039400): Train loss 3.671, Val loss 3.862\n",
      "Ep 6 (Step 039410): Train loss 3.630, Val loss 3.862\n",
      "Ep 6 (Step 039420): Train loss 3.548, Val loss 3.862\n",
      "Ep 6 (Step 039430): Train loss 3.717, Val loss 3.862\n",
      "Ep 6 (Step 039440): Train loss 3.673, Val loss 3.862\n",
      "Ep 6 (Step 039450): Train loss 3.538, Val loss 3.862\n",
      "Ep 6 (Step 039460): Train loss 3.613, Val loss 3.862\n",
      "Ep 6 (Step 039470): Train loss 3.569, Val loss 3.862\n",
      "Ep 6 (Step 039480): Train loss 3.618, Val loss 3.862\n",
      "Ep 6 (Step 039490): Train loss 3.528, Val loss 3.862\n",
      "Ep 6 (Step 039500): Train loss 3.658, Val loss 3.862\n",
      "Ep 6 (Step 039510): Train loss 3.647, Val loss 3.862\n",
      "Ep 6 (Step 039520): Train loss 3.627, Val loss 3.862\n",
      "Ep 6 (Step 039530): Train loss 3.737, Val loss 3.862\n",
      "Ep 6 (Step 039540): Train loss 3.661, Val loss 3.862\n",
      "Ep 6 (Step 039550): Train loss 3.750, Val loss 3.862\n",
      "Ep 6 (Step 039560): Train loss 3.547, Val loss 3.862\n",
      "Ep 6 (Step 039570): Train loss 3.559, Val loss 3.862\n",
      "Ep 6 (Step 039580): Train loss 3.714, Val loss 3.862\n",
      "Ep 6 (Step 039590): Train loss 3.630, Val loss 3.862\n",
      "Ep 6 (Step 039600): Train loss 3.655, Val loss 3.862\n",
      "Ep 6 (Step 039610): Train loss 3.708, Val loss 3.862\n",
      "Ep 6 (Step 039620): Train loss 3.645, Val loss 3.862\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_81582/2232791355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model on all works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train(train_loader, val_loader, num_epochs=6,\n\u001b[0m\u001b[1;32m      4\u001b[0m       eval_iter=10, model_prefix=\"model_768_12_12\");\n",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_81582/4111494681.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, num_epochs, eval_iter, lr, generate_sample_text, sample_text, model_prefix)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Pass train_losses and val_losses as references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     train_model_simple(\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/Build GPT from scratch/pre_train.py\u001b[0m in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs, eval_iter, cfg, train_losses, val_losses, track_tokens_seen, start_context, tokenizer, generate_sample_text, model_prefix)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reset gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=6,\n",
    "      eval_iter=10, model_prefix=\"model_768_12_12\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8539769d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.187, Val loss 10.130\n",
      "Ep 1 (Step 000010): Train loss 8.149, Val loss 8.061\n",
      "Ep 1 (Step 000020): Train loss 6.993, Val loss 6.950\n",
      "Ep 1 (Step 000030): Train loss 6.759, Val loss 6.643\n",
      "Ep 1 (Step 000040): Train loss 6.492, Val loss 6.514\n",
      "Ep 1 (Step 000050): Train loss 6.360, Val loss 6.387\n",
      "Ep 1 (Step 000060): Train loss 6.197, Val loss 6.259\n",
      "Ep 1 (Step 000070): Train loss 6.069, Val loss 6.151\n",
      "Ep 1 (Step 000080): Train loss 6.023, Val loss 6.030\n",
      "Ep 1 (Step 000090): Train loss 5.912, Val loss 5.981\n",
      "Ep 1 (Step 000100): Train loss 5.880, Val loss 5.877\n",
      "Ep 1 (Step 000110): Train loss 5.740, Val loss 5.806\n",
      "Ep 1 (Step 000120): Train loss 5.697, Val loss 5.740\n",
      "Ep 1 (Step 000130): Train loss 5.682, Val loss 5.696\n",
      "Ep 1 (Step 000140): Train loss 5.636, Val loss 5.658\n",
      "Ep 1 (Step 000150): Train loss 5.696, Val loss 5.644\n",
      "Ep 1 (Step 000160): Train loss 5.498, Val loss 5.630\n",
      "Ep 1 (Step 000170): Train loss 5.555, Val loss 5.595\n",
      "Ep 1 (Step 000180): Train loss 5.470, Val loss 5.548\n",
      "Ep 1 (Step 000190): Train loss 5.389, Val loss 5.497\n",
      "Ep 1 (Step 000200): Train loss 5.379, Val loss 5.469\n",
      "Ep 1 (Step 000210): Train loss 5.366, Val loss 5.472\n",
      "Ep 1 (Step 000220): Train loss 5.333, Val loss 5.469\n",
      "Ep 1 (Step 000230): Train loss 5.286, Val loss 5.420\n",
      "Ep 1 (Step 000240): Train loss 5.310, Val loss 5.384\n",
      "Ep 1 (Step 000250): Train loss 5.277, Val loss 5.345\n",
      "Ep 1 (Step 000260): Train loss 5.327, Val loss 5.324\n",
      "Ep 1 (Step 000270): Train loss 5.269, Val loss 5.317\n",
      "Ep 1 (Step 000280): Train loss 5.274, Val loss 5.311\n",
      "Ep 1 (Step 000290): Train loss 5.394, Val loss 5.274\n",
      "Ep 1 (Step 000300): Train loss 5.235, Val loss 5.251\n",
      "Ep 1 (Step 000310): Train loss 5.233, Val loss 5.217\n",
      "Ep 1 (Step 000320): Train loss 5.197, Val loss 5.218\n",
      "Ep 1 (Step 000330): Train loss 5.222, Val loss 5.223\n",
      "Ep 1 (Step 000340): Train loss 5.263, Val loss 5.166\n",
      "Ep 1 (Step 000350): Train loss 5.068, Val loss 5.136\n",
      "Ep 1 (Step 000360): Train loss 5.246, Val loss 5.141\n",
      "Ep 1 (Step 000370): Train loss 5.211, Val loss 5.108\n",
      "Ep 1 (Step 000380): Train loss 5.129, Val loss 5.091\n",
      "Ep 1 (Step 000390): Train loss 5.063, Val loss 5.088\n",
      "Ep 1 (Step 000400): Train loss 5.053, Val loss 5.091\n",
      "Ep 1 (Step 000410): Train loss 5.084, Val loss 5.073\n",
      "Ep 1 (Step 000420): Train loss 5.035, Val loss 5.045\n",
      "Ep 1 (Step 000430): Train loss 5.056, Val loss 5.034\n",
      "Ep 1 (Step 000440): Train loss 5.131, Val loss 5.010\n",
      "Ep 1 (Step 000450): Train loss 5.167, Val loss 5.006\n",
      "Ep 1 (Step 000460): Train loss 4.995, Val loss 5.016\n",
      "Ep 1 (Step 000470): Train loss 4.967, Val loss 5.023\n",
      "Ep 1 (Step 000480): Train loss 5.026, Val loss 4.997\n",
      "Ep 1 (Step 000490): Train loss 4.971, Val loss 4.974\n",
      "Ep 1 (Step 000500): Train loss 4.961, Val loss 4.963\n",
      "Ep 1 (Step 000510): Train loss 4.931, Val loss 4.950\n",
      "Ep 1 (Step 000520): Train loss 4.993, Val loss 4.966\n",
      "Ep 1 (Step 000530): Train loss 4.927, Val loss 4.946\n",
      "Ep 1 (Step 000540): Train loss 5.103, Val loss 4.938\n",
      "Ep 1 (Step 000550): Train loss 4.957, Val loss 4.940\n",
      "Ep 1 (Step 000560): Train loss 4.956, Val loss 4.964\n",
      "Ep 1 (Step 000570): Train loss 4.965, Val loss 4.939\n",
      "Ep 1 (Step 000580): Train loss 5.004, Val loss 4.920\n",
      "Ep 1 (Step 000590): Train loss 4.943, Val loss 4.931\n",
      "Ep 1 (Step 000600): Train loss 5.034, Val loss 4.928\n",
      "Ep 1 (Step 000610): Train loss 5.001, Val loss 4.906\n",
      "Ep 1 (Step 000620): Train loss 4.861, Val loss 4.912\n",
      "Ep 1 (Step 000630): Train loss 4.837, Val loss 4.889\n",
      "Ep 1 (Step 000640): Train loss 4.859, Val loss 4.898\n",
      "Ep 1 (Step 000650): Train loss 4.901, Val loss 4.894\n",
      "Ep 1 (Step 000660): Train loss 4.782, Val loss 4.882\n",
      "Ep 1 (Step 000670): Train loss 4.900, Val loss 4.894\n",
      "Ep 1 (Step 000680): Train loss 4.832, Val loss 4.886\n",
      "Ep 1 (Step 000690): Train loss 4.821, Val loss 4.900\n",
      "Ep 1 (Step 000700): Train loss 4.877, Val loss 4.909\n",
      "Ep 1 (Step 000710): Train loss 4.833, Val loss 4.899\n",
      "Ep 1 (Step 000720): Train loss 4.792, Val loss 4.895\n",
      "Ep 1 (Step 000730): Train loss 4.887, Val loss 4.903\n",
      "Ep 1 (Step 000740): Train loss 4.850, Val loss 4.896\n",
      "Ep 1 (Step 000750): Train loss 4.927, Val loss 4.877\n",
      "Ep 1 (Step 000760): Train loss 4.724, Val loss 4.839\n",
      "Ep 1 (Step 000770): Train loss 4.810, Val loss 4.845\n",
      "Ep 1 (Step 000780): Train loss 4.832, Val loss 4.844\n",
      "Ep 1 (Step 000790): Train loss 4.783, Val loss 4.834\n",
      "Ep 1 (Step 000800): Train loss 4.776, Val loss 4.838\n",
      "Ep 1 (Step 000810): Train loss 4.790, Val loss 4.825\n",
      "Ep 1 (Step 000820): Train loss 4.837, Val loss 4.816\n",
      "Ep 2 (Step 000830): Train loss 4.805, Val loss 4.806\n",
      "Ep 2 (Step 000840): Train loss 4.774, Val loss 4.798\n",
      "Ep 2 (Step 000850): Train loss 4.794, Val loss 4.828\n",
      "Ep 2 (Step 000860): Train loss 4.792, Val loss 4.827\n",
      "Ep 2 (Step 000870): Train loss 4.701, Val loss 4.809\n",
      "Ep 2 (Step 000880): Train loss 4.726, Val loss 4.789\n",
      "Ep 2 (Step 000890): Train loss 4.710, Val loss 4.788\n",
      "Ep 2 (Step 000900): Train loss 4.682, Val loss 4.779\n",
      "Ep 2 (Step 000910): Train loss 4.726, Val loss 4.790\n",
      "Ep 2 (Step 000920): Train loss 4.687, Val loss 4.779\n",
      "Ep 2 (Step 000930): Train loss 4.650, Val loss 4.777\n",
      "Ep 2 (Step 000940): Train loss 4.702, Val loss 4.778\n",
      "Ep 2 (Step 000950): Train loss 4.595, Val loss 4.776\n",
      "Ep 2 (Step 000960): Train loss 4.618, Val loss 4.754\n",
      "Ep 2 (Step 000970): Train loss 4.691, Val loss 4.759\n",
      "Ep 2 (Step 000980): Train loss 4.678, Val loss 4.752\n",
      "Ep 2 (Step 000990): Train loss 4.776, Val loss 4.748\n",
      "Ep 2 (Step 001000): Train loss 4.586, Val loss 4.753\n",
      "Ep 2 (Step 001010): Train loss 4.618, Val loss 4.744\n",
      "Ep 2 (Step 001020): Train loss 4.787, Val loss 4.765\n",
      "Ep 2 (Step 001030): Train loss 4.678, Val loss 4.747\n",
      "Ep 2 (Step 001040): Train loss 4.624, Val loss 4.734\n",
      "Ep 2 (Step 001050): Train loss 4.716, Val loss 4.739\n",
      "Ep 2 (Step 001060): Train loss 4.580, Val loss 4.735\n",
      "Ep 2 (Step 001070): Train loss 4.603, Val loss 4.721\n",
      "Ep 2 (Step 001080): Train loss 4.581, Val loss 4.738\n",
      "Ep 2 (Step 001090): Train loss 4.640, Val loss 4.738\n",
      "Ep 2 (Step 001100): Train loss 4.644, Val loss 4.718\n",
      "Ep 2 (Step 001110): Train loss 4.681, Val loss 4.726\n",
      "Ep 2 (Step 001120): Train loss 4.617, Val loss 4.731\n",
      "Ep 2 (Step 001130): Train loss 4.593, Val loss 4.724\n",
      "Ep 2 (Step 001140): Train loss 4.635, Val loss 4.723\n",
      "Ep 2 (Step 001150): Train loss 4.544, Val loss 4.719\n",
      "Ep 2 (Step 001160): Train loss 4.532, Val loss 4.698\n",
      "Ep 2 (Step 001170): Train loss 4.499, Val loss 4.699\n",
      "Ep 2 (Step 001180): Train loss 4.569, Val loss 4.688\n",
      "Ep 2 (Step 001190): Train loss 4.604, Val loss 4.698\n",
      "Ep 2 (Step 001200): Train loss 4.601, Val loss 4.701\n",
      "Ep 2 (Step 001210): Train loss 4.552, Val loss 4.691\n",
      "Ep 2 (Step 001220): Train loss 4.540, Val loss 4.686\n",
      "Ep 2 (Step 001230): Train loss 4.567, Val loss 4.691\n",
      "Ep 2 (Step 001240): Train loss 4.499, Val loss 4.677\n",
      "Ep 2 (Step 001250): Train loss 4.498, Val loss 4.674\n",
      "Ep 2 (Step 001260): Train loss 4.534, Val loss 4.667\n",
      "Ep 2 (Step 001270): Train loss 4.514, Val loss 4.662\n",
      "Ep 2 (Step 001280): Train loss 4.443, Val loss 4.669\n",
      "Ep 2 (Step 001290): Train loss 4.541, Val loss 4.672\n",
      "Ep 2 (Step 001300): Train loss 4.507, Val loss 4.678\n",
      "Ep 2 (Step 001310): Train loss 4.520, Val loss 4.665\n",
      "Ep 2 (Step 001320): Train loss 4.589, Val loss 4.663\n",
      "Ep 2 (Step 001330): Train loss 4.521, Val loss 4.649\n",
      "Ep 2 (Step 001340): Train loss 4.475, Val loss 4.645\n",
      "Ep 2 (Step 001350): Train loss 4.498, Val loss 4.651\n",
      "Ep 2 (Step 001360): Train loss 4.513, Val loss 4.649\n",
      "Ep 2 (Step 001370): Train loss 4.539, Val loss 4.643\n",
      "Ep 2 (Step 001380): Train loss 4.526, Val loss 4.634\n",
      "Ep 2 (Step 001390): Train loss 4.556, Val loss 4.633\n",
      "Ep 2 (Step 001400): Train loss 4.470, Val loss 4.645\n",
      "Ep 2 (Step 001410): Train loss 4.528, Val loss 4.629\n",
      "Ep 2 (Step 001420): Train loss 4.564, Val loss 4.624\n",
      "Ep 2 (Step 001430): Train loss 4.548, Val loss 4.624\n",
      "Ep 2 (Step 001440): Train loss 4.500, Val loss 4.626\n",
      "Ep 2 (Step 001450): Train loss 4.440, Val loss 4.620\n",
      "Ep 2 (Step 001460): Train loss 4.436, Val loss 4.615\n",
      "Ep 2 (Step 001470): Train loss 4.407, Val loss 4.603\n",
      "Ep 2 (Step 001480): Train loss 4.483, Val loss 4.592\n",
      "Ep 2 (Step 001490): Train loss 4.401, Val loss 4.598\n",
      "Ep 2 (Step 001500): Train loss 4.383, Val loss 4.583\n",
      "Ep 2 (Step 001510): Train loss 4.412, Val loss 4.594\n",
      "Ep 2 (Step 001520): Train loss 4.435, Val loss 4.594\n",
      "Ep 2 (Step 001530): Train loss 4.451, Val loss 4.590\n",
      "Ep 2 (Step 001540): Train loss 4.433, Val loss 4.589\n",
      "Ep 2 (Step 001550): Train loss 4.403, Val loss 4.585\n",
      "Ep 2 (Step 001560): Train loss 4.401, Val loss 4.580\n",
      "Ep 2 (Step 001570): Train loss 4.376, Val loss 4.572\n",
      "Ep 2 (Step 001580): Train loss 4.361, Val loss 4.563\n",
      "Ep 2 (Step 001590): Train loss 4.412, Val loss 4.579\n",
      "Ep 2 (Step 001600): Train loss 4.435, Val loss 4.585\n",
      "Ep 2 (Step 001610): Train loss 4.472, Val loss 4.577\n",
      "Ep 2 (Step 001620): Train loss 4.427, Val loss 4.585\n",
      "Ep 2 (Step 001630): Train loss 4.430, Val loss 4.577\n",
      "Ep 2 (Step 001640): Train loss 4.462, Val loss 4.561\n",
      "Ep 3 (Step 001650): Train loss 4.357, Val loss 4.560\n",
      "Ep 3 (Step 001660): Train loss 4.412, Val loss 4.568\n",
      "Ep 3 (Step 001670): Train loss 4.424, Val loss 4.560\n",
      "Ep 3 (Step 001680): Train loss 4.356, Val loss 4.572\n",
      "Ep 3 (Step 001690): Train loss 4.354, Val loss 4.558\n",
      "Ep 3 (Step 001700): Train loss 4.418, Val loss 4.545\n",
      "Ep 3 (Step 001710): Train loss 4.367, Val loss 4.558\n",
      "Ep 3 (Step 001720): Train loss 4.451, Val loss 4.547\n",
      "Ep 3 (Step 001730): Train loss 4.412, Val loss 4.553\n",
      "Ep 3 (Step 001740): Train loss 4.346, Val loss 4.540\n",
      "Ep 3 (Step 001750): Train loss 4.359, Val loss 4.540\n",
      "Ep 3 (Step 001760): Train loss 4.312, Val loss 4.528\n",
      "Ep 3 (Step 001770): Train loss 4.292, Val loss 4.528\n",
      "Ep 3 (Step 001780): Train loss 4.353, Val loss 4.540\n",
      "Ep 3 (Step 001790): Train loss 4.368, Val loss 4.542\n",
      "Ep 3 (Step 001800): Train loss 4.254, Val loss 4.536\n",
      "Ep 3 (Step 001810): Train loss 4.383, Val loss 4.542\n",
      "Ep 3 (Step 001820): Train loss 4.311, Val loss 4.523\n",
      "Ep 3 (Step 001830): Train loss 4.356, Val loss 4.523\n",
      "Ep 3 (Step 001840): Train loss 4.270, Val loss 4.523\n",
      "Ep 3 (Step 001850): Train loss 4.350, Val loss 4.520\n",
      "Ep 3 (Step 001860): Train loss 4.407, Val loss 4.523\n",
      "Ep 3 (Step 001870): Train loss 4.398, Val loss 4.525\n",
      "Ep 3 (Step 001880): Train loss 4.202, Val loss 4.529\n",
      "Ep 3 (Step 001890): Train loss 4.382, Val loss 4.522\n",
      "Ep 3 (Step 001900): Train loss 4.212, Val loss 4.516\n",
      "Ep 3 (Step 001910): Train loss 4.231, Val loss 4.521\n",
      "Ep 3 (Step 001920): Train loss 4.290, Val loss 4.515\n",
      "Ep 3 (Step 001930): Train loss 4.256, Val loss 4.506\n",
      "Ep 3 (Step 001940): Train loss 4.235, Val loss 4.502\n",
      "Ep 3 (Step 001950): Train loss 4.276, Val loss 4.508\n",
      "Ep 3 (Step 001960): Train loss 4.258, Val loss 4.499\n",
      "Ep 3 (Step 001970): Train loss 4.234, Val loss 4.504\n",
      "Ep 3 (Step 001980): Train loss 4.225, Val loss 4.499\n",
      "Ep 3 (Step 001990): Train loss 4.368, Val loss 4.504\n",
      "Ep 3 (Step 002000): Train loss 4.188, Val loss 4.509\n",
      "Ep 3 (Step 002010): Train loss 4.312, Val loss 4.499\n",
      "Ep 3 (Step 002020): Train loss 4.278, Val loss 4.504\n",
      "Ep 3 (Step 002030): Train loss 4.207, Val loss 4.497\n",
      "Ep 3 (Step 002040): Train loss 4.316, Val loss 4.504\n",
      "Ep 3 (Step 002050): Train loss 4.235, Val loss 4.500\n",
      "Ep 3 (Step 002060): Train loss 4.193, Val loss 4.496\n",
      "Ep 3 (Step 002070): Train loss 4.276, Val loss 4.492\n",
      "Ep 3 (Step 002080): Train loss 4.235, Val loss 4.481\n",
      "Ep 3 (Step 002090): Train loss 4.246, Val loss 4.475\n",
      "Ep 3 (Step 002100): Train loss 4.257, Val loss 4.491\n",
      "Ep 3 (Step 002110): Train loss 4.228, Val loss 4.479\n",
      "Ep 3 (Step 002120): Train loss 4.281, Val loss 4.486\n",
      "Ep 3 (Step 002130): Train loss 4.273, Val loss 4.482\n",
      "Ep 3 (Step 002140): Train loss 4.262, Val loss 4.482\n",
      "Ep 3 (Step 002150): Train loss 4.216, Val loss 4.476\n",
      "Ep 3 (Step 002160): Train loss 4.137, Val loss 4.484\n",
      "Ep 3 (Step 002170): Train loss 4.180, Val loss 4.474\n",
      "Ep 3 (Step 002180): Train loss 4.279, Val loss 4.479\n",
      "Ep 3 (Step 002190): Train loss 4.207, Val loss 4.470\n",
      "Ep 3 (Step 002200): Train loss 4.182, Val loss 4.477\n",
      "Ep 3 (Step 002210): Train loss 4.191, Val loss 4.465\n",
      "Ep 3 (Step 002220): Train loss 4.157, Val loss 4.455\n",
      "Ep 3 (Step 002230): Train loss 4.140, Val loss 4.462\n",
      "Ep 3 (Step 002240): Train loss 4.225, Val loss 4.465\n",
      "Ep 3 (Step 002250): Train loss 4.202, Val loss 4.447\n",
      "Ep 3 (Step 002260): Train loss 4.068, Val loss 4.446\n",
      "Ep 3 (Step 002270): Train loss 4.271, Val loss 4.438\n",
      "Ep 3 (Step 002280): Train loss 4.215, Val loss 4.445\n",
      "Ep 3 (Step 002290): Train loss 4.152, Val loss 4.443\n",
      "Ep 3 (Step 002300): Train loss 4.163, Val loss 4.443\n",
      "Ep 3 (Step 002310): Train loss 4.255, Val loss 4.451\n",
      "Ep 3 (Step 002320): Train loss 4.150, Val loss 4.430\n",
      "Ep 3 (Step 002330): Train loss 4.211, Val loss 4.450\n",
      "Ep 3 (Step 002340): Train loss 4.181, Val loss 4.429\n",
      "Ep 3 (Step 002350): Train loss 4.204, Val loss 4.419\n",
      "Ep 3 (Step 002360): Train loss 4.166, Val loss 4.429\n",
      "Ep 3 (Step 002370): Train loss 4.171, Val loss 4.429\n",
      "Ep 3 (Step 002380): Train loss 4.184, Val loss 4.426\n",
      "Ep 3 (Step 002390): Train loss 4.169, Val loss 4.420\n",
      "Ep 3 (Step 002400): Train loss 4.111, Val loss 4.425\n",
      "Ep 3 (Step 002410): Train loss 4.082, Val loss 4.414\n",
      "Ep 3 (Step 002420): Train loss 4.168, Val loss 4.427\n",
      "Ep 3 (Step 002430): Train loss 4.142, Val loss 4.413\n",
      "Ep 3 (Step 002440): Train loss 4.189, Val loss 4.431\n",
      "Ep 3 (Step 002450): Train loss 4.116, Val loss 4.419\n",
      "Ep 3 (Step 002460): Train loss 4.084, Val loss 4.420\n",
      "Ep 4 (Step 002470): Train loss 4.077, Val loss 4.414\n",
      "Ep 4 (Step 002480): Train loss 4.130, Val loss 4.417\n",
      "Ep 4 (Step 002490): Train loss 4.098, Val loss 4.414\n",
      "Ep 4 (Step 002500): Train loss 4.256, Val loss 4.417\n",
      "Ep 4 (Step 002510): Train loss 4.158, Val loss 4.410\n",
      "Ep 4 (Step 002520): Train loss 4.132, Val loss 4.399\n",
      "Ep 4 (Step 002530): Train loss 4.016, Val loss 4.403\n",
      "Ep 4 (Step 002540): Train loss 4.109, Val loss 4.404\n",
      "Ep 4 (Step 002550): Train loss 4.141, Val loss 4.413\n",
      "Ep 4 (Step 002560): Train loss 4.125, Val loss 4.411\n",
      "Ep 4 (Step 002570): Train loss 4.056, Val loss 4.406\n",
      "Ep 4 (Step 002580): Train loss 4.081, Val loss 4.408\n",
      "Ep 4 (Step 002590): Train loss 4.073, Val loss 4.406\n",
      "Ep 4 (Step 002600): Train loss 4.155, Val loss 4.402\n",
      "Ep 4 (Step 002610): Train loss 4.054, Val loss 4.398\n",
      "Ep 4 (Step 002620): Train loss 4.033, Val loss 4.397\n",
      "Ep 4 (Step 002630): Train loss 4.054, Val loss 4.391\n",
      "Ep 4 (Step 002640): Train loss 4.103, Val loss 4.398\n",
      "Ep 4 (Step 002650): Train loss 4.010, Val loss 4.396\n",
      "Ep 4 (Step 002660): Train loss 4.069, Val loss 4.403\n",
      "Ep 4 (Step 002670): Train loss 4.062, Val loss 4.396\n",
      "Ep 4 (Step 002680): Train loss 4.029, Val loss 4.402\n",
      "Ep 4 (Step 002690): Train loss 4.001, Val loss 4.404\n",
      "Ep 4 (Step 002700): Train loss 4.035, Val loss 4.405\n",
      "Ep 4 (Step 002710): Train loss 4.037, Val loss 4.407\n",
      "Ep 4 (Step 002720): Train loss 4.065, Val loss 4.399\n",
      "Ep 4 (Step 002730): Train loss 4.073, Val loss 4.400\n",
      "Ep 4 (Step 002740): Train loss 4.049, Val loss 4.409\n",
      "Ep 4 (Step 002750): Train loss 3.960, Val loss 4.391\n",
      "Ep 4 (Step 002760): Train loss 3.967, Val loss 4.399\n",
      "Ep 4 (Step 002770): Train loss 4.008, Val loss 4.403\n",
      "Ep 4 (Step 002780): Train loss 4.006, Val loss 4.390\n",
      "Ep 4 (Step 002790): Train loss 4.116, Val loss 4.388\n",
      "Ep 4 (Step 002800): Train loss 4.039, Val loss 4.385\n",
      "Ep 4 (Step 002810): Train loss 4.032, Val loss 4.388\n",
      "Ep 4 (Step 002820): Train loss 4.066, Val loss 4.391\n",
      "Ep 4 (Step 002830): Train loss 3.985, Val loss 4.375\n",
      "Ep 4 (Step 002840): Train loss 4.020, Val loss 4.382\n",
      "Ep 4 (Step 002850): Train loss 3.945, Val loss 4.377\n",
      "Ep 4 (Step 002860): Train loss 4.021, Val loss 4.379\n",
      "Ep 4 (Step 002870): Train loss 4.045, Val loss 4.381\n",
      "Ep 4 (Step 002880): Train loss 3.976, Val loss 4.382\n",
      "Ep 4 (Step 002890): Train loss 4.066, Val loss 4.382\n",
      "Ep 4 (Step 002900): Train loss 3.991, Val loss 4.383\n",
      "Ep 4 (Step 002910): Train loss 4.046, Val loss 4.377\n",
      "Ep 4 (Step 002920): Train loss 3.906, Val loss 4.378\n",
      "Ep 4 (Step 002930): Train loss 3.996, Val loss 4.377\n",
      "Ep 4 (Step 002940): Train loss 3.910, Val loss 4.377\n",
      "Ep 4 (Step 002950): Train loss 4.023, Val loss 4.378\n",
      "Ep 4 (Step 002960): Train loss 4.060, Val loss 4.369\n",
      "Ep 4 (Step 002970): Train loss 4.012, Val loss 4.365\n",
      "Ep 4 (Step 002980): Train loss 4.031, Val loss 4.366\n",
      "Ep 4 (Step 002990): Train loss 3.933, Val loss 4.360\n",
      "Ep 4 (Step 003000): Train loss 3.890, Val loss 4.359\n",
      "Ep 4 (Step 003010): Train loss 4.012, Val loss 4.365\n",
      "Ep 4 (Step 003020): Train loss 3.998, Val loss 4.360\n",
      "Ep 4 (Step 003030): Train loss 3.875, Val loss 4.352\n",
      "Ep 4 (Step 003040): Train loss 3.979, Val loss 4.363\n",
      "Ep 4 (Step 003050): Train loss 3.945, Val loss 4.359\n",
      "Ep 4 (Step 003060): Train loss 4.005, Val loss 4.366\n",
      "Ep 4 (Step 003070): Train loss 3.916, Val loss 4.363\n",
      "Ep 4 (Step 003080): Train loss 3.999, Val loss 4.362\n",
      "Ep 4 (Step 003090): Train loss 3.953, Val loss 4.356\n",
      "Ep 4 (Step 003100): Train loss 3.911, Val loss 4.356\n",
      "Ep 4 (Step 003110): Train loss 3.961, Val loss 4.349\n",
      "Ep 4 (Step 003120): Train loss 3.906, Val loss 4.355\n",
      "Ep 4 (Step 003130): Train loss 4.035, Val loss 4.355\n",
      "Ep 4 (Step 003140): Train loss 3.991, Val loss 4.347\n",
      "Ep 4 (Step 003150): Train loss 4.047, Val loss 4.351\n",
      "Ep 4 (Step 003160): Train loss 3.997, Val loss 4.354\n",
      "Ep 4 (Step 003170): Train loss 3.929, Val loss 4.351\n",
      "Ep 4 (Step 003180): Train loss 3.998, Val loss 4.348\n",
      "Ep 4 (Step 003190): Train loss 3.842, Val loss 4.352\n",
      "Ep 4 (Step 003200): Train loss 3.923, Val loss 4.337\n",
      "Ep 4 (Step 003210): Train loss 3.928, Val loss 4.340\n",
      "Ep 4 (Step 003220): Train loss 3.963, Val loss 4.341\n",
      "Ep 4 (Step 003230): Train loss 4.083, Val loss 4.339\n",
      "Ep 4 (Step 003240): Train loss 3.996, Val loss 4.335\n",
      "Ep 4 (Step 003250): Train loss 3.879, Val loss 4.345\n",
      "Ep 4 (Step 003260): Train loss 3.891, Val loss 4.348\n",
      "Ep 4 (Step 003270): Train loss 3.910, Val loss 4.339\n",
      "Ep 4 (Step 003280): Train loss 3.945, Val loss 4.339\n",
      "Ep 5 (Step 003290): Train loss 3.914, Val loss 4.341\n",
      "Ep 5 (Step 003300): Train loss 3.920, Val loss 4.341\n",
      "Ep 5 (Step 003310): Train loss 3.837, Val loss 4.342\n",
      "Ep 5 (Step 003320): Train loss 3.901, Val loss 4.336\n",
      "Ep 5 (Step 003330): Train loss 3.944, Val loss 4.336\n",
      "Ep 5 (Step 003340): Train loss 3.994, Val loss 4.340\n",
      "Ep 5 (Step 003350): Train loss 3.890, Val loss 4.342\n",
      "Ep 5 (Step 003360): Train loss 3.920, Val loss 4.341\n",
      "Ep 5 (Step 003370): Train loss 3.879, Val loss 4.338\n",
      "Ep 5 (Step 003380): Train loss 3.916, Val loss 4.341\n",
      "Ep 5 (Step 003390): Train loss 3.805, Val loss 4.344\n",
      "Ep 5 (Step 003400): Train loss 3.869, Val loss 4.340\n",
      "Ep 5 (Step 003410): Train loss 3.891, Val loss 4.335\n",
      "Ep 5 (Step 003420): Train loss 3.894, Val loss 4.343\n",
      "Ep 5 (Step 003430): Train loss 3.872, Val loss 4.346\n",
      "Ep 5 (Step 003440): Train loss 3.857, Val loss 4.341\n",
      "Ep 5 (Step 003450): Train loss 3.913, Val loss 4.342\n",
      "Ep 5 (Step 003460): Train loss 3.851, Val loss 4.338\n",
      "Ep 5 (Step 003470): Train loss 3.833, Val loss 4.342\n",
      "Ep 5 (Step 003480): Train loss 3.938, Val loss 4.343\n",
      "Ep 5 (Step 003490): Train loss 3.876, Val loss 4.343\n",
      "Ep 5 (Step 003500): Train loss 3.821, Val loss 4.339\n",
      "Ep 5 (Step 003510): Train loss 3.873, Val loss 4.345\n",
      "Ep 5 (Step 003520): Train loss 3.872, Val loss 4.341\n",
      "Ep 5 (Step 003530): Train loss 3.858, Val loss 4.338\n",
      "Ep 5 (Step 003540): Train loss 3.805, Val loss 4.337\n",
      "Ep 5 (Step 003550): Train loss 3.902, Val loss 4.337\n",
      "Ep 5 (Step 003560): Train loss 3.917, Val loss 4.339\n",
      "Ep 5 (Step 003570): Train loss 3.945, Val loss 4.343\n",
      "Ep 5 (Step 003580): Train loss 3.799, Val loss 4.342\n",
      "Ep 5 (Step 003590): Train loss 3.864, Val loss 4.331\n",
      "Ep 5 (Step 003600): Train loss 3.937, Val loss 4.331\n",
      "Ep 5 (Step 003610): Train loss 3.917, Val loss 4.328\n",
      "Ep 5 (Step 003620): Train loss 3.811, Val loss 4.329\n",
      "Ep 5 (Step 003630): Train loss 3.878, Val loss 4.327\n",
      "Ep 5 (Step 003640): Train loss 3.884, Val loss 4.324\n",
      "Ep 5 (Step 003650): Train loss 3.896, Val loss 4.331\n",
      "Ep 5 (Step 003660): Train loss 3.924, Val loss 4.331\n",
      "Ep 5 (Step 003670): Train loss 3.869, Val loss 4.330\n",
      "Ep 5 (Step 003680): Train loss 3.812, Val loss 4.326\n",
      "Ep 5 (Step 003690): Train loss 3.916, Val loss 4.329\n",
      "Ep 5 (Step 003700): Train loss 3.813, Val loss 4.330\n",
      "Ep 5 (Step 003710): Train loss 3.864, Val loss 4.331\n",
      "Ep 5 (Step 003720): Train loss 3.953, Val loss 4.323\n",
      "Ep 5 (Step 003730): Train loss 3.891, Val loss 4.320\n",
      "Ep 5 (Step 003740): Train loss 3.870, Val loss 4.321\n",
      "Ep 5 (Step 003750): Train loss 3.887, Val loss 4.321\n",
      "Ep 5 (Step 003760): Train loss 3.801, Val loss 4.327\n",
      "Ep 5 (Step 003770): Train loss 3.774, Val loss 4.324\n",
      "Ep 5 (Step 003780): Train loss 3.940, Val loss 4.320\n",
      "Ep 5 (Step 003790): Train loss 3.905, Val loss 4.323\n",
      "Ep 5 (Step 003800): Train loss 3.819, Val loss 4.323\n",
      "Ep 5 (Step 003810): Train loss 3.802, Val loss 4.318\n",
      "Ep 5 (Step 003820): Train loss 3.818, Val loss 4.316\n",
      "Ep 5 (Step 003830): Train loss 3.809, Val loss 4.320\n",
      "Ep 5 (Step 003840): Train loss 3.897, Val loss 4.319\n",
      "Ep 5 (Step 003850): Train loss 3.822, Val loss 4.317\n",
      "Ep 5 (Step 003860): Train loss 3.832, Val loss 4.316\n",
      "Ep 5 (Step 003870): Train loss 3.839, Val loss 4.318\n",
      "Ep 5 (Step 003880): Train loss 3.818, Val loss 4.323\n",
      "Ep 5 (Step 003890): Train loss 3.849, Val loss 4.319\n",
      "Ep 5 (Step 003900): Train loss 3.849, Val loss 4.319\n",
      "Ep 5 (Step 003910): Train loss 3.861, Val loss 4.322\n",
      "Ep 5 (Step 003920): Train loss 3.801, Val loss 4.317\n",
      "Ep 5 (Step 003930): Train loss 3.789, Val loss 4.320\n",
      "Ep 5 (Step 003940): Train loss 3.869, Val loss 4.316\n",
      "Ep 5 (Step 003950): Train loss 3.840, Val loss 4.314\n",
      "Ep 5 (Step 003960): Train loss 3.795, Val loss 4.315\n",
      "Ep 5 (Step 003970): Train loss 3.808, Val loss 4.314\n",
      "Ep 5 (Step 003980): Train loss 3.791, Val loss 4.316\n",
      "Ep 5 (Step 003990): Train loss 3.879, Val loss 4.315\n",
      "Ep 5 (Step 004000): Train loss 3.916, Val loss 4.312\n",
      "Ep 5 (Step 004010): Train loss 3.764, Val loss 4.310\n",
      "Ep 5 (Step 004020): Train loss 3.821, Val loss 4.313\n",
      "Ep 5 (Step 004030): Train loss 3.795, Val loss 4.316\n",
      "Ep 5 (Step 004040): Train loss 3.830, Val loss 4.313\n",
      "Ep 5 (Step 004050): Train loss 3.788, Val loss 4.312\n",
      "Ep 5 (Step 004060): Train loss 3.776, Val loss 4.312\n",
      "Ep 5 (Step 004070): Train loss 3.794, Val loss 4.310\n",
      "Ep 5 (Step 004080): Train loss 3.774, Val loss 4.306\n",
      "Ep 5 (Step 004090): Train loss 3.813, Val loss 4.304\n",
      "Ep 5 (Step 004100): Train loss 3.842, Val loss 4.310\n",
      "Ep 6 (Step 004110): Train loss 3.855, Val loss 4.308\n",
      "Ep 6 (Step 004120): Train loss 3.845, Val loss 4.304\n",
      "Ep 6 (Step 004130): Train loss 3.778, Val loss 4.304\n",
      "Ep 6 (Step 004140): Train loss 3.787, Val loss 4.306\n",
      "Ep 6 (Step 004150): Train loss 3.763, Val loss 4.305\n",
      "Ep 6 (Step 004160): Train loss 3.820, Val loss 4.307\n",
      "Ep 6 (Step 004170): Train loss 3.795, Val loss 4.307\n",
      "Ep 6 (Step 004180): Train loss 3.810, Val loss 4.310\n",
      "Ep 6 (Step 004190): Train loss 3.770, Val loss 4.305\n",
      "Ep 6 (Step 004200): Train loss 3.763, Val loss 4.302\n",
      "Ep 6 (Step 004210): Train loss 3.764, Val loss 4.303\n",
      "Ep 6 (Step 004220): Train loss 3.851, Val loss 4.305\n",
      "Ep 6 (Step 004230): Train loss 3.771, Val loss 4.306\n",
      "Ep 6 (Step 004240): Train loss 3.820, Val loss 4.307\n",
      "Ep 6 (Step 004250): Train loss 3.855, Val loss 4.308\n",
      "Ep 6 (Step 004260): Train loss 3.822, Val loss 4.307\n",
      "Ep 6 (Step 004270): Train loss 3.828, Val loss 4.307\n",
      "Ep 6 (Step 004280): Train loss 3.884, Val loss 4.309\n",
      "Ep 6 (Step 004290): Train loss 3.816, Val loss 4.308\n",
      "Ep 6 (Step 004300): Train loss 3.823, Val loss 4.307\n",
      "Ep 6 (Step 004310): Train loss 3.817, Val loss 4.308\n",
      "Ep 6 (Step 004320): Train loss 3.789, Val loss 4.308\n",
      "Ep 6 (Step 004330): Train loss 3.800, Val loss 4.308\n",
      "Ep 6 (Step 004340): Train loss 3.851, Val loss 4.309\n",
      "Ep 6 (Step 004350): Train loss 3.807, Val loss 4.310\n",
      "Ep 6 (Step 004360): Train loss 3.781, Val loss 4.310\n",
      "Ep 6 (Step 004370): Train loss 3.765, Val loss 4.309\n",
      "Ep 6 (Step 004380): Train loss 3.779, Val loss 4.309\n",
      "Ep 6 (Step 004390): Train loss 3.667, Val loss 4.308\n",
      "Ep 6 (Step 004400): Train loss 3.799, Val loss 4.308\n",
      "Ep 6 (Step 004410): Train loss 3.829, Val loss 4.308\n",
      "Ep 6 (Step 004420): Train loss 3.788, Val loss 4.309\n",
      "Ep 6 (Step 004430): Train loss 3.817, Val loss 4.309\n",
      "Ep 6 (Step 004440): Train loss 3.701, Val loss 4.308\n",
      "Ep 6 (Step 004450): Train loss 3.763, Val loss 4.308\n",
      "Ep 6 (Step 004460): Train loss 3.900, Val loss 4.308\n",
      "Ep 6 (Step 004470): Train loss 3.767, Val loss 4.308\n",
      "Ep 6 (Step 004480): Train loss 3.735, Val loss 4.307\n",
      "Ep 6 (Step 004490): Train loss 3.874, Val loss 4.306\n",
      "Ep 6 (Step 004500): Train loss 3.836, Val loss 4.307\n",
      "Ep 6 (Step 004510): Train loss 3.733, Val loss 4.307\n",
      "Ep 6 (Step 004520): Train loss 3.851, Val loss 4.307\n",
      "Ep 6 (Step 004530): Train loss 3.832, Val loss 4.308\n",
      "Ep 6 (Step 004540): Train loss 3.790, Val loss 4.309\n",
      "Ep 6 (Step 004550): Train loss 3.715, Val loss 4.309\n",
      "Ep 6 (Step 004560): Train loss 3.753, Val loss 4.308\n",
      "Ep 6 (Step 004570): Train loss 3.744, Val loss 4.307\n",
      "Ep 6 (Step 004580): Train loss 3.871, Val loss 4.307\n",
      "Ep 6 (Step 004590): Train loss 3.748, Val loss 4.307\n",
      "Ep 6 (Step 004600): Train loss 3.757, Val loss 4.307\n",
      "Ep 6 (Step 004610): Train loss 3.705, Val loss 4.307\n",
      "Ep 6 (Step 004620): Train loss 3.826, Val loss 4.307\n",
      "Ep 6 (Step 004630): Train loss 3.832, Val loss 4.307\n",
      "Ep 6 (Step 004640): Train loss 3.853, Val loss 4.306\n",
      "Ep 6 (Step 004650): Train loss 3.741, Val loss 4.306\n",
      "Ep 6 (Step 004660): Train loss 3.735, Val loss 4.306\n",
      "Ep 6 (Step 004670): Train loss 3.769, Val loss 4.306\n",
      "Ep 6 (Step 004680): Train loss 3.846, Val loss 4.307\n",
      "Ep 6 (Step 004690): Train loss 3.849, Val loss 4.307\n",
      "Ep 6 (Step 004700): Train loss 3.796, Val loss 4.307\n",
      "Ep 6 (Step 004710): Train loss 3.757, Val loss 4.307\n",
      "Ep 6 (Step 004720): Train loss 3.833, Val loss 4.307\n",
      "Ep 6 (Step 004730): Train loss 3.845, Val loss 4.307\n",
      "Ep 6 (Step 004740): Train loss 3.723, Val loss 4.307\n",
      "Ep 6 (Step 004750): Train loss 3.813, Val loss 4.307\n",
      "Ep 6 (Step 004760): Train loss 3.807, Val loss 4.307\n",
      "Ep 6 (Step 004770): Train loss 3.766, Val loss 4.307\n",
      "Ep 6 (Step 004780): Train loss 3.854, Val loss 4.307\n",
      "Ep 6 (Step 004790): Train loss 3.779, Val loss 4.307\n",
      "Ep 6 (Step 004800): Train loss 3.714, Val loss 4.307\n",
      "Ep 6 (Step 004810): Train loss 3.781, Val loss 4.307\n",
      "Ep 6 (Step 004820): Train loss 3.733, Val loss 4.307\n",
      "Ep 6 (Step 004830): Train loss 3.750, Val loss 4.307\n",
      "Ep 6 (Step 004840): Train loss 3.806, Val loss 4.307\n",
      "Ep 6 (Step 004850): Train loss 3.814, Val loss 4.307\n",
      "Ep 6 (Step 004860): Train loss 3.778, Val loss 4.307\n",
      "Ep 6 (Step 004870): Train loss 3.886, Val loss 4.307\n",
      "Ep 6 (Step 004880): Train loss 3.761, Val loss 4.307\n",
      "Ep 6 (Step 004890): Train loss 3.731, Val loss 4.307\n",
      "Ep 6 (Step 004900): Train loss 3.807, Val loss 4.307\n",
      "Ep 6 (Step 004910): Train loss 3.763, Val loss 4.307\n",
      "Ep 6 (Step 004920): Train loss 3.803, Val loss 4.307\n",
      "Training completed in 14.31 minutes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_49791/679252246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model on all works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train(train_loader, val_loader, num_epochs=10,\n\u001b[0m\u001b[1;32m      4\u001b[0m       eval_iter=10, checkpoint_path=\"model_and_optimizer_best_old_tok.pth\");\n",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_49791/584259918.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, num_epochs, eval_iter, lr, generate_sample_text, sample_text, checkpoint_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Pass train_losses and val_losses as references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     train_model_simple(\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/Build GPT from scratch/pre_train.py\u001b[0m in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs, eval_iter, start_context, cfg, train_losses, val_losses, track_tokens_seen, generate_sample_text, checkpoint_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reset gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=6,\n",
    "      eval_iter=10, model_prefix=\"model_768_12_12_old_tok\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6651aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0002, weight_decay=0.05)\n",
    "\n",
    "checkpoint = torch.load(\"model_768_12_12.pth\", weights_only=True, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37c1b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "071757e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(27979) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jaxlib in /Users/mariam/opt/anaconda3/lib/python3.9/site-packages (0.4.30)\n",
      "Requirement already satisfied: jax[cpu] in /Users/mariam/opt/anaconda3/lib/python3.9/site-packages (0.4.30)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /Users/mariam/opt/anaconda3/lib/python3.9/site-packages (from jax[cpu]) (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.22 in /Users/mariam/opt/anaconda3/lib/python3.9/site-packages (from jax[cpu]) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum in /Users/mariam/opt/anaconda3/lib/python3.9/site-packages (from jax[cpu]) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /Users/mariam/opt/anaconda3/lib/python3.9/site-packages (from jax[cpu]) (1.9.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/mariam/opt/anaconda3/lib/python3.9/site-packages (from jax[cpu]) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/mariam/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.6->jax[cpu]) (3.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"jax[cpu]\" jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ecee23b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'jax' has no attribute 'version' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_23009/843098088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/evaluate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluation_suite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationSuite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m from .evaluator import (\n\u001b[1;32m     31\u001b[0m     \u001b[0mAudioClassificationEvaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/evaluate/evaluation_suite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluation_module_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/evaluate/evaluator/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSUPPORTED_TASKS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSUPPORTED_PIPELINE_TASKS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTASK_ALIASES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_task\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcheck_pipeline_task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_class_from_dynamic_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseImageProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFEATURE_EXTRACTOR_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_processing_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageProcessingMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_image_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpreter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpHint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauthoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelAnalyzer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpResolverType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/authoring/authoring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverter_error_data_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrap_toco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_phase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxla_computation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_xla_computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0m_xla_computation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Set Cloud TPU env vars if necessary before transitively loading C++ backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud_tpu_init\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcloud_tpu_init\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_cloud_tpu_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0m_cloud_tpu_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/cloud_tpu_init.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardware_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransfer_guard_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mversion_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m version = check_jaxlib_version(\n\u001b[0;32m---> 76\u001b[0;31m   \u001b[0mjax_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m   \u001b[0mjaxlib_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m   minimum_jaxlib_version=jax.version._minimum_jaxlib_version)\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'jax' has no attribute 'version' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from itertools import combinations\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb79401",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file_path = 'eval_text_data.txt'\n",
    "\n",
    "with open(eval_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    eval_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d523e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)  # Forward pass\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * target_ids.numel()\n",
    "            total_tokens += target_ids.numel()\n",
    "\n",
    "    perplexity = np.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "837f7534",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_81582/1973960337.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_81582/160537806.py\u001b[0m in \u001b[0;36mcompute_perplexity\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "compute_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c61225-1077-4019-984a-564aa7ba6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "def weat_score(model, target_words_1, target_words_2, attribute_words_1, attribute_words_2, tokenizer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Measures bias by comparing how close different groups of words are in embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding(word):\n",
    "        token_id = tokenizer.encode(word, allowed_special={'<|endoftext|>'})[0]\n",
    "        with torch.no_grad():\n",
    "            embed = model.tok_emb(torch.tensor([token_id], device=device)).cpu().numpy()\n",
    "        return embed.flatten()\n",
    "\n",
    "    # Get embeddings\n",
    "    target_1_embs = [get_embedding(w) for w in target_words_1]\n",
    "    target_2_embs = [get_embedding(w) for w in target_words_2]\n",
    "    attr_1_embs = [get_embedding(w) for w in attribute_words_1]\n",
    "    attr_2_embs = [get_embedding(w) for w in attribute_words_2]\n",
    "\n",
    "    def association(t, A, B):\n",
    "        return np.mean([cosine_similarity(t, a) for a in A]) - np.mean([cosine_similarity(t, b) for b in B])\n",
    "\n",
    "    # Compute WEAT score\n",
    "    s1 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_1_embs])\n",
    "    s2 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_2_embs])\n",
    "    \n",
    "    weat_score = s1 - s2\n",
    "    return weat_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a1406-0bd9-409a-9fd8-8b4f343d2c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_male = [\"gentleman\", \"officer\", \"clergyman\", \"husband\", \"captain\"]\n",
    "target_female = [\"lady\", \"governess\", \"girl\", \"wife\", \"widow\"]\n",
    "\n",
    "attribute_male = [\"honour\", \"duty\", \"wisdom\", \"fortitude\", \"independence\"]\n",
    "attribute_female = [\"grace\", \"affection\", \"beauty\", \"delicacy\", \"modesty\"]\n",
    "\n",
    "weat_score(model, target_male, target_female, attribute_male, attribute_female, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcce2e-326f-4cc6-8033-e77e0a2ba270",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d845171-c8f1-47a0-a60b-1f9847a22590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import re\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_bleu_rouge_from_val(model, device=\"cpu\"):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    # Step 1: Load the validation set\n",
    "    with open('val_text_data_all_txt.txt', 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Step 2: Split into sentences & filter\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', data)\n",
    "    filtered_sentences = [s.strip() for s in sentences if 5 <= len(s.split()) <= 60]\n",
    "    filtered_sentences = filtered_sentences[:1000]\n",
    "\n",
    "    # Step 3: Split each sentence into two halves and store as tuples\n",
    "    sentence_tuples = []\n",
    "    for sent in filtered_sentences:\n",
    "        words = sent.split()\n",
    "        mid = len(words) // 2\n",
    "        first_half = ' '.join(words[:mid])\n",
    "        second_half = ' '.join(words[mid:])\n",
    "        sentence_tuples.append((first_half, second_half))\n",
    "\n",
    "    # Step 4: For each (first_half, second_half), generate prediction\n",
    "    for first_half, second_half in sentence_tuples:\n",
    "        generated_text = generate(\n",
    "            model=model, prompt=first_half,\n",
    "            max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "            device=device,\n",
    "            temperature=0.7,\n",
    "            top_k=50\n",
    "        )\n",
    "\n",
    "        # Build reference and prediction\n",
    "        reference = first_half + \" \" + second_half\n",
    "        prediction = generated_text\n",
    "\n",
    "        references.append(reference)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Step 5-6: Compute BLEU and ROUGE\n",
    "    # Format references correctly for BLEU\n",
    "    references_formatted = [[ref] for ref in references]\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references_formatted)['bleu']\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}, ROUGE-L Score: {rouge_score['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404bbea-7821-4b52-a5e9-f240bf5c01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_bleu_rouge_from_val(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Miss Bennet has inherited the estate from her aunt, so she must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Mr. Darcy has inherited the estate from his aunt, so he must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81220f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"A wife is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"A husband is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"I shall now go\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"He said\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"She was\",\n",
    "    max_new_tokens=200, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb810-cfc6-49fe-bfd0-66c035e0707e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78fd8b69-ae98-4300-b876-93bf84fa0d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to be the very day, and I am sure I am sure I am sure I should have been a very much obliged to be very happy. I am'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "583fd4d9-4273-435a-a2fe-651162acb64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to go and Mrs. Weston.\\n\"I am very glad to think of your own family.\"\\n\"I will not like you. I am afraid'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c8a060b-6d8a-4dc8-ade9-bfde78b38900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she is wild to get married; she can be better off than half an hour.\"\\nMrs. Gibson tried to talk on the subject.\\n\"Well, I am sure I'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"she is wild to get married\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=1,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba961e03-e482-4601-bef7-daab0c1dac77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I must beg to speak to you, and I will not be able to say that you are very much mistaken. I will not, therefore, be your friend'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"I must\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f99b81-a4be-4994-9c00-8898a408f06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
