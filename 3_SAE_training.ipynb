{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b225ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from clean_gutenberg_text import clean_gutenberg_text\n",
    "from train_sae import train_sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b0d461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3191ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.2,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd8e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "checkpoint = torch.load(\"model_and_optimizer_5.pth\", weights_only=True)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f4fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb48a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_and_clean_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads a text file and splits it into sentences while cleaning the text.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the text file.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of cleaned sentences from the book.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = clean_gutenberg_text(file_path)\n",
    "\n",
    "    # Split text into sentences (simple heuristic using punctuation)\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "\n",
    "    # Remove very short or long sentences\n",
    "    sentences = [s.strip() for s in sentences if 5 < len(s.split()) < 50]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f25032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"original_texts/\"\n",
    "dataset = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        sentences = load_and_clean_text(os.path.join(directory, filename))\n",
    "        dataset += sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f23472ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def get_token_embeddings(text, model, tokenizer, layers=[6, 12]):\n",
    "    \"\"\"\n",
    "    Extracts token embeddings from specified transformer layers.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "    - model: Custom GPT model.\n",
    "    - tokenizer: tiktoken encoding object.\n",
    "    - layers (list): Transformer layers to extract embeddings from.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Layer-wise token embeddings {layer_number: embeddings}\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = text_to_token_ids(text, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, hidden_states = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    embeddings = {}\n",
    "    for layer in layers:\n",
    "        if layer - 1 < len(hidden_states):\n",
    "            embeddings[layer] = hidden_states[layer - 1].squeeze(0).cpu().numpy()\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: Layer {layer} is out of range (max index {len(hidden_states) - 1})\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer6_embeddings = []\n",
    "layer12_embeddings = []\n",
    "\n",
    "for sentence in dataset:\n",
    "    embeddings = get_token_embeddings(sentence, model, tokenizer)\n",
    "    layer6_embeddings.append(embeddings[6])\n",
    "    layer12_embeddings.append(embeddings[12])\n",
    "\n",
    "# Convert to NumPy and flatten tokens into dataset\n",
    "layer6_embeddings = np.vstack(layer6_embeddings)\n",
    "layer12_embeddings = np.vstack(layer12_embeddings)\n",
    "\n",
    "os.makedirs(\"sae_data\", exist_ok=True)\n",
    "np.save(\"sae_data/layer6_embeddings.npy\", layer6_embeddings)\n",
    "np.save(\"sae_data/layer12_embeddings.npy\", layer12_embeddings)\n",
    "\n",
    "print(\"Saved token embeddings:\")\n",
    "print(f\"Layer 6: {layer6_embeddings.shape}\")\n",
    "print(f\"Layer 12: {layer12_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c72b1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer6_embeddings = np.load(\"sae_data/layer6_embeddings.npy\")\n",
    "layer12_embeddings = np.load(\"sae_data/layer12_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62df3de3-c92d-4399-9906-5675803c20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sparse_auto_encoder import SparseAutoencoder\n",
    "\n",
    "def objective(trial, device=\"cpu\", embeddings_path=\"training_embeddings.npy\"):\n",
    "    # Hyperparameter search space\n",
    "    print(50*\"=\")\n",
    "    print(f\"Trial number {trial.number + 1}\")\n",
    "    print(50*\"=\")\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 512, step=64)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    # Load your embeddings\n",
    "    embeddings = np.load(embeddings_path)  # Replace with actual file\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "    input_dim = embeddings.shape[1]\n",
    "    sae = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(sae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    dataset = TensorDataset(embeddings)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience, early_stop_counter = 10, 0\n",
    "\n",
    "    for epoch in range(30):\n",
    "        sae.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, encoded = sae(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            sparsity_loss = torch.norm(encoded, p=1) * 1e-4\n",
    "            total_loss = loss + sparsity_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += total_loss.item()\n",
    "\n",
    "        sae.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch[0].to(device)\n",
    "                outputs, encoded = sae(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                sparsity_loss = torch.norm(encoded, p=1) * 1e-4\n",
    "                total_loss = loss + sparsity_loss\n",
    "                val_loss += total_loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch}: Train loss {train_loss:.4f}, Val loss {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            break\n",
    "\n",
    "    return best_val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ad1e0d-2ec6-4e76-b20a-85949d7930d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 05:49:50,242] A new study created in memory with name: no-name-debab08a-1ae8-4354-b1d7-2e8a93fcaf53\n",
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Trial number 1\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.5013, Val loss 2.7664\n",
      "Epoch 1: Train loss 2.6397, Val loss 2.5404\n",
      "Epoch 2: Train loss 2.5101, Val loss 2.4382\n",
      "Epoch 3: Train loss 2.4494, Val loss 2.3934\n",
      "Epoch 4: Train loss 2.4061, Val loss 2.3609\n",
      "Epoch 5: Train loss 2.3730, Val loss 2.3305\n",
      "Epoch 6: Train loss 2.3432, Val loss 2.3005\n",
      "Epoch 7: Train loss 2.3171, Val loss 2.2783\n",
      "Epoch 8: Train loss 2.2942, Val loss 2.2555\n",
      "Epoch 9: Train loss 2.2737, Val loss 2.2374\n",
      "Epoch 10: Train loss 2.2572, Val loss 2.2252\n",
      "Epoch 11: Train loss 2.2438, Val loss 2.2149\n",
      "Epoch 12: Train loss 2.2321, Val loss 2.2054\n",
      "Epoch 13: Train loss 2.2236, Val loss 2.2009\n",
      "Epoch 14: Train loss 2.2164, Val loss 2.1995\n",
      "Epoch 15: Train loss 2.2110, Val loss 2.1948\n",
      "Epoch 16: Train loss 2.2063, Val loss 2.1908\n",
      "Epoch 17: Train loss 2.2020, Val loss 2.1906\n",
      "Epoch 18: Train loss 2.1982, Val loss 2.1864\n",
      "Epoch 19: Train loss 2.1944, Val loss 2.1859\n",
      "Epoch 20: Train loss 2.1912, Val loss 2.1838\n",
      "Epoch 21: Train loss 2.1882, Val loss 2.1842\n",
      "Epoch 22: Train loss 2.1858, Val loss 2.1784\n",
      "Epoch 23: Train loss 2.1831, Val loss 2.1818\n",
      "Epoch 24: Train loss 2.1809, Val loss 2.1789\n",
      "Epoch 25: Train loss 2.1786, Val loss 2.1784\n",
      "Epoch 26: Train loss 2.1770, Val loss 2.1826\n",
      "Epoch 27: Train loss 2.1748, Val loss 2.1746\n",
      "Epoch 28: Train loss 2.1729, Val loss 2.1777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:02:12,080] Trial 0 finished with value: 2.170551270788366 and parameters: {'batch_size': 128, 'lr': 0.0002688498549528964, 'hidden_dim': 320, 'weight_decay': 0.00024172216807324387}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1711, Val loss 2.1706\n",
      "==================================================\n",
      "Trial number 2\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
      "/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_78150/160072725.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss 2.4140, Val loss 2.3188\n",
      "Epoch 1: Train loss 2.3013, Val loss 2.3061\n",
      "Epoch 2: Train loss 2.2921, Val loss 2.2908\n",
      "Epoch 3: Train loss 2.2870, Val loss 2.3128\n",
      "Epoch 4: Train loss 2.2838, Val loss 2.3026\n",
      "Epoch 5: Train loss 2.2817, Val loss 2.3122\n",
      "Epoch 6: Train loss 2.2804, Val loss 2.2955\n",
      "Epoch 7: Train loss 2.2793, Val loss 2.2935\n",
      "Epoch 8: Train loss 2.2782, Val loss 2.2808\n",
      "Epoch 9: Train loss 2.2774, Val loss 2.3122\n",
      "Epoch 10: Train loss 2.2773, Val loss 2.2876\n",
      "Epoch 11: Train loss 2.2768, Val loss 2.2953\n",
      "Epoch 12: Train loss 2.2759, Val loss 2.2960\n",
      "Epoch 13: Train loss 2.2757, Val loss 2.2897\n",
      "Epoch 14: Train loss 2.2754, Val loss 2.2881\n",
      "Epoch 15: Train loss 2.2601, Val loss 2.2521\n",
      "Epoch 16: Train loss 2.2585, Val loss 2.2471\n",
      "Epoch 17: Train loss 2.2585, Val loss 2.2550\n",
      "Epoch 18: Train loss 2.2587, Val loss 2.2596\n",
      "Epoch 19: Train loss 2.2585, Val loss 2.2635\n",
      "Epoch 20: Train loss 2.2582, Val loss 2.2584\n",
      "Epoch 21: Train loss 2.2581, Val loss 2.2611\n",
      "Epoch 22: Train loss 2.2582, Val loss 2.2607\n",
      "Epoch 23: Train loss 2.2479, Val loss 2.2250\n",
      "Epoch 24: Train loss 2.2469, Val loss 2.2332\n",
      "Epoch 25: Train loss 2.2468, Val loss 2.2317\n",
      "Epoch 26: Train loss 2.2467, Val loss 2.2267\n",
      "Epoch 27: Train loss 2.2465, Val loss 2.2312\n",
      "Epoch 28: Train loss 2.2465, Val loss 2.2292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:22:54,658] Trial 1 finished with value: 2.224995661976669 and parameters: {'batch_size': 64, 'lr': 0.004508739037375544, 'hidden_dim': 192, 'weight_decay': 0.0001467563404464736}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.2463, Val loss 2.2263\n",
      "==================================================\n",
      "Trial number 3\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.1771, Val loss 2.8172\n",
      "Epoch 1: Train loss 2.7800, Val loss 2.6882\n",
      "Epoch 2: Train loss 2.6947, Val loss 2.6175\n",
      "Epoch 3: Train loss 2.6386, Val loss 2.5764\n",
      "Epoch 4: Train loss 2.6049, Val loss 2.5540\n",
      "Epoch 5: Train loss 2.5865, Val loss 2.5463\n",
      "Epoch 6: Train loss 2.5802, Val loss 2.5443\n",
      "Epoch 7: Train loss 2.5765, Val loss 2.5453\n",
      "Epoch 8: Train loss 2.5741, Val loss 2.5435\n",
      "Epoch 9: Train loss 2.5720, Val loss 2.5434\n",
      "Epoch 10: Train loss 2.5707, Val loss 2.5442\n",
      "Epoch 11: Train loss 2.5698, Val loss 2.5418\n",
      "Epoch 12: Train loss 2.5687, Val loss 2.5428\n",
      "Epoch 13: Train loss 2.5678, Val loss 2.5433\n",
      "Epoch 14: Train loss 2.5671, Val loss 2.5422\n",
      "Epoch 15: Train loss 2.5665, Val loss 2.5457\n",
      "Epoch 16: Train loss 2.5662, Val loss 2.5436\n",
      "Epoch 17: Train loss 2.5657, Val loss 2.5425\n",
      "Epoch 18: Train loss 2.5633, Val loss 2.5331\n",
      "Epoch 19: Train loss 2.5630, Val loss 2.5343\n",
      "Epoch 20: Train loss 2.5628, Val loss 2.5353\n",
      "Epoch 21: Train loss 2.5624, Val loss 2.5335\n",
      "Epoch 22: Train loss 2.5624, Val loss 2.5336\n",
      "Epoch 23: Train loss 2.5620, Val loss 2.5333\n",
      "Epoch 24: Train loss 2.5618, Val loss 2.5328\n",
      "Epoch 25: Train loss 2.5608, Val loss 2.5299\n",
      "Epoch 26: Train loss 2.5606, Val loss 2.5301\n",
      "Epoch 27: Train loss 2.5606, Val loss 2.5311\n",
      "Epoch 28: Train loss 2.5605, Val loss 2.5301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:43:08,899] Trial 2 finished with value: 2.5284160660719817 and parameters: {'batch_size': 64, 'lr': 0.00020725071060021414, 'hidden_dim': 64, 'weight_decay': 1.9243970614633706e-06}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.5604, Val loss 2.5284\n",
      "==================================================\n",
      "Trial number 4\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.8168, Val loss 2.6210\n",
      "Epoch 1: Train loss 2.5877, Val loss 2.6165\n",
      "Epoch 2: Train loss 2.5764, Val loss 2.5997\n",
      "Epoch 3: Train loss 2.5724, Val loss 2.5933\n",
      "Epoch 4: Train loss 2.5704, Val loss 2.6060\n",
      "Epoch 5: Train loss 2.5694, Val loss 2.6090\n",
      "Epoch 6: Train loss 2.5685, Val loss 2.5808\n",
      "Epoch 7: Train loss 2.5677, Val loss 2.5884\n",
      "Epoch 8: Train loss 2.5670, Val loss 2.5909\n",
      "Epoch 9: Train loss 2.5670, Val loss 2.5738\n",
      "Epoch 10: Train loss 2.5666, Val loss 2.5881\n",
      "Epoch 11: Train loss 2.5661, Val loss 2.5779\n",
      "Epoch 12: Train loss 2.5656, Val loss 2.5883\n",
      "Epoch 13: Train loss 2.5655, Val loss 2.5677\n",
      "Epoch 14: Train loss 2.5651, Val loss 2.5784\n",
      "Epoch 15: Train loss 2.5650, Val loss 2.5797\n",
      "Epoch 16: Train loss 2.5648, Val loss 2.5900\n",
      "Epoch 17: Train loss 2.5645, Val loss 2.5777\n",
      "Epoch 18: Train loss 2.5645, Val loss 2.5801\n",
      "Epoch 19: Train loss 2.5645, Val loss 2.5810\n",
      "Epoch 20: Train loss 2.5585, Val loss 2.5615\n",
      "Epoch 21: Train loss 2.5579, Val loss 2.5576\n",
      "Epoch 22: Train loss 2.5575, Val loss 2.5573\n",
      "Epoch 23: Train loss 2.5575, Val loss 2.5587\n",
      "Epoch 24: Train loss 2.5575, Val loss 2.5647\n",
      "Epoch 25: Train loss 2.5574, Val loss 2.5576\n",
      "Epoch 26: Train loss 2.5575, Val loss 2.5544\n",
      "Epoch 27: Train loss 2.5573, Val loss 2.5627\n",
      "Epoch 28: Train loss 2.5575, Val loss 2.5561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 06:55:02,432] Trial 3 finished with value: 2.5543929240920327 and parameters: {'batch_size': 128, 'lr': 0.0017955378450738243, 'hidden_dim': 64, 'weight_decay': 1.980034613217017e-05}. Best is trial 0 with value: 2.170551270788366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.5572, Val loss 2.5640\n",
      "==================================================\n",
      "Trial number 5\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.7470, Val loss 2.3950\n",
      "Epoch 1: Train loss 2.3955, Val loss 2.2924\n",
      "Epoch 2: Train loss 2.3048, Val loss 2.2097\n",
      "Epoch 3: Train loss 2.2311, Val loss 2.1628\n",
      "Epoch 4: Train loss 2.1922, Val loss 2.1437\n",
      "Epoch 5: Train loss 2.1740, Val loss 2.1422\n",
      "Epoch 6: Train loss 2.1642, Val loss 2.1383\n",
      "Epoch 7: Train loss 2.1578, Val loss 2.1427\n",
      "Epoch 8: Train loss 2.1526, Val loss 2.1437\n",
      "Epoch 9: Train loss 2.1479, Val loss 2.1372\n",
      "Epoch 10: Train loss 2.1445, Val loss 2.1387\n",
      "Epoch 11: Train loss 2.1415, Val loss 2.1432\n",
      "Epoch 12: Train loss 2.1381, Val loss 2.1517\n",
      "Epoch 13: Train loss 2.1356, Val loss 2.1578\n",
      "Epoch 14: Train loss 2.1332, Val loss 2.1525\n",
      "Epoch 15: Train loss 2.1310, Val loss 2.1545\n",
      "Epoch 16: Train loss 2.1234, Val loss 2.1080\n",
      "Epoch 17: Train loss 2.1219, Val loss 2.1100\n",
      "Epoch 18: Train loss 2.1206, Val loss 2.1085\n",
      "Epoch 19: Train loss 2.1194, Val loss 2.1072\n",
      "Epoch 20: Train loss 2.1182, Val loss 2.1102\n",
      "Epoch 21: Train loss 2.1174, Val loss 2.1108\n",
      "Epoch 22: Train loss 2.1164, Val loss 2.1132\n",
      "Epoch 23: Train loss 2.1155, Val loss 2.1107\n",
      "Epoch 24: Train loss 2.1145, Val loss 2.1094\n",
      "Epoch 25: Train loss 2.1136, Val loss 2.1119\n",
      "Epoch 26: Train loss 2.1095, Val loss 2.0839\n",
      "Epoch 27: Train loss 2.1087, Val loss 2.0843\n",
      "Epoch 28: Train loss 2.1081, Val loss 2.0863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 07:16:17,436] Trial 4 finished with value: 2.083881515848338 and parameters: {'batch_size': 64, 'lr': 0.0003881008304162215, 'hidden_dim': 384, 'weight_decay': 2.3923055206292162e-05}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1077, Val loss 2.0868\n",
      "==================================================\n",
      "Trial number 6\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.3573, Val loss 2.2031\n",
      "Epoch 1: Train loss 2.2544, Val loss 2.1934\n",
      "Epoch 2: Train loss 2.2470, Val loss 2.1944\n",
      "Epoch 3: Train loss 2.2429, Val loss 2.1903\n",
      "Epoch 4: Train loss 2.2405, Val loss 2.1887\n",
      "Epoch 5: Train loss 2.2391, Val loss 2.1896\n",
      "Epoch 6: Train loss 2.2378, Val loss 2.1853\n",
      "Epoch 7: Train loss 2.2372, Val loss 2.1904\n",
      "Epoch 8: Train loss 2.2364, Val loss 2.1852\n",
      "Epoch 9: Train loss 2.2354, Val loss 2.1833\n",
      "Epoch 10: Train loss 2.2351, Val loss 2.1861\n",
      "Epoch 11: Train loss 2.2346, Val loss 2.1805\n",
      "Epoch 12: Train loss 2.2338, Val loss 2.1858\n",
      "Epoch 13: Train loss 2.2336, Val loss 2.1823\n",
      "Epoch 14: Train loss 2.2334, Val loss 2.1866\n",
      "Epoch 15: Train loss 2.2330, Val loss 2.1816\n",
      "Epoch 16: Train loss 2.2328, Val loss 2.1794\n",
      "Epoch 17: Train loss 2.2324, Val loss 2.1818\n",
      "Epoch 18: Train loss 2.2323, Val loss 2.1867\n",
      "Epoch 19: Train loss 2.2321, Val loss 2.1766\n",
      "Epoch 20: Train loss 2.2320, Val loss 2.1891\n",
      "Epoch 21: Train loss 2.2317, Val loss 2.1924\n",
      "Epoch 22: Train loss 2.2316, Val loss 2.1749\n",
      "Epoch 23: Train loss 2.2314, Val loss 2.1798\n",
      "Epoch 24: Train loss 2.2308, Val loss 2.1811\n",
      "Epoch 25: Train loss 2.2312, Val loss 2.1879\n",
      "Epoch 26: Train loss 2.2308, Val loss 2.1752\n",
      "Epoch 27: Train loss 2.2309, Val loss 2.1794\n",
      "Epoch 28: Train loss 2.2306, Val loss 2.1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 07:55:47,399] Trial 5 finished with value: 2.1656301614224165 and parameters: {'batch_size': 32, 'lr': 0.0021729419335939316, 'hidden_dim': 256, 'weight_decay': 0.00048608168804415455}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.2222, Val loss 2.1656\n",
      "==================================================\n",
      "Trial number 7\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.5723, Val loss 2.2847\n",
      "Epoch 1: Train loss 2.2485, Val loss 2.2205\n",
      "Epoch 2: Train loss 2.1990, Val loss 2.2098\n",
      "Epoch 3: Train loss 2.1830, Val loss 2.1955\n",
      "Epoch 4: Train loss 2.1738, Val loss 2.2090\n",
      "Epoch 5: Train loss 2.1684, Val loss 2.2012\n",
      "Epoch 6: Train loss 2.1642, Val loss 2.2004\n",
      "Epoch 7: Train loss 2.1604, Val loss 2.2010\n",
      "Epoch 8: Train loss 2.1579, Val loss 2.1832\n",
      "Epoch 9: Train loss 2.1553, Val loss 2.1774\n",
      "Epoch 10: Train loss 2.1524, Val loss 2.2113\n",
      "Epoch 11: Train loss 2.1509, Val loss 2.2076\n",
      "Epoch 12: Train loss 2.1490, Val loss 2.2049\n",
      "Epoch 13: Train loss 2.1477, Val loss 2.2049\n",
      "Epoch 14: Train loss 2.1454, Val loss 2.1735\n",
      "Epoch 15: Train loss 2.1450, Val loss 2.1901\n",
      "Epoch 16: Train loss 2.1440, Val loss 2.1863\n",
      "Epoch 17: Train loss 2.1427, Val loss 2.2090\n",
      "Epoch 18: Train loss 2.1412, Val loss 2.1864\n",
      "Epoch 19: Train loss 2.1403, Val loss 2.2033\n",
      "Epoch 20: Train loss 2.1400, Val loss 2.5091\n",
      "Epoch 21: Train loss 2.1314, Val loss 2.1538\n",
      "Epoch 22: Train loss 2.1290, Val loss 2.1983\n",
      "Epoch 23: Train loss 2.1280, Val loss 2.1678\n",
      "Epoch 24: Train loss 2.1274, Val loss 2.1459\n",
      "Epoch 25: Train loss 2.1267, Val loss 2.1677\n",
      "Epoch 26: Train loss 2.1267, Val loss 2.1792\n",
      "Epoch 27: Train loss 2.1258, Val loss 2.1469\n",
      "Epoch 28: Train loss 2.1254, Val loss 2.1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 08:08:39,194] Trial 6 finished with value: 2.1458648746663873 and parameters: {'batch_size': 128, 'lr': 0.002262804895465889, 'hidden_dim': 512, 'weight_decay': 1.2074671078795074e-05}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1242, Val loss 2.1671\n",
      "==================================================\n",
      "Trial number 8\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.6830, Val loss 2.9287\n",
      "Epoch 1: Train loss 2.6973, Val loss 2.5486\n",
      "Epoch 2: Train loss 2.5121, Val loss 2.4390\n",
      "Epoch 3: Train loss 2.4410, Val loss 2.3816\n",
      "Epoch 4: Train loss 2.3982, Val loss 2.3592\n",
      "Epoch 5: Train loss 2.3705, Val loss 2.3356\n",
      "Epoch 6: Train loss 2.3475, Val loss 2.3143\n",
      "Epoch 7: Train loss 2.3279, Val loss 2.2913\n",
      "Epoch 8: Train loss 2.3100, Val loss 2.2707\n",
      "Epoch 9: Train loss 2.2940, Val loss 2.2571\n",
      "Epoch 10: Train loss 2.2788, Val loss 2.2455\n",
      "Epoch 11: Train loss 2.2643, Val loss 2.2292\n",
      "Epoch 12: Train loss 2.2505, Val loss 2.2204\n",
      "Epoch 13: Train loss 2.2369, Val loss 2.2035\n",
      "Epoch 14: Train loss 2.2259, Val loss 2.1959\n",
      "Epoch 15: Train loss 2.2149, Val loss 2.1837\n",
      "Epoch 16: Train loss 2.2055, Val loss 2.1777\n",
      "Epoch 17: Train loss 2.1969, Val loss 2.1710\n",
      "Epoch 18: Train loss 2.1900, Val loss 2.1665\n",
      "Epoch 19: Train loss 2.1841, Val loss 2.1618\n",
      "Epoch 20: Train loss 2.1790, Val loss 2.1631\n",
      "Epoch 21: Train loss 2.1749, Val loss 2.1544\n",
      "Epoch 22: Train loss 2.1704, Val loss 2.1520\n",
      "Epoch 23: Train loss 2.1667, Val loss 2.1490\n",
      "Epoch 24: Train loss 2.1631, Val loss 2.1474\n",
      "Epoch 25: Train loss 2.1603, Val loss 2.1428\n",
      "Epoch 26: Train loss 2.1573, Val loss 2.1437\n",
      "Epoch 27: Train loss 2.1547, Val loss 2.1403\n",
      "Epoch 28: Train loss 2.1524, Val loss 2.1394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 08:21:38,398] Trial 7 finished with value: 2.1394470236518166 and parameters: {'batch_size': 128, 'lr': 0.00019293742668148652, 'hidden_dim': 448, 'weight_decay': 0.00022579186946369568}. Best is trial 4 with value: 2.083881515848338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1504, Val loss 2.1417\n",
      "==================================================\n",
      "Trial number 9\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4441, Val loss 2.1521\n",
      "Epoch 1: Train loss 2.1927, Val loss 2.1148\n",
      "Epoch 2: Train loss 2.1680, Val loss 2.1177\n",
      "Epoch 3: Train loss 2.1613, Val loss 2.1051\n",
      "Epoch 4: Train loss 2.1571, Val loss 2.1005\n",
      "Epoch 5: Train loss 2.1540, Val loss 2.1026\n",
      "Epoch 6: Train loss 2.1515, Val loss 2.0959\n",
      "Epoch 7: Train loss 2.1492, Val loss 2.0967\n",
      "Epoch 8: Train loss 2.1474, Val loss 2.0888\n",
      "Epoch 9: Train loss 2.1458, Val loss 2.0864\n",
      "Epoch 10: Train loss 2.1449, Val loss 2.0961\n",
      "Epoch 11: Train loss 2.1438, Val loss 2.0909\n",
      "Epoch 12: Train loss 2.1426, Val loss 2.0885\n",
      "Epoch 13: Train loss 2.1418, Val loss 2.0868\n",
      "Epoch 14: Train loss 2.1407, Val loss 2.0861\n",
      "Epoch 15: Train loss 2.1403, Val loss 2.0882\n",
      "Epoch 16: Train loss 2.1397, Val loss 2.0880\n",
      "Epoch 17: Train loss 2.1392, Val loss 2.0777\n",
      "Epoch 18: Train loss 2.1386, Val loss 2.0790\n",
      "Epoch 19: Train loss 2.1383, Val loss 2.0814\n",
      "Epoch 20: Train loss 2.1380, Val loss 2.0770\n",
      "Epoch 21: Train loss 2.1374, Val loss 2.0794\n",
      "Epoch 22: Train loss 2.1374, Val loss 2.0781\n",
      "Epoch 23: Train loss 2.1368, Val loss 2.0749\n",
      "Epoch 24: Train loss 2.1363, Val loss 2.0786\n",
      "Epoch 25: Train loss 2.1359, Val loss 2.0803\n",
      "Epoch 26: Train loss 2.1354, Val loss 2.0753\n",
      "Epoch 27: Train loss 2.1348, Val loss 2.0771\n",
      "Epoch 28: Train loss 2.1344, Val loss 2.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 09:00:44,986] Trial 8 finished with value: 2.07491034575172 and parameters: {'batch_size': 32, 'lr': 0.000816547713285386, 'hidden_dim': 512, 'weight_decay': 2.9090240560596477e-06}. Best is trial 8 with value: 2.07491034575172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1341, Val loss 2.0782\n",
      "==================================================\n",
      "Trial number 10\n",
      "==================================================\n",
      "Epoch 0: Train loss 3.6428, Val loss 2.9041\n",
      "Epoch 1: Train loss 2.6809, Val loss 2.5343\n",
      "Epoch 2: Train loss 2.5082, Val loss 2.4249\n",
      "Epoch 3: Train loss 2.4365, Val loss 2.3805\n",
      "Epoch 4: Train loss 2.3950, Val loss 2.3383\n",
      "Epoch 5: Train loss 2.3643, Val loss 2.3227\n",
      "Epoch 6: Train loss 2.3415, Val loss 2.3036\n",
      "Epoch 7: Train loss 2.3214, Val loss 2.2759\n",
      "Epoch 8: Train loss 2.3022, Val loss 2.2607\n",
      "Epoch 9: Train loss 2.2851, Val loss 2.2427\n",
      "Epoch 10: Train loss 2.2683, Val loss 2.2313\n",
      "Epoch 11: Train loss 2.2530, Val loss 2.2129\n",
      "Epoch 12: Train loss 2.2384, Val loss 2.2007\n",
      "Epoch 13: Train loss 2.2255, Val loss 2.1905\n",
      "Epoch 14: Train loss 2.2141, Val loss 2.1795\n",
      "Epoch 15: Train loss 2.2038, Val loss 2.1727\n",
      "Epoch 16: Train loss 2.1950, Val loss 2.1652\n",
      "Epoch 17: Train loss 2.1873, Val loss 2.1620\n",
      "Epoch 18: Train loss 2.1811, Val loss 2.1559\n",
      "Epoch 19: Train loss 2.1750, Val loss 2.1516\n",
      "Epoch 20: Train loss 2.1699, Val loss 2.1459\n",
      "Epoch 21: Train loss 2.1655, Val loss 2.1439\n",
      "Epoch 22: Train loss 2.1620, Val loss 2.1426\n",
      "Epoch 23: Train loss 2.1590, Val loss 2.1400\n",
      "Epoch 24: Train loss 2.1558, Val loss 2.1384\n",
      "Epoch 25: Train loss 2.1535, Val loss 2.1373\n",
      "Epoch 26: Train loss 2.1515, Val loss 2.1362\n",
      "Epoch 27: Train loss 2.1494, Val loss 2.1331\n",
      "Epoch 28: Train loss 2.1475, Val loss 2.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 09:13:30,328] Trial 9 finished with value: 2.133140113136985 and parameters: {'batch_size': 128, 'lr': 0.00020385491002032069, 'hidden_dim': 448, 'weight_decay': 0.0009293870195375112}. Best is trial 8 with value: 2.07491034575172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1461, Val loss 2.1341\n",
      "==================================================\n",
      "Trial number 11\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4641, Val loss 2.1610\n",
      "Epoch 1: Train loss 2.1981, Val loss 2.1177\n",
      "Epoch 2: Train loss 2.1680, Val loss 2.1113\n",
      "Epoch 3: Train loss 2.1603, Val loss 2.1073\n",
      "Epoch 4: Train loss 2.1552, Val loss 2.1021\n",
      "Epoch 5: Train loss 2.1526, Val loss 2.1009\n",
      "Epoch 6: Train loss 2.1501, Val loss 2.1042\n",
      "Epoch 7: Train loss 2.1482, Val loss 2.0891\n",
      "Epoch 8: Train loss 2.1468, Val loss 2.0926\n",
      "Epoch 9: Train loss 2.1453, Val loss 2.0885\n",
      "Epoch 10: Train loss 2.1440, Val loss 2.0865\n",
      "Epoch 11: Train loss 2.1432, Val loss 2.0876\n",
      "Epoch 12: Train loss 2.1423, Val loss 2.0805\n",
      "Epoch 13: Train loss 2.1417, Val loss 2.0836\n",
      "Epoch 14: Train loss 2.1412, Val loss 2.0824\n",
      "Epoch 15: Train loss 2.1406, Val loss 2.0840\n",
      "Epoch 16: Train loss 2.1400, Val loss 2.0843\n",
      "Epoch 17: Train loss 2.1391, Val loss 2.0755\n",
      "Epoch 18: Train loss 2.1386, Val loss 2.0769\n",
      "Epoch 19: Train loss 2.1380, Val loss 2.0733\n",
      "Epoch 20: Train loss 2.1372, Val loss 2.0794\n",
      "Epoch 21: Train loss 2.1368, Val loss 2.0805\n",
      "Epoch 22: Train loss 2.1367, Val loss 2.0795\n",
      "Epoch 23: Train loss 2.1365, Val loss 2.0813\n",
      "Epoch 24: Train loss 2.1361, Val loss 2.0719\n",
      "Epoch 25: Train loss 2.1360, Val loss 2.0764\n",
      "Epoch 26: Train loss 2.1357, Val loss 2.0776\n",
      "Epoch 27: Train loss 2.1354, Val loss 2.0761\n",
      "Epoch 28: Train loss 2.1354, Val loss 2.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 09:52:38,080] Trial 10 finished with value: 2.070989271797911 and parameters: {'batch_size': 32, 'lr': 0.0007246691209707045, 'hidden_dim': 512, 'weight_decay': 1.0552397856143292e-06}. Best is trial 10 with value: 2.070989271797911.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1352, Val loss 2.0710\n",
      "==================================================\n",
      "Trial number 12\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4702, Val loss 2.1798\n",
      "Epoch 1: Train loss 2.2019, Val loss 2.1129\n",
      "Epoch 2: Train loss 2.1672, Val loss 2.1201\n",
      "Epoch 3: Train loss 2.1594, Val loss 2.1079\n",
      "Epoch 4: Train loss 2.1543, Val loss 2.1025\n",
      "Epoch 5: Train loss 2.1510, Val loss 2.1049\n",
      "Epoch 6: Train loss 2.1492, Val loss 2.0979\n",
      "Epoch 7: Train loss 2.1475, Val loss 2.0987\n",
      "Epoch 8: Train loss 2.1465, Val loss 2.0943\n",
      "Epoch 9: Train loss 2.1456, Val loss 2.0965\n",
      "Epoch 10: Train loss 2.1447, Val loss 2.0882\n",
      "Epoch 11: Train loss 2.1438, Val loss 2.0918\n",
      "Epoch 12: Train loss 2.1429, Val loss 2.0872\n",
      "Epoch 13: Train loss 2.1420, Val loss 2.0878\n",
      "Epoch 14: Train loss 2.1413, Val loss 2.0828\n",
      "Epoch 15: Train loss 2.1404, Val loss 2.0836\n",
      "Epoch 16: Train loss 2.1395, Val loss 2.0862\n",
      "Epoch 17: Train loss 2.1389, Val loss 2.0900\n",
      "Epoch 18: Train loss 2.1385, Val loss 2.0852\n",
      "Epoch 19: Train loss 2.1379, Val loss 2.0827\n",
      "Epoch 20: Train loss 2.1371, Val loss 2.0838\n",
      "Epoch 21: Train loss 2.1289, Val loss 2.0680\n",
      "Epoch 22: Train loss 2.1273, Val loss 2.0689\n",
      "Epoch 23: Train loss 2.1269, Val loss 2.0667\n",
      "Epoch 24: Train loss 2.1266, Val loss 2.0674\n",
      "Epoch 25: Train loss 2.1263, Val loss 2.0709\n",
      "Epoch 26: Train loss 2.1263, Val loss 2.0661\n",
      "Epoch 27: Train loss 2.1258, Val loss 2.0678\n",
      "Epoch 28: Train loss 2.1256, Val loss 2.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 10:31:43,896] Trial 11 finished with value: 2.0660949138034574 and parameters: {'batch_size': 32, 'lr': 0.0007016224567866245, 'hidden_dim': 512, 'weight_decay': 1.1004487564277892e-06}. Best is trial 11 with value: 2.0660949138034574.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1256, Val loss 2.0701\n",
      "==================================================\n",
      "Trial number 13\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.4893, Val loss 2.1768\n",
      "Epoch 1: Train loss 2.2109, Val loss 2.1531\n",
      "Epoch 2: Train loss 2.1821, Val loss 2.1659\n",
      "Epoch 3: Train loss 2.1730, Val loss 2.1861\n",
      "Epoch 4: Train loss 2.1683, Val loss 2.1886\n",
      "Epoch 5: Train loss 2.1654, Val loss 2.2205\n",
      "Epoch 6: Train loss 2.1636, Val loss 2.2249\n",
      "Epoch 7: Train loss 2.1614, Val loss 2.2632\n",
      "Epoch 8: Train loss 2.1478, Val loss 2.1724\n",
      "Epoch 9: Train loss 2.1455, Val loss 2.1609\n",
      "Epoch 10: Train loss 2.1442, Val loss 2.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 10:47:19,730] Trial 12 finished with value: 2.153145467405852 and parameters: {'batch_size': 32, 'lr': 0.0006288969424953504, 'hidden_dim': 384, 'weight_decay': 1.0034603972640884e-06}. Best is trial 11 with value: 2.0660949138034574.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train loss 2.1431, Val loss 2.2010\n",
      "==================================================\n",
      "Trial number 14\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.5095, Val loss 2.2279\n",
      "Epoch 1: Train loss 2.2277, Val loss 2.1206\n",
      "Epoch 2: Train loss 2.1678, Val loss 2.1205\n",
      "Epoch 3: Train loss 2.1543, Val loss 2.1307\n",
      "Epoch 4: Train loss 2.1478, Val loss 2.1580\n",
      "Epoch 5: Train loss 2.1434, Val loss 2.1555\n",
      "Epoch 6: Train loss 2.1400, Val loss 2.1728\n",
      "Epoch 7: Train loss 2.1374, Val loss 2.1752\n",
      "Epoch 8: Train loss 2.1244, Val loss 2.0986\n",
      "Epoch 9: Train loss 2.1213, Val loss 2.1100\n",
      "Epoch 10: Train loss 2.1196, Val loss 2.1081\n",
      "Epoch 11: Train loss 2.1185, Val loss 2.1205\n",
      "Epoch 12: Train loss 2.1173, Val loss 2.1234\n",
      "Epoch 13: Train loss 2.1164, Val loss 2.1237\n",
      "Epoch 14: Train loss 2.1156, Val loss 2.1255\n",
      "Epoch 15: Train loss 2.1085, Val loss 2.0756\n",
      "Epoch 16: Train loss 2.1080, Val loss 2.0798\n",
      "Epoch 17: Train loss 2.1071, Val loss 2.0785\n",
      "Epoch 18: Train loss 2.1066, Val loss 2.0757\n",
      "Epoch 19: Train loss 2.1058, Val loss 2.0778\n",
      "Epoch 20: Train loss 2.1053, Val loss 2.0760\n",
      "Epoch 21: Train loss 2.1048, Val loss 2.0761\n",
      "Epoch 22: Train loss 2.1011, Val loss 2.0572\n",
      "Epoch 23: Train loss 2.1008, Val loss 2.0589\n",
      "Epoch 24: Train loss 2.1005, Val loss 2.0608\n",
      "Epoch 25: Train loss 2.1003, Val loss 2.0577\n",
      "Epoch 26: Train loss 2.1000, Val loss 2.0595\n",
      "Epoch 27: Train loss 2.0998, Val loss 2.0610\n",
      "Epoch 28: Train loss 2.0993, Val loss 2.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 11:26:55,833] Trial 13 finished with value: 2.05181546670681 and parameters: {'batch_size': 32, 'lr': 0.0005587614855267997, 'hidden_dim': 512, 'weight_decay': 6.056437420027853e-06}. Best is trial 13 with value: 2.05181546670681.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.0975, Val loss 2.0518\n",
      "==================================================\n",
      "Trial number 15\n",
      "==================================================\n",
      "Epoch 0: Train loss 2.8744, Val loss 2.4504\n",
      "Epoch 1: Train loss 2.4838, Val loss 2.3448\n",
      "Epoch 2: Train loss 2.4184, Val loss 2.2957\n",
      "Epoch 3: Train loss 2.3694, Val loss 2.2535\n",
      "Epoch 4: Train loss 2.3175, Val loss 2.1969\n",
      "Epoch 5: Train loss 2.2606, Val loss 2.1474\n",
      "Epoch 6: Train loss 2.2106, Val loss 2.1180\n",
      "Epoch 7: Train loss 2.1856, Val loss 2.1069\n",
      "Epoch 8: Train loss 2.1742, Val loss 2.1009\n",
      "Epoch 9: Train loss 2.1669, Val loss 2.0953\n",
      "Epoch 10: Train loss 2.1617, Val loss 2.0941\n",
      "Epoch 11: Train loss 2.1573, Val loss 2.0918\n",
      "Epoch 12: Train loss 2.1538, Val loss 2.0895\n",
      "Epoch 13: Train loss 2.1505, Val loss 2.0892\n",
      "Epoch 14: Train loss 2.1486, Val loss 2.0874\n",
      "Epoch 15: Train loss 2.1460, Val loss 2.0858\n",
      "Epoch 16: Train loss 2.1435, Val loss 2.0855\n",
      "Epoch 17: Train loss 2.1417, Val loss 2.0863\n",
      "Epoch 18: Train loss 2.1397, Val loss 2.0826\n",
      "Epoch 19: Train loss 2.1383, Val loss 2.0815\n",
      "Epoch 20: Train loss 2.1371, Val loss 2.0823\n",
      "Epoch 21: Train loss 2.1358, Val loss 2.0834\n",
      "Epoch 22: Train loss 2.1352, Val loss 2.0832\n",
      "Epoch 23: Train loss 2.1346, Val loss 2.0843\n",
      "Epoch 24: Train loss 2.1339, Val loss 2.0844\n",
      "Epoch 25: Train loss 2.1330, Val loss 2.0864\n",
      "Epoch 26: Train loss 2.1294, Val loss 2.0745\n",
      "Epoch 27: Train loss 2.1293, Val loss 2.0760\n",
      "Epoch 28: Train loss 2.1289, Val loss 2.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 12:07:36,156] Trial 14 finished with value: 2.0745204591968585 and parameters: {'batch_size': 32, 'lr': 0.00010999744095048953, 'hidden_dim': 384, 'weight_decay': 5.383174846292139e-06}. Best is trial 13 with value: 2.05181546670681.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train loss 2.1285, Val loss 2.0749\n",
      "Best hyperparameters: {'batch_size': 32, 'lr': 0.0005587614855267997, 'hidden_dim': 512, 'weight_decay': 6.056437420027853e-06}\n",
      "Search completed in 377.77 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda trial: objective(trial, device, embeddings_path=\"sae_data/layer6_embeddings.npy\"), n_trials=15)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Search completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae61ddeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 3.1780, Val Loss: 2.6685\n",
      "Epoch [2/500], Train Loss: 2.6029, Val Loss: 2.4346\n",
      "Epoch [3/500], Train Loss: 2.4867, Val Loss: 2.3634\n",
      "Epoch [4/500], Train Loss: 2.4400, Val Loss: 2.3277\n",
      "Epoch [5/500], Train Loss: 2.4119, Val Loss: 2.3049\n",
      "Epoch [6/500], Train Loss: 2.3914, Val Loss: 2.2882\n",
      "Epoch [7/500], Train Loss: 2.3737, Val Loss: 2.2770\n",
      "Epoch [8/500], Train Loss: 2.3581, Val Loss: 2.2617\n",
      "Epoch [9/500], Train Loss: 2.3431, Val Loss: 2.2493\n",
      "Epoch [10/500], Train Loss: 2.3268, Val Loss: 2.2341\n",
      "Epoch [11/500], Train Loss: 2.3101, Val Loss: 2.2132\n",
      "Epoch [12/500], Train Loss: 2.2910, Val Loss: 2.2035\n",
      "Epoch [13/500], Train Loss: 2.2705, Val Loss: 2.1773\n",
      "Epoch [14/500], Train Loss: 2.2491, Val Loss: 2.1564\n",
      "Epoch [15/500], Train Loss: 2.2269, Val Loss: 2.1326\n",
      "Epoch [16/500], Train Loss: 2.2051, Val Loss: 2.1178\n",
      "Epoch [17/500], Train Loss: 2.1846, Val Loss: 2.1013\n",
      "Epoch [18/500], Train Loss: 2.1700, Val Loss: 2.0936\n",
      "Epoch [19/500], Train Loss: 2.1605, Val Loss: 2.0907\n",
      "Epoch [20/500], Train Loss: 2.1538, Val Loss: 2.0829\n",
      "Epoch [21/500], Train Loss: 2.1487, Val Loss: 2.0816\n",
      "Epoch [22/500], Train Loss: 2.1445, Val Loss: 2.0785\n",
      "Epoch [23/500], Train Loss: 2.1407, Val Loss: 2.0809\n",
      "Epoch [24/500], Train Loss: 2.1381, Val Loss: 2.0751\n",
      "Epoch [25/500], Train Loss: 2.1358, Val Loss: 2.0764\n",
      "Epoch [26/500], Train Loss: 2.1338, Val Loss: 2.0770\n",
      "Epoch [27/500], Train Loss: 2.1320, Val Loss: 2.0779\n",
      "Epoch [28/500], Train Loss: 2.1304, Val Loss: 2.0754\n",
      "Epoch [29/500], Train Loss: 2.1288, Val Loss: 2.0759\n",
      "Epoch [30/500], Train Loss: 2.1271, Val Loss: 2.0751\n",
      "Epoch [31/500], Train Loss: 2.1239, Val Loss: 2.0802\n",
      "Epoch [32/500], Train Loss: 2.1232, Val Loss: 2.0777\n",
      "Epoch [33/500], Train Loss: 2.1227, Val Loss: 2.0790\n",
      "Epoch [34/500], Train Loss: 2.1220, Val Loss: 2.0746\n",
      "Epoch [35/500], Train Loss: 2.1215, Val Loss: 2.0815\n",
      "Epoch [36/500], Train Loss: 2.1209, Val Loss: 2.0797\n",
      "Epoch [37/500], Train Loss: 2.1202, Val Loss: 2.0798\n",
      "Epoch [38/500], Train Loss: 2.1196, Val Loss: 2.0751\n",
      "Epoch [39/500], Train Loss: 2.1192, Val Loss: 2.0832\n",
      "Epoch [40/500], Train Loss: 2.1188, Val Loss: 2.0767\n",
      "Epoch [41/500], Train Loss: 2.1172, Val Loss: 2.0805\n",
      "Epoch [42/500], Train Loss: 2.1170, Val Loss: 2.0793\n",
      "Epoch [43/500], Train Loss: 2.1169, Val Loss: 2.0845\n",
      "Epoch [44/500], Train Loss: 2.1166, Val Loss: 2.0826\n",
      "⏳ Early stopping at epoch 44. No improvement for 10 epochs.\n",
      "✅ SAE training completed. Best model saved as sae_layer6.pth.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "train_sae(layer6_embeddings, \"sae_layer6_1.pth\", epochs=500,\n",
    "          batch_size=32, lr=5e-5, hidden_dim=512, weight_decay=1e-03, \n",
    "          train_losses=[],val_losses=val_losses, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6be99e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 9.3784, Val Loss: 8.6603\n",
      "Epoch [2/500], Train Loss: 8.5349, Val Loss: 8.2008\n",
      "Epoch [3/500], Train Loss: 8.1273, Val Loss: 7.8981\n",
      "Epoch [4/500], Train Loss: 7.8221, Val Loss: 7.6962\n",
      "Epoch [5/500], Train Loss: 7.5788, Val Loss: 7.4449\n",
      "Epoch [6/500], Train Loss: 7.3745, Val Loss: 7.2593\n",
      "Epoch [7/500], Train Loss: 7.1991, Val Loss: 7.0678\n",
      "Epoch [8/500], Train Loss: 7.0452, Val Loss: 6.8777\n",
      "Epoch [9/500], Train Loss: 6.9088, Val Loss: 6.7717\n",
      "Epoch [10/500], Train Loss: 6.7874, Val Loss: 6.6290\n",
      "Epoch [11/500], Train Loss: 6.6799, Val Loss: 6.5192\n",
      "Epoch [12/500], Train Loss: 6.5856, Val Loss: 6.3898\n",
      "Epoch [13/500], Train Loss: 6.5007, Val Loss: 6.3215\n",
      "Epoch [14/500], Train Loss: 6.4268, Val Loss: 6.2082\n",
      "Epoch [15/500], Train Loss: 6.3615, Val Loss: 6.1749\n",
      "Epoch [16/500], Train Loss: 6.3047, Val Loss: 6.1079\n",
      "Epoch [17/500], Train Loss: 6.2549, Val Loss: 6.0426\n",
      "Epoch [18/500], Train Loss: 6.2121, Val Loss: 6.0207\n",
      "Epoch [19/500], Train Loss: 6.1741, Val Loss: 5.9502\n",
      "Epoch [20/500], Train Loss: 6.1426, Val Loss: 5.9204\n",
      "Epoch [21/500], Train Loss: 6.1137, Val Loss: 5.9035\n",
      "Epoch [22/500], Train Loss: 6.0912, Val Loss: 5.8688\n",
      "Epoch [23/500], Train Loss: 6.0682, Val Loss: 5.8455\n",
      "Epoch [24/500], Train Loss: 6.0516, Val Loss: 5.8259\n",
      "Epoch [25/500], Train Loss: 6.0346, Val Loss: 5.8177\n",
      "Epoch [26/500], Train Loss: 6.0192, Val Loss: 5.8152\n",
      "Epoch [27/500], Train Loss: 6.0068, Val Loss: 5.7729\n",
      "Epoch [28/500], Train Loss: 5.9945, Val Loss: 5.7581\n",
      "Epoch [29/500], Train Loss: 5.9837, Val Loss: 5.7516\n",
      "Epoch [30/500], Train Loss: 5.9728, Val Loss: 5.7452\n",
      "Epoch [31/500], Train Loss: 5.9627, Val Loss: 5.7299\n",
      "Epoch [32/500], Train Loss: 5.9539, Val Loss: 5.7112\n",
      "Epoch [33/500], Train Loss: 5.9444, Val Loss: 5.7008\n",
      "Epoch [34/500], Train Loss: 5.9364, Val Loss: 5.7059\n",
      "Epoch [35/500], Train Loss: 5.9277, Val Loss: 5.7007\n",
      "Epoch [36/500], Train Loss: 5.9196, Val Loss: 5.6674\n",
      "Epoch [37/500], Train Loss: 5.9111, Val Loss: 5.6785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_79434/843324810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m train_sae(layer12_embeddings, \"sae_layer12.pth\", epochs=500,\n\u001b[0m\u001b[1;32m      6\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           train_losses=[],val_losses=val_losses, device=device)\n",
      "\u001b[0;32m~/development/Build GPT from scratch/train_sae.py\u001b[0m in \u001b[0;36mtrain_sae\u001b[0;34m(embeddings, model_name, train_losses, val_losses, batch_size, epochs, lr, hidden_dim, weight_decay, device, patience)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtotal_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msparsity_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                             )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    185\u001b[0m             )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "train_sae(layer12_embeddings, \"sae_layer12.pth\", epochs=500,\n",
    "          batch_size=32, lr=1e-5, hidden_dim=512, weight_decay=1e-02, \n",
    "          train_losses=[],val_losses=val_losses, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c637e474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 4.6714, Val Loss: 4.0114\n",
      "Epoch [2/500], Train Loss: 3.8162, Val Loss: 3.4267\n",
      "Epoch [3/500], Train Loss: 3.3766, Val Loss: 3.0739\n",
      "Epoch [4/500], Train Loss: 3.0600, Val Loss: 2.8439\n",
      "Epoch [5/500], Train Loss: 2.8320, Val Loss: 2.6764\n",
      "Epoch [6/500], Train Loss: 2.6863, Val Loss: 2.5769\n",
      "Epoch [7/500], Train Loss: 2.6010, Val Loss: 2.4972\n",
      "Epoch [8/500], Train Loss: 2.5442, Val Loss: 2.4359\n",
      "Epoch [9/500], Train Loss: 2.5016, Val Loss: 2.3885\n",
      "Epoch [10/500], Train Loss: 2.4683, Val Loss: 2.3595\n",
      "Epoch [11/500], Train Loss: 2.4423, Val Loss: 2.3330\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_79434/2332630519.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m train_sae(layer6_embeddings, \"sae_layer6_expanded.pth\", epochs=500,\n\u001b[0m\u001b[1;32m      6\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-04\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           train_losses=[],val_losses=val_losses, device=device)\n",
      "\u001b[0;32m~/development/Build GPT from scratch/train_sae.py\u001b[0m in \u001b[0;36mtrain_sae\u001b[0;34m(embeddings, model_name, train_losses, val_losses, batch_size, epochs, lr, hidden_dim, weight_decay, device, patience)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0msparsity_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtotal_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msparsity_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3336\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_VF.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "train_sae(layer6_embeddings, \"sae_layer6_expanded.pth\", epochs=500,\n",
    "          batch_size=32, lr=1e-5, hidden_dim=1536, weight_decay=1e-04, \n",
    "          train_losses=[],val_losses=val_losses, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21bec9e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 10.2528, Val Loss: 9.4597\n",
      "Epoch [2/500], Train Loss: 9.1150, Val Loss: 8.7378\n",
      "Epoch [3/500], Train Loss: 8.4605, Val Loss: 8.2728\n",
      "Epoch [4/500], Train Loss: 7.9646, Val Loss: 7.8654\n",
      "Epoch [5/500], Train Loss: 7.5986, Val Loss: 7.6130\n",
      "Epoch [6/500], Train Loss: 7.3334, Val Loss: 7.4305\n",
      "Epoch [7/500], Train Loss: 7.1272, Val Loss: 7.0853\n",
      "Epoch [8/500], Train Loss: 6.9541, Val Loss: 7.0251\n",
      "Epoch [9/500], Train Loss: 6.8049, Val Loss: 6.7875\n",
      "Epoch [10/500], Train Loss: 6.6743, Val Loss: 6.6703\n",
      "Epoch [11/500], Train Loss: 6.5586, Val Loss: 6.5076\n",
      "Epoch [12/500], Train Loss: 6.4574, Val Loss: 6.4108\n",
      "Epoch [13/500], Train Loss: 6.3698, Val Loss: 6.2961\n",
      "Epoch [14/500], Train Loss: 6.2927, Val Loss: 6.1800\n",
      "Epoch [15/500], Train Loss: 6.2243, Val Loss: 6.0854\n",
      "Epoch [16/500], Train Loss: 6.1671, Val Loss: 6.0580\n",
      "Epoch [17/500], Train Loss: 6.1168, Val Loss: 5.9766\n",
      "Epoch [18/500], Train Loss: 6.0739, Val Loss: 5.9582\n",
      "Epoch [19/500], Train Loss: 6.0375, Val Loss: 5.8765\n",
      "Epoch [20/500], Train Loss: 6.0081, Val Loss: 5.8419\n",
      "Epoch [21/500], Train Loss: 5.9820, Val Loss: 5.8041\n",
      "Epoch [22/500], Train Loss: 5.9598, Val Loss: 5.7608\n",
      "Epoch [23/500], Train Loss: 5.9408, Val Loss: 5.7330\n",
      "Epoch [24/500], Train Loss: 5.9259, Val Loss: 5.7223\n",
      "Epoch [25/500], Train Loss: 5.9122, Val Loss: 5.7230\n",
      "Epoch [26/500], Train Loss: 5.8996, Val Loss: 5.6935\n",
      "Epoch [27/500], Train Loss: 5.8898, Val Loss: 5.7098\n",
      "Epoch [28/500], Train Loss: 5.8800, Val Loss: 5.6719\n",
      "Epoch [29/500], Train Loss: 5.8727, Val Loss: 5.6658\n",
      "Epoch [30/500], Train Loss: 5.8653, Val Loss: 5.6590\n",
      "Epoch [31/500], Train Loss: 5.8592, Val Loss: 5.6572\n",
      "Epoch [32/500], Train Loss: 5.8540, Val Loss: 5.6581\n",
      "Epoch [33/500], Train Loss: 5.8473, Val Loss: 5.6347\n",
      "Epoch [34/500], Train Loss: 5.8415, Val Loss: 5.6329\n",
      "Epoch [35/500], Train Loss: 5.8365, Val Loss: 5.6255\n",
      "Epoch [36/500], Train Loss: 5.8317, Val Loss: 5.6416\n",
      "Epoch [37/500], Train Loss: 5.8274, Val Loss: 5.6184\n",
      "Epoch [38/500], Train Loss: 5.8236, Val Loss: 5.6166\n",
      "Epoch [39/500], Train Loss: 5.8192, Val Loss: 5.6101\n",
      "Epoch [40/500], Train Loss: 5.8154, Val Loss: 5.6094\n",
      "Epoch [41/500], Train Loss: 5.8117, Val Loss: 5.6192\n",
      "Epoch [42/500], Train Loss: 5.8080, Val Loss: 5.6058\n",
      "Epoch [43/500], Train Loss: 5.8044, Val Loss: 5.6006\n",
      "Epoch [44/500], Train Loss: 5.8004, Val Loss: 5.5964\n",
      "Epoch [45/500], Train Loss: 5.7963, Val Loss: 5.5907\n",
      "Epoch [46/500], Train Loss: 5.7918, Val Loss: 5.5857\n",
      "Epoch [47/500], Train Loss: 5.7894, Val Loss: 5.5820\n",
      "Epoch [48/500], Train Loss: 5.7867, Val Loss: 5.5801\n",
      "Epoch [49/500], Train Loss: 5.7834, Val Loss: 5.5932\n",
      "Epoch [50/500], Train Loss: 5.7808, Val Loss: 5.5755\n",
      "Epoch [51/500], Train Loss: 5.7773, Val Loss: 5.5737\n",
      "Epoch [52/500], Train Loss: 5.7737, Val Loss: 5.5764\n",
      "Epoch [53/500], Train Loss: 5.7716, Val Loss: 5.5687\n",
      "Epoch [54/500], Train Loss: 5.7676, Val Loss: 5.5648\n",
      "Epoch [55/500], Train Loss: 5.7666, Val Loss: 5.5682\n",
      "Epoch [56/500], Train Loss: 5.7625, Val Loss: 5.5616\n",
      "Epoch [57/500], Train Loss: 5.7591, Val Loss: 5.5740\n",
      "Epoch [58/500], Train Loss: 5.7580, Val Loss: 5.5533\n",
      "Epoch [59/500], Train Loss: 5.7541, Val Loss: 5.5636\n",
      "Epoch [60/500], Train Loss: 5.7516, Val Loss: 5.5506\n",
      "Epoch [61/500], Train Loss: 5.7485, Val Loss: 5.5468\n",
      "Epoch [62/500], Train Loss: 5.7463, Val Loss: 5.5455\n",
      "Epoch [63/500], Train Loss: 5.7441, Val Loss: 5.5391\n",
      "Epoch [64/500], Train Loss: 5.7406, Val Loss: 5.5416\n",
      "Epoch [65/500], Train Loss: 5.7364, Val Loss: 5.5409\n",
      "Epoch [66/500], Train Loss: 5.7347, Val Loss: 5.5318\n",
      "Epoch [67/500], Train Loss: 5.7320, Val Loss: 5.5447\n",
      "Epoch [68/500], Train Loss: 5.7283, Val Loss: 5.5312\n",
      "Epoch [69/500], Train Loss: 5.7272, Val Loss: 5.5253\n",
      "Epoch [70/500], Train Loss: 5.7240, Val Loss: 5.5245\n",
      "Epoch [71/500], Train Loss: 5.7207, Val Loss: 5.5200\n",
      "Epoch [72/500], Train Loss: 5.7179, Val Loss: 5.5164\n",
      "Epoch [73/500], Train Loss: 5.7157, Val Loss: 5.5126\n",
      "Epoch [74/500], Train Loss: 5.7129, Val Loss: 5.5185\n",
      "Epoch [75/500], Train Loss: 5.7104, Val Loss: 5.5192\n",
      "Epoch [76/500], Train Loss: 5.7067, Val Loss: 5.5179\n",
      "Epoch [77/500], Train Loss: 5.7037, Val Loss: 5.5082\n",
      "Epoch [78/500], Train Loss: 5.7011, Val Loss: 5.5011\n",
      "Epoch [79/500], Train Loss: 5.6982, Val Loss: 5.5007\n",
      "Epoch [80/500], Train Loss: 5.6945, Val Loss: 5.4939\n",
      "Epoch [81/500], Train Loss: 5.6924, Val Loss: 5.4981\n",
      "Epoch [82/500], Train Loss: 5.6892, Val Loss: 5.4936\n",
      "Epoch [83/500], Train Loss: 5.6860, Val Loss: 5.4911\n",
      "Epoch [84/500], Train Loss: 5.6836, Val Loss: 5.4826\n",
      "Epoch [85/500], Train Loss: 5.6801, Val Loss: 5.4852\n",
      "Epoch [86/500], Train Loss: 5.6769, Val Loss: 5.4798\n",
      "Epoch [87/500], Train Loss: 5.6742, Val Loss: 5.4710\n",
      "Epoch [88/500], Train Loss: 5.6715, Val Loss: 5.4699\n",
      "Epoch [89/500], Train Loss: 5.6683, Val Loss: 5.4716\n",
      "Epoch [90/500], Train Loss: 5.6640, Val Loss: 5.4678\n",
      "Epoch [91/500], Train Loss: 5.6606, Val Loss: 5.4581\n",
      "Epoch [92/500], Train Loss: 5.6574, Val Loss: 5.4639\n",
      "Epoch [93/500], Train Loss: 5.6545, Val Loss: 5.4526\n",
      "Epoch [94/500], Train Loss: 5.6510, Val Loss: 5.4753\n",
      "Epoch [95/500], Train Loss: 5.6481, Val Loss: 5.4436\n",
      "Epoch [96/500], Train Loss: 5.6442, Val Loss: 5.4483\n",
      "Epoch [97/500], Train Loss: 5.6397, Val Loss: 5.4428\n",
      "Epoch [98/500], Train Loss: 5.6369, Val Loss: 5.4466\n",
      "Epoch [99/500], Train Loss: 5.6325, Val Loss: 5.4341\n",
      "Epoch [100/500], Train Loss: 5.6298, Val Loss: 5.4299\n",
      "Epoch [101/500], Train Loss: 5.6256, Val Loss: 5.4316\n",
      "Epoch [102/500], Train Loss: 5.6221, Val Loss: 5.4229\n",
      "Epoch [103/500], Train Loss: 5.6174, Val Loss: 5.4133\n",
      "Epoch [104/500], Train Loss: 5.6144, Val Loss: 5.4142\n",
      "Epoch [105/500], Train Loss: 5.6097, Val Loss: 5.4122\n",
      "Epoch [106/500], Train Loss: 5.6059, Val Loss: 5.4086\n",
      "Epoch [107/500], Train Loss: 5.6019, Val Loss: 5.4019\n",
      "Epoch [108/500], Train Loss: 5.5984, Val Loss: 5.3951\n",
      "Epoch [109/500], Train Loss: 5.5928, Val Loss: 5.3954\n",
      "Epoch [110/500], Train Loss: 5.5884, Val Loss: 5.3994\n",
      "Epoch [111/500], Train Loss: 5.5842, Val Loss: 5.3860\n",
      "Epoch [112/500], Train Loss: 5.5797, Val Loss: 5.3822\n",
      "Epoch [113/500], Train Loss: 5.5744, Val Loss: 5.3781\n",
      "Epoch [114/500], Train Loss: 5.5699, Val Loss: 5.3781\n",
      "Epoch [115/500], Train Loss: 5.5636, Val Loss: 5.3640\n",
      "Epoch [116/500], Train Loss: 5.5599, Val Loss: 5.3692\n",
      "Epoch [117/500], Train Loss: 5.5538, Val Loss: 5.3729\n",
      "Epoch [118/500], Train Loss: 5.5497, Val Loss: 5.3665\n",
      "Epoch [119/500], Train Loss: 5.5436, Val Loss: 5.3578\n",
      "Epoch [120/500], Train Loss: 5.5388, Val Loss: 5.3528\n",
      "Epoch [121/500], Train Loss: 5.5334, Val Loss: 5.3501\n",
      "Epoch [122/500], Train Loss: 5.5296, Val Loss: 5.3449\n",
      "Epoch [123/500], Train Loss: 5.5253, Val Loss: 5.3412\n",
      "Epoch [124/500], Train Loss: 5.5220, Val Loss: 5.3469\n",
      "Epoch [125/500], Train Loss: 5.5172, Val Loss: 5.3335\n",
      "Epoch [126/500], Train Loss: 5.5134, Val Loss: 5.3364\n",
      "Epoch [127/500], Train Loss: 5.5096, Val Loss: 5.3301\n",
      "Epoch [128/500], Train Loss: 5.5062, Val Loss: 5.3328\n",
      "Epoch [129/500], Train Loss: 5.5032, Val Loss: 5.3269\n",
      "Epoch [130/500], Train Loss: 5.4998, Val Loss: 5.3347\n",
      "Epoch [131/500], Train Loss: 5.4959, Val Loss: 5.3237\n",
      "Epoch [132/500], Train Loss: 5.4942, Val Loss: 5.3229\n",
      "Epoch [133/500], Train Loss: 5.4908, Val Loss: 5.3231\n",
      "Epoch [134/500], Train Loss: 5.4876, Val Loss: 5.3182\n",
      "Epoch [135/500], Train Loss: 5.4845, Val Loss: 5.3194\n",
      "Epoch [136/500], Train Loss: 5.4823, Val Loss: 5.3209\n",
      "Epoch [137/500], Train Loss: 5.4794, Val Loss: 5.3144\n",
      "Epoch [138/500], Train Loss: 5.4773, Val Loss: 5.3126\n",
      "Epoch [139/500], Train Loss: 5.4744, Val Loss: 5.3088\n",
      "Epoch [140/500], Train Loss: 5.4723, Val Loss: 5.3105\n",
      "Epoch [141/500], Train Loss: 5.4693, Val Loss: 5.3055\n",
      "Epoch [142/500], Train Loss: 5.4687, Val Loss: 5.3126\n",
      "Epoch [143/500], Train Loss: 5.4657, Val Loss: 5.3092\n",
      "Epoch [144/500], Train Loss: 5.4639, Val Loss: 5.3019\n",
      "Epoch [145/500], Train Loss: 5.4625, Val Loss: 5.3092\n",
      "Epoch [146/500], Train Loss: 5.4607, Val Loss: 5.3024\n",
      "Epoch [147/500], Train Loss: 5.4588, Val Loss: 5.3024\n",
      "Epoch [148/500], Train Loss: 5.4577, Val Loss: 5.3096\n",
      "Epoch [149/500], Train Loss: 5.4552, Val Loss: 5.3000\n",
      "Epoch [150/500], Train Loss: 5.4546, Val Loss: 5.2963\n",
      "Epoch [151/500], Train Loss: 5.4531, Val Loss: 5.2961\n",
      "Epoch [152/500], Train Loss: 5.4508, Val Loss: 5.3059\n",
      "Epoch [153/500], Train Loss: 5.4495, Val Loss: 5.2962\n",
      "Epoch [154/500], Train Loss: 5.4487, Val Loss: 5.2973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [155/500], Train Loss: 5.4472, Val Loss: 5.2968\n",
      "Epoch [156/500], Train Loss: 5.4463, Val Loss: 5.2974\n",
      "Epoch [157/500], Train Loss: 5.4433, Val Loss: 5.2900\n",
      "Epoch [158/500], Train Loss: 5.4432, Val Loss: 5.3019\n",
      "Epoch [159/500], Train Loss: 5.4421, Val Loss: 5.2970\n",
      "Epoch [160/500], Train Loss: 5.4419, Val Loss: 5.2957\n",
      "Epoch [161/500], Train Loss: 5.4415, Val Loss: 5.2974\n",
      "Epoch [162/500], Train Loss: 5.4405, Val Loss: 5.2977\n",
      "Epoch [163/500], Train Loss: 5.4403, Val Loss: 5.3023\n",
      "Epoch [164/500], Train Loss: 5.4392, Val Loss: 5.2934\n",
      "Epoch [165/500], Train Loss: 5.4390, Val Loss: 5.3098\n",
      "Epoch [166/500], Train Loss: 5.4385, Val Loss: 5.3026\n",
      "Epoch [167/500], Train Loss: 5.4382, Val Loss: 5.2926\n",
      "⏳ Early stopping at epoch 167. No improvement for 10 epochs.\n",
      "✅ SAE training completed. Best model saved as sae_layer12_expanded.pth.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "train_sae(layer12_embeddings, \"sae_layer12_expanded_1700.pth\", epochs=500,\n",
    "          batch_size=32, lr=1e-5, hidden_dim=1700, weight_decay=1e-04, \n",
    "          train_losses=[],val_losses=val_losses, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7c4eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"losses_sae.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "train_losses = data[\"train_losses\"]\n",
    "val_losses = data[\"val_losses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7032254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparse_auto_encoder import SparseAutoencoder\n",
    "\n",
    "sae_layer12_expanded = SparseAutoencoder(input_dim=768, hidden_dim=1700)\n",
    "sae_layer12_expanded.load_state_dict(torch.load(\"sae_layer12_expanded.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e49d48a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 10.7967, Val Loss: 9.8300\n",
      "Epoch [2/500], Train Loss: 9.4837, Val Loss: 9.0222\n",
      "Epoch [3/500], Train Loss: 8.7017, Val Loss: 8.3775\n",
      "Epoch [4/500], Train Loss: 8.1056, Val Loss: 8.0037\n",
      "Epoch [5/500], Train Loss: 7.6678, Val Loss: 7.7563\n",
      "Epoch [6/500], Train Loss: 7.3639, Val Loss: 7.4141\n",
      "Epoch [7/500], Train Loss: 7.1397, Val Loss: 7.2778\n",
      "Epoch [8/500], Train Loss: 6.9595, Val Loss: 7.0649\n",
      "Epoch [9/500], Train Loss: 6.8049, Val Loss: 6.8241\n",
      "Epoch [10/500], Train Loss: 6.6685, Val Loss: 6.6474\n",
      "Epoch [11/500], Train Loss: 6.5516, Val Loss: 6.5122\n",
      "Epoch [12/500], Train Loss: 6.4494, Val Loss: 6.3105\n",
      "Epoch [13/500], Train Loss: 6.3583, Val Loss: 6.2694\n",
      "Epoch [14/500], Train Loss: 6.2799, Val Loss: 6.1230\n",
      "Epoch [15/500], Train Loss: 6.2110, Val Loss: 6.0314\n",
      "Epoch [16/500], Train Loss: 6.1518, Val Loss: 5.9883\n",
      "Epoch [17/500], Train Loss: 6.1017, Val Loss: 5.9333\n",
      "Epoch [18/500], Train Loss: 6.0584, Val Loss: 5.8741\n",
      "Epoch [19/500], Train Loss: 6.0212, Val Loss: 5.8591\n",
      "Epoch [20/500], Train Loss: 5.9915, Val Loss: 5.8080\n",
      "Epoch [21/500], Train Loss: 5.9632, Val Loss: 5.7748\n",
      "Epoch [22/500], Train Loss: 5.9416, Val Loss: 5.7516\n",
      "Epoch [23/500], Train Loss: 5.9234, Val Loss: 5.7198\n",
      "Epoch [24/500], Train Loss: 5.9079, Val Loss: 5.6766\n",
      "Epoch [25/500], Train Loss: 5.8937, Val Loss: 5.6792\n",
      "Epoch [26/500], Train Loss: 5.8820, Val Loss: 5.6714\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_56525/3295904009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_sae(layer12_embeddings, model_name=\"sae_layer12_expanded_1.pth\", epochs=500,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2304\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-03\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           train_losses=[],val_losses=[], device=device)\n",
      "\u001b[0;32m~/development/Build GPT from scratch/train_sae.py\u001b[0m in \u001b[0;36mtrain_sae\u001b[0;34m(embeddings, sae, model_name, train_losses, val_losses, batch_size, epochs, lr, hidden_dim, weight_decay, device, patience)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0msparsity_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mtotal_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msparsity_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_sae(layer12_embeddings, model_name=\"sae_layer12_expanded_1.pth\", epochs=500,\n",
    "          patience=30, batch_size=32, lr=1e-5, hidden_dim=2304, weight_decay=1e-03, \n",
    "          train_losses=[],val_losses=[], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b9bdfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.419478  ,  1.097434  , -5.971122  , ..., -2.1528153 ,\n",
       "         4.6942825 ,  0.84316725],\n",
       "       [-0.55278456, -2.5868275 , -4.9650073 , ..., -4.4642773 ,\n",
       "         8.33209   , -6.3761005 ],\n",
       "       [11.52194   , -7.388887  , -7.3803024 , ..., -2.946519  ,\n",
       "        -0.92312884, -1.3777622 ],\n",
       "       ...,\n",
       "       [ 5.6442814 , -3.999446  , -2.193171  , ..., -2.052844  ,\n",
       "         0.05622917,  0.82508975],\n",
       "       [ 1.7008734 ,  1.0291717 ,  1.7188739 , ...,  0.36227274,\n",
       "         3.265392  , -0.6340358 ],\n",
       "       [ 4.86724   , -3.615003  , -0.6352348 , ...,  3.7851796 ,\n",
       "         1.1924815 , -1.6266764 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer12_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e31cb7e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariam/opt/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 10.7911, Val Loss: 9.8234\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [2/500], Train Loss: 9.4748, Val Loss: 8.9847\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [3/500], Train Loss: 8.6924, Val Loss: 8.4460\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [4/500], Train Loss: 8.0968, Val Loss: 7.9623\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [5/500], Train Loss: 7.6660, Val Loss: 7.7041\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [6/500], Train Loss: 7.3656, Val Loss: 7.4455\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [7/500], Train Loss: 7.1416, Val Loss: 7.1965\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [8/500], Train Loss: 6.9612, Val Loss: 7.0236\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [9/500], Train Loss: 6.8082, Val Loss: 6.9582\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [10/500], Train Loss: 6.6735, Val Loss: 6.6553\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [11/500], Train Loss: 6.5566, Val Loss: 6.5508\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [12/500], Train Loss: 6.4546, Val Loss: 6.4480\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [13/500], Train Loss: 6.3648, Val Loss: 6.2476\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [14/500], Train Loss: 6.2853, Val Loss: 6.1997\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [15/500], Train Loss: 6.2168, Val Loss: 6.1305\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [16/500], Train Loss: 6.1573, Val Loss: 6.0226\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [17/500], Train Loss: 6.1064, Val Loss: 5.9572\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [18/500], Train Loss: 6.0622, Val Loss: 5.8939\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [19/500], Train Loss: 6.0246, Val Loss: 5.8552\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [20/500], Train Loss: 5.9928, Val Loss: 5.8023\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [21/500], Train Loss: 5.9647, Val Loss: 5.8220\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [22/500], Train Loss: 5.9416, Val Loss: 5.7446\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [23/500], Train Loss: 5.9232, Val Loss: 5.7339\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [24/500], Train Loss: 5.9068, Val Loss: 5.6976\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [25/500], Train Loss: 5.8931, Val Loss: 5.6718\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [26/500], Train Loss: 5.8795, Val Loss: 5.6815\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [27/500], Train Loss: 5.8693, Val Loss: 5.6950\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [28/500], Train Loss: 5.8603, Val Loss: 5.6604\n",
      "🔹 New Learning Rate: 1e-05\n",
      "Epoch [29/500], Train Loss: 5.8517, Val Loss: 5.6327\n",
      "🔹 New Learning Rate: 1e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_77814/3146884291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_sae(layer12_embeddings, model_name=\"sae_layer12_expanded_2.pth\", epochs=500,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2304\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           train_losses=[],val_losses=[], device=device)\n",
      "\u001b[0;32m~/development/Build GPT from scratch/train_sae.py\u001b[0m in \u001b[0;36mtrain_sae\u001b[0;34m(embeddings, sae, model_name, train_losses, val_losses, batch_size, epochs, lr, hidden_dim, weight_decay, device, patience)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mtotal_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msparsity_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                             )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    185\u001b[0m             )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_sae(layer12_embeddings, model_name=\"sae_layer12_expanded_2.pth\", epochs=500,\n",
    "          patience=30, batch_size=32, lr=1e-5, hidden_dim=2304, weight_decay=1e-02, \n",
    "          train_losses=[],val_losses=[], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4396b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 5.4032, Val Loss: 4.8838\n",
      "Epoch [2/500], Train Loss: 4.8314, Val Loss: 4.5803\n",
      "Epoch [3/500], Train Loss: 4.6227, Val Loss: 4.3705\n",
      "Epoch [4/500], Train Loss: 4.4661, Val Loss: 4.2232\n",
      "Epoch [5/500], Train Loss: 4.3308, Val Loss: 4.0835\n",
      "Epoch [6/500], Train Loss: 4.2070, Val Loss: 3.9584\n",
      "Epoch [7/500], Train Loss: 4.0939, Val Loss: 3.8387\n",
      "Epoch [8/500], Train Loss: 3.9889, Val Loss: 3.7355\n",
      "Epoch [9/500], Train Loss: 3.8890, Val Loss: 3.6505\n",
      "Epoch [10/500], Train Loss: 3.7948, Val Loss: 3.5522\n",
      "Epoch [11/500], Train Loss: 3.7048, Val Loss: 3.4585\n",
      "Epoch [12/500], Train Loss: 3.6188, Val Loss: 3.3752\n",
      "Epoch [13/500], Train Loss: 3.5362, Val Loss: 3.3095\n",
      "Epoch [14/500], Train Loss: 3.4574, Val Loss: 3.2342\n",
      "Epoch [15/500], Train Loss: 3.3810, Val Loss: 3.1437\n",
      "Epoch [16/500], Train Loss: 3.3091, Val Loss: 3.0838\n",
      "Epoch [17/500], Train Loss: 3.2393, Val Loss: 3.0454\n",
      "Epoch [18/500], Train Loss: 3.1739, Val Loss: 2.9571\n",
      "Epoch [19/500], Train Loss: 3.1122, Val Loss: 2.9221\n",
      "Epoch [20/500], Train Loss: 3.0534, Val Loss: 2.8719\n",
      "Epoch [21/500], Train Loss: 2.9997, Val Loss: 2.8324\n",
      "Epoch [22/500], Train Loss: 2.9484, Val Loss: 2.7815\n",
      "Epoch [23/500], Train Loss: 2.9033, Val Loss: 2.7553\n",
      "Epoch [24/500], Train Loss: 2.8621, Val Loss: 2.7304\n",
      "Epoch [25/500], Train Loss: 2.8245, Val Loss: 2.6951\n",
      "Epoch [26/500], Train Loss: 2.7932, Val Loss: 2.6664\n",
      "Epoch [27/500], Train Loss: 2.7636, Val Loss: 2.6491\n",
      "Epoch [28/500], Train Loss: 2.7386, Val Loss: 2.6487\n",
      "Epoch [29/500], Train Loss: 2.7154, Val Loss: 2.6069\n",
      "Epoch [30/500], Train Loss: 2.6948, Val Loss: 2.5818\n",
      "Epoch [31/500], Train Loss: 2.6770, Val Loss: 2.5935\n",
      "Epoch [32/500], Train Loss: 2.6596, Val Loss: 2.5670\n",
      "Epoch [33/500], Train Loss: 2.6443, Val Loss: 2.5564\n",
      "Epoch [34/500], Train Loss: 2.6305, Val Loss: 2.5508\n",
      "Epoch [35/500], Train Loss: 2.6181, Val Loss: 2.5182\n",
      "Epoch [36/500], Train Loss: 2.6063, Val Loss: 2.4987\n",
      "Epoch [37/500], Train Loss: 2.5954, Val Loss: 2.4808\n",
      "Epoch [38/500], Train Loss: 2.5850, Val Loss: 2.4758\n",
      "Epoch [39/500], Train Loss: 2.5749, Val Loss: 2.4758\n",
      "Epoch [40/500], Train Loss: 2.5662, Val Loss: 2.4595\n",
      "Epoch [41/500], Train Loss: 2.5578, Val Loss: 2.4502\n",
      "Epoch [42/500], Train Loss: 2.5498, Val Loss: 2.4327\n",
      "Epoch [43/500], Train Loss: 2.5421, Val Loss: 2.4309\n",
      "Epoch [44/500], Train Loss: 2.5348, Val Loss: 2.4060\n",
      "Epoch [45/500], Train Loss: 2.5281, Val Loss: 2.4051\n",
      "Epoch [46/500], Train Loss: 2.5213, Val Loss: 2.3890\n",
      "Epoch [47/500], Train Loss: 2.5153, Val Loss: 2.3934\n",
      "Epoch [48/500], Train Loss: 2.5096, Val Loss: 2.3822\n",
      "Epoch [49/500], Train Loss: 2.5038, Val Loss: 2.3713\n",
      "Epoch [50/500], Train Loss: 2.4981, Val Loss: 2.3746\n",
      "Epoch [51/500], Train Loss: 2.4932, Val Loss: 2.3545\n",
      "Epoch [52/500], Train Loss: 2.4882, Val Loss: 2.3585\n",
      "Epoch [53/500], Train Loss: 2.4830, Val Loss: 2.3612\n",
      "Epoch [54/500], Train Loss: 2.4789, Val Loss: 2.3486\n",
      "Epoch [55/500], Train Loss: 2.4747, Val Loss: 2.3362\n",
      "Epoch [56/500], Train Loss: 2.4704, Val Loss: 2.3300\n",
      "Epoch [57/500], Train Loss: 2.4665, Val Loss: 2.3159\n",
      "Epoch [58/500], Train Loss: 2.4629, Val Loss: 2.3163\n",
      "Epoch [59/500], Train Loss: 2.4587, Val Loss: 2.3144\n",
      "Epoch [60/500], Train Loss: 2.4556, Val Loss: 2.3000\n",
      "Epoch [61/500], Train Loss: 2.4523, Val Loss: 2.2937\n",
      "Epoch [62/500], Train Loss: 2.4492, Val Loss: 2.2939\n",
      "Epoch [63/500], Train Loss: 2.4456, Val Loss: 2.2936\n",
      "Epoch [64/500], Train Loss: 2.4427, Val Loss: 2.2853\n",
      "Epoch [65/500], Train Loss: 2.4397, Val Loss: 2.2831\n",
      "Epoch [66/500], Train Loss: 2.4367, Val Loss: 2.2918\n",
      "Epoch [67/500], Train Loss: 2.4341, Val Loss: 2.2811\n",
      "Epoch [68/500], Train Loss: 2.4312, Val Loss: 2.2792\n",
      "Epoch [69/500], Train Loss: 2.4291, Val Loss: 2.2743\n",
      "Epoch [70/500], Train Loss: 2.4268, Val Loss: 2.2785\n",
      "Epoch [71/500], Train Loss: 2.4242, Val Loss: 2.2710\n",
      "Epoch [72/500], Train Loss: 2.4219, Val Loss: 2.2713\n",
      "Epoch [73/500], Train Loss: 2.4197, Val Loss: 2.2678\n",
      "Epoch [74/500], Train Loss: 2.4178, Val Loss: 2.2679\n",
      "Epoch [75/500], Train Loss: 2.4161, Val Loss: 2.2676\n",
      "Epoch [76/500], Train Loss: 2.4137, Val Loss: 2.2637\n",
      "Epoch [77/500], Train Loss: 2.4119, Val Loss: 2.2652\n",
      "Epoch [78/500], Train Loss: 2.4099, Val Loss: 2.2636\n",
      "Epoch [79/500], Train Loss: 2.4085, Val Loss: 2.2616\n",
      "Epoch [80/500], Train Loss: 2.4068, Val Loss: 2.2599\n",
      "Epoch [81/500], Train Loss: 2.4046, Val Loss: 2.2588\n",
      "Epoch [82/500], Train Loss: 2.4035, Val Loss: 2.2614\n",
      "Epoch [83/500], Train Loss: 2.4013, Val Loss: 2.2563\n",
      "Epoch [84/500], Train Loss: 2.4001, Val Loss: 2.2587\n",
      "Epoch [85/500], Train Loss: 2.3987, Val Loss: 2.2576\n",
      "Epoch [86/500], Train Loss: 2.3965, Val Loss: 2.2611\n",
      "Epoch [87/500], Train Loss: 2.3957, Val Loss: 2.2602\n",
      "Epoch [88/500], Train Loss: 2.3942, Val Loss: 2.2544\n",
      "Epoch [89/500], Train Loss: 2.3930, Val Loss: 2.2612\n",
      "Epoch [90/500], Train Loss: 2.3918, Val Loss: 2.2553\n",
      "Epoch [91/500], Train Loss: 2.3904, Val Loss: 2.2672\n",
      "Epoch [92/500], Train Loss: 2.3892, Val Loss: 2.2577\n",
      "Epoch [93/500], Train Loss: 2.3883, Val Loss: 2.2595\n",
      "Epoch [94/500], Train Loss: 2.3867, Val Loss: 2.2592\n",
      "Epoch [95/500], Train Loss: 2.3857, Val Loss: 2.2560\n",
      "Epoch [96/500], Train Loss: 2.3849, Val Loss: 2.2554\n",
      "Epoch [97/500], Train Loss: 2.3838, Val Loss: 2.2577\n",
      "Epoch [98/500], Train Loss: 2.3825, Val Loss: 2.2665\n",
      "Epoch [99/500], Train Loss: 2.3814, Val Loss: 2.2623\n",
      "Epoch [100/500], Train Loss: 2.3807, Val Loss: 2.2632\n",
      "Epoch [101/500], Train Loss: 2.3795, Val Loss: 2.2535\n",
      "Epoch [102/500], Train Loss: 2.3786, Val Loss: 2.2654\n",
      "Epoch [103/500], Train Loss: 2.3777, Val Loss: 2.2649\n",
      "Epoch [104/500], Train Loss: 2.3771, Val Loss: 2.2706\n",
      "Epoch [105/500], Train Loss: 2.3759, Val Loss: 2.2680\n",
      "Epoch [106/500], Train Loss: 2.3751, Val Loss: 2.2704\n",
      "Epoch [107/500], Train Loss: 2.3746, Val Loss: 2.2621\n",
      "Epoch [108/500], Train Loss: 2.3739, Val Loss: 2.2782\n",
      "Epoch [109/500], Train Loss: 2.3723, Val Loss: 2.2634\n",
      "Epoch [110/500], Train Loss: 2.3717, Val Loss: 2.2713\n",
      "Epoch [111/500], Train Loss: 2.3711, Val Loss: 2.2798\n",
      "Epoch [112/500], Train Loss: 2.3704, Val Loss: 2.2711\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_62465/2350761703.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_sae(layer6_embeddings, model_name=\"sae_layer6_expanded.pth\", epochs=500,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3072\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           train_losses=[],val_losses=[], device=device)\n",
      "\u001b[0;32m~/development/Build GPT from scratch/train_sae.py\u001b[0m in \u001b[0;36mtrain_sae\u001b[0;34m(embeddings, sae, model_name, train_losses, val_losses, batch_size, epochs, lr, hidden_dim, weight_decay, device, patience)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0msparsity_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/Build GPT from scratch/sparse_auto_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_sae(layer6_embeddings, model_name=\"sae_layer6_expanded.pth\", epochs=500,\n",
    "          patience=30, batch_size=16, lr=1e-6, hidden_dim=3072, weight_decay=0.01, \n",
    "          train_losses=[],val_losses=[], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13db1e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 10.1094, Val Loss: 9.3052\n",
      "Epoch [2/500], Train Loss: 9.0258, Val Loss: 8.6694\n",
      "Epoch [3/500], Train Loss: 8.4043, Val Loss: 8.2044\n",
      "Epoch [4/500], Train Loss: 7.9314, Val Loss: 7.9031\n",
      "Epoch [5/500], Train Loss: 7.5860, Val Loss: 7.5911\n",
      "Epoch [6/500], Train Loss: 7.3290, Val Loss: 7.3647\n",
      "Epoch [7/500], Train Loss: 7.1274, Val Loss: 7.1575\n",
      "Epoch [8/500], Train Loss: 6.9569, Val Loss: 6.8590\n",
      "Epoch [9/500], Train Loss: 6.8095, Val Loss: 6.7924\n",
      "Epoch [10/500], Train Loss: 6.6798, Val Loss: 6.5947\n",
      "Epoch [11/500], Train Loss: 6.5644, Val Loss: 6.4972\n",
      "Epoch [12/500], Train Loss: 6.4640, Val Loss: 6.3730\n",
      "Epoch [13/500], Train Loss: 6.3754, Val Loss: 6.2535\n",
      "Epoch [14/500], Train Loss: 6.2993, Val Loss: 6.1770\n",
      "Epoch [15/500], Train Loss: 6.2305, Val Loss: 6.0726\n",
      "Epoch [16/500], Train Loss: 6.1739, Val Loss: 6.0339\n",
      "Epoch [17/500], Train Loss: 6.1243, Val Loss: 5.9524\n",
      "Epoch [18/500], Train Loss: 6.0824, Val Loss: 5.8978\n",
      "Epoch [19/500], Train Loss: 6.0473, Val Loss: 5.8906\n",
      "Epoch [20/500], Train Loss: 6.0161, Val Loss: 5.8252\n",
      "Epoch [21/500], Train Loss: 5.9889, Val Loss: 5.7863\n",
      "Epoch [22/500], Train Loss: 5.9686, Val Loss: 5.7420\n",
      "Epoch [23/500], Train Loss: 5.9489, Val Loss: 5.7490\n",
      "Epoch [24/500], Train Loss: 5.9331, Val Loss: 5.7152\n",
      "Epoch [25/500], Train Loss: 5.9199, Val Loss: 5.7020\n",
      "Epoch [26/500], Train Loss: 5.9079, Val Loss: 5.7135\n",
      "Epoch [27/500], Train Loss: 5.8979, Val Loss: 5.6795\n",
      "Epoch [28/500], Train Loss: 5.8904, Val Loss: 5.6628\n",
      "Epoch [29/500], Train Loss: 5.8817, Val Loss: 5.6610\n",
      "Epoch [30/500], Train Loss: 5.8744, Val Loss: 5.6587\n",
      "Epoch [31/500], Train Loss: 5.8676, Val Loss: 5.6541\n",
      "Epoch [32/500], Train Loss: 5.8622, Val Loss: 5.6405\n",
      "Epoch [33/500], Train Loss: 5.8559, Val Loss: 5.6393\n",
      "Epoch [34/500], Train Loss: 5.8513, Val Loss: 5.6304\n",
      "Epoch [35/500], Train Loss: 5.8460, Val Loss: 5.6283\n",
      "Epoch [36/500], Train Loss: 5.8406, Val Loss: 5.6170\n",
      "Epoch [37/500], Train Loss: 5.8362, Val Loss: 5.6157\n",
      "Epoch [38/500], Train Loss: 5.8316, Val Loss: 5.6138\n",
      "Epoch [39/500], Train Loss: 5.8262, Val Loss: 5.6070\n",
      "Epoch [40/500], Train Loss: 5.8229, Val Loss: 5.6107\n",
      "Epoch [41/500], Train Loss: 5.8196, Val Loss: 5.6089\n",
      "Epoch [42/500], Train Loss: 5.8160, Val Loss: 5.6059\n",
      "Epoch [43/500], Train Loss: 5.8123, Val Loss: 5.5899\n",
      "Epoch [44/500], Train Loss: 5.8075, Val Loss: 5.5999\n",
      "Epoch [45/500], Train Loss: 5.8047, Val Loss: 5.5958\n",
      "Epoch [46/500], Train Loss: 5.8006, Val Loss: 5.5816\n",
      "Epoch [47/500], Train Loss: 5.7964, Val Loss: 5.5846\n",
      "Epoch [48/500], Train Loss: 5.7937, Val Loss: 5.5780\n",
      "Epoch [49/500], Train Loss: 5.7905, Val Loss: 5.5764\n",
      "Epoch [50/500], Train Loss: 5.7867, Val Loss: 5.5821\n",
      "Epoch [51/500], Train Loss: 5.7839, Val Loss: 5.5703\n",
      "Epoch [52/500], Train Loss: 5.7808, Val Loss: 5.5685\n",
      "Epoch [53/500], Train Loss: 5.7782, Val Loss: 5.5630\n",
      "Epoch [54/500], Train Loss: 5.7734, Val Loss: 5.5594\n",
      "Epoch [55/500], Train Loss: 5.7713, Val Loss: 5.5592\n",
      "Epoch [56/500], Train Loss: 5.7682, Val Loss: 5.5571\n",
      "Epoch [57/500], Train Loss: 5.7656, Val Loss: 5.5482\n",
      "Epoch [58/500], Train Loss: 5.7624, Val Loss: 5.5511\n",
      "Epoch [59/500], Train Loss: 5.7591, Val Loss: 5.5471\n",
      "Epoch [60/500], Train Loss: 5.7559, Val Loss: 5.5468\n",
      "Epoch [61/500], Train Loss: 5.7526, Val Loss: 5.5364\n",
      "Epoch [62/500], Train Loss: 5.7495, Val Loss: 5.5372\n",
      "Epoch [63/500], Train Loss: 5.7468, Val Loss: 5.5362\n",
      "Epoch [64/500], Train Loss: 5.7448, Val Loss: 5.5417\n",
      "Epoch [65/500], Train Loss: 5.7423, Val Loss: 5.5341\n",
      "Epoch [66/500], Train Loss: 5.7379, Val Loss: 5.5268\n",
      "Epoch [67/500], Train Loss: 5.7358, Val Loss: 5.5294\n",
      "Epoch [68/500], Train Loss: 5.7315, Val Loss: 5.5152\n",
      "Epoch [69/500], Train Loss: 5.7283, Val Loss: 5.5146\n",
      "Epoch [70/500], Train Loss: 5.7265, Val Loss: 5.5115\n",
      "Epoch [71/500], Train Loss: 5.7231, Val Loss: 5.5118\n",
      "Epoch [72/500], Train Loss: 5.7206, Val Loss: 5.5090\n",
      "Epoch [73/500], Train Loss: 5.7167, Val Loss: 5.5039\n",
      "Epoch [74/500], Train Loss: 5.7139, Val Loss: 5.5148\n",
      "Epoch [75/500], Train Loss: 5.7112, Val Loss: 5.5206\n",
      "Epoch [76/500], Train Loss: 5.7070, Val Loss: 5.4964\n",
      "Epoch [77/500], Train Loss: 5.7048, Val Loss: 5.4945\n",
      "Epoch [78/500], Train Loss: 5.7013, Val Loss: 5.4861\n",
      "Epoch [79/500], Train Loss: 5.6965, Val Loss: 5.4894\n",
      "Epoch [80/500], Train Loss: 5.6953, Val Loss: 5.4810\n",
      "Epoch [81/500], Train Loss: 5.6920, Val Loss: 5.4828\n",
      "Epoch [82/500], Train Loss: 5.6879, Val Loss: 5.4829\n",
      "Epoch [83/500], Train Loss: 5.6844, Val Loss: 5.4768\n",
      "Epoch [84/500], Train Loss: 5.6802, Val Loss: 5.4713\n",
      "Epoch [85/500], Train Loss: 5.6779, Val Loss: 5.4810\n",
      "Epoch [86/500], Train Loss: 5.6745, Val Loss: 5.4624\n",
      "Epoch [87/500], Train Loss: 5.6714, Val Loss: 5.4645\n",
      "Epoch [88/500], Train Loss: 5.6674, Val Loss: 5.4553\n",
      "Epoch [89/500], Train Loss: 5.6639, Val Loss: 5.4521\n",
      "Epoch [90/500], Train Loss: 5.6600, Val Loss: 5.4472\n",
      "Epoch [91/500], Train Loss: 5.6563, Val Loss: 5.4460\n",
      "Epoch [92/500], Train Loss: 5.6520, Val Loss: 5.4444\n",
      "Epoch [93/500], Train Loss: 5.6480, Val Loss: 5.4394\n",
      "Epoch [94/500], Train Loss: 5.6455, Val Loss: 5.4312\n",
      "Epoch [95/500], Train Loss: 5.6404, Val Loss: 5.4262\n",
      "Epoch [96/500], Train Loss: 5.6363, Val Loss: 5.4252\n",
      "Epoch [97/500], Train Loss: 5.6327, Val Loss: 5.4196\n",
      "Epoch [98/500], Train Loss: 5.6286, Val Loss: 5.4180\n",
      "Epoch [99/500], Train Loss: 5.6254, Val Loss: 5.4094\n",
      "Epoch [100/500], Train Loss: 5.6202, Val Loss: 5.4140\n",
      "Epoch [101/500], Train Loss: 5.6155, Val Loss: 5.4078\n",
      "Epoch [102/500], Train Loss: 5.6112, Val Loss: 5.3977\n",
      "Epoch [103/500], Train Loss: 5.6066, Val Loss: 5.3967\n",
      "Epoch [104/500], Train Loss: 5.6022, Val Loss: 5.3897\n",
      "Epoch [105/500], Train Loss: 5.5970, Val Loss: 5.3810\n",
      "Epoch [106/500], Train Loss: 5.5936, Val Loss: 5.3853\n",
      "Epoch [107/500], Train Loss: 5.5890, Val Loss: 5.3774\n",
      "Epoch [108/500], Train Loss: 5.5831, Val Loss: 5.3687\n",
      "Epoch [109/500], Train Loss: 5.5777, Val Loss: 5.3646\n",
      "Epoch [110/500], Train Loss: 5.5723, Val Loss: 5.3616\n",
      "Epoch [111/500], Train Loss: 5.5661, Val Loss: 5.3654\n",
      "Epoch [112/500], Train Loss: 5.5612, Val Loss: 5.3563\n",
      "Epoch [113/500], Train Loss: 5.5559, Val Loss: 5.3576\n",
      "Epoch [114/500], Train Loss: 5.5501, Val Loss: 5.3750\n",
      "Epoch [115/500], Train Loss: 5.5445, Val Loss: 5.3459\n",
      "Epoch [116/500], Train Loss: 5.5390, Val Loss: 5.3397\n",
      "Epoch [117/500], Train Loss: 5.5347, Val Loss: 5.3399\n",
      "Epoch [118/500], Train Loss: 5.5299, Val Loss: 5.3338\n",
      "Epoch [119/500], Train Loss: 5.5268, Val Loss: 5.3386\n",
      "Epoch [120/500], Train Loss: 5.5209, Val Loss: 5.3364\n",
      "Epoch [121/500], Train Loss: 5.5171, Val Loss: 5.3261\n",
      "Epoch [122/500], Train Loss: 5.5128, Val Loss: 5.3220\n",
      "Epoch [123/500], Train Loss: 5.5099, Val Loss: 5.3189\n",
      "Epoch [124/500], Train Loss: 5.5065, Val Loss: 5.3192\n",
      "Epoch [125/500], Train Loss: 5.5028, Val Loss: 5.3144\n",
      "Epoch [126/500], Train Loss: 5.5001, Val Loss: 5.3205\n",
      "Epoch [127/500], Train Loss: 5.4965, Val Loss: 5.3127\n",
      "Epoch [128/500], Train Loss: 5.4926, Val Loss: 5.3124\n",
      "Epoch [129/500], Train Loss: 5.4897, Val Loss: 5.3084\n",
      "Epoch [130/500], Train Loss: 5.4869, Val Loss: 5.3079\n",
      "Epoch [131/500], Train Loss: 5.4840, Val Loss: 5.3004\n",
      "Epoch [132/500], Train Loss: 5.4816, Val Loss: 5.2980\n",
      "Epoch [133/500], Train Loss: 5.4790, Val Loss: 5.3056\n",
      "Epoch [134/500], Train Loss: 5.4765, Val Loss: 5.3031\n",
      "Epoch [135/500], Train Loss: 5.4743, Val Loss: 5.2925\n",
      "Epoch [136/500], Train Loss: 5.4720, Val Loss: 5.2937\n",
      "Epoch [137/500], Train Loss: 5.4687, Val Loss: 5.2893\n",
      "Epoch [138/500], Train Loss: 5.4671, Val Loss: 5.2898\n",
      "Epoch [139/500], Train Loss: 5.4653, Val Loss: 5.2891\n",
      "Epoch [140/500], Train Loss: 5.4638, Val Loss: 5.2926\n",
      "Epoch [141/500], Train Loss: 5.4613, Val Loss: 5.2962\n",
      "Epoch [142/500], Train Loss: 5.4600, Val Loss: 5.2891\n",
      "Epoch [143/500], Train Loss: 5.4580, Val Loss: 5.2947\n",
      "Epoch [144/500], Train Loss: 5.4562, Val Loss: 5.2833\n",
      "Epoch [145/500], Train Loss: 5.4548, Val Loss: 5.3003\n",
      "Epoch [146/500], Train Loss: 5.4531, Val Loss: 5.2886\n",
      "Epoch [147/500], Train Loss: 5.4518, Val Loss: 5.2820\n",
      "Epoch [148/500], Train Loss: 5.4505, Val Loss: 5.2859\n",
      "Epoch [149/500], Train Loss: 5.4494, Val Loss: 5.2854\n",
      "Epoch [150/500], Train Loss: 5.4482, Val Loss: 5.2814\n",
      "Epoch [151/500], Train Loss: 5.4470, Val Loss: 5.2881\n",
      "Epoch [152/500], Train Loss: 5.4455, Val Loss: 5.2816\n",
      "Epoch [153/500], Train Loss: 5.4441, Val Loss: 5.2797\n",
      "Epoch [154/500], Train Loss: 5.4432, Val Loss: 5.2851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [155/500], Train Loss: 5.4428, Val Loss: 5.2777\n",
      "Epoch [156/500], Train Loss: 5.4410, Val Loss: 5.2800\n",
      "Epoch [157/500], Train Loss: 5.4395, Val Loss: 5.2777\n",
      "Epoch [158/500], Train Loss: 5.4396, Val Loss: 5.2845\n",
      "Epoch [159/500], Train Loss: 5.4385, Val Loss: 5.2857\n",
      "Epoch [160/500], Train Loss: 5.4374, Val Loss: 5.2818\n",
      "Epoch [161/500], Train Loss: 5.4358, Val Loss: 5.2874\n",
      "Epoch [162/500], Train Loss: 5.4355, Val Loss: 5.2789\n",
      "Epoch [163/500], Train Loss: 5.4345, Val Loss: 5.2802\n",
      "Epoch [164/500], Train Loss: 5.4332, Val Loss: 5.2761\n",
      "Epoch [165/500], Train Loss: 5.4331, Val Loss: 5.2804\n",
      "Epoch [166/500], Train Loss: 5.4323, Val Loss: 5.2760\n",
      "Epoch [167/500], Train Loss: 5.4311, Val Loss: 5.2861\n",
      "Epoch [168/500], Train Loss: 5.4307, Val Loss: 5.2873\n",
      "Epoch [169/500], Train Loss: 5.4296, Val Loss: 5.2856\n",
      "Epoch [170/500], Train Loss: 5.4291, Val Loss: 5.2734\n",
      "Epoch [171/500], Train Loss: 5.4285, Val Loss: 5.2973\n",
      "Epoch [172/500], Train Loss: 5.4278, Val Loss: 5.2846\n",
      "Epoch [173/500], Train Loss: 5.4270, Val Loss: 5.2932\n",
      "Epoch [174/500], Train Loss: 5.4264, Val Loss: 5.2964\n",
      "Epoch [175/500], Train Loss: 5.4266, Val Loss: 5.3032\n",
      "Epoch [176/500], Train Loss: 5.4250, Val Loss: 5.2836\n",
      "Epoch [177/500], Train Loss: 5.4254, Val Loss: 5.2855\n",
      "Epoch [178/500], Train Loss: 5.4250, Val Loss: 5.2886\n",
      "Epoch [179/500], Train Loss: 5.4235, Val Loss: 5.3019\n",
      "Epoch [180/500], Train Loss: 5.4231, Val Loss: 5.2776\n",
      "⏳ Early stopping at epoch 180. No improvement for 10 epochs.\n",
      "✅ SAE training completed. Best model saved as sae_layer12_expanded.pth.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "train_sae(layer12_embeddings, model_name=\"sae_layer12_expanded.pth\", epochs=500,\n",
    "          batch_size=32, lr=1e-5, hidden_dim=1536, weight_decay=1e-04, \n",
    "          train_losses=[],val_losses=val_losses, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ce61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
