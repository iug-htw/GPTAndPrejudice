{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b225ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from clean_gutenberg_text import clean_gutenberg_text\n",
    "from train_sae import train_sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b0d461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3191ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.2,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd8e016",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_and_optimizer_5.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m GPTModel(GPT_CONFIG_124M)\n\u001b[1;32m----> 2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_and_optimizer_5.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_and_optimizer_5.pth'"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "checkpoint = torch.load(\"model_and_optimizer_5.pth\", weights_only=True)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb48a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_and_clean_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads a text file and splits it into sentences while cleaning the text.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the text file.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of cleaned sentences from the book.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = clean_gutenberg_text(file_path)\n",
    "\n",
    "    # Split text into sentences (simple heuristic using punctuation)\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "\n",
    "    # Remove very short or long sentences\n",
    "    sentences = [s.strip() for s in sentences if 5 < len(s.split()) < 50]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"original_texts/\"\n",
    "dataset = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        sentences = load_and_clean_text(os.path.join(directory, filename))\n",
    "        dataset += sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23472ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def get_token_embeddings(text, model, tokenizer, layers=[6, 12]):\n",
    "    \"\"\"\n",
    "    Extracts token embeddings from specified transformer layers.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "    - model: Custom GPT model.\n",
    "    - tokenizer: tiktoken encoding object.\n",
    "    - layers (list): Transformer layers to extract embeddings from.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Layer-wise token embeddings {layer_number: embeddings}\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = text_to_token_ids(text, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, hidden_states = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    embeddings = {}\n",
    "    for layer in layers:\n",
    "        if layer - 1 < len(hidden_states):\n",
    "            embeddings[layer] = hidden_states[layer - 1].squeeze(0).cpu().numpy()\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: Layer {layer} is out of range (max index {len(hidden_states) - 1})\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer6_embeddings = []\n",
    "layer12_embeddings = []\n",
    "\n",
    "for sentence in dataset:\n",
    "    embeddings = get_token_embeddings(sentence, model, tokenizer)\n",
    "    layer6_embeddings.append(embeddings[6])\n",
    "    layer12_embeddings.append(embeddings[12])\n",
    "\n",
    "# Convert to NumPy and flatten tokens into dataset\n",
    "layer6_embeddings = np.vstack(layer6_embeddings)\n",
    "layer12_embeddings = np.vstack(layer12_embeddings)\n",
    "\n",
    "os.makedirs(\"sae_data\", exist_ok=True)\n",
    "np.save(\"sae_data/layer6_embeddings.npy\", layer6_embeddings)\n",
    "np.save(\"sae_data/layer12_embeddings.npy\", layer12_embeddings)\n",
    "\n",
    "print(\"Saved token embeddings:\")\n",
    "print(f\"Layer 6: {layer6_embeddings.shape}\")\n",
    "print(f\"Layer 12: {layer12_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer6_embeddings = np.load(\"sae_data/layer6_embeddings.npy\")\n",
    "layer12_embeddings = np.load(\"sae_data/layer12_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae61ddeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariam/opt/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 2.3618, Val Loss: 2.2800\n",
      "Epoch [2/500], Train Loss: 2.2735, Val Loss: 2.2984\n",
      "Epoch [3/500], Train Loss: 2.2630, Val Loss: 2.2754\n",
      "Epoch [4/500], Train Loss: 2.2570, Val Loss: 2.2771\n",
      "Epoch [5/500], Train Loss: 2.2525, Val Loss: 2.2562\n",
      "Epoch [6/500], Train Loss: 2.2502, Val Loss: 2.2787\n",
      "Epoch [7/500], Train Loss: 2.2485, Val Loss: 2.2574\n",
      "Epoch [8/500], Train Loss: 2.2471, Val Loss: 2.2740\n",
      "Epoch [9/500], Train Loss: 2.2461, Val Loss: 2.2814\n",
      "Epoch [10/500], Train Loss: 2.2452, Val Loss: 2.2925\n",
      "Epoch [11/500], Train Loss: 2.2444, Val Loss: 2.2744\n",
      "Epoch [12/500], Train Loss: 2.2260, Val Loss: 2.2265\n",
      "Epoch [13/500], Train Loss: 2.2237, Val Loss: 2.2189\n",
      "Epoch [14/500], Train Loss: 2.2233, Val Loss: 2.2273\n",
      "Epoch [15/500], Train Loss: 2.2230, Val Loss: 2.2203\n",
      "Epoch [16/500], Train Loss: 2.2226, Val Loss: 2.2392\n",
      "Epoch [17/500], Train Loss: 2.2220, Val Loss: 2.2464\n",
      "Epoch [18/500], Train Loss: 2.2219, Val Loss: 2.2346\n",
      "Epoch [19/500], Train Loss: 2.2212, Val Loss: 2.2189\n",
      "Epoch [20/500], Train Loss: 2.2090, Val Loss: 2.2067\n",
      "Epoch [21/500], Train Loss: 2.2080, Val Loss: 2.2087\n",
      "Epoch [22/500], Train Loss: 2.2075, Val Loss: 2.1985\n",
      "Epoch [23/500], Train Loss: 2.2070, Val Loss: 2.1986\n",
      "Epoch [24/500], Train Loss: 2.2066, Val Loss: 2.1990\n",
      "Epoch [25/500], Train Loss: 2.2062, Val Loss: 2.2083\n",
      "Epoch [26/500], Train Loss: 2.2061, Val Loss: 2.2040\n",
      "Epoch [27/500], Train Loss: 2.2058, Val Loss: 2.1983\n",
      "Epoch [28/500], Train Loss: 2.2054, Val Loss: 2.1963\n",
      "Epoch [29/500], Train Loss: 2.2052, Val Loss: 2.1903\n",
      "Epoch [30/500], Train Loss: 2.2050, Val Loss: 2.1972\n",
      "Epoch [31/500], Train Loss: 2.2046, Val Loss: 2.1971\n",
      "Epoch [32/500], Train Loss: 2.2046, Val Loss: 2.2015\n",
      "Epoch [33/500], Train Loss: 2.2042, Val Loss: 2.2026\n",
      "Epoch [34/500], Train Loss: 2.2042, Val Loss: 2.2161\n",
      "Epoch [35/500], Train Loss: 2.2039, Val Loss: 2.1954\n",
      "Epoch [36/500], Train Loss: 2.1951, Val Loss: 2.1725\n",
      "Epoch [37/500], Train Loss: 2.1942, Val Loss: 2.1784\n",
      "Epoch [38/500], Train Loss: 2.1939, Val Loss: 2.1742\n",
      "Epoch [39/500], Train Loss: 2.1937, Val Loss: 2.1789\n",
      "Epoch [40/500], Train Loss: 2.1936, Val Loss: 2.1723\n",
      "Epoch [41/500], Train Loss: 2.1935, Val Loss: 2.1762\n",
      "Epoch [42/500], Train Loss: 2.1935, Val Loss: 2.1673\n",
      "Epoch [43/500], Train Loss: 2.1933, Val Loss: 2.1806\n",
      "Epoch [44/500], Train Loss: 2.1933, Val Loss: 2.1809\n",
      "Epoch [45/500], Train Loss: 2.1931, Val Loss: 2.1787\n",
      "Epoch [46/500], Train Loss: 2.1931, Val Loss: 2.1806\n",
      "Epoch [47/500], Train Loss: 2.1929, Val Loss: 2.1817\n",
      "Epoch [48/500], Train Loss: 2.1929, Val Loss: 2.1803\n",
      "Epoch [49/500], Train Loss: 2.1866, Val Loss: 2.1574\n",
      "Epoch [50/500], Train Loss: 2.1859, Val Loss: 2.1606\n",
      "Epoch [51/500], Train Loss: 2.1857, Val Loss: 2.1599\n",
      "Epoch [52/500], Train Loss: 2.1860, Val Loss: 2.1592\n",
      "Epoch [53/500], Train Loss: 2.1858, Val Loss: 2.1596\n",
      "Epoch [54/500], Train Loss: 2.1857, Val Loss: 2.1601\n",
      "Epoch [55/500], Train Loss: 2.1858, Val Loss: 2.1575\n",
      "Epoch [56/500], Train Loss: 2.1815, Val Loss: 2.1503\n",
      "Epoch [57/500], Train Loss: 2.1807, Val Loss: 2.1461\n",
      "Epoch [58/500], Train Loss: 2.1807, Val Loss: 2.1520\n",
      "Epoch [59/500], Train Loss: 2.1806, Val Loss: 2.1476\n",
      "Epoch [60/500], Train Loss: 2.1806, Val Loss: 2.1498\n",
      "Epoch [61/500], Train Loss: 2.1807, Val Loss: 2.1492\n",
      "Epoch [62/500], Train Loss: 2.1804, Val Loss: 2.1464\n",
      "Epoch [63/500], Train Loss: 2.1806, Val Loss: 2.1478\n",
      "Epoch [64/500], Train Loss: 2.1775, Val Loss: 2.1397\n",
      "Epoch [65/500], Train Loss: 2.1770, Val Loss: 2.1387\n",
      "Epoch [66/500], Train Loss: 2.1769, Val Loss: 2.1411\n",
      "Epoch [67/500], Train Loss: 2.1767, Val Loss: 2.1389\n",
      "Epoch [68/500], Train Loss: 2.1768, Val Loss: 2.1412\n",
      "Epoch [69/500], Train Loss: 2.1767, Val Loss: 2.1410\n",
      "Epoch [70/500], Train Loss: 2.1767, Val Loss: 2.1388\n",
      "Epoch [71/500], Train Loss: 2.1770, Val Loss: 2.1409\n",
      "Epoch [72/500], Train Loss: 2.1746, Val Loss: 2.1345\n",
      "Epoch [73/500], Train Loss: 2.1742, Val Loss: 2.1341\n",
      "Epoch [74/500], Train Loss: 2.1744, Val Loss: 2.1357\n",
      "Epoch [75/500], Train Loss: 2.1742, Val Loss: 2.1340\n",
      "Epoch [76/500], Train Loss: 2.1741, Val Loss: 2.1344\n",
      "Epoch [77/500], Train Loss: 2.1741, Val Loss: 2.1342\n",
      "Epoch [78/500], Train Loss: 2.1740, Val Loss: 2.1351\n",
      "Epoch [79/500], Train Loss: 2.1742, Val Loss: 2.1348\n",
      "Epoch [80/500], Train Loss: 2.1727, Val Loss: 2.1321\n",
      "Epoch [81/500], Train Loss: 2.1723, Val Loss: 2.1321\n",
      "Epoch [82/500], Train Loss: 2.1724, Val Loss: 2.1330\n",
      "Epoch [83/500], Train Loss: 2.1722, Val Loss: 2.1327\n",
      "Epoch [84/500], Train Loss: 2.1721, Val Loss: 2.1321\n",
      "Epoch [85/500], Train Loss: 2.1720, Val Loss: 2.1306\n",
      "Epoch [86/500], Train Loss: 2.1720, Val Loss: 2.1320\n",
      "Epoch [87/500], Train Loss: 2.1721, Val Loss: 2.1308\n",
      "Epoch [88/500], Train Loss: 2.1723, Val Loss: 2.1319\n",
      "Epoch [89/500], Train Loss: 2.1722, Val Loss: 2.1309\n",
      "Epoch [90/500], Train Loss: 2.1723, Val Loss: 2.1313\n",
      "Epoch [91/500], Train Loss: 2.1721, Val Loss: 2.1325\n",
      "Epoch [92/500], Train Loss: 2.1711, Val Loss: 2.1277\n",
      "Epoch [93/500], Train Loss: 2.1710, Val Loss: 2.1288\n",
      "Epoch [94/500], Train Loss: 2.1709, Val Loss: 2.1285\n",
      "Epoch [95/500], Train Loss: 2.1705, Val Loss: 2.1292\n",
      "Epoch [96/500], Train Loss: 2.1708, Val Loss: 2.1295\n",
      "Epoch [97/500], Train Loss: 2.1705, Val Loss: 2.1303\n",
      "Epoch [98/500], Train Loss: 2.1706, Val Loss: 2.1298\n",
      "Epoch [99/500], Train Loss: 2.1699, Val Loss: 2.1279\n",
      "Epoch [100/500], Train Loss: 2.1697, Val Loss: 2.1273\n",
      "Epoch [101/500], Train Loss: 2.1695, Val Loss: 2.1281\n",
      "Epoch [102/500], Train Loss: 2.1694, Val Loss: 2.1282\n",
      "Epoch [103/500], Train Loss: 2.1693, Val Loss: 2.1294\n",
      "Epoch [104/500], Train Loss: 2.1693, Val Loss: 2.1276\n",
      "Epoch [105/500], Train Loss: 2.1692, Val Loss: 2.1281\n",
      "Epoch [106/500], Train Loss: 2.1692, Val Loss: 2.1295\n",
      "Epoch [107/500], Train Loss: 2.1685, Val Loss: 2.1276\n",
      "Epoch [108/500], Train Loss: 2.1686, Val Loss: 2.1268\n",
      "Epoch [109/500], Train Loss: 2.1681, Val Loss: 2.1270\n",
      "Epoch [110/500], Train Loss: 2.1682, Val Loss: 2.1277\n",
      "Epoch [111/500], Train Loss: 2.1681, Val Loss: 2.1276\n",
      "Epoch [112/500], Train Loss: 2.1680, Val Loss: 2.1280\n",
      "Epoch [113/500], Train Loss: 2.1683, Val Loss: 2.1271\n",
      "Epoch [114/500], Train Loss: 2.1678, Val Loss: 2.1278\n",
      "Epoch [115/500], Train Loss: 2.1674, Val Loss: 2.1275\n",
      "Epoch [116/500], Train Loss: 2.1674, Val Loss: 2.1283\n",
      "Epoch [117/500], Train Loss: 2.1673, Val Loss: 2.1271\n",
      "Epoch [118/500], Train Loss: 2.1675, Val Loss: 2.1278\n",
      "⏳ Early stopping at epoch 118. No improvement for 10 epochs.\n",
      "✅ SAE training completed. Best model saved as sae_layer6_1.pth.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "train_sae(layer6_embeddings, \"sae_layer6_1.pth\", train_losses=[],\n",
    "          val_losses=val_losses, epochs=500, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be99e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 5.9004, Val Loss: 5.7176\n",
      "Epoch [2/500], Train Loss: 5.7059, Val Loss: 5.6882\n",
      "Epoch [3/500], Train Loss: 5.6882, Val Loss: 5.7546\n",
      "Epoch [4/500], Train Loss: 5.6805, Val Loss: 5.7202\n",
      "Epoch [5/500], Train Loss: 5.6752, Val Loss: 5.7325\n",
      "Epoch [6/500], Train Loss: 5.6718, Val Loss: 5.6864\n",
      "Epoch [7/500], Train Loss: 5.6699, Val Loss: 5.6657\n",
      "Epoch [8/500], Train Loss: 5.6674, Val Loss: 5.6849\n",
      "Epoch [9/500], Train Loss: 5.6654, Val Loss: 5.7040\n",
      "Epoch [10/500], Train Loss: 5.6632, Val Loss: 5.7012\n",
      "Epoch [11/500], Train Loss: 5.6616, Val Loss: 5.7231\n",
      "Epoch [12/500], Train Loss: 5.6603, Val Loss: 5.6998\n",
      "Epoch [13/500], Train Loss: 5.6603, Val Loss: 5.6749\n",
      "Epoch [14/500], Train Loss: 5.6296, Val Loss: 5.5976\n",
      "Epoch [15/500], Train Loss: 5.6270, Val Loss: 5.6285\n",
      "Epoch [16/500], Train Loss: 5.6266, Val Loss: 5.6242\n",
      "Epoch [17/500], Train Loss: 5.6256, Val Loss: 5.6264\n",
      "Epoch [18/500], Train Loss: 5.6253, Val Loss: 5.6167\n",
      "Epoch [19/500], Train Loss: 5.6253, Val Loss: 5.5838\n",
      "Epoch [20/500], Train Loss: 5.6247, Val Loss: 5.6130\n",
      "Epoch [21/500], Train Loss: 5.6255, Val Loss: 5.5968\n",
      "Epoch [22/500], Train Loss: 5.6257, Val Loss: 5.6001\n",
      "Epoch [23/500], Train Loss: 5.6245, Val Loss: 5.6243\n",
      "Epoch [24/500], Train Loss: 5.6245, Val Loss: 5.6153\n",
      "Epoch [25/500], Train Loss: 5.6240, Val Loss: 5.6048\n",
      "Epoch [26/500], Train Loss: 5.6044, Val Loss: 5.5500\n",
      "Epoch [27/500], Train Loss: 5.6028, Val Loss: 5.5353\n",
      "Epoch [28/500], Train Loss: 5.6017, Val Loss: 5.5751\n",
      "Epoch [29/500], Train Loss: 5.6013, Val Loss: 5.5356\n",
      "Epoch [30/500], Train Loss: 5.6027, Val Loss: 5.5579\n",
      "Epoch [31/500], Train Loss: 5.6021, Val Loss: 5.5591\n",
      "Epoch [32/500], Train Loss: 5.6025, Val Loss: 5.5636\n",
      "Epoch [33/500], Train Loss: 5.6017, Val Loss: 5.5495\n",
      "Epoch [34/500], Train Loss: 5.5871, Val Loss: 5.5226\n",
      "Epoch [35/500], Train Loss: 5.5866, Val Loss: 5.5079\n",
      "Epoch [36/500], Train Loss: 5.5859, Val Loss: 5.5056\n",
      "Epoch [37/500], Train Loss: 5.5863, Val Loss: 5.5129\n",
      "Epoch [38/500], Train Loss: 5.5860, Val Loss: 5.5148\n",
      "Epoch [39/500], Train Loss: 5.5859, Val Loss: 5.5074\n",
      "Epoch [40/500], Train Loss: 5.5865, Val Loss: 5.4980\n",
      "Epoch [41/500], Train Loss: 5.5862, Val Loss: 5.5171\n",
      "Epoch [42/500], Train Loss: 5.5857, Val Loss: 5.4974\n",
      "Epoch [43/500], Train Loss: 5.5857, Val Loss: 5.5141\n",
      "Epoch [44/500], Train Loss: 5.5858, Val Loss: 5.5050\n",
      "Epoch [45/500], Train Loss: 5.5853, Val Loss: 5.5134\n",
      "Epoch [46/500], Train Loss: 5.5852, Val Loss: 5.5144\n",
      "Epoch [47/500], Train Loss: 5.5853, Val Loss: 5.5020\n",
      "Epoch [48/500], Train Loss: 5.5859, Val Loss: 5.5148\n",
      "Epoch [49/500], Train Loss: 5.5762, Val Loss: 5.4837\n",
      "Epoch [50/500], Train Loss: 5.5749, Val Loss: 5.4792\n",
      "Epoch [51/500], Train Loss: 5.5744, Val Loss: 5.4973\n",
      "Epoch [52/500], Train Loss: 5.5741, Val Loss: 5.4798\n",
      "Epoch [53/500], Train Loss: 5.5752, Val Loss: 5.4902\n",
      "Epoch [54/500], Train Loss: 5.5744, Val Loss: 5.4761\n",
      "Epoch [55/500], Train Loss: 5.5746, Val Loss: 5.4833\n",
      "Epoch [56/500], Train Loss: 5.5744, Val Loss: 5.4901\n",
      "Epoch [57/500], Train Loss: 5.5748, Val Loss: 5.4802\n",
      "Epoch [58/500], Train Loss: 5.5740, Val Loss: 5.4770\n",
      "Epoch [59/500], Train Loss: 5.5741, Val Loss: 5.4846\n",
      "Epoch [60/500], Train Loss: 5.5745, Val Loss: 5.4942\n",
      "Epoch [61/500], Train Loss: 5.5673, Val Loss: 5.4654\n",
      "Epoch [62/500], Train Loss: 5.5668, Val Loss: 5.4658\n",
      "Epoch [63/500], Train Loss: 5.5662, Val Loss: 5.4652\n",
      "Epoch [64/500], Train Loss: 5.5663, Val Loss: 5.4714\n",
      "Epoch [65/500], Train Loss: 5.5662, Val Loss: 5.4666\n",
      "Epoch [66/500], Train Loss: 5.5661, Val Loss: 5.4634\n",
      "Epoch [67/500], Train Loss: 5.5656, Val Loss: 5.4634\n",
      "Epoch [68/500], Train Loss: 5.5659, Val Loss: 5.4682\n",
      "Epoch [69/500], Train Loss: 5.5665, Val Loss: 5.4623\n",
      "Epoch [70/500], Train Loss: 5.5660, Val Loss: 5.4601\n",
      "Epoch [71/500], Train Loss: 5.5660, Val Loss: 5.4634\n",
      "Epoch [72/500], Train Loss: 5.5661, Val Loss: 5.4604\n",
      "Epoch [73/500], Train Loss: 5.5661, Val Loss: 5.4644\n",
      "Epoch [74/500], Train Loss: 5.5654, Val Loss: 5.4639\n",
      "Epoch [75/500], Train Loss: 5.5667, Val Loss: 5.4630\n",
      "Epoch [76/500], Train Loss: 5.5656, Val Loss: 5.4657\n",
      "Epoch [77/500], Train Loss: 5.5612, Val Loss: 5.4539\n",
      "Epoch [78/500], Train Loss: 5.5608, Val Loss: 5.4525\n",
      "Epoch [79/500], Train Loss: 5.5596, Val Loss: 5.4524\n",
      "Epoch [80/500], Train Loss: 5.5598, Val Loss: 5.4543\n",
      "Epoch [81/500], Train Loss: 5.5597, Val Loss: 5.4553\n",
      "Epoch [82/500], Train Loss: 5.5595, Val Loss: 5.4523\n",
      "Epoch [83/500], Train Loss: 5.5603, Val Loss: 5.4519\n",
      "Epoch [84/500], Train Loss: 5.5600, Val Loss: 5.4606\n",
      "Epoch [85/500], Train Loss: 5.5601, Val Loss: 5.4509\n",
      "Epoch [86/500], Train Loss: 5.5599, Val Loss: 5.4472\n",
      "Epoch [87/500], Train Loss: 5.5608, Val Loss: 5.4569\n",
      "Epoch [88/500], Train Loss: 5.5594, Val Loss: 5.4523\n",
      "Epoch [89/500], Train Loss: 5.5603, Val Loss: 5.4534\n",
      "Epoch [90/500], Train Loss: 5.5599, Val Loss: 5.4507\n",
      "Epoch [91/500], Train Loss: 5.5598, Val Loss: 5.4501\n",
      "Epoch [92/500], Train Loss: 5.5597, Val Loss: 5.4520\n",
      "Epoch [93/500], Train Loss: 5.5572, Val Loss: 5.4470\n",
      "Epoch [94/500], Train Loss: 5.5558, Val Loss: 5.4439\n",
      "Epoch [95/500], Train Loss: 5.5555, Val Loss: 5.4453\n",
      "Epoch [96/500], Train Loss: 5.5561, Val Loss: 5.4463\n",
      "Epoch [97/500], Train Loss: 5.5562, Val Loss: 5.4434\n",
      "Epoch [98/500], Train Loss: 5.5556, Val Loss: 5.4443\n",
      "Epoch [99/500], Train Loss: 5.5554, Val Loss: 5.4430\n",
      "Epoch [100/500], Train Loss: 5.5562, Val Loss: 5.4458\n",
      "Epoch [101/500], Train Loss: 5.5554, Val Loss: 5.4415\n",
      "Epoch [102/500], Train Loss: 5.5554, Val Loss: 5.4490\n",
      "Epoch [103/500], Train Loss: 5.5557, Val Loss: 5.4462\n",
      "Epoch [104/500], Train Loss: 5.5550, Val Loss: 5.4395\n",
      "Epoch [105/500], Train Loss: 5.5552, Val Loss: 5.4470\n",
      "Epoch [106/500], Train Loss: 5.5552, Val Loss: 5.4424\n",
      "Epoch [107/500], Train Loss: 5.5555, Val Loss: 5.4443\n",
      "Epoch [108/500], Train Loss: 5.5549, Val Loss: 5.4440\n",
      "Epoch [109/500], Train Loss: 5.5550, Val Loss: 5.4383\n",
      "Epoch [110/500], Train Loss: 5.5556, Val Loss: 5.4445\n",
      "Epoch [111/500], Train Loss: 5.5551, Val Loss: 5.4443\n",
      "Epoch [112/500], Train Loss: 5.5557, Val Loss: 5.4446\n",
      "Epoch [113/500], Train Loss: 5.5551, Val Loss: 5.4443\n",
      "Epoch [114/500], Train Loss: 5.5551, Val Loss: 5.4443\n",
      "Epoch [115/500], Train Loss: 5.5553, Val Loss: 5.4452\n",
      "Epoch [116/500], Train Loss: 5.5530, Val Loss: 5.4357\n",
      "Epoch [117/500], Train Loss: 5.5525, Val Loss: 5.4397\n",
      "Epoch [118/500], Train Loss: 5.5525, Val Loss: 5.4405\n",
      "Epoch [119/500], Train Loss: 5.5523, Val Loss: 5.4402\n",
      "Epoch [120/500], Train Loss: 5.5525, Val Loss: 5.4363\n",
      "Epoch [121/500], Train Loss: 5.5530, Val Loss: 5.4512\n",
      "Epoch [122/500], Train Loss: 5.5520, Val Loss: 5.4346\n",
      "Epoch [123/500], Train Loss: 5.5523, Val Loss: 5.4381\n",
      "Epoch [124/500], Train Loss: 5.5522, Val Loss: 5.4368\n",
      "Epoch [125/500], Train Loss: 5.5529, Val Loss: 5.4387\n",
      "Epoch [126/500], Train Loss: 5.5523, Val Loss: 5.4384\n",
      "Epoch [127/500], Train Loss: 5.5522, Val Loss: 5.4387\n",
      "Epoch [128/500], Train Loss: 5.5522, Val Loss: 5.4377\n",
      "Epoch [129/500], Train Loss: 5.5510, Val Loss: 5.4365\n",
      "Epoch [130/500], Train Loss: 5.5502, Val Loss: 5.4368\n",
      "Epoch [131/500], Train Loss: 5.5504, Val Loss: 5.4375\n",
      "Epoch [132/500], Train Loss: 5.5499, Val Loss: 5.4363\n",
      "⏳ Early stopping at epoch 132. No improvement for 10 epochs.\n",
      "✅ SAE training completed. Best model saved as sae_layer12.pth.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "global train_losses, val_losses\n",
    "\n",
    "train_sae(layer12_embeddings, \"sae_layer12.pth\", train_losses=[],\n",
    "          val_losses=val_losses, epochs=500, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62df3de3-c92d-4399-9906-5675803c20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sparse_auto_encoder import SparseAutoencoder\n",
    "\n",
    "def objective(trial, embeddings_path):\n",
    "    # Hyperparameter search space\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 512, step=64)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load your embeddings\n",
    "    embeddings = np.load(embeddings_path)  # Replace with actual file\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "    input_dim = embeddings.shape[1]\n",
    "    sae = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(sae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    dataset = TensorDataset(embeddings)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience, early_stop_counter = 10, 0\n",
    "\n",
    "    for epoch in range(30):  # Tune for a fixed number of epochs\n",
    "        sae.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, encoded = sae(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            sparsity_loss = torch.norm(encoded, p=1) * 1e-4\n",
    "            total_loss = loss + sparsity_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += total_loss.item()\n",
    "\n",
    "        sae.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch[0].to(device)\n",
    "                outputs, encoded = sae(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                sparsity_loss = torch.norm(encoded, p=1) * 1e-4\n",
    "                total_loss = loss + sparsity_loss\n",
    "                val_loss += total_loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            break\n",
    "\n",
    "    return best_val_loss  # Optuna minimizes this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ad1e0d-2ec6-4e76-b20a-85949d7930d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 15:29:09,452] A new study created in memory with name: no-name-043ed9f3-7d38-41b2-9d5e-0003d269b796\n",
      "C:\\Users\\IuG_Lap1\\AppData\\Local\\Temp\\ipykernel_15332\\4231555641.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
      "C:\\Users\\IuG_Lap1\\AppData\\Local\\Temp\\ipykernel_15332\\4231555641.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
      "[W 2025-03-06 15:29:09,458] Trial 0 failed with parameters: {'batch_size': 32, 'lr': 0.005764291861533475, 'hidden_dim': 448, 'weight_decay': 5.804798232732968e-06} because of the following error: FileNotFoundError(2, 'No such file or directory').\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\IuG_Lap1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\IuG_Lap1\\AppData\\Local\\Temp\\ipykernel_15332\\2580150686.py\", line 3, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, embeddings_path=\"sae_data/layer6_embeddings.npy\"), n_trials=30)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\IuG_Lap1\\AppData\\Local\\Temp\\ipykernel_15332\\4231555641.py\", line 19, in objective\n",
      "    embeddings = np.load(embeddings_path)  # Replace with actual file\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\IuG_Lap1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py\", line 455, in load\n",
      "    fid = stack.enter_context(open(os.fspath(file), \"rb\"))\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sae_data/layer6_embeddings.npy'\n",
      "[W 2025-03-06 15:29:09,461] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sae_data/layer6_embeddings.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run hyperparameter tuning\u001b[39;00m\n\u001b[0;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msae_data/layer6_embeddings.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Print best hyperparameters\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run hyperparameter tuning\u001b[39;00m\n\u001b[0;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msae_data/layer6_embeddings.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Print best hyperparameters\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, embeddings_path)\u001b[0m\n\u001b[0;32m     16\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load your embeddings\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with actual file\u001b[39;00m\n\u001b[0;32m     20\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(embeddings, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:455\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    453\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 455\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    456\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sae_data/layer6_embeddings.npy'"
     ]
    }
   ],
   "source": [
    "# Run hyperparameter tuning\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda trial: objective(trial, embeddings_path=\"sae_data/layer6_embeddings.npy\"), n_trials=30)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1ea44-f87f-4dd1-979f-b283935c29b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
