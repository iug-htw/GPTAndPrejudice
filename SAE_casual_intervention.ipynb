{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d797b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from sparse_auto_encoder import SparseAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff9a97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.2,\n",
    "    \"qkv_bias\": True,\n",
    "    \"device\": \"cpu\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624e271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "checkpoint = torch.load(\"model_768_12_12_old_tok.pth\", weights_only=True, map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce8e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13960944",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_6 = SparseAutoencoder(input_dim=768, hidden_dim=3072).to(device)\n",
    "sae_6.load_state_dict(torch.load(\"sae_model_6_3072.pth\", map_location=torch.device('cpu')))\n",
    "sae_6.eval();\n",
    "\n",
    "sae_12 = SparseAutoencoder(input_dim=768, hidden_dim=3072).to(device)\n",
    "sae_12.load_state_dict(torch.load(\"sae_model_12_3072.pth\", map_location=torch.device('cpu')))\n",
    "sae_12.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0917a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276b734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_embeddings(text, model, tokenizer, layers=[6, 12]):\n",
    "    \"\"\"\n",
    "    Extracts token embeddings from specified transformer layers.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "    - model: Custom GPT model.\n",
    "    - tokenizer: tiktoken encoding object.\n",
    "    - layers (list): Transformer layers to extract embeddings from.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Layer-wise token embeddings {layer_number: embeddings}\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = text_to_token_ids(text, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, hidden_states = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    embeddings = {} \n",
    "    for layer in layers:\n",
    "        if layer - 1 < len(hidden_states):\n",
    "            embeddings[layer] = hidden_states[layer - 1].squeeze(0).cpu().numpy()\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: Layer {layer} is out of range (max index {len(hidden_states) - 1})\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94611fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_top_activating_neurons(concept_to_texts, model, tokenizer, sae, get_token_embeddings, layer=6, top_k=5, device='cpu'):\n",
    "    concept_top_neurons = {}\n",
    "\n",
    "    for concept, sentences in concept_to_texts.items():\n",
    "        print(f\"Processing concept: {concept}\")\n",
    "        neuron_activation_counts = defaultdict(int)\n",
    "        total_tokens = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            embeddings_np = get_token_embeddings(sentence, model, tokenizer, layers=[layer])[layer]\n",
    "            embeddings = torch.tensor(embeddings_np, dtype=torch.float32).to(device)\n",
    "\n",
    "            decoded, encoded = sae(embeddings)  # encoded shape: (seq_len, n_features)\n",
    "            top_neuron_indices = torch.argmax(encoded, dim=1).cpu().numpy()\n",
    "\n",
    "            for idx in top_neuron_indices:\n",
    "                neuron_activation_counts[idx] += 1\n",
    "\n",
    "            total_tokens += encoded.shape[0]\n",
    "\n",
    "        neuron_avg_activation = {k: v / total_tokens for k, v in neuron_activation_counts.items()}\n",
    "        top_neurons = sorted(neuron_avg_activation.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        concept_top_neurons[concept] = [neuron for neuron, _ in top_neurons]\n",
    "\n",
    "    return concept_top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b561dadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing concept: marriage_as_duty\n",
      "Processing concept: romantic_love\n",
      "Processing concept: social_class\n",
      "Processing concept: moral_superiority\n",
      "Processing concept: stigma_of_spinsterhood\n",
      "Processing concept: wealth_and_inheritance\n",
      "Processing concept: female_professions\n",
      "Processing concept: male_professions\n",
      "Processing concept: reputation_and_gossip\n",
      "Processing concept: truth_and_honesty\n",
      "Processing concept: vanity_and_appearance\n",
      "Processing concept: matchmaking_positive\n",
      "Processing concept: matchmaking_negative\n",
      "Processing concept: social_hierarchy\n",
      "{'marriage_as_duty': [720, 639, 443, 2899, 1996], 'romantic_love': [720, 1091, 639, 1313, 1228], 'social_class': [720, 1996, 1080, 639, 779], 'moral_superiority': [1080, 1091, 2899, 639, 443], 'stigma_of_spinsterhood': [1080, 639, 1313, 317, 811], 'wealth_and_inheritance': [720, 1313, 604, 1091, 2604], 'female_professions': [779, 1441, 720, 2604, 5], 'male_professions': [779, 1441, 720, 2755, 2604], 'reputation_and_gossip': [2755, 720, 1441, 1313, 443], 'truth_and_honesty': [720, 2755, 1091, 1313, 639], 'vanity_and_appearance': [720, 2755, 2248, 2604, 2268], 'matchmaking_positive': [720, 2755, 1441, 2604, 779], 'matchmaking_negative': [2755, 720, 1441, 1313, 1091], 'social_hierarchy': [720, 1441, 2755, 639, 1996]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"concepts_to_text.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    concept_to_texts = json.load(f)\n",
    "\n",
    "top_neurons = find_top_activating_neurons(\n",
    "    concept_to_texts=concept_to_texts,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    sae=sae_6,\n",
    "    get_token_embeddings=get_token_embeddings,\n",
    "    layer=6,\n",
    "    top_k=5,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "print(top_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0372a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = text_to_token_ids(\"Marriage is\", tokenizer).to(device)\n",
    "    \n",
    "for _ in range(10):\n",
    "    idx_cond = idx[:, -GPT_CONFIG_124M['context_length']:]\n",
    "\n",
    "    # 1. Run forward to get hidden state at layer 6\n",
    "    _, hiddens = model(idx_cond, output_hidden_states=True)\n",
    "    layer6_hidden = hiddens[5].detach().clone()\n",
    "\n",
    "    # 2. Inject neuron 720 with a high value\n",
    "    layer6_hidden[:, :, 720] += 5.0  # boost neuron 720 activation\n",
    "\n",
    "    # 3. Run from layer 6 onward using intervene_layer=6\n",
    "    logits = model(\n",
    "        idx_cond,\n",
    "        intervene_layer=6,\n",
    "        edited_hidden=layer6_hidden,\n",
    "        output_hidden_states=False\n",
    "    )\n",
    "    idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "    idx_next = idx_next[:, -1, :]\n",
    "\n",
    "    # Same as before: append sampled index to the running sequence\n",
    "    idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    ouput_text = token_ids_to_text(idx, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c88f9532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Marriage is a very good deal of the world. I am'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ouput_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2273c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Marriage is parallels\\nanthafield Parkynch Hallrietriet'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = text_to_token_ids(\"Marriage is\", tokenizer).to(device)\n",
    "boost_value = 5.0\n",
    "layer = 6\n",
    "    \n",
    "for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "        _, hidden_states = model(idx, output_hidden_states=True)\n",
    "        layer_hidden = hidden_states[layer - 1].squeeze(0)\n",
    "\n",
    "        intervened_hidden = sae_12.intervene_and_decode(layer_hidden, 1090, boost=5)\n",
    "\n",
    "#         encoded[:, 2899] += boost_value\n",
    "\n",
    "#         intervened_hidden = sae_6.decoder(encoded)\n",
    "#         intervened_hidden = intervened_hidden.unsqueeze(0)  # shape: (batch, seq_len, emb_dim)\n",
    "\n",
    "        logits = model(idx, intervene_layer=(layer - 1), edited_hidden=intervened_hidden.unsqueeze(0))\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "#         # Filter logits with top_k sampling\n",
    "#         top_logits, _ = torch.topk(logits, 50)\n",
    "#         min_val = top_logits[:, -1]\n",
    "#         logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "#         # temperature scaling\n",
    "#         logits = logits / 0.3\n",
    "\n",
    "#         # Apply softmax to get probabilities\n",
    "#         probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "#         # Sample from the distribution\n",
    "#         idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "        ouput_text = token_ids_to_text(idx, tokenizer)\n",
    "        \n",
    "ouput_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62b465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
