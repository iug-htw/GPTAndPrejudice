/home/cluster_home/simbeck/conda/envs/Cramming_example/bin/python
Python 3.13.2
Mon Mar 10 08:57:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:41:00.0 Off |                    0 |
| N/A   26C    P0             64W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
#############################################################
### starting Training with Rilke books on GPU ###
#############################################################
import GPTModel
import data_loader_v1
import generate
Using cuda device.
Training model with tiktoken tokenizer...
Traceback (most recent call last):
  File "/home/cluster_home/simbeck/RilkeLM/GPTAndPrejudice/2_pretrain.py", line 145, in <module>
    train(train_loader, val_loader, num_epochs=7,
    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          eval_iter=25, sample_text="Im Park ist",
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          checkpoint_path=f"models/model_and_optimizer_{tokenizer}v2.pth", tokenizer=tokenizer)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cluster_home/simbeck/RilkeLM/GPTAndPrejudice/2_pretrain.py", line 99, in train
    model = GPTModel(GPT_CONFIG_124M)
  File "/home/cluster_home/simbeck/RilkeLM/GPTAndPrejudice/gpt_model.py", line 15, in __init__
    *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])
      ~~~~~~~~~~~~~~~~^^^^^
  File "/home/cluster_home/simbeck/RilkeLM/GPTAndPrejudice/dummy_transformer_block.py", line 10, in __init__
    self.att = MultiHeadAttention(
               ~~~~~~~~~~~~~~~~~~^
        d_in=cfg["emb_dim"],
        ^^^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        dropout=cfg["drop_rate"],
        ^^^^^^^^^^^^^^^^^^^^^^^^^
        qkv_bias=cfg["qkv_bias"])
        ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cluster_home/simbeck/RilkeLM/GPTAndPrejudice/multi_head_attention.py", line 7, in __init__
    assert d_out % num_heads == 0, "d_out must be divisible by num_heads"
           ^^^^^^^^^^^^^^^^^^^^^^
AssertionError: d_out must be divisible by num_heads
