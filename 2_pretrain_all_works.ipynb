{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d339",
   "metadata": {},
   "source": [
    "### Detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d281a",
   "metadata": {},
   "source": [
    "### Set up model configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72561797-a3f0-4d84-9883-64c447482389",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 16000,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.2,       # Dropout rate\n",
    "    \"qkv_bias\": True,      # Query-Key-Value bias\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383eb92",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d1c16",
   "metadata": {},
   "source": [
    "#### GPT-2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75227c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6642b10",
   "metadata": {},
   "source": [
    "#### Custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b2a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2281b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='all_books.txt',\n",
    "    model_prefix='gpt_custom_tokenizer',\n",
    "    vocab_size=GPT_CONFIG_124M[\"vocab_size\"],\n",
    "    model_type='bpe',\n",
    "    character_coverage=0.9995,\n",
    "    hard_vocab_limit=False,\n",
    "    bos_id=-1,\n",
    "    eos_id=-1,\n",
    "    user_defined_symbols=[\"<|endoftext|>\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d8fac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.load('gpt_custom_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a88bc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_used_in_this_trial=\"GPT2\"\n",
    "tokenizer_used_in_this_trial=\"CUSTOM\"\n",
    "\n",
    "def encode(full_text):\n",
    "    if tokenizer_used_in_this_trial == \"GPT2\":\n",
    "        return tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})\n",
    "    else:\n",
    "        return tokenizer.encode(full_text, out_type=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fbf11",
   "metadata": {},
   "source": [
    "### Load training and validation data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b562de-efe1-40d2-a5ba-350b1edb7a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data_all_txt.txt'\n",
    "val_file_path = 'val_text_data_all_txt.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf608bf",
   "metadata": {},
   "source": [
    "### Initialize data loaders for training\n",
    "Data loaders implementation can be found in `./data_loader_v1.py`.\n",
    "\n",
    "This implementation follows the omplementation detailed in _Raschka, Sebastian. Build a Large Language Model (From Scratch). Manning Publications, 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bddf6dae-302d-4fc7-853b-2806a0c7d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f915a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 740186\n",
      "Tokens: 891494\n",
      "Unique Tokens Used: 13944\n"
     ]
    }
   ],
   "source": [
    "full_text = train_data + val_data\n",
    "\n",
    "word_count = len(full_text.split())\n",
    "\n",
    "# tiktoken tokenizer ->\n",
    "# tokens = tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "# Custom tokenizer ->\n",
    "tokens = tokenizer.encode(full_text, out_type=int)\n",
    "\n",
    "token_count = len(tokens)\n",
    "unique_token_count = len(set(tokens))\n",
    "\n",
    "print(\"Words:\", word_count)\n",
    "print(\"Tokens:\", token_count)\n",
    "print(\"Unique Tokens Used:\", unique_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f969edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean(): \n",
    "    \"\"\"\n",
    "    This is a function for GPU data claening before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    \n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.mps.empty_cache()  # Attempt to release MPS memory\n",
    "    \n",
    "    # Move tensors to CPU\n",
    "    for tensor in list(globals().values()):\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device == torch.device(\"mps\"):\n",
    "            tensor.to(\"cpu\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    del tensor\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "    print(\"Allocated Memory:\", torch.mps.current_allocated_memory() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da82d2c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f2c9bfd-5c57-4af6-98e8-5da47988d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train import train_model_simple\n",
    "import time\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\n",
    "def train(train_loader, val_loader,\n",
    "          num_epochs=10, eval_iter=5, lr=0.0002,\n",
    "          generate_sample_text=False,\n",
    "          sample_text=\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be\",\n",
    "          model_prefix=\"model_and_optimizer\"):\n",
    "\n",
    "    global train_losses, val_losses, track_tokens_seen  # Ensure these are updated globally\n",
    "\n",
    "    if device == \"mps\":\n",
    "        clean()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-08, weight_decay=0.05)\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=num_epochs, eval_iter=eval_iter,\n",
    "        start_context=sample_text, cfg=GPT_CONFIG_124M,\n",
    "        generate_sample_text=generate_sample_text,\n",
    "        model_prefix=model_prefix,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        print(50 * \"=\")\n",
    "        clean()\n",
    "    if device == \"cuda\":\n",
    "        print(50 * \"=\")\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d594966-9781-4ea2-9a68-01196f5111b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae6fc",
   "metadata": {},
   "source": [
    "### Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda45148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.075, Val loss 9.044\n",
      "Ep 1 (Step 000010): Train loss 7.224, Val loss 7.250\n",
      "Ep 1 (Step 000020): Train loss 6.591, Val loss 6.620\n",
      "Ep 1 (Step 000030): Train loss 6.388, Val loss 6.477\n",
      "Ep 1 (Step 000040): Train loss 6.237, Val loss 6.331\n",
      "Ep 1 (Step 000050): Train loss 6.111, Val loss 6.144\n",
      "Ep 1 (Step 000060): Train loss 5.943, Val loss 6.014\n",
      "Ep 1 (Step 000070): Train loss 5.824, Val loss 5.925\n",
      "Ep 1 (Step 000080): Train loss 5.764, Val loss 5.837\n",
      "Ep 1 (Step 000090): Train loss 5.733, Val loss 5.770\n",
      "Ep 1 (Step 000100): Train loss 5.637, Val loss 5.725\n",
      "Ep 1 (Step 000110): Train loss 5.557, Val loss 5.674\n",
      "Ep 1 (Step 000120): Train loss 5.635, Val loss 5.639\n",
      "Ep 1 (Step 000130): Train loss 5.573, Val loss 5.596\n",
      "Ep 1 (Step 000140): Train loss 5.433, Val loss 5.565\n",
      "Ep 1 (Step 000150): Train loss 5.429, Val loss 5.531\n",
      "Ep 1 (Step 000160): Train loss 5.406, Val loss 5.495\n",
      "Ep 1 (Step 000170): Train loss 5.411, Val loss 5.484\n",
      "Ep 1 (Step 000180): Train loss 5.428, Val loss 5.450\n",
      "Ep 1 (Step 000190): Train loss 5.437, Val loss 5.419\n",
      "Ep 1 (Step 000200): Train loss 5.341, Val loss 5.413\n",
      "Ep 1 (Step 000210): Train loss 5.322, Val loss 5.378\n",
      "Ep 1 (Step 000220): Train loss 5.284, Val loss 5.360\n",
      "Ep 1 (Step 000230): Train loss 5.322, Val loss 5.339\n",
      "Ep 1 (Step 000240): Train loss 5.237, Val loss 5.343\n",
      "Ep 1 (Step 000250): Train loss 5.240, Val loss 5.344\n",
      "Ep 1 (Step 000260): Train loss 5.218, Val loss 5.309\n",
      "Ep 1 (Step 000270): Train loss 5.285, Val loss 5.310\n",
      "Ep 1 (Step 000280): Train loss 5.315, Val loss 5.304\n",
      "Ep 1 (Step 000290): Train loss 5.200, Val loss 5.285\n",
      "Ep 1 (Step 000300): Train loss 5.187, Val loss 5.277\n",
      "Ep 1 (Step 000310): Train loss 5.181, Val loss 5.250\n",
      "Ep 1 (Step 000320): Train loss 5.237, Val loss 5.231\n",
      "Ep 1 (Step 000330): Train loss 5.160, Val loss 5.239\n",
      "Ep 1 (Step 000340): Train loss 5.182, Val loss 5.231\n",
      "Ep 1 (Step 000350): Train loss 5.201, Val loss 5.228\n",
      "Ep 1 (Step 000360): Train loss 5.141, Val loss 5.210\n",
      "Ep 1 (Step 000370): Train loss 5.182, Val loss 5.212\n",
      "Ep 1 (Step 000380): Train loss 5.163, Val loss 5.208\n",
      "Ep 1 (Step 000390): Train loss 5.092, Val loss 5.197\n",
      "Ep 1 (Step 000400): Train loss 5.169, Val loss 5.188\n",
      "Ep 1 (Step 000410): Train loss 5.100, Val loss 5.178\n",
      "Ep 1 (Step 000420): Train loss 4.995, Val loss 5.181\n",
      "Ep 1 (Step 000430): Train loss 5.115, Val loss 5.163\n",
      "Ep 1 (Step 000440): Train loss 5.134, Val loss 5.144\n",
      "Ep 1 (Step 000450): Train loss 5.126, Val loss 5.141\n",
      "Ep 1 (Step 000460): Train loss 5.089, Val loss 5.148\n",
      "Ep 1 (Step 000470): Train loss 5.115, Val loss 5.158\n",
      "Ep 1 (Step 000480): Train loss 5.034, Val loss 5.173\n",
      "Ep 1 (Step 000490): Train loss 5.043, Val loss 5.152\n",
      "Ep 1 (Step 000500): Train loss 5.103, Val loss 5.159\n",
      "Ep 1 (Step 000510): Train loss 5.057, Val loss 5.141\n",
      "Ep 1 (Step 000520): Train loss 5.161, Val loss 5.138\n",
      "Ep 1 (Step 000530): Train loss 5.022, Val loss 5.113\n",
      "Ep 1 (Step 000540): Train loss 5.040, Val loss 5.111\n",
      "Ep 1 (Step 000550): Train loss 5.067, Val loss 5.116\n",
      "Ep 1 (Step 000560): Train loss 5.002, Val loss 5.103\n",
      "Ep 1 (Step 000570): Train loss 5.007, Val loss 5.097\n",
      "Ep 1 (Step 000580): Train loss 4.987, Val loss 5.090\n",
      "Ep 1 (Step 000590): Train loss 5.008, Val loss 5.089\n",
      "Ep 1 (Step 000600): Train loss 4.986, Val loss 5.070\n",
      "Ep 1 (Step 000610): Train loss 5.055, Val loss 5.084\n",
      "Ep 1 (Step 000620): Train loss 4.975, Val loss 5.056\n",
      "Ep 1 (Step 000630): Train loss 4.890, Val loss 5.058\n",
      "Ep 1 (Step 000640): Train loss 4.935, Val loss 5.063\n",
      "Ep 1 (Step 000650): Train loss 4.970, Val loss 5.066\n",
      "Ep 1 (Step 000660): Train loss 4.917, Val loss 5.069\n",
      "Ep 1 (Step 000670): Train loss 5.002, Val loss 5.065\n",
      "Ep 1 (Step 000680): Train loss 4.923, Val loss 5.055\n",
      "Ep 1 (Step 000690): Train loss 4.898, Val loss 5.060\n",
      "Ep 1 (Step 000700): Train loss 5.023, Val loss 5.046\n",
      "Ep 1 (Step 000710): Train loss 4.950, Val loss 5.045\n",
      "Ep 1 (Step 000720): Train loss 4.955, Val loss 5.035\n",
      "Ep 1 (Step 000730): Train loss 4.928, Val loss 5.016\n",
      "Ep 1 (Step 000740): Train loss 4.938, Val loss 4.999\n",
      "Ep 1 (Step 000750): Train loss 4.900, Val loss 5.009\n",
      "Ep 1 (Step 000760): Train loss 4.935, Val loss 5.015\n",
      "Ep 1 (Step 000770): Train loss 4.870, Val loss 5.009\n",
      "Ep 1 (Step 000780): Train loss 4.956, Val loss 5.006\n",
      "Ep 2 (Step 000790): Train loss 4.989, Val loss 5.013\n",
      "Ep 2 (Step 000800): Train loss 4.898, Val loss 4.995\n",
      "Ep 2 (Step 000810): Train loss 4.871, Val loss 4.998\n",
      "Ep 2 (Step 000820): Train loss 4.929, Val loss 5.002\n",
      "Ep 2 (Step 000830): Train loss 4.929, Val loss 4.998\n",
      "Ep 2 (Step 000840): Train loss 4.908, Val loss 4.983\n",
      "Ep 2 (Step 000850): Train loss 4.923, Val loss 4.976\n",
      "Ep 2 (Step 000860): Train loss 4.802, Val loss 4.982\n",
      "Ep 2 (Step 000870): Train loss 4.815, Val loss 4.977\n",
      "Ep 2 (Step 000880): Train loss 4.896, Val loss 4.995\n",
      "Ep 2 (Step 000890): Train loss 4.862, Val loss 4.976\n",
      "Ep 2 (Step 000900): Train loss 4.908, Val loss 4.976\n",
      "Ep 2 (Step 000910): Train loss 4.805, Val loss 4.960\n",
      "Ep 2 (Step 000920): Train loss 4.727, Val loss 4.942\n",
      "Ep 2 (Step 000930): Train loss 4.733, Val loss 4.944\n",
      "Ep 2 (Step 000940): Train loss 4.832, Val loss 4.945\n",
      "Ep 2 (Step 000950): Train loss 4.810, Val loss 4.923\n",
      "Ep 2 (Step 000960): Train loss 4.801, Val loss 4.934\n",
      "Ep 2 (Step 000970): Train loss 4.773, Val loss 4.953\n",
      "Ep 2 (Step 000980): Train loss 4.828, Val loss 4.946\n",
      "Ep 2 (Step 000990): Train loss 4.885, Val loss 4.936\n",
      "Ep 2 (Step 001000): Train loss 4.832, Val loss 4.941\n",
      "Ep 2 (Step 001010): Train loss 4.811, Val loss 4.923\n",
      "Ep 2 (Step 001020): Train loss 4.774, Val loss 4.924\n",
      "Ep 2 (Step 001030): Train loss 4.748, Val loss 4.941\n",
      "Ep 2 (Step 001040): Train loss 4.729, Val loss 4.921\n",
      "Ep 2 (Step 001050): Train loss 4.854, Val loss 4.914\n",
      "Ep 2 (Step 001060): Train loss 4.835, Val loss 4.923\n",
      "Ep 2 (Step 001070): Train loss 4.747, Val loss 4.912\n",
      "Ep 2 (Step 001080): Train loss 4.766, Val loss 4.917\n",
      "Ep 2 (Step 001090): Train loss 4.798, Val loss 4.930\n",
      "Ep 2 (Step 001100): Train loss 4.810, Val loss 4.920\n",
      "Ep 2 (Step 001110): Train loss 4.726, Val loss 4.919\n",
      "Ep 2 (Step 001120): Train loss 4.720, Val loss 4.920\n",
      "Ep 2 (Step 001130): Train loss 4.734, Val loss 4.929\n",
      "Ep 2 (Step 001140): Train loss 4.705, Val loss 4.915\n",
      "Ep 2 (Step 001150): Train loss 4.738, Val loss 4.919\n",
      "Ep 2 (Step 001160): Train loss 4.761, Val loss 4.893\n",
      "Ep 2 (Step 001170): Train loss 4.678, Val loss 4.904\n",
      "Ep 2 (Step 001180): Train loss 4.683, Val loss 4.899\n",
      "Ep 2 (Step 001190): Train loss 4.646, Val loss 4.891\n",
      "Ep 2 (Step 001200): Train loss 4.777, Val loss 4.902\n",
      "Ep 2 (Step 001210): Train loss 4.653, Val loss 4.885\n",
      "Ep 2 (Step 001220): Train loss 4.653, Val loss 4.867\n",
      "Ep 2 (Step 001230): Train loss 4.674, Val loss 4.872\n",
      "Ep 2 (Step 001240): Train loss 4.701, Val loss 4.870\n",
      "Ep 2 (Step 001250): Train loss 4.681, Val loss 4.866\n",
      "Ep 2 (Step 001260): Train loss 4.701, Val loss 4.875\n",
      "Ep 2 (Step 001270): Train loss 4.795, Val loss 4.888\n",
      "Ep 2 (Step 001280): Train loss 4.702, Val loss 4.874\n",
      "Ep 2 (Step 001290): Train loss 4.675, Val loss 4.872\n",
      "Ep 2 (Step 001300): Train loss 4.613, Val loss 4.869\n",
      "Ep 2 (Step 001310): Train loss 4.699, Val loss 4.866\n",
      "Ep 2 (Step 001320): Train loss 4.730, Val loss 4.866\n",
      "Ep 2 (Step 001330): Train loss 4.681, Val loss 4.859\n",
      "Ep 2 (Step 001340): Train loss 4.578, Val loss 4.856\n",
      "Ep 2 (Step 001350): Train loss 4.729, Val loss 4.850\n",
      "Ep 2 (Step 001360): Train loss 4.696, Val loss 4.854\n",
      "Ep 2 (Step 001370): Train loss 4.640, Val loss 4.841\n",
      "Ep 2 (Step 001380): Train loss 4.618, Val loss 4.828\n",
      "Ep 2 (Step 001390): Train loss 4.549, Val loss 4.822\n",
      "Ep 2 (Step 001400): Train loss 4.645, Val loss 4.840\n",
      "Ep 2 (Step 001410): Train loss 4.606, Val loss 4.839\n",
      "Ep 2 (Step 001420): Train loss 4.606, Val loss 4.834\n",
      "Ep 2 (Step 001430): Train loss 4.585, Val loss 4.821\n",
      "Ep 2 (Step 001440): Train loss 4.671, Val loss 4.828\n",
      "Ep 2 (Step 001450): Train loss 4.579, Val loss 4.832\n",
      "Ep 2 (Step 001460): Train loss 4.580, Val loss 4.810\n",
      "Ep 2 (Step 001470): Train loss 4.689, Val loss 4.806\n",
      "Ep 2 (Step 001480): Train loss 4.607, Val loss 4.808\n",
      "Ep 2 (Step 001490): Train loss 4.610, Val loss 4.810\n",
      "Ep 2 (Step 001500): Train loss 4.576, Val loss 4.795\n",
      "Ep 2 (Step 001510): Train loss 4.576, Val loss 4.782\n",
      "Ep 2 (Step 001520): Train loss 4.625, Val loss 4.784\n",
      "Ep 2 (Step 001530): Train loss 4.677, Val loss 4.773\n",
      "Ep 2 (Step 001540): Train loss 4.634, Val loss 4.772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2 (Step 001550): Train loss 4.604, Val loss 4.767\n",
      "Ep 2 (Step 001560): Train loss 4.653, Val loss 4.783\n",
      "Ep 3 (Step 001570): Train loss 4.536, Val loss 4.772\n",
      "Ep 3 (Step 001580): Train loss 4.589, Val loss 4.784\n",
      "Ep 3 (Step 001590): Train loss 4.592, Val loss 4.776\n",
      "Ep 3 (Step 001600): Train loss 4.612, Val loss 4.761\n",
      "Ep 3 (Step 001610): Train loss 4.515, Val loss 4.773\n",
      "Ep 3 (Step 001620): Train loss 4.599, Val loss 4.757\n",
      "Ep 3 (Step 001630): Train loss 4.552, Val loss 4.757\n",
      "Ep 3 (Step 001640): Train loss 4.616, Val loss 4.763\n",
      "Ep 3 (Step 001650): Train loss 4.558, Val loss 4.777\n",
      "Ep 3 (Step 001660): Train loss 4.560, Val loss 4.761\n",
      "Ep 3 (Step 001670): Train loss 4.557, Val loss 4.762\n",
      "Ep 3 (Step 001680): Train loss 4.645, Val loss 4.753\n",
      "Ep 3 (Step 001690): Train loss 4.505, Val loss 4.741\n",
      "Ep 3 (Step 001700): Train loss 4.547, Val loss 4.754\n",
      "Ep 3 (Step 001710): Train loss 4.574, Val loss 4.757\n",
      "Ep 3 (Step 001720): Train loss 4.502, Val loss 4.750\n",
      "Ep 3 (Step 001730): Train loss 4.505, Val loss 4.728\n",
      "Ep 3 (Step 001740): Train loss 4.451, Val loss 4.751\n",
      "Ep 3 (Step 001750): Train loss 4.405, Val loss 4.756\n",
      "Ep 3 (Step 001760): Train loss 4.442, Val loss 4.760\n",
      "Ep 3 (Step 001770): Train loss 4.451, Val loss 4.741\n",
      "Ep 3 (Step 001780): Train loss 4.542, Val loss 4.750\n",
      "Ep 3 (Step 001790): Train loss 4.411, Val loss 4.740\n",
      "Ep 3 (Step 001800): Train loss 4.481, Val loss 4.736\n",
      "Ep 3 (Step 001810): Train loss 4.420, Val loss 4.738\n",
      "Ep 3 (Step 001820): Train loss 4.432, Val loss 4.725\n",
      "Ep 3 (Step 001830): Train loss 4.436, Val loss 4.749\n",
      "Ep 3 (Step 001840): Train loss 4.393, Val loss 4.730\n",
      "Ep 3 (Step 001850): Train loss 4.483, Val loss 4.728\n",
      "Ep 3 (Step 001860): Train loss 4.458, Val loss 4.743\n",
      "Ep 3 (Step 001870): Train loss 4.340, Val loss 4.740\n",
      "Ep 3 (Step 001880): Train loss 4.400, Val loss 4.716\n",
      "Ep 3 (Step 001890): Train loss 4.543, Val loss 4.732\n",
      "Ep 3 (Step 001900): Train loss 4.423, Val loss 4.712\n",
      "Ep 3 (Step 001910): Train loss 4.496, Val loss 4.714\n",
      "Ep 3 (Step 001920): Train loss 4.436, Val loss 4.707\n",
      "Ep 3 (Step 001930): Train loss 4.494, Val loss 4.707\n",
      "Ep 3 (Step 001940): Train loss 4.406, Val loss 4.718\n",
      "Ep 3 (Step 001950): Train loss 4.401, Val loss 4.713\n",
      "Ep 3 (Step 001960): Train loss 4.426, Val loss 4.717\n",
      "Ep 3 (Step 001970): Train loss 4.444, Val loss 4.707\n",
      "Ep 3 (Step 001980): Train loss 4.489, Val loss 4.701\n",
      "Ep 3 (Step 001990): Train loss 4.378, Val loss 4.708\n",
      "Ep 3 (Step 002000): Train loss 4.489, Val loss 4.710\n",
      "Ep 3 (Step 002010): Train loss 4.348, Val loss 4.688\n",
      "Ep 3 (Step 002020): Train loss 4.414, Val loss 4.686\n",
      "Ep 3 (Step 002030): Train loss 4.357, Val loss 4.693\n",
      "Ep 3 (Step 002040): Train loss 4.500, Val loss 4.696\n",
      "Ep 3 (Step 002050): Train loss 4.355, Val loss 4.703\n"
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=10,\n",
    "      eval_iter=10, model_prefix=\"model_768_12_12\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8539769d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.484, Val loss 10.461\n",
      "Ep 1 (Step 000010): Train loss 8.235, Val loss 8.203\n",
      "Ep 1 (Step 000020): Train loss 7.011, Val loss 6.983\n",
      "Ep 1 (Step 000030): Train loss 6.659, Val loss 6.617\n",
      "Ep 1 (Step 000040): Train loss 6.569, Val loss 6.562\n",
      "Ep 1 (Step 000050): Train loss 6.469, Val loss 6.395\n",
      "Ep 1 (Step 000060): Train loss 6.120, Val loss 6.250\n",
      "Ep 1 (Step 000070): Train loss 5.919, Val loss 6.099\n",
      "Ep 1 (Step 000080): Train loss 5.901, Val loss 6.016\n",
      "Ep 1 (Step 000090): Train loss 5.774, Val loss 5.884\n",
      "Ep 1 (Step 000100): Train loss 5.647, Val loss 5.786\n",
      "Ep 1 (Step 000110): Train loss 5.595, Val loss 5.718\n",
      "Ep 1 (Step 000120): Train loss 5.488, Val loss 5.658\n",
      "Ep 1 (Step 000130): Train loss 5.502, Val loss 5.618\n",
      "Ep 1 (Step 000140): Train loss 5.392, Val loss 5.549\n",
      "Ep 1 (Step 000150): Train loss 5.318, Val loss 5.516\n",
      "Ep 1 (Step 000160): Train loss 5.285, Val loss 5.481\n",
      "Ep 1 (Step 000170): Train loss 5.227, Val loss 5.443\n",
      "Ep 1 (Step 000180): Train loss 5.306, Val loss 5.396\n",
      "Ep 1 (Step 000190): Train loss 5.247, Val loss 5.377\n",
      "Ep 1 (Step 000200): Train loss 5.217, Val loss 5.341\n",
      "Ep 1 (Step 000210): Train loss 5.176, Val loss 5.298\n",
      "Ep 1 (Step 000220): Train loss 5.142, Val loss 5.294\n",
      "Ep 1 (Step 000230): Train loss 5.034, Val loss 5.271\n",
      "Ep 1 (Step 000240): Train loss 4.985, Val loss 5.246\n",
      "Ep 1 (Step 000250): Train loss 5.101, Val loss 5.250\n",
      "Ep 1 (Step 000260): Train loss 5.071, Val loss 5.218\n",
      "Ep 1 (Step 000270): Train loss 4.969, Val loss 5.199\n",
      "Ep 1 (Step 000280): Train loss 4.883, Val loss 5.187\n",
      "Ep 1 (Step 000290): Train loss 4.936, Val loss 5.175\n",
      "Ep 1 (Step 000300): Train loss 4.807, Val loss 5.161\n",
      "Ep 1 (Step 000310): Train loss 5.005, Val loss 5.135\n",
      "Ep 1 (Step 000320): Train loss 4.994, Val loss 5.121\n",
      "Ep 1 (Step 000330): Train loss 4.881, Val loss 5.115\n",
      "Ep 1 (Step 000340): Train loss 4.900, Val loss 5.090\n",
      "Ep 1 (Step 000350): Train loss 4.848, Val loss 5.086\n",
      "Ep 1 (Step 000360): Train loss 4.834, Val loss 5.077\n",
      "Ep 2 (Step 000370): Train loss 4.873, Val loss 5.068\n",
      "Ep 2 (Step 000380): Train loss 4.761, Val loss 5.057\n",
      "Ep 2 (Step 000390): Train loss 4.721, Val loss 5.048\n",
      "Ep 2 (Step 000400): Train loss 4.733, Val loss 5.047\n",
      "Ep 2 (Step 000410): Train loss 4.797, Val loss 5.034\n",
      "Ep 2 (Step 000420): Train loss 4.706, Val loss 5.028\n",
      "Ep 2 (Step 000430): Train loss 4.739, Val loss 5.026\n",
      "Ep 2 (Step 000440): Train loss 4.692, Val loss 5.009\n",
      "Ep 2 (Step 000450): Train loss 4.779, Val loss 5.010\n",
      "Ep 2 (Step 000460): Train loss 4.692, Val loss 4.986\n",
      "Ep 2 (Step 000470): Train loss 4.721, Val loss 5.002\n",
      "Ep 2 (Step 000480): Train loss 4.804, Val loss 4.990\n",
      "Ep 2 (Step 000490): Train loss 4.616, Val loss 4.969\n",
      "Ep 2 (Step 000500): Train loss 4.645, Val loss 4.963\n",
      "Ep 2 (Step 000510): Train loss 4.606, Val loss 4.966\n",
      "Ep 2 (Step 000520): Train loss 4.591, Val loss 4.967\n",
      "Ep 2 (Step 000530): Train loss 4.634, Val loss 4.952\n",
      "Ep 2 (Step 000540): Train loss 4.548, Val loss 4.938\n",
      "Ep 2 (Step 000550): Train loss 4.511, Val loss 4.943\n",
      "Ep 2 (Step 000560): Train loss 4.516, Val loss 4.937\n",
      "Ep 2 (Step 000570): Train loss 4.621, Val loss 4.916\n",
      "Ep 2 (Step 000580): Train loss 4.530, Val loss 4.927\n",
      "Ep 2 (Step 000590): Train loss 4.500, Val loss 4.934\n",
      "Ep 2 (Step 000600): Train loss 4.532, Val loss 4.936\n",
      "Ep 2 (Step 000610): Train loss 4.565, Val loss 4.917\n",
      "Ep 2 (Step 000620): Train loss 4.582, Val loss 4.921\n",
      "Ep 2 (Step 000630): Train loss 4.416, Val loss 4.910\n",
      "Ep 2 (Step 000640): Train loss 4.507, Val loss 4.908\n",
      "Ep 2 (Step 000650): Train loss 4.495, Val loss 4.882\n",
      "Ep 2 (Step 000660): Train loss 4.565, Val loss 4.882\n",
      "Ep 2 (Step 000670): Train loss 4.466, Val loss 4.881\n",
      "Ep 2 (Step 000680): Train loss 4.459, Val loss 4.863\n",
      "Ep 2 (Step 000690): Train loss 4.456, Val loss 4.867\n",
      "Ep 2 (Step 000700): Train loss 4.483, Val loss 4.864\n",
      "Ep 2 (Step 000710): Train loss 4.411, Val loss 4.859\n",
      "Ep 2 (Step 000720): Train loss 4.316, Val loss 4.867\n",
      "Ep 3 (Step 000730): Train loss 4.358, Val loss 4.836\n",
      "Ep 3 (Step 000740): Train loss 4.385, Val loss 4.843\n",
      "Ep 3 (Step 000750): Train loss 4.416, Val loss 4.839\n",
      "Ep 3 (Step 000760): Train loss 4.240, Val loss 4.856\n",
      "Ep 3 (Step 000770): Train loss 4.377, Val loss 4.852\n",
      "Ep 3 (Step 000780): Train loss 4.465, Val loss 4.862\n",
      "Ep 3 (Step 000790): Train loss 4.390, Val loss 4.840\n",
      "Ep 3 (Step 000800): Train loss 4.274, Val loss 4.820\n",
      "Ep 3 (Step 000810): Train loss 4.292, Val loss 4.827\n",
      "Ep 3 (Step 000820): Train loss 4.381, Val loss 4.808\n",
      "Ep 3 (Step 000830): Train loss 4.316, Val loss 4.809\n",
      "Ep 3 (Step 000840): Train loss 4.357, Val loss 4.803\n",
      "Ep 3 (Step 000850): Train loss 4.401, Val loss 4.795\n",
      "Ep 3 (Step 000860): Train loss 4.158, Val loss 4.806\n",
      "Ep 3 (Step 000870): Train loss 4.228, Val loss 4.798\n",
      "Ep 3 (Step 000880): Train loss 4.387, Val loss 4.809\n",
      "Ep 3 (Step 000890): Train loss 4.297, Val loss 4.804\n",
      "Ep 3 (Step 000900): Train loss 4.235, Val loss 4.795\n",
      "Ep 3 (Step 000910): Train loss 4.404, Val loss 4.796\n",
      "Ep 3 (Step 000920): Train loss 4.248, Val loss 4.784\n",
      "Ep 3 (Step 000930): Train loss 4.329, Val loss 4.782\n",
      "Ep 3 (Step 000940): Train loss 4.209, Val loss 4.794\n",
      "Ep 3 (Step 000950): Train loss 4.237, Val loss 4.788\n",
      "Ep 3 (Step 000960): Train loss 4.255, Val loss 4.780\n",
      "Ep 3 (Step 000970): Train loss 4.320, Val loss 4.776\n",
      "Ep 3 (Step 000980): Train loss 4.135, Val loss 4.765\n",
      "Ep 3 (Step 000990): Train loss 4.280, Val loss 4.766\n",
      "Ep 3 (Step 001000): Train loss 4.206, Val loss 4.777\n",
      "Ep 3 (Step 001010): Train loss 4.135, Val loss 4.747\n",
      "Ep 3 (Step 001020): Train loss 4.227, Val loss 4.757\n",
      "Ep 3 (Step 001030): Train loss 4.136, Val loss 4.759\n",
      "Ep 3 (Step 001040): Train loss 4.173, Val loss 4.750\n",
      "Ep 3 (Step 001050): Train loss 4.167, Val loss 4.742\n",
      "Ep 3 (Step 001060): Train loss 4.228, Val loss 4.743\n",
      "Ep 3 (Step 001070): Train loss 4.194, Val loss 4.744\n",
      "Ep 3 (Step 001080): Train loss 4.149, Val loss 4.760\n",
      "Ep 3 (Step 001090): Train loss 4.108, Val loss 4.735\n",
      "Ep 4 (Step 001100): Train loss 4.138, Val loss 4.731\n",
      "Ep 4 (Step 001110): Train loss 4.160, Val loss 4.748\n",
      "Ep 4 (Step 001120): Train loss 4.109, Val loss 4.755\n",
      "Ep 4 (Step 001130): Train loss 4.184, Val loss 4.750\n",
      "Ep 4 (Step 001140): Train loss 4.127, Val loss 4.749\n",
      "Ep 4 (Step 001150): Train loss 4.147, Val loss 4.741\n",
      "Ep 4 (Step 001160): Train loss 4.093, Val loss 4.743\n",
      "Ep 4 (Step 001170): Train loss 4.147, Val loss 4.736\n",
      "Ep 4 (Step 001180): Train loss 4.098, Val loss 4.733\n",
      "Ep 4 (Step 001190): Train loss 4.039, Val loss 4.722\n",
      "Ep 4 (Step 001200): Train loss 4.071, Val loss 4.731\n",
      "Ep 4 (Step 001210): Train loss 3.924, Val loss 4.727\n",
      "Ep 4 (Step 001220): Train loss 4.106, Val loss 4.729\n",
      "Ep 4 (Step 001230): Train loss 4.132, Val loss 4.733\n",
      "Ep 4 (Step 001240): Train loss 3.987, Val loss 4.712\n",
      "Ep 4 (Step 001250): Train loss 4.012, Val loss 4.711\n",
      "Ep 4 (Step 001260): Train loss 4.097, Val loss 4.726\n",
      "Ep 4 (Step 001270): Train loss 4.011, Val loss 4.725\n",
      "Ep 4 (Step 001280): Train loss 4.021, Val loss 4.725\n",
      "Ep 4 (Step 001290): Train loss 4.038, Val loss 4.718\n",
      "Ep 4 (Step 001300): Train loss 4.047, Val loss 4.725\n",
      "Ep 4 (Step 001310): Train loss 3.957, Val loss 4.713\n",
      "Ep 4 (Step 001320): Train loss 3.999, Val loss 4.714\n",
      "Ep 4 (Step 001330): Train loss 3.906, Val loss 4.710\n",
      "Ep 4 (Step 001340): Train loss 4.015, Val loss 4.699\n",
      "Ep 4 (Step 001350): Train loss 3.994, Val loss 4.707\n",
      "Ep 4 (Step 001360): Train loss 3.981, Val loss 4.694\n",
      "Ep 4 (Step 001370): Train loss 3.921, Val loss 4.701\n",
      "Ep 4 (Step 001380): Train loss 3.939, Val loss 4.691\n",
      "Ep 4 (Step 001390): Train loss 3.924, Val loss 4.700\n",
      "Ep 4 (Step 001400): Train loss 3.878, Val loss 4.691\n",
      "Ep 4 (Step 001410): Train loss 3.927, Val loss 4.691\n",
      "Ep 4 (Step 001420): Train loss 3.957, Val loss 4.712\n",
      "Ep 4 (Step 001430): Train loss 3.901, Val loss 4.699\n",
      "Ep 4 (Step 001440): Train loss 3.913, Val loss 4.697\n",
      "Ep 4 (Step 001450): Train loss 3.846, Val loss 4.690\n",
      "Ep 5 (Step 001460): Train loss 3.930, Val loss 4.700\n",
      "Ep 5 (Step 001470): Train loss 3.898, Val loss 4.689\n",
      "Ep 5 (Step 001480): Train loss 3.831, Val loss 4.699\n",
      "Ep 5 (Step 001490): Train loss 3.866, Val loss 4.692\n",
      "Ep 5 (Step 001500): Train loss 3.867, Val loss 4.698\n",
      "Ep 5 (Step 001510): Train loss 3.874, Val loss 4.698\n",
      "Ep 5 (Step 001520): Train loss 3.861, Val loss 4.699\n",
      "Ep 5 (Step 001530): Train loss 3.831, Val loss 4.697\n",
      "Ep 5 (Step 001540): Train loss 3.894, Val loss 4.701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5 (Step 001550): Train loss 3.853, Val loss 4.701\n",
      "Ep 5 (Step 001560): Train loss 3.830, Val loss 4.711\n",
      "Ep 5 (Step 001570): Train loss 3.903, Val loss 4.702\n",
      "Ep 5 (Step 001580): Train loss 3.810, Val loss 4.697\n",
      "Ep 5 (Step 001590): Train loss 3.739, Val loss 4.700\n",
      "Ep 5 (Step 001600): Train loss 3.831, Val loss 4.685\n",
      "Ep 5 (Step 001610): Train loss 3.745, Val loss 4.690\n",
      "Ep 5 (Step 001620): Train loss 3.849, Val loss 4.693\n",
      "Ep 5 (Step 001630): Train loss 3.725, Val loss 4.681\n",
      "Ep 5 (Step 001640): Train loss 3.857, Val loss 4.701\n",
      "Ep 5 (Step 001650): Train loss 3.746, Val loss 4.691\n",
      "Ep 5 (Step 001660): Train loss 3.781, Val loss 4.686\n",
      "Ep 5 (Step 001670): Train loss 3.831, Val loss 4.691\n",
      "Ep 5 (Step 001680): Train loss 3.774, Val loss 4.696\n",
      "Ep 5 (Step 001690): Train loss 3.775, Val loss 4.697\n",
      "Ep 5 (Step 001700): Train loss 3.673, Val loss 4.691\n",
      "Ep 5 (Step 001710): Train loss 3.683, Val loss 4.689\n",
      "Ep 5 (Step 001720): Train loss 3.720, Val loss 4.691\n",
      "Ep 5 (Step 001730): Train loss 3.750, Val loss 4.689\n",
      "Ep 5 (Step 001740): Train loss 3.762, Val loss 4.686\n",
      "Ep 5 (Step 001750): Train loss 3.750, Val loss 4.678\n",
      "Ep 5 (Step 001760): Train loss 3.736, Val loss 4.674\n",
      "Ep 5 (Step 001770): Train loss 3.704, Val loss 4.681\n",
      "Ep 5 (Step 001780): Train loss 3.678, Val loss 4.678\n",
      "Ep 5 (Step 001790): Train loss 3.712, Val loss 4.670\n",
      "Ep 5 (Step 001800): Train loss 3.636, Val loss 4.669\n",
      "Ep 5 (Step 001810): Train loss 3.692, Val loss 4.670\n",
      "Ep 5 (Step 001820): Train loss 3.640, Val loss 4.679\n",
      "Ep 6 (Step 001830): Train loss 3.597, Val loss 4.675\n",
      "Ep 6 (Step 001840): Train loss 3.683, Val loss 4.682\n",
      "Ep 6 (Step 001850): Train loss 3.626, Val loss 4.672\n",
      "Ep 6 (Step 001860): Train loss 3.574, Val loss 4.682\n",
      "Ep 6 (Step 001870): Train loss 3.657, Val loss 4.688\n",
      "Ep 6 (Step 001880): Train loss 3.600, Val loss 4.687\n",
      "Ep 6 (Step 001890): Train loss 3.646, Val loss 4.689\n",
      "Ep 6 (Step 001900): Train loss 3.659, Val loss 4.684\n",
      "Ep 6 (Step 001910): Train loss 3.547, Val loss 4.700\n",
      "Ep 6 (Step 001920): Train loss 3.611, Val loss 4.699\n",
      "Ep 6 (Step 001930): Train loss 3.547, Val loss 4.698\n",
      "Ep 6 (Step 001940): Train loss 3.617, Val loss 4.697\n",
      "Ep 6 (Step 001950): Train loss 3.590, Val loss 4.699\n",
      "Ep 6 (Step 001960): Train loss 3.649, Val loss 4.698\n",
      "Ep 6 (Step 001970): Train loss 3.649, Val loss 4.698\n",
      "Ep 6 (Step 001980): Train loss 3.632, Val loss 4.695\n",
      "Ep 6 (Step 001990): Train loss 3.543, Val loss 4.693\n",
      "Ep 6 (Step 002000): Train loss 3.641, Val loss 4.697\n",
      "Ep 6 (Step 002010): Train loss 3.528, Val loss 4.703\n",
      "Ep 6 (Step 002020): Train loss 3.499, Val loss 4.695\n",
      "Ep 6 (Step 002030): Train loss 3.605, Val loss 4.695\n",
      "Ep 6 (Step 002040): Train loss 3.497, Val loss 4.690\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_49791/679252246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model on all works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train(train_loader, val_loader, num_epochs=10,\n\u001b[0m\u001b[1;32m      4\u001b[0m       eval_iter=10, checkpoint_path=\"model_and_optimizer_best_old_tok.pth\");\n",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_49791/584259918.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, num_epochs, eval_iter, lr, generate_sample_text, sample_text, checkpoint_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Pass train_losses and val_losses as references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     train_model_simple(\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/Build GPT from scratch/pre_train.py\u001b[0m in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs, eval_iter, start_context, cfg, train_losses, val_losses, track_tokens_seen, generate_sample_text, checkpoint_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reset gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=10,\n",
    "      eval_iter=10, model_prefix=\"model_768_12_12_old_tok\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6651aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0002, weight_decay=0.05)\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer_best_old_tok.pth\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ecee23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from itertools import combinations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d523e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)  # Forward pass\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * target_ids.numel()\n",
    "            total_tokens += target_ids.numel()\n",
    "\n",
    "    perplexity = np.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "837f7534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.75698927127588"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adb0c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss Bennet has inherited the estate from her aunt, so she must have been rather.\n",
      "\"And I think you have no longer to be sure. My poor mother will be so very well known to be sure of Edward? And will not be able to tell you.\"\n",
      "\"Yes. You have given me in\n",
      "==================================================\n",
      "Mr. Darcy has inherited the estate from his aunt, so he must have no less well as he, though in his behaviour, and the country. Mr. Bennet was obliged to be a way by his behaviour to the room, as he was a very often wished to have happened; and Mr. Bennet,\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    prompt=\"Miss Bennet has inherited the estate from her aunt, so she must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model,  tokenizer=tokenizer,\n",
    "    prompt=\"Mr. Darcy has inherited the estate from his aunt, so he must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81220f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wife is not to her.\n",
      "“And you are very well.”\n",
      "“I have not know, “I am not you,\n",
      "==================================================\n",
      "A husband is not to be in the of this, I am sure I shall not mean to be sure I am sure to say to the very little to be so\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    prompt=\"A wife is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    prompt=\"A husband is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "714f6606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good lady ought to be done. He saw him in his manner of his own judgment. He was not to him, he had a man, he had he had not been\n",
      "==================================================\n",
      "A highly respectable man ought to be quite as she had seen him to take it. The house had passed, the greatest part of the house was not been the whole party of his own\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    prompt=\"A good lady ought to be\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    prompt=\"A highly respectable man ought to be\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67a25f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['state', 'param_groups'])\n",
      "Learning Rate (lr): 0.0002\n",
      "Weight Decay: 0.01\n",
      "Betas: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('model_and_optimizer_all_txt_updated.pth', map_location='cpu')\n",
    "\n",
    "# Extract optimizer state dict\n",
    "optimizer_state = checkpoint['optimizer_state_dict']\n",
    "\n",
    "# Optional: print all optimizer keys to explore\n",
    "print(optimizer_state.keys())\n",
    "\n",
    "# Extract settings (if AdamW or Adam)\n",
    "for param_group in optimizer_state['param_groups']:\n",
    "    print(\"Learning Rate (lr):\", param_group['lr'])\n",
    "    print(\"Weight Decay:\", param_group['weight_decay'])\n",
    "    print(\"Betas:\", optimizer_state['state'][list(optimizer_state['state'].keys())[0]]['exp_avg'])  # Optional state content\n",
    "    print(\"Eps (may not be saved):\", 'Check model code, not always stored')\n",
    "    print(param_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc249e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
