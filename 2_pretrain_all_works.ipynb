{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d339",
   "metadata": {},
   "source": [
    "### Detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d281a",
   "metadata": {},
   "source": [
    "### Set up model configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72561797-a3f0-4d84-9883-64c447482389",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 896,         # Embedding dimension\n",
    "    \"n_heads\": 14,          # Number of attention heads\n",
    "    \"n_layers\": 8,         # Number of layers\n",
    "    \"drop_rate\": 0.2,       # Dropout rate\n",
    "    \"qkv_bias\": True,      # Query-Key-Value bias\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383eb92",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d1c16",
   "metadata": {},
   "source": [
    "#### GPT-2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75227c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88bc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(full_text):\n",
    "    return tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fbf11",
   "metadata": {},
   "source": [
    "### Load training and validation data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2b562de-efe1-40d2-a5ba-350b1edb7a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data.txt'\n",
    "val_file_path = 'val_text_data.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf608bf",
   "metadata": {},
   "source": [
    "### Initialize data loaders for training\n",
    "Data loaders implementation can be found in `./data_loader_v1.py`.\n",
    "\n",
    "This implementation follows the omplementation detailed in _Raschka, Sebastian. Build a Large Language Model (From Scratch). Manning Publications, 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bddf6dae-302d-4fc7-853b-2806a0c7d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f915a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 5789730\n",
      "Characters: 32372094\n",
      "Tokens: 7608098\n",
      "Unique Tokens Used: 28960\n"
     ]
    }
   ],
   "source": [
    "full_text = train_data + val_data\n",
    "\n",
    "word_count = len(full_text.split())\n",
    "char_count = len(full_text)\n",
    "\n",
    "tokens = tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "token_count = len(tokens)\n",
    "unique_token_count = len(set(tokens))\n",
    "\n",
    "print(\"Words:\", word_count)\n",
    "print(\"Characters:\", char_count)\n",
    "print(\"Tokens:\", token_count)\n",
    "print(\"Unique Tokens Used:\", unique_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f969edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean(): \n",
    "    \"\"\"\n",
    "    This is a function for GPU data claening before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    \n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.mps.empty_cache()  # Attempt to release MPS memory\n",
    "    \n",
    "    # Move tensors to CPU\n",
    "    for tensor in list(globals().values()):\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device == torch.device(\"mps\"):\n",
    "            tensor.to(\"cpu\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    del tensor\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "    print(\"Allocated Memory:\", torch.mps.current_allocated_memory() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da82d2c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f2c9bfd-5c57-4af6-98e8-5da47988d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train import train_model_simple\n",
    "import time\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\n",
    "def train(train_loader, val_loader,\n",
    "          num_epochs=10, eval_iter=5, lr=0.0002,\n",
    "          generate_sample_text=False,\n",
    "          sample_text=\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be\",\n",
    "          model_prefix=\"model_and_optimizer\"):\n",
    "\n",
    "    global train_losses, val_losses, track_tokens_seen  # Ensure these are updated globally\n",
    "\n",
    "    if device == \"mps\":\n",
    "        clean()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-08, weight_decay=0.05)\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=num_epochs, eval_iter=eval_iter,\n",
    "        start_context=sample_text, cfg=GPT_CONFIG_124M,\n",
    "        generate_sample_text=generate_sample_text,\n",
    "        model_prefix=model_prefix,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        print(50 * \"=\")\n",
    "        clean()\n",
    "    if device == \"cuda\":\n",
    "        print(50 * \"=\")\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d594966-9781-4ea2-9a68-01196f5111b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae6fc",
   "metadata": {},
   "source": [
    "### Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda45148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.449, Val loss 10.457\n",
      "Ep 1 (Step 000150): Train loss 6.141, Val loss 6.075\n",
      "Ep 1 (Step 000300): Train loss 5.762, Val loss 5.716\n",
      "Ep 1 (Step 000450): Train loss 5.631, Val loss 5.572\n",
      "Ep 1 (Step 000600): Train loss 5.530, Val loss 5.458\n",
      "Ep 1 (Step 000750): Train loss 5.393, Val loss 5.399\n",
      "Ep 1 (Step 000900): Train loss 5.349, Val loss 5.337\n",
      "Ep 1 (Step 001050): Train loss 5.309, Val loss 5.282\n",
      "Ep 1 (Step 001200): Train loss 5.212, Val loss 5.193\n",
      "Ep 1 (Step 001350): Train loss 5.194, Val loss 5.169\n",
      "Ep 1 (Step 001500): Train loss 5.145, Val loss 5.067\n",
      "Ep 1 (Step 001650): Train loss 5.045, Val loss 5.060\n",
      "Ep 1 (Step 001800): Train loss 5.056, Val loss 4.991\n",
      "Ep 1 (Step 001950): Train loss 4.967, Val loss 4.930\n",
      "Ep 1 (Step 002100): Train loss 4.948, Val loss 4.903\n",
      "Ep 1 (Step 002250): Train loss 4.875, Val loss 4.900\n",
      "Ep 1 (Step 002400): Train loss 4.897, Val loss 4.862\n",
      "Ep 1 (Step 002550): Train loss 4.870, Val loss 4.860\n",
      "Ep 1 (Step 002700): Train loss 4.812, Val loss 4.812\n",
      "Ep 1 (Step 002850): Train loss 4.801, Val loss 4.808\n",
      "Ep 1 (Step 003000): Train loss 4.756, Val loss 4.759\n",
      "Ep 1 (Step 003150): Train loss 4.751, Val loss 4.735\n",
      "Ep 1 (Step 003300): Train loss 4.739, Val loss 4.707\n",
      "Ep 1 (Step 003450): Train loss 4.729, Val loss 4.693\n",
      "Ep 1 (Step 003600): Train loss 4.678, Val loss 4.699\n",
      "Ep 1 (Step 003750): Train loss 4.663, Val loss 4.667\n",
      "Ep 1 (Step 003900): Train loss 4.595, Val loss 4.634\n",
      "Ep 1 (Step 004050): Train loss 4.586, Val loss 4.636\n",
      "Ep 1 (Step 004200): Train loss 4.573, Val loss 4.609\n",
      "Ep 1 (Step 004350): Train loss 4.544, Val loss 4.604\n",
      "Ep 2 (Step 004500): Train loss 4.540, Val loss 4.599\n",
      "Ep 2 (Step 004650): Train loss 4.544, Val loss 4.563\n",
      "Ep 2 (Step 004800): Train loss 4.496, Val loss 4.547\n",
      "Ep 2 (Step 004950): Train loss 4.478, Val loss 4.560\n",
      "Ep 2 (Step 005100): Train loss 4.478, Val loss 4.530\n",
      "Ep 2 (Step 005250): Train loss 4.448, Val loss 4.530\n",
      "Ep 2 (Step 005400): Train loss 4.437, Val loss 4.516\n",
      "Ep 2 (Step 005550): Train loss 4.461, Val loss 4.504\n",
      "Ep 2 (Step 005700): Train loss 4.435, Val loss 4.477\n",
      "Ep 2 (Step 005850): Train loss 4.413, Val loss 4.461\n",
      "Ep 2 (Step 006000): Train loss 4.377, Val loss 4.442\n",
      "Ep 2 (Step 006150): Train loss 4.372, Val loss 4.432\n",
      "Ep 2 (Step 006300): Train loss 4.374, Val loss 4.416\n",
      "Ep 2 (Step 006450): Train loss 4.349, Val loss 4.401\n",
      "Ep 2 (Step 006600): Train loss 4.332, Val loss 4.395\n",
      "Ep 2 (Step 006750): Train loss 4.321, Val loss 4.396\n",
      "Ep 2 (Step 006900): Train loss 4.287, Val loss 4.401\n",
      "Ep 2 (Step 007050): Train loss 4.309, Val loss 4.391\n",
      "Ep 2 (Step 007200): Train loss 4.269, Val loss 4.389\n",
      "Ep 2 (Step 007350): Train loss 4.243, Val loss 4.384\n",
      "Ep 2 (Step 007500): Train loss 4.257, Val loss 4.371\n",
      "Ep 2 (Step 007650): Train loss 4.278, Val loss 4.368\n",
      "Ep 2 (Step 007800): Train loss 4.228, Val loss 4.348\n",
      "Ep 2 (Step 007950): Train loss 4.211, Val loss 4.339\n",
      "Ep 2 (Step 008100): Train loss 4.181, Val loss 4.332\n",
      "Ep 2 (Step 008250): Train loss 4.186, Val loss 4.320\n",
      "Ep 2 (Step 008400): Train loss 4.194, Val loss 4.309\n",
      "Ep 2 (Step 008550): Train loss 4.198, Val loss 4.280\n",
      "Ep 2 (Step 008700): Train loss 4.175, Val loss 4.281\n",
      "Ep 2 (Step 008850): Train loss 4.179, Val loss 4.268\n",
      "Ep 3 (Step 009000): Train loss 4.184, Val loss 4.274\n",
      "Ep 3 (Step 009150): Train loss 4.142, Val loss 4.257\n",
      "Ep 3 (Step 009300): Train loss 4.159, Val loss 4.236\n",
      "Ep 3 (Step 009450): Train loss 4.145, Val loss 4.247\n",
      "Ep 3 (Step 009600): Train loss 4.099, Val loss 4.245\n",
      "Ep 3 (Step 009750): Train loss 4.117, Val loss 4.247\n",
      "Ep 3 (Step 009900): Train loss 4.084, Val loss 4.231\n",
      "Ep 3 (Step 010050): Train loss 4.075, Val loss 4.227\n",
      "Ep 3 (Step 010200): Train loss 4.074, Val loss 4.229\n",
      "Ep 3 (Step 010350): Train loss 4.054, Val loss 4.233\n",
      "Ep 3 (Step 010500): Train loss 4.046, Val loss 4.216\n",
      "Ep 3 (Step 010650): Train loss 4.050, Val loss 4.213\n",
      "Ep 3 (Step 010800): Train loss 4.043, Val loss 4.203\n",
      "Ep 3 (Step 010950): Train loss 4.055, Val loss 4.207\n",
      "Ep 3 (Step 011100): Train loss 4.014, Val loss 4.210\n",
      "Ep 3 (Step 011250): Train loss 4.000, Val loss 4.201\n",
      "Ep 3 (Step 011400): Train loss 4.008, Val loss 4.205\n",
      "Ep 3 (Step 011550): Train loss 4.007, Val loss 4.193\n",
      "Ep 3 (Step 011700): Train loss 3.987, Val loss 4.186\n",
      "Ep 3 (Step 011850): Train loss 3.991, Val loss 4.194\n",
      "Ep 3 (Step 012000): Train loss 4.007, Val loss 4.190\n",
      "Ep 3 (Step 012150): Train loss 3.964, Val loss 4.180\n",
      "Ep 3 (Step 012300): Train loss 3.943, Val loss 4.160\n",
      "Ep 3 (Step 012450): Train loss 3.950, Val loss 4.166\n",
      "Ep 3 (Step 012600): Train loss 3.960, Val loss 4.155\n",
      "Ep 3 (Step 012750): Train loss 3.949, Val loss 4.153\n",
      "Ep 3 (Step 012900): Train loss 3.975, Val loss 4.153\n",
      "Ep 3 (Step 013050): Train loss 3.925, Val loss 4.154\n",
      "Ep 3 (Step 013200): Train loss 3.898, Val loss 4.158\n",
      "Ep 3 (Step 013350): Train loss 3.897, Val loss 4.152\n",
      "Ep 4 (Step 013500): Train loss 3.918, Val loss 4.172\n",
      "Ep 4 (Step 013650): Train loss 3.895, Val loss 4.167\n",
      "Ep 4 (Step 013800): Train loss 3.916, Val loss 4.160\n",
      "Ep 4 (Step 013950): Train loss 3.894, Val loss 4.149\n",
      "Ep 4 (Step 014100): Train loss 3.858, Val loss 4.146\n",
      "Ep 4 (Step 014250): Train loss 3.847, Val loss 4.134\n",
      "Ep 4 (Step 014400): Train loss 3.869, Val loss 4.127\n",
      "Ep 4 (Step 014550): Train loss 3.850, Val loss 4.128\n",
      "Ep 4 (Step 014700): Train loss 3.857, Val loss 4.120\n",
      "Ep 4 (Step 014850): Train loss 3.846, Val loss 4.119\n",
      "Ep 4 (Step 015000): Train loss 3.844, Val loss 4.112\n",
      "Ep 4 (Step 015150): Train loss 3.831, Val loss 4.094\n",
      "Ep 4 (Step 015300): Train loss 3.792, Val loss 4.103\n",
      "Ep 4 (Step 015450): Train loss 3.794, Val loss 4.096\n",
      "Ep 4 (Step 015600): Train loss 3.838, Val loss 4.098\n",
      "Ep 4 (Step 015750): Train loss 3.835, Val loss 4.089\n",
      "Ep 4 (Step 015900): Train loss 3.803, Val loss 4.087\n",
      "Ep 4 (Step 016050): Train loss 3.832, Val loss 4.091\n",
      "Ep 4 (Step 016200): Train loss 3.814, Val loss 4.090\n",
      "Ep 4 (Step 016350): Train loss 3.804, Val loss 4.088\n",
      "Ep 4 (Step 016500): Train loss 3.781, Val loss 4.084\n",
      "Ep 4 (Step 016650): Train loss 3.774, Val loss 4.087\n",
      "Ep 4 (Step 016800): Train loss 3.752, Val loss 4.080\n",
      "Ep 4 (Step 016950): Train loss 3.764, Val loss 4.075\n",
      "Ep 4 (Step 017100): Train loss 3.745, Val loss 4.073\n",
      "Ep 4 (Step 017250): Train loss 3.734, Val loss 4.072\n",
      "Ep 4 (Step 017400): Train loss 3.744, Val loss 4.066\n",
      "Ep 4 (Step 017550): Train loss 3.723, Val loss 4.072\n",
      "Ep 4 (Step 017700): Train loss 3.746, Val loss 4.066\n",
      "Ep 5 (Step 017850): Train loss 3.749, Val loss 4.072\n",
      "Ep 5 (Step 018000): Train loss 3.750, Val loss 4.074\n",
      "Ep 5 (Step 018150): Train loss 3.736, Val loss 4.075\n",
      "Ep 5 (Step 018300): Train loss 3.704, Val loss 4.078\n",
      "Ep 5 (Step 018450): Train loss 3.750, Val loss 4.075\n",
      "Ep 5 (Step 018600): Train loss 3.714, Val loss 4.072\n",
      "Ep 5 (Step 018750): Train loss 3.726, Val loss 4.072\n",
      "Ep 5 (Step 018900): Train loss 3.731, Val loss 4.061\n",
      "Ep 5 (Step 019050): Train loss 3.723, Val loss 4.064\n",
      "Ep 5 (Step 019200): Train loss 3.738, Val loss 4.063\n",
      "Ep 5 (Step 019350): Train loss 3.677, Val loss 4.060\n",
      "Ep 5 (Step 019500): Train loss 3.693, Val loss 4.057\n",
      "Ep 5 (Step 019650): Train loss 3.653, Val loss 4.052\n",
      "Ep 5 (Step 019800): Train loss 3.683, Val loss 4.055\n",
      "Ep 5 (Step 019950): Train loss 3.680, Val loss 4.046\n",
      "Ep 5 (Step 020100): Train loss 3.681, Val loss 4.047\n",
      "Ep 5 (Step 020250): Train loss 3.676, Val loss 4.044\n",
      "Ep 5 (Step 020400): Train loss 3.684, Val loss 4.043\n",
      "Ep 5 (Step 020550): Train loss 3.645, Val loss 4.041\n",
      "Ep 5 (Step 020700): Train loss 3.665, Val loss 4.041\n",
      "Ep 5 (Step 020850): Train loss 3.643, Val loss 4.039\n",
      "Ep 5 (Step 021000): Train loss 3.675, Val loss 4.041\n",
      "Ep 5 (Step 021150): Train loss 3.657, Val loss 4.038\n",
      "Ep 5 (Step 021300): Train loss 3.666, Val loss 4.034\n",
      "Ep 5 (Step 021450): Train loss 3.652, Val loss 4.035\n",
      "Ep 5 (Step 021600): Train loss 3.661, Val loss 4.032\n",
      "Ep 5 (Step 021750): Train loss 3.678, Val loss 4.035\n",
      "Ep 5 (Step 021900): Train loss 3.644, Val loss 4.035\n",
      "Ep 5 (Step 022050): Train loss 3.636, Val loss 4.032\n",
      "Ep 5 (Step 022200): Train loss 3.635, Val loss 4.030\n",
      "Ep 6 (Step 022350): Train loss 3.645, Val loss 4.032\n",
      "Ep 6 (Step 022500): Train loss 3.637, Val loss 4.034\n",
      "Ep 6 (Step 022650): Train loss 3.648, Val loss 4.037\n",
      "Ep 6 (Step 022800): Train loss 3.644, Val loss 4.033\n",
      "Ep 6 (Step 022950): Train loss 3.617, Val loss 4.035\n",
      "Ep 6 (Step 023100): Train loss 3.597, Val loss 4.034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6 (Step 023250): Train loss 3.627, Val loss 4.034\n",
      "Ep 6 (Step 023400): Train loss 3.616, Val loss 4.032\n",
      "Ep 6 (Step 023550): Train loss 3.626, Val loss 4.031\n",
      "Ep 6 (Step 023700): Train loss 3.645, Val loss 4.034\n",
      "Ep 6 (Step 023850): Train loss 3.635, Val loss 4.036\n",
      "Ep 6 (Step 024000): Train loss 3.629, Val loss 4.032\n",
      "Ep 6 (Step 024150): Train loss 3.625, Val loss 4.031\n",
      "Ep 6 (Step 024300): Train loss 3.601, Val loss 4.032\n",
      "Ep 6 (Step 024450): Train loss 3.619, Val loss 4.031\n",
      "Ep 6 (Step 024600): Train loss 3.620, Val loss 4.031\n",
      "Ep 6 (Step 024750): Train loss 3.614, Val loss 4.033\n",
      "Ep 6 (Step 024900): Train loss 3.649, Val loss 4.032\n",
      "Ep 6 (Step 025050): Train loss 3.629, Val loss 4.032\n",
      "Ep 6 (Step 025200): Train loss 3.619, Val loss 4.031\n",
      "Ep 6 (Step 025350): Train loss 3.599, Val loss 4.031\n",
      "Ep 6 (Step 025500): Train loss 3.652, Val loss 4.032\n",
      "Ep 6 (Step 025650): Train loss 3.641, Val loss 4.032\n",
      "Ep 6 (Step 025800): Train loss 3.603, Val loss 4.031\n",
      "Ep 6 (Step 025950): Train loss 3.624, Val loss 4.031\n",
      "Ep 6 (Step 026100): Train loss 3.651, Val loss 4.031\n",
      "Ep 6 (Step 026250): Train loss 3.617, Val loss 4.031\n",
      "Ep 6 (Step 026400): Train loss 3.618, Val loss 4.031\n",
      "Ep 6 (Step 026550): Train loss 3.614, Val loss 4.031\n",
      "Ep 6 (Step 026700): Train loss 3.602, Val loss 4.031\n",
      "Training completed in 400.43 minutes.\n"
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=6,\n",
    "      eval_iter=150, model_prefix=\"model_640_10_8_0_1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6651aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0002, weight_decay=0.05)\n",
    "\n",
    "checkpoint = torch.load(\"model_896_14_8_256.pth\", weights_only=True, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc47f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ecee23b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py:128\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[1;34m(coro)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3286\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[1;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[0;32m   3284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_history:\n\u001b[0;32m   3285\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3286\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[0;32m   3288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(cell, raw_cell)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\history.py:793\u001b[0m, in \u001b[0;36mHistoryManager.store_inputs\u001b[1;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_hist_raw\u001b[38;5;241m.\u001b[39mappend(source_raw)\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_input_cache_lock:\n\u001b[1;32m--> 793\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb_input_cache\u001b[49m\u001b[38;5;241m.\u001b[39mappend((line_num, source, source_raw))\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;66;03m# Trigger to flush cache and write to DB.\u001b[39;00m\n\u001b[0;32m    795\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_input_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_cache_size:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\traitlets\\traitlets.py:676\u001b[0m, in \u001b[0;36mTraitType.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;129m@t\u001b[39m\u001b[38;5;241m.\u001b[39moverload\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[t\u001b[38;5;241m.\u001b[39mAny]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m G:\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m--> 676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: HasTraits \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[t\u001b[38;5;241m.\u001b[39mAny]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self \u001b[38;5;241m|\u001b[39m G:\n\u001b[0;32m    677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the value of the trait by self.name for the instance.\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03m    Default values are instantiated when :meth:`HasTraits.__new__`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;124;03m    is in the :class:`HasTraits` instance.\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from itertools import combinations\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)  # Forward pass\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * target_ids.numel()\n",
    "            total_tokens += target_ids.numel()\n",
    "\n",
    "    perplexity = np.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c61225-1077-4019-984a-564aa7ba6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "def weat_score(model, target_words_1, target_words_2, attribute_words_1, attribute_words_2, tokenizer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Measures bias by comparing how close different groups of words are in embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding(word):\n",
    "        token_id = tokenizer.encode(word, allowed_special={'<|endoftext|>'})[0]\n",
    "        with torch.no_grad():\n",
    "            embed = model.tok_emb(torch.tensor([token_id], device=device)).cpu().numpy()\n",
    "        return embed.flatten()\n",
    "\n",
    "    # Get embeddings\n",
    "    target_1_embs = [get_embedding(w) for w in target_words_1]\n",
    "    target_2_embs = [get_embedding(w) for w in target_words_2]\n",
    "    attr_1_embs = [get_embedding(w) for w in attribute_words_1]\n",
    "    attr_2_embs = [get_embedding(w) for w in attribute_words_2]\n",
    "\n",
    "    def association(t, A, B):\n",
    "        return np.mean([cosine_similarity(t, a) for a in A]) - np.mean([cosine_similarity(t, b) for b in B])\n",
    "\n",
    "    # Compute WEAT score\n",
    "    s1 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_1_embs])\n",
    "    s2 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_2_embs])\n",
    "    \n",
    "    weat_score = s1 - s2\n",
    "    return weat_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a1406-0bd9-409a-9fd8-8b4f343d2c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_male = [\"gentleman\", \"officer\", \"clergyman\", \"husband\", \"captain\"]\n",
    "target_female = [\"lady\", \"governess\", \"girl\", \"wife\", \"widow\"]\n",
    "\n",
    "attribute_male = [\"honour\", \"duty\", \"wisdom\", \"fortitude\", \"independence\"]\n",
    "attribute_female = [\"grace\", \"affection\", \"beauty\", \"delicacy\", \"modesty\"]\n",
    "\n",
    "weat_score(model, target_male, target_female, attribute_male, attribute_female, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcce2e-326f-4cc6-8033-e77e0a2ba270",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d845171-c8f1-47a0-a60b-1f9847a22590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import re\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_bleu_rouge_from_val(model, device=\"cpu\"):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    # Step 1: Load the validation set\n",
    "    with open('val_text_data_all_txt.txt', 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Step 2: Split into sentences & filter\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', data)\n",
    "    filtered_sentences = [s.strip() for s in sentences if 5 <= len(s.split()) <= 60]\n",
    "    filtered_sentences = filtered_sentences[:1000]\n",
    "\n",
    "    # Step 3: Split each sentence into two halves and store as tuples\n",
    "    sentence_tuples = []\n",
    "    for sent in filtered_sentences:\n",
    "        words = sent.split()\n",
    "        mid = len(words) // 2\n",
    "        first_half = ' '.join(words[:mid])\n",
    "        second_half = ' '.join(words[mid:])\n",
    "        sentence_tuples.append((first_half, second_half))\n",
    "\n",
    "    # Step 4: For each (first_half, second_half), generate prediction\n",
    "    for first_half, second_half in sentence_tuples:\n",
    "        generated_text = generate(\n",
    "            model=model, prompt=first_half,\n",
    "            max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "            device=device,\n",
    "            temperature=0.7,\n",
    "            top_k=50\n",
    "        )\n",
    "\n",
    "        # Build reference and prediction\n",
    "        reference = first_half + \" \" + second_half\n",
    "        prediction = generated_text\n",
    "\n",
    "        references.append(reference)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Step 5-6: Compute BLEU and ROUGE\n",
    "    # Format references correctly for BLEU\n",
    "    references_formatted = [[ref] for ref in references]\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references_formatted)['bleu']\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}, ROUGE-L Score: {rouge_score['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404bbea-7821-4b52-a5e9-f240bf5c01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_bleu_rouge_from_val(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Miss Bennet has inherited the estate from her aunt, so she must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Mr. Darcy has inherited the estate from his aunt, so he must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81220f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"A wife is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"A husband is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"I shall now go\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"He said\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"She was\",\n",
    "    max_new_tokens=200, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb810-cfc6-49fe-bfd0-66c035e0707e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78fd8b69-ae98-4300-b876-93bf84fa0d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to be the very day, and I am sure I am sure I am sure I should have been a very much obliged to be very happy. I am'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "583fd4d9-4273-435a-a2fe-651162acb64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to go and Mrs. Weston.\\n\"I am very glad to think of your own family.\"\\n\"I will not like you. I am afraid'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2c8a060b-6d8a-4dc8-ade9-bfde78b38900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she is wild to get married from me to do the best of them. I think it is quite forgot to write to be a great deal. You must own. There is nothing'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"she is wild to get married\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=1,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba961e03-e482-4601-bef7-daab0c1dac77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f99b81-a4be-4994-9c00-8898a408f06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
