{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d339",
   "metadata": {},
   "source": [
    "### Detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d281a",
   "metadata": {},
   "source": [
    "### Set up model configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72561797-a3f0-4d84-9883-64c447482389",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "#   \"vocab_size\": 14000,    # Vocabulary size (custom tokenizer)\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.2,       # Dropout rate\n",
    "    \"qkv_bias\": True,      # Query-Key-Value bias\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383eb92",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d1c16",
   "metadata": {},
   "source": [
    "#### GPT-2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75227c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6642b10",
   "metadata": {},
   "source": [
    "#### Custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b2a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2281b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='all_books.txt',\n",
    "    model_prefix='gpt_custom_tokenizer',\n",
    "    vocab_size=GPT_CONFIG_124M[\"vocab_size\"],\n",
    "    model_type='bpe',\n",
    "    character_coverage=0.9995,\n",
    "    hard_vocab_limit=False,\n",
    "    bos_id=-1,\n",
    "    eos_id=-1,\n",
    "    user_defined_symbols=[\"<|endoftext|>\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d8fac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.load('gpt_custom_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88bc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_used_in_this_trial=\"GPT2\"\n",
    "# tokenizer_used_in_this_trial=\"CUSTOM\"\n",
    "\n",
    "def encode(full_text):\n",
    "    if tokenizer_used_in_this_trial == \"GPT2\":\n",
    "        return tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})\n",
    "    else:\n",
    "        return tokenizer.encode(full_text, out_type=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fbf11",
   "metadata": {},
   "source": [
    "### Load training and validation data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2b562de-efe1-40d2-a5ba-350b1edb7a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data_all_txt.txt'\n",
    "val_file_path = 'val_text_data_all_txt.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf608bf",
   "metadata": {},
   "source": [
    "### Initialize data loaders for training\n",
    "Data loaders implementation can be found in `./data_loader_v1.py`.\n",
    "\n",
    "This implementation follows the omplementation detailed in _Raschka, Sebastian. Build a Large Language Model (From Scratch). Manning Publications, 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bddf6dae-302d-4fc7-853b-2806a0c7d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f915a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 740186\n",
      "Characters: 4132331\n",
      "Tokens: 935517\n",
      "Unique Tokens Used: 13902\n"
     ]
    }
   ],
   "source": [
    "full_text = train_data + val_data\n",
    "\n",
    "word_count = len(full_text.split())\n",
    "char_count = len(full_text)\n",
    "\n",
    "# tiktoken tokenizer ->\n",
    "tokens = tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "# Custom tokenizer ->\n",
    "# tokens = tokenizer.encode(full_text, out_type=int)\n",
    "\n",
    "token_count = len(tokens)\n",
    "unique_token_count = len(set(tokens))\n",
    "\n",
    "print(\"Words:\", word_count)\n",
    "print(\"Characters:\", char_count)\n",
    "print(\"Tokens:\", token_count)\n",
    "print(\"Unique Tokens Used:\", unique_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f969edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean(): \n",
    "    \"\"\"\n",
    "    This is a function for GPU data claening before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    \n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.mps.empty_cache()  # Attempt to release MPS memory\n",
    "    \n",
    "    # Move tensors to CPU\n",
    "    for tensor in list(globals().values()):\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device == torch.device(\"mps\"):\n",
    "            tensor.to(\"cpu\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    del tensor\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "    print(\"Allocated Memory:\", torch.mps.current_allocated_memory() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da82d2c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f2c9bfd-5c57-4af6-98e8-5da47988d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train import train_model_simple\n",
    "import time\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\n",
    "def train(train_loader, val_loader,\n",
    "          num_epochs=10, eval_iter=5, lr=0.0002,\n",
    "          generate_sample_text=False,\n",
    "          sample_text=\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be\",\n",
    "          model_prefix=\"model_and_optimizer\"):\n",
    "\n",
    "    global train_losses, val_losses, track_tokens_seen  # Ensure these are updated globally\n",
    "\n",
    "    if device == \"mps\":\n",
    "        clean()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-08, weight_decay=0.05)\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=num_epochs, eval_iter=eval_iter,\n",
    "        start_context=sample_text, cfg=GPT_CONFIG_124M,\n",
    "        generate_sample_text=generate_sample_text,\n",
    "        model_prefix=model_prefix,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        print(50 * \"=\")\n",
    "        clean()\n",
    "    if device == \"cuda\":\n",
    "        print(50 * \"=\")\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d594966-9781-4ea2-9a68-01196f5111b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae6fc",
   "metadata": {},
   "source": [
    "### Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dda45148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 8.911, Val loss 8.861\n",
      "Ep 1 (Step 000010): Train loss 7.138, Val loss 7.140\n",
      "Ep 1 (Step 000020): Train loss 6.511, Val loss 6.545\n",
      "Ep 1 (Step 000030): Train loss 6.403, Val loss 6.398\n",
      "Ep 1 (Step 000040): Train loss 6.367, Val loss 6.271\n",
      "Ep 1 (Step 000050): Train loss 6.112, Val loss 6.128\n",
      "Ep 1 (Step 000060): Train loss 6.114, Val loss 5.995\n",
      "Ep 1 (Step 000070): Train loss 5.921, Val loss 5.898\n",
      "Ep 1 (Step 000080): Train loss 5.786, Val loss 5.821\n",
      "Ep 1 (Step 000090): Train loss 5.758, Val loss 5.751\n",
      "Ep 1 (Step 000100): Train loss 5.703, Val loss 5.692\n",
      "Ep 1 (Step 000110): Train loss 5.625, Val loss 5.647\n",
      "Ep 1 (Step 000120): Train loss 5.643, Val loss 5.607\n",
      "Ep 1 (Step 000130): Train loss 5.553, Val loss 5.555\n",
      "Ep 1 (Step 000140): Train loss 5.505, Val loss 5.530\n",
      "Ep 1 (Step 000150): Train loss 5.494, Val loss 5.508\n",
      "Ep 1 (Step 000160): Train loss 5.471, Val loss 5.494\n",
      "Ep 1 (Step 000170): Train loss 5.360, Val loss 5.459\n",
      "Ep 1 (Step 000180): Train loss 5.465, Val loss 5.442\n",
      "Ep 1 (Step 000190): Train loss 5.316, Val loss 5.431\n",
      "Ep 1 (Step 000200): Train loss 5.415, Val loss 5.422\n",
      "Ep 1 (Step 000210): Train loss 5.305, Val loss 5.387\n",
      "Ep 1 (Step 000220): Train loss 5.452, Val loss 5.390\n",
      "Ep 1 (Step 000230): Train loss 5.321, Val loss 5.375\n",
      "Ep 1 (Step 000240): Train loss 5.312, Val loss 5.364\n",
      "Ep 1 (Step 000250): Train loss 5.238, Val loss 5.347\n",
      "Ep 1 (Step 000260): Train loss 5.353, Val loss 5.329\n",
      "Ep 1 (Step 000270): Train loss 5.264, Val loss 5.345\n",
      "Ep 1 (Step 000280): Train loss 5.327, Val loss 5.296\n",
      "Ep 1 (Step 000290): Train loss 5.288, Val loss 5.289\n",
      "Ep 1 (Step 000300): Train loss 5.281, Val loss 5.267\n",
      "Ep 1 (Step 000310): Train loss 5.278, Val loss 5.266\n",
      "Ep 1 (Step 000320): Train loss 5.247, Val loss 5.247\n",
      "Ep 1 (Step 000330): Train loss 5.291, Val loss 5.251\n",
      "Ep 1 (Step 000340): Train loss 5.207, Val loss 5.236\n",
      "Ep 1 (Step 000350): Train loss 5.108, Val loss 5.235\n",
      "Ep 1 (Step 000360): Train loss 5.106, Val loss 5.201\n",
      "Ep 1 (Step 000370): Train loss 5.179, Val loss 5.197\n",
      "Ep 1 (Step 000380): Train loss 5.062, Val loss 5.193\n",
      "Ep 1 (Step 000390): Train loss 5.092, Val loss 5.193\n",
      "Ep 1 (Step 000400): Train loss 5.121, Val loss 5.210\n",
      "Ep 1 (Step 000410): Train loss 5.080, Val loss 5.197\n",
      "Ep 1 (Step 000420): Train loss 5.130, Val loss 5.193\n",
      "Ep 1 (Step 000430): Train loss 5.103, Val loss 5.181\n",
      "Ep 1 (Step 000440): Train loss 5.176, Val loss 5.193\n",
      "Ep 1 (Step 000450): Train loss 5.030, Val loss 5.177\n",
      "Ep 1 (Step 000460): Train loss 5.127, Val loss 5.155\n",
      "Ep 1 (Step 000470): Train loss 5.048, Val loss 5.167\n",
      "Ep 1 (Step 000480): Train loss 5.081, Val loss 5.149\n",
      "Ep 1 (Step 000490): Train loss 5.083, Val loss 5.168\n",
      "Ep 1 (Step 000500): Train loss 5.114, Val loss 5.157\n",
      "Ep 1 (Step 000510): Train loss 5.039, Val loss 5.160\n",
      "Ep 1 (Step 000520): Train loss 5.070, Val loss 5.141\n",
      "Ep 1 (Step 000530): Train loss 4.999, Val loss 5.130\n",
      "Ep 1 (Step 000540): Train loss 5.083, Val loss 5.126\n",
      "Ep 1 (Step 000550): Train loss 4.942, Val loss 5.137\n",
      "Ep 1 (Step 000560): Train loss 4.982, Val loss 5.112\n",
      "Ep 1 (Step 000570): Train loss 5.043, Val loss 5.119\n",
      "Ep 1 (Step 000580): Train loss 5.067, Val loss 5.122\n",
      "Ep 1 (Step 000590): Train loss 5.127, Val loss 5.101\n",
      "Ep 1 (Step 000600): Train loss 5.026, Val loss 5.110\n",
      "Ep 1 (Step 000610): Train loss 5.026, Val loss 5.093\n",
      "Ep 1 (Step 000620): Train loss 5.026, Val loss 5.081\n",
      "Ep 1 (Step 000630): Train loss 4.911, Val loss 5.077\n",
      "Ep 1 (Step 000640): Train loss 4.906, Val loss 5.065\n",
      "Ep 1 (Step 000650): Train loss 5.031, Val loss 5.063\n",
      "Ep 1 (Step 000660): Train loss 4.973, Val loss 5.056\n",
      "Ep 1 (Step 000670): Train loss 4.933, Val loss 5.054\n",
      "Ep 1 (Step 000680): Train loss 5.000, Val loss 5.058\n",
      "Ep 1 (Step 000690): Train loss 4.943, Val loss 5.049\n",
      "Ep 1 (Step 000700): Train loss 5.013, Val loss 5.056\n",
      "Ep 1 (Step 000710): Train loss 4.892, Val loss 5.046\n",
      "Ep 1 (Step 000720): Train loss 4.927, Val loss 5.037\n",
      "Ep 1 (Step 000730): Train loss 5.012, Val loss 5.036\n",
      "Ep 1 (Step 000740): Train loss 4.913, Val loss 5.025\n",
      "Ep 1 (Step 000750): Train loss 4.818, Val loss 5.030\n",
      "Ep 1 (Step 000760): Train loss 4.953, Val loss 5.027\n",
      "Ep 1 (Step 000770): Train loss 5.016, Val loss 5.012\n",
      "Ep 1 (Step 000780): Train loss 4.904, Val loss 5.000\n",
      "Ep 2 (Step 000790): Train loss 5.033, Val loss 5.026\n",
      "Ep 2 (Step 000800): Train loss 4.906, Val loss 5.003\n",
      "Ep 2 (Step 000810): Train loss 4.885, Val loss 4.999\n",
      "Ep 2 (Step 000820): Train loss 4.921, Val loss 4.992\n",
      "Ep 2 (Step 000830): Train loss 4.914, Val loss 4.985\n",
      "Ep 2 (Step 000840): Train loss 4.810, Val loss 4.987\n",
      "Ep 2 (Step 000850): Train loss 4.895, Val loss 4.985\n",
      "Ep 2 (Step 000860): Train loss 4.840, Val loss 4.989\n",
      "Ep 2 (Step 000870): Train loss 4.952, Val loss 4.972\n",
      "Ep 2 (Step 000880): Train loss 4.814, Val loss 4.985\n",
      "Ep 2 (Step 000890): Train loss 4.899, Val loss 4.980\n",
      "Ep 2 (Step 000900): Train loss 4.915, Val loss 4.953\n",
      "Ep 2 (Step 000910): Train loss 4.906, Val loss 4.957\n",
      "Ep 2 (Step 000920): Train loss 4.862, Val loss 4.959\n",
      "Ep 2 (Step 000930): Train loss 4.840, Val loss 4.962\n",
      "Ep 2 (Step 000940): Train loss 4.842, Val loss 4.948\n",
      "Ep 2 (Step 000950): Train loss 4.861, Val loss 4.953\n",
      "Ep 2 (Step 000960): Train loss 4.826, Val loss 4.962\n",
      "Ep 2 (Step 000970): Train loss 4.826, Val loss 4.940\n",
      "Ep 2 (Step 000980): Train loss 4.814, Val loss 4.936\n",
      "Ep 2 (Step 000990): Train loss 4.694, Val loss 4.944\n",
      "Ep 2 (Step 001000): Train loss 4.829, Val loss 4.931\n",
      "Ep 2 (Step 001010): Train loss 4.844, Val loss 4.950\n",
      "Ep 2 (Step 001020): Train loss 4.866, Val loss 4.946\n",
      "Ep 2 (Step 001030): Train loss 4.775, Val loss 4.957\n",
      "Ep 2 (Step 001040): Train loss 4.775, Val loss 4.927\n",
      "Ep 2 (Step 001050): Train loss 4.851, Val loss 4.926\n",
      "Ep 2 (Step 001060): Train loss 4.772, Val loss 4.925\n",
      "Ep 2 (Step 001070): Train loss 4.772, Val loss 4.942\n",
      "Ep 2 (Step 001080): Train loss 4.838, Val loss 4.942\n",
      "Ep 2 (Step 001090): Train loss 4.807, Val loss 4.948\n",
      "Ep 2 (Step 001100): Train loss 4.854, Val loss 4.923\n",
      "Ep 2 (Step 001110): Train loss 4.745, Val loss 4.916\n",
      "Ep 2 (Step 001120): Train loss 4.771, Val loss 4.918\n",
      "Ep 2 (Step 001130): Train loss 4.741, Val loss 4.895\n",
      "Ep 2 (Step 001140): Train loss 4.763, Val loss 4.911\n",
      "Ep 2 (Step 001150): Train loss 4.751, Val loss 4.896\n",
      "Ep 2 (Step 001160): Train loss 4.803, Val loss 4.900\n",
      "Ep 2 (Step 001170): Train loss 4.845, Val loss 4.890\n",
      "Ep 2 (Step 001180): Train loss 4.675, Val loss 4.886\n",
      "Ep 2 (Step 001190): Train loss 4.784, Val loss 4.898\n",
      "Ep 2 (Step 001200): Train loss 4.844, Val loss 4.886\n",
      "Ep 2 (Step 001210): Train loss 4.667, Val loss 4.875\n",
      "Ep 2 (Step 001220): Train loss 4.816, Val loss 4.889\n",
      "Ep 2 (Step 001230): Train loss 4.758, Val loss 4.874\n",
      "Ep 2 (Step 001240): Train loss 4.730, Val loss 4.865\n",
      "Ep 2 (Step 001250): Train loss 4.699, Val loss 4.875\n",
      "Ep 2 (Step 001260): Train loss 4.716, Val loss 4.861\n",
      "Ep 2 (Step 001270): Train loss 4.601, Val loss 4.846\n",
      "Ep 2 (Step 001280): Train loss 4.627, Val loss 4.858\n",
      "Ep 2 (Step 001290): Train loss 4.640, Val loss 4.866\n",
      "Ep 2 (Step 001300): Train loss 4.658, Val loss 4.851\n",
      "Ep 2 (Step 001310): Train loss 4.626, Val loss 4.842\n",
      "Ep 2 (Step 001320): Train loss 4.782, Val loss 4.848\n",
      "Ep 2 (Step 001330): Train loss 4.642, Val loss 4.850\n",
      "Ep 2 (Step 001340): Train loss 4.708, Val loss 4.843\n",
      "Ep 2 (Step 001350): Train loss 4.692, Val loss 4.835\n",
      "Ep 2 (Step 001360): Train loss 4.706, Val loss 4.848\n",
      "Ep 2 (Step 001370): Train loss 4.674, Val loss 4.836\n",
      "Ep 2 (Step 001380): Train loss 4.639, Val loss 4.837\n",
      "Ep 2 (Step 001390): Train loss 4.645, Val loss 4.828\n",
      "Ep 2 (Step 001400): Train loss 4.660, Val loss 4.829\n",
      "Ep 2 (Step 001410): Train loss 4.656, Val loss 4.829\n",
      "Ep 2 (Step 001420): Train loss 4.695, Val loss 4.822\n",
      "Ep 2 (Step 001430): Train loss 4.564, Val loss 4.819\n",
      "Ep 2 (Step 001440): Train loss 4.669, Val loss 4.827\n",
      "Ep 2 (Step 001450): Train loss 4.642, Val loss 4.817\n",
      "Ep 2 (Step 001460): Train loss 4.693, Val loss 4.815\n",
      "Ep 2 (Step 001470): Train loss 4.613, Val loss 4.812\n",
      "Ep 2 (Step 001480): Train loss 4.556, Val loss 4.808\n",
      "Ep 2 (Step 001490): Train loss 4.661, Val loss 4.795\n",
      "Ep 2 (Step 001500): Train loss 4.578, Val loss 4.800\n",
      "Ep 2 (Step 001510): Train loss 4.668, Val loss 4.788\n",
      "Ep 2 (Step 001520): Train loss 4.572, Val loss 4.786\n",
      "Ep 2 (Step 001530): Train loss 4.640, Val loss 4.783\n",
      "Ep 2 (Step 001540): Train loss 4.561, Val loss 4.793\n",
      "Ep 2 (Step 001550): Train loss 4.586, Val loss 4.784\n",
      "Ep 2 (Step 001560): Train loss 4.504, Val loss 4.778\n",
      "Ep 2 (Step 001570): Train loss 4.622, Val loss 4.771\n",
      "Ep 3 (Step 001580): Train loss 4.574, Val loss 4.770\n",
      "Ep 3 (Step 001590): Train loss 4.523, Val loss 4.763\n",
      "Ep 3 (Step 001600): Train loss 4.492, Val loss 4.767\n",
      "Ep 3 (Step 001610): Train loss 4.521, Val loss 4.756\n",
      "Ep 3 (Step 001620): Train loss 4.620, Val loss 4.751\n",
      "Ep 3 (Step 001630): Train loss 4.529, Val loss 4.751\n",
      "Ep 3 (Step 001640): Train loss 4.509, Val loss 4.750\n",
      "Ep 3 (Step 001650): Train loss 4.502, Val loss 4.746\n",
      "Ep 3 (Step 001660): Train loss 4.552, Val loss 4.758\n",
      "Ep 3 (Step 001670): Train loss 4.544, Val loss 4.742\n",
      "Ep 3 (Step 001680): Train loss 4.627, Val loss 4.755\n",
      "Ep 3 (Step 001690): Train loss 4.555, Val loss 4.754\n",
      "Ep 3 (Step 001700): Train loss 4.454, Val loss 4.756\n",
      "Ep 3 (Step 001710): Train loss 4.514, Val loss 4.755\n",
      "Ep 3 (Step 001720): Train loss 4.580, Val loss 4.744\n",
      "Ep 3 (Step 001730): Train loss 4.520, Val loss 4.754\n",
      "Ep 3 (Step 001740): Train loss 4.434, Val loss 4.744\n",
      "Ep 3 (Step 001750): Train loss 4.447, Val loss 4.745\n",
      "Ep 3 (Step 001760): Train loss 4.504, Val loss 4.752\n",
      "Ep 3 (Step 001770): Train loss 4.435, Val loss 4.747\n",
      "Ep 3 (Step 001780): Train loss 4.563, Val loss 4.734\n",
      "Ep 3 (Step 001790): Train loss 4.481, Val loss 4.735\n",
      "Ep 3 (Step 001800): Train loss 4.538, Val loss 4.755\n",
      "Ep 3 (Step 001810): Train loss 4.468, Val loss 4.748\n",
      "Ep 3 (Step 001820): Train loss 4.401, Val loss 4.749\n",
      "Ep 3 (Step 001830): Train loss 4.441, Val loss 4.735\n",
      "Ep 3 (Step 001840): Train loss 4.506, Val loss 4.734\n",
      "Ep 3 (Step 001850): Train loss 4.457, Val loss 4.719\n",
      "Ep 3 (Step 001860): Train loss 4.455, Val loss 4.732\n",
      "Ep 3 (Step 001870): Train loss 4.384, Val loss 4.722\n",
      "Ep 3 (Step 001880): Train loss 4.538, Val loss 4.718\n",
      "Ep 3 (Step 001890): Train loss 4.425, Val loss 4.716\n",
      "Ep 3 (Step 001900): Train loss 4.415, Val loss 4.709\n",
      "Ep 3 (Step 001910): Train loss 4.455, Val loss 4.715\n",
      "Ep 3 (Step 001920): Train loss 4.476, Val loss 4.711\n",
      "Ep 3 (Step 001930): Train loss 4.469, Val loss 4.710\n",
      "Ep 3 (Step 001940): Train loss 4.468, Val loss 4.704\n",
      "Ep 3 (Step 001950): Train loss 4.377, Val loss 4.685\n",
      "Ep 3 (Step 001960): Train loss 4.367, Val loss 4.693\n",
      "Ep 3 (Step 001970): Train loss 4.407, Val loss 4.695\n",
      "Ep 3 (Step 001980): Train loss 4.406, Val loss 4.697\n",
      "Ep 3 (Step 001990): Train loss 4.414, Val loss 4.689\n",
      "Ep 3 (Step 002000): Train loss 4.392, Val loss 4.692\n",
      "Ep 3 (Step 002010): Train loss 4.345, Val loss 4.675\n",
      "Ep 3 (Step 002020): Train loss 4.361, Val loss 4.687\n",
      "Ep 3 (Step 002030): Train loss 4.423, Val loss 4.682\n",
      "Ep 3 (Step 002040): Train loss 4.406, Val loss 4.698\n",
      "Ep 3 (Step 002050): Train loss 4.397, Val loss 4.683\n",
      "Ep 3 (Step 002060): Train loss 4.400, Val loss 4.686\n",
      "Ep 3 (Step 002070): Train loss 4.422, Val loss 4.684\n",
      "Ep 3 (Step 002080): Train loss 4.456, Val loss 4.684\n",
      "Ep 3 (Step 002090): Train loss 4.395, Val loss 4.676\n",
      "Ep 3 (Step 002100): Train loss 4.340, Val loss 4.668\n",
      "Ep 3 (Step 002110): Train loss 4.378, Val loss 4.677\n",
      "Ep 3 (Step 002120): Train loss 4.415, Val loss 4.665\n",
      "Ep 3 (Step 002130): Train loss 4.340, Val loss 4.680\n",
      "Ep 3 (Step 002140): Train loss 4.461, Val loss 4.682\n",
      "Ep 3 (Step 002150): Train loss 4.373, Val loss 4.676\n",
      "Ep 3 (Step 002160): Train loss 4.373, Val loss 4.686\n",
      "Ep 3 (Step 002170): Train loss 4.469, Val loss 4.673\n",
      "Ep 3 (Step 002180): Train loss 4.446, Val loss 4.676\n",
      "Ep 3 (Step 002190): Train loss 4.337, Val loss 4.664\n",
      "Ep 3 (Step 002200): Train loss 4.271, Val loss 4.659\n",
      "Ep 3 (Step 002210): Train loss 4.306, Val loss 4.670\n",
      "Ep 3 (Step 002220): Train loss 4.391, Val loss 4.651\n",
      "Ep 3 (Step 002230): Train loss 4.348, Val loss 4.651\n",
      "Ep 3 (Step 002240): Train loss 4.389, Val loss 4.635\n",
      "Ep 3 (Step 002250): Train loss 4.354, Val loss 4.657\n",
      "Ep 3 (Step 002260): Train loss 4.386, Val loss 4.637\n",
      "Ep 3 (Step 002270): Train loss 4.283, Val loss 4.632\n",
      "Ep 3 (Step 002280): Train loss 4.300, Val loss 4.644\n",
      "Ep 3 (Step 002290): Train loss 4.263, Val loss 4.631\n",
      "Ep 3 (Step 002300): Train loss 4.305, Val loss 4.641\n",
      "Ep 3 (Step 002310): Train loss 4.354, Val loss 4.635\n",
      "Ep 3 (Step 002320): Train loss 4.368, Val loss 4.627\n",
      "Ep 3 (Step 002330): Train loss 4.308, Val loss 4.628\n",
      "Ep 3 (Step 002340): Train loss 4.263, Val loss 4.622\n",
      "Ep 3 (Step 002350): Train loss 4.367, Val loss 4.629\n",
      "Ep 3 (Step 002360): Train loss 4.330, Val loss 4.622\n",
      "Ep 4 (Step 002370): Train loss 4.374, Val loss 4.620\n",
      "Ep 4 (Step 002380): Train loss 4.445, Val loss 4.649\n",
      "Ep 4 (Step 002390): Train loss 4.266, Val loss 4.634\n",
      "Ep 4 (Step 002400): Train loss 4.248, Val loss 4.629\n",
      "Ep 4 (Step 002410): Train loss 4.289, Val loss 4.636\n",
      "Ep 4 (Step 002420): Train loss 4.246, Val loss 4.628\n",
      "Ep 4 (Step 002430): Train loss 4.282, Val loss 4.628\n",
      "Ep 4 (Step 002440): Train loss 4.367, Val loss 4.632\n",
      "Ep 4 (Step 002450): Train loss 4.287, Val loss 4.639\n",
      "Ep 4 (Step 002460): Train loss 4.303, Val loss 4.633\n",
      "Ep 4 (Step 002470): Train loss 4.223, Val loss 4.625\n",
      "Ep 4 (Step 002480): Train loss 4.311, Val loss 4.624\n",
      "Ep 4 (Step 002490): Train loss 4.216, Val loss 4.626\n",
      "Ep 4 (Step 002500): Train loss 4.371, Val loss 4.631\n",
      "Ep 4 (Step 002510): Train loss 4.235, Val loss 4.634\n",
      "Ep 4 (Step 002520): Train loss 4.373, Val loss 4.619\n",
      "Ep 4 (Step 002530): Train loss 4.235, Val loss 4.628\n",
      "Ep 4 (Step 002540): Train loss 4.277, Val loss 4.626\n",
      "Ep 4 (Step 002550): Train loss 4.300, Val loss 4.625\n",
      "Ep 4 (Step 002560): Train loss 4.249, Val loss 4.612\n",
      "Ep 4 (Step 002570): Train loss 4.223, Val loss 4.634\n",
      "Ep 4 (Step 002580): Train loss 4.269, Val loss 4.615\n",
      "Ep 4 (Step 002590): Train loss 4.297, Val loss 4.615\n",
      "Ep 4 (Step 002600): Train loss 4.205, Val loss 4.617\n",
      "Ep 4 (Step 002610): Train loss 4.318, Val loss 4.612\n",
      "Ep 4 (Step 002620): Train loss 4.291, Val loss 4.611\n",
      "Ep 4 (Step 002630): Train loss 4.214, Val loss 4.612\n",
      "Ep 4 (Step 002640): Train loss 4.151, Val loss 4.619\n",
      "Ep 4 (Step 002650): Train loss 4.238, Val loss 4.617\n",
      "Ep 4 (Step 002660): Train loss 4.205, Val loss 4.616\n",
      "Ep 4 (Step 002670): Train loss 4.219, Val loss 4.604\n",
      "Ep 4 (Step 002680): Train loss 4.231, Val loss 4.608\n",
      "Ep 4 (Step 002690): Train loss 4.263, Val loss 4.616\n",
      "Ep 4 (Step 002700): Train loss 4.111, Val loss 4.612\n",
      "Ep 4 (Step 002710): Train loss 4.146, Val loss 4.608\n",
      "Ep 4 (Step 002720): Train loss 4.301, Val loss 4.609\n",
      "Ep 4 (Step 002730): Train loss 4.174, Val loss 4.601\n",
      "Ep 4 (Step 002740): Train loss 4.245, Val loss 4.603\n",
      "Ep 4 (Step 002750): Train loss 4.141, Val loss 4.602\n",
      "Ep 4 (Step 002760): Train loss 4.192, Val loss 4.606\n",
      "Ep 4 (Step 002770): Train loss 4.223, Val loss 4.605\n",
      "Ep 4 (Step 002780): Train loss 4.169, Val loss 4.607\n",
      "Ep 4 (Step 002790): Train loss 4.104, Val loss 4.590\n",
      "Ep 4 (Step 002800): Train loss 4.169, Val loss 4.597\n",
      "Ep 4 (Step 002810): Train loss 4.088, Val loss 4.591\n",
      "Ep 4 (Step 002820): Train loss 4.193, Val loss 4.587\n",
      "Ep 4 (Step 002830): Train loss 4.121, Val loss 4.577\n",
      "Ep 4 (Step 002840): Train loss 4.241, Val loss 4.580\n",
      "Ep 4 (Step 002850): Train loss 4.172, Val loss 4.589\n",
      "Ep 4 (Step 002860): Train loss 4.167, Val loss 4.585\n",
      "Ep 4 (Step 002870): Train loss 4.193, Val loss 4.574\n",
      "Ep 4 (Step 002880): Train loss 4.250, Val loss 4.582\n",
      "Ep 4 (Step 002890): Train loss 4.109, Val loss 4.572\n",
      "Ep 4 (Step 002900): Train loss 4.139, Val loss 4.572\n",
      "Ep 4 (Step 002910): Train loss 4.221, Val loss 4.581\n",
      "Ep 4 (Step 002920): Train loss 4.169, Val loss 4.576\n",
      "Ep 4 (Step 002930): Train loss 4.144, Val loss 4.576\n",
      "Ep 4 (Step 002940): Train loss 4.233, Val loss 4.575\n",
      "Ep 4 (Step 002950): Train loss 4.137, Val loss 4.563\n",
      "Ep 4 (Step 002960): Train loss 4.114, Val loss 4.575\n",
      "Ep 4 (Step 002970): Train loss 4.131, Val loss 4.567\n",
      "Ep 4 (Step 002980): Train loss 4.092, Val loss 4.575\n",
      "Ep 4 (Step 002990): Train loss 4.083, Val loss 4.576\n",
      "Ep 4 (Step 003000): Train loss 4.058, Val loss 4.570\n",
      "Ep 4 (Step 003010): Train loss 4.123, Val loss 4.573\n",
      "Ep 4 (Step 003020): Train loss 4.149, Val loss 4.568\n",
      "Ep 4 (Step 003030): Train loss 4.188, Val loss 4.569\n",
      "Ep 4 (Step 003040): Train loss 4.094, Val loss 4.570\n",
      "Ep 4 (Step 003050): Train loss 4.132, Val loss 4.558\n",
      "Ep 4 (Step 003060): Train loss 4.116, Val loss 4.557\n",
      "Ep 4 (Step 003070): Train loss 4.152, Val loss 4.567\n",
      "Ep 4 (Step 003080): Train loss 4.137, Val loss 4.559\n",
      "Ep 4 (Step 003090): Train loss 4.143, Val loss 4.578\n",
      "Ep 4 (Step 003100): Train loss 4.135, Val loss 4.569\n",
      "Ep 4 (Step 003110): Train loss 4.086, Val loss 4.558\n",
      "Ep 4 (Step 003120): Train loss 4.033, Val loss 4.560\n",
      "Ep 4 (Step 003130): Train loss 4.059, Val loss 4.561\n",
      "Ep 4 (Step 003140): Train loss 4.127, Val loss 4.548\n",
      "Ep 5 (Step 003150): Train loss 4.169, Val loss 4.559\n",
      "Ep 5 (Step 003160): Train loss 4.159, Val loss 4.548\n",
      "Ep 5 (Step 003170): Train loss 4.090, Val loss 4.556\n",
      "Ep 5 (Step 003180): Train loss 4.125, Val loss 4.556\n",
      "Ep 5 (Step 003190): Train loss 4.063, Val loss 4.554\n",
      "Ep 5 (Step 003200): Train loss 4.069, Val loss 4.562\n",
      "Ep 5 (Step 003210): Train loss 4.146, Val loss 4.559\n",
      "Ep 5 (Step 003220): Train loss 4.125, Val loss 4.561\n",
      "Ep 5 (Step 003230): Train loss 4.062, Val loss 4.558\n",
      "Ep 5 (Step 003240): Train loss 4.054, Val loss 4.557\n",
      "Ep 5 (Step 003250): Train loss 4.087, Val loss 4.549\n",
      "Ep 5 (Step 003260): Train loss 4.065, Val loss 4.560\n",
      "Ep 5 (Step 003270): Train loss 4.060, Val loss 4.551\n",
      "Ep 5 (Step 003280): Train loss 4.122, Val loss 4.552\n",
      "Ep 5 (Step 003290): Train loss 4.127, Val loss 4.551\n",
      "Ep 5 (Step 003300): Train loss 4.069, Val loss 4.541\n",
      "Ep 5 (Step 003310): Train loss 4.164, Val loss 4.543\n",
      "Ep 5 (Step 003320): Train loss 4.138, Val loss 4.548\n",
      "Ep 5 (Step 003330): Train loss 4.157, Val loss 4.544\n",
      "Ep 5 (Step 003340): Train loss 4.053, Val loss 4.544\n",
      "Ep 5 (Step 003350): Train loss 4.172, Val loss 4.557\n",
      "Ep 5 (Step 003360): Train loss 4.096, Val loss 4.552\n",
      "Ep 5 (Step 003370): Train loss 4.023, Val loss 4.545\n",
      "Ep 5 (Step 003380): Train loss 4.026, Val loss 4.541\n",
      "Ep 5 (Step 003390): Train loss 4.106, Val loss 4.551\n",
      "Ep 5 (Step 003400): Train loss 4.099, Val loss 4.547\n",
      "Ep 5 (Step 003410): Train loss 3.997, Val loss 4.548\n",
      "Ep 5 (Step 003420): Train loss 4.133, Val loss 4.554\n",
      "Ep 5 (Step 003430): Train loss 4.034, Val loss 4.546\n",
      "Ep 5 (Step 003440): Train loss 4.049, Val loss 4.555\n",
      "Ep 5 (Step 003450): Train loss 4.005, Val loss 4.549\n",
      "Ep 5 (Step 003460): Train loss 3.986, Val loss 4.547\n",
      "Ep 5 (Step 003470): Train loss 4.054, Val loss 4.549\n",
      "Ep 5 (Step 003480): Train loss 3.997, Val loss 4.541\n",
      "Ep 5 (Step 003490): Train loss 4.082, Val loss 4.543\n",
      "Ep 5 (Step 003500): Train loss 4.072, Val loss 4.543\n",
      "Ep 5 (Step 003510): Train loss 3.978, Val loss 4.540\n",
      "Ep 5 (Step 003520): Train loss 4.092, Val loss 4.539\n",
      "Ep 5 (Step 003530): Train loss 4.134, Val loss 4.548\n",
      "Ep 5 (Step 003540): Train loss 3.988, Val loss 4.548\n",
      "Ep 5 (Step 003550): Train loss 4.009, Val loss 4.546\n",
      "Ep 5 (Step 003560): Train loss 3.956, Val loss 4.544\n",
      "Ep 5 (Step 003570): Train loss 4.034, Val loss 4.536\n",
      "Ep 5 (Step 003580): Train loss 3.964, Val loss 4.535\n",
      "Ep 5 (Step 003590): Train loss 4.024, Val loss 4.538\n",
      "Ep 5 (Step 003600): Train loss 4.065, Val loss 4.536\n",
      "Ep 5 (Step 003610): Train loss 4.034, Val loss 4.531\n",
      "Ep 5 (Step 003620): Train loss 4.020, Val loss 4.533\n",
      "Ep 5 (Step 003630): Train loss 4.023, Val loss 4.536\n",
      "Ep 5 (Step 003640): Train loss 4.059, Val loss 4.534\n",
      "Ep 5 (Step 003650): Train loss 4.090, Val loss 4.529\n",
      "Ep 5 (Step 003660): Train loss 4.066, Val loss 4.529\n",
      "Ep 5 (Step 003670): Train loss 4.068, Val loss 4.534\n",
      "Ep 5 (Step 003680): Train loss 4.008, Val loss 4.536\n",
      "Ep 5 (Step 003690): Train loss 4.048, Val loss 4.532\n",
      "Ep 5 (Step 003700): Train loss 4.054, Val loss 4.529\n",
      "Ep 5 (Step 003710): Train loss 4.019, Val loss 4.527\n",
      "Ep 5 (Step 003720): Train loss 4.008, Val loss 4.527\n",
      "Ep 5 (Step 003730): Train loss 3.983, Val loss 4.531\n",
      "Ep 5 (Step 003740): Train loss 3.989, Val loss 4.528\n",
      "Ep 5 (Step 003750): Train loss 4.009, Val loss 4.528\n",
      "Ep 5 (Step 003760): Train loss 4.011, Val loss 4.529\n",
      "Ep 5 (Step 003770): Train loss 4.009, Val loss 4.527\n",
      "Ep 5 (Step 003780): Train loss 4.129, Val loss 4.529\n",
      "Ep 5 (Step 003790): Train loss 4.055, Val loss 4.532\n",
      "Ep 5 (Step 003800): Train loss 4.036, Val loss 4.531\n",
      "Ep 5 (Step 003810): Train loss 3.998, Val loss 4.525\n",
      "Ep 5 (Step 003820): Train loss 4.013, Val loss 4.525\n",
      "Ep 5 (Step 003830): Train loss 4.023, Val loss 4.524\n",
      "Ep 5 (Step 003840): Train loss 3.970, Val loss 4.523\n",
      "Ep 5 (Step 003850): Train loss 4.036, Val loss 4.526\n",
      "Ep 5 (Step 003860): Train loss 4.063, Val loss 4.525\n",
      "Ep 5 (Step 003870): Train loss 4.029, Val loss 4.529\n",
      "Ep 5 (Step 003880): Train loss 4.102, Val loss 4.528\n",
      "Ep 5 (Step 003890): Train loss 3.950, Val loss 4.523\n",
      "Ep 5 (Step 003900): Train loss 3.989, Val loss 4.526\n",
      "Ep 5 (Step 003910): Train loss 4.079, Val loss 4.527\n",
      "Ep 5 (Step 003920): Train loss 4.066, Val loss 4.527\n",
      "Ep 5 (Step 003930): Train loss 3.985, Val loss 4.527\n",
      "Ep 6 (Step 003940): Train loss 3.956, Val loss 4.527\n",
      "Ep 6 (Step 003950): Train loss 4.023, Val loss 4.526\n",
      "Ep 6 (Step 003960): Train loss 3.973, Val loss 4.525\n",
      "Ep 6 (Step 003970): Train loss 4.024, Val loss 4.525\n",
      "Ep 6 (Step 003980): Train loss 3.962, Val loss 4.527\n",
      "Ep 6 (Step 003990): Train loss 3.943, Val loss 4.526\n",
      "Ep 6 (Step 004000): Train loss 4.062, Val loss 4.527\n",
      "Ep 6 (Step 004010): Train loss 3.931, Val loss 4.526\n",
      "Ep 6 (Step 004020): Train loss 3.985, Val loss 4.525\n",
      "Ep 6 (Step 004030): Train loss 3.978, Val loss 4.526\n",
      "Ep 6 (Step 004040): Train loss 3.986, Val loss 4.526\n",
      "Ep 6 (Step 004050): Train loss 3.941, Val loss 4.529\n",
      "Ep 6 (Step 004060): Train loss 3.974, Val loss 4.529\n",
      "Ep 6 (Step 004070): Train loss 4.047, Val loss 4.529\n",
      "Ep 6 (Step 004080): Train loss 4.066, Val loss 4.526\n",
      "Ep 6 (Step 004090): Train loss 4.008, Val loss 4.523\n",
      "Ep 6 (Step 004100): Train loss 3.917, Val loss 4.525\n",
      "Ep 6 (Step 004110): Train loss 3.979, Val loss 4.527\n",
      "Ep 6 (Step 004120): Train loss 4.020, Val loss 4.529\n",
      "Ep 6 (Step 004130): Train loss 4.043, Val loss 4.529\n",
      "Ep 6 (Step 004140): Train loss 3.999, Val loss 4.530\n",
      "Ep 6 (Step 004150): Train loss 3.979, Val loss 4.531\n",
      "Ep 6 (Step 004160): Train loss 4.000, Val loss 4.532\n",
      "Ep 6 (Step 004170): Train loss 4.038, Val loss 4.529\n",
      "Ep 6 (Step 004180): Train loss 3.967, Val loss 4.527\n",
      "Ep 6 (Step 004190): Train loss 4.036, Val loss 4.527\n",
      "Ep 6 (Step 004200): Train loss 3.895, Val loss 4.527\n",
      "Ep 6 (Step 004210): Train loss 3.996, Val loss 4.529\n",
      "Ep 6 (Step 004220): Train loss 3.905, Val loss 4.530\n",
      "Ep 6 (Step 004230): Train loss 3.946, Val loss 4.530\n",
      "Ep 6 (Step 004240): Train loss 4.046, Val loss 4.529\n",
      "Ep 6 (Step 004250): Train loss 3.942, Val loss 4.528\n",
      "Ep 6 (Step 004260): Train loss 3.963, Val loss 4.527\n",
      "Ep 6 (Step 004270): Train loss 4.008, Val loss 4.527\n",
      "Ep 6 (Step 004280): Train loss 3.985, Val loss 4.528\n",
      "Ep 6 (Step 004290): Train loss 3.978, Val loss 4.528\n",
      "Ep 6 (Step 004300): Train loss 4.019, Val loss 4.528\n",
      "Ep 6 (Step 004310): Train loss 4.132, Val loss 4.528\n",
      "Ep 6 (Step 004320): Train loss 3.952, Val loss 4.527\n",
      "Ep 6 (Step 004330): Train loss 4.020, Val loss 4.527\n",
      "Ep 6 (Step 004340): Train loss 4.027, Val loss 4.527\n",
      "Ep 6 (Step 004350): Train loss 3.981, Val loss 4.527\n",
      "Ep 6 (Step 004360): Train loss 3.976, Val loss 4.526\n",
      "Ep 6 (Step 004370): Train loss 4.007, Val loss 4.525\n",
      "Ep 6 (Step 004380): Train loss 4.052, Val loss 4.525\n",
      "Ep 6 (Step 004390): Train loss 3.958, Val loss 4.525\n",
      "Ep 6 (Step 004400): Train loss 3.975, Val loss 4.525\n",
      "Ep 6 (Step 004410): Train loss 3.948, Val loss 4.525\n",
      "Ep 6 (Step 004420): Train loss 4.003, Val loss 4.525\n",
      "Ep 6 (Step 004430): Train loss 4.067, Val loss 4.525\n",
      "Ep 6 (Step 004440): Train loss 3.950, Val loss 4.525\n",
      "Ep 6 (Step 004450): Train loss 3.981, Val loss 4.525\n",
      "Ep 6 (Step 004460): Train loss 4.012, Val loss 4.525\n",
      "Ep 6 (Step 004470): Train loss 4.006, Val loss 4.525\n",
      "Ep 6 (Step 004480): Train loss 3.970, Val loss 4.525\n",
      "Ep 6 (Step 004490): Train loss 4.006, Val loss 4.525\n",
      "Ep 6 (Step 004500): Train loss 4.013, Val loss 4.525\n",
      "Ep 6 (Step 004510): Train loss 4.010, Val loss 4.524\n",
      "Ep 6 (Step 004520): Train loss 3.952, Val loss 4.524\n",
      "Ep 6 (Step 004530): Train loss 3.918, Val loss 4.525\n",
      "Ep 6 (Step 004540): Train loss 4.013, Val loss 4.525\n",
      "Ep 6 (Step 004550): Train loss 3.944, Val loss 4.525\n",
      "Ep 6 (Step 004560): Train loss 4.040, Val loss 4.525\n",
      "Ep 6 (Step 004570): Train loss 3.978, Val loss 4.525\n",
      "Ep 6 (Step 004580): Train loss 3.994, Val loss 4.525\n",
      "Ep 6 (Step 004590): Train loss 4.031, Val loss 4.524\n",
      "Ep 6 (Step 004600): Train loss 3.971, Val loss 4.524\n",
      "Ep 6 (Step 004610): Train loss 4.001, Val loss 4.524\n",
      "Ep 6 (Step 004620): Train loss 4.008, Val loss 4.524\n",
      "Ep 6 (Step 004630): Train loss 3.958, Val loss 4.524\n",
      "Ep 6 (Step 004640): Train loss 4.001, Val loss 4.524\n",
      "Ep 6 (Step 004650): Train loss 3.964, Val loss 4.524\n",
      "Ep 6 (Step 004660): Train loss 4.000, Val loss 4.524\n",
      "Ep 6 (Step 004670): Train loss 3.987, Val loss 4.524\n",
      "Ep 6 (Step 004680): Train loss 3.961, Val loss 4.524\n",
      "Ep 6 (Step 004690): Train loss 3.979, Val loss 4.524\n",
      "Ep 6 (Step 004700): Train loss 3.953, Val loss 4.524\n",
      "Ep 6 (Step 004710): Train loss 3.991, Val loss 4.524\n",
      "Ep 6 (Step 004720): Train loss 4.015, Val loss 4.524\n",
      "Training completed in 10.89 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2 (Step 001550): Train loss 4.604, Val loss 4.767\n",
      "Ep 2 (Step 001560): Train loss 4.653, Val loss 4.783\n",
      "Ep 3 (Step 001570): Train loss 4.536, Val loss 4.772\n",
      "Ep 3 (Step 001580): Train loss 4.589, Val loss 4.784\n",
      "Ep 3 (Step 001590): Train loss 4.592, Val loss 4.776\n",
      "Ep 3 (Step 001600): Train loss 4.612, Val loss 4.761\n",
      "Ep 3 (Step 001610): Train loss 4.515, Val loss 4.773\n",
      "Ep 3 (Step 001620): Train loss 4.599, Val loss 4.757\n",
      "Ep 3 (Step 001630): Train loss 4.552, Val loss 4.757\n",
      "Ep 3 (Step 001640): Train loss 4.616, Val loss 4.763\n",
      "Ep 3 (Step 001650): Train loss 4.558, Val loss 4.777\n",
      "Ep 3 (Step 001660): Train loss 4.560, Val loss 4.761\n",
      "Ep 3 (Step 001670): Train loss 4.557, Val loss 4.762\n",
      "Ep 3 (Step 001680): Train loss 4.645, Val loss 4.753\n",
      "Ep 3 (Step 001690): Train loss 4.505, Val loss 4.741\n",
      "Ep 3 (Step 001700): Train loss 4.547, Val loss 4.754\n",
      "Ep 3 (Step 001710): Train loss 4.574, Val loss 4.757\n",
      "Ep 3 (Step 001720): Train loss 4.502, Val loss 4.750\n",
      "Ep 3 (Step 001730): Train loss 4.505, Val loss 4.728\n",
      "Ep 3 (Step 001740): Train loss 4.451, Val loss 4.751\n",
      "Ep 3 (Step 001750): Train loss 4.405, Val loss 4.756\n",
      "Ep 3 (Step 001760): Train loss 4.442, Val loss 4.760\n",
      "Ep 3 (Step 001770): Train loss 4.451, Val loss 4.741\n",
      "Ep 3 (Step 001780): Train loss 4.542, Val loss 4.750\n",
      "Ep 3 (Step 001790): Train loss 4.411, Val loss 4.740\n",
      "Ep 3 (Step 001800): Train loss 4.481, Val loss 4.736\n",
      "Ep 3 (Step 001810): Train loss 4.420, Val loss 4.738\n",
      "Ep 3 (Step 001820): Train loss 4.432, Val loss 4.725\n",
      "Ep 3 (Step 001830): Train loss 4.436, Val loss 4.749\n",
      "Ep 3 (Step 001840): Train loss 4.393, Val loss 4.730\n",
      "Ep 3 (Step 001850): Train loss 4.483, Val loss 4.728\n",
      "Ep 3 (Step 001860): Train loss 4.458, Val loss 4.743\n",
      "Ep 3 (Step 001870): Train loss 4.340, Val loss 4.740\n",
      "Ep 3 (Step 001880): Train loss 4.400, Val loss 4.716\n",
      "Ep 3 (Step 001890): Train loss 4.543, Val loss 4.732\n",
      "Ep 3 (Step 001900): Train loss 4.423, Val loss 4.712\n",
      "Ep 3 (Step 001910): Train loss 4.496, Val loss 4.714\n",
      "Ep 3 (Step 001920): Train loss 4.436, Val loss 4.707\n",
      "Ep 3 (Step 001930): Train loss 4.494, Val loss 4.707\n",
      "Ep 3 (Step 001940): Train loss 4.406, Val loss 4.718\n",
      "Ep 3 (Step 001950): Train loss 4.401, Val loss 4.713\n",
      "Ep 3 (Step 001960): Train loss 4.426, Val loss 4.717\n",
      "Ep 3 (Step 001970): Train loss 4.444, Val loss 4.707\n",
      "Ep 3 (Step 001980): Train loss 4.489, Val loss 4.701\n",
      "Ep 3 (Step 001990): Train loss 4.378, Val loss 4.708\n",
      "Ep 3 (Step 002000): Train loss 4.489, Val loss 4.710\n",
      "Ep 3 (Step 002010): Train loss 4.348, Val loss 4.688\n",
      "Ep 3 (Step 002020): Train loss 4.414, Val loss 4.686\n",
      "Ep 3 (Step 002030): Train loss 4.357, Val loss 4.693\n",
      "Ep 3 (Step 002040): Train loss 4.500, Val loss 4.696\n",
      "Ep 3 (Step 002050): Train loss 4.355, Val loss 4.703\n"
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=6,\n",
    "      eval_iter=10, model_prefix=\"model_768_12_12\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8539769d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.187, Val loss 10.130\n",
      "Ep 1 (Step 000010): Train loss 8.149, Val loss 8.061\n",
      "Ep 1 (Step 000020): Train loss 6.993, Val loss 6.950\n",
      "Ep 1 (Step 000030): Train loss 6.759, Val loss 6.643\n",
      "Ep 1 (Step 000040): Train loss 6.492, Val loss 6.514\n",
      "Ep 1 (Step 000050): Train loss 6.360, Val loss 6.387\n",
      "Ep 1 (Step 000060): Train loss 6.197, Val loss 6.259\n",
      "Ep 1 (Step 000070): Train loss 6.069, Val loss 6.151\n",
      "Ep 1 (Step 000080): Train loss 6.023, Val loss 6.030\n",
      "Ep 1 (Step 000090): Train loss 5.912, Val loss 5.981\n",
      "Ep 1 (Step 000100): Train loss 5.880, Val loss 5.877\n",
      "Ep 1 (Step 000110): Train loss 5.740, Val loss 5.806\n",
      "Ep 1 (Step 000120): Train loss 5.697, Val loss 5.740\n",
      "Ep 1 (Step 000130): Train loss 5.682, Val loss 5.696\n",
      "Ep 1 (Step 000140): Train loss 5.636, Val loss 5.658\n",
      "Ep 1 (Step 000150): Train loss 5.696, Val loss 5.644\n",
      "Ep 1 (Step 000160): Train loss 5.498, Val loss 5.630\n",
      "Ep 1 (Step 000170): Train loss 5.555, Val loss 5.595\n",
      "Ep 1 (Step 000180): Train loss 5.470, Val loss 5.548\n",
      "Ep 1 (Step 000190): Train loss 5.389, Val loss 5.497\n",
      "Ep 1 (Step 000200): Train loss 5.379, Val loss 5.469\n",
      "Ep 1 (Step 000210): Train loss 5.366, Val loss 5.472\n",
      "Ep 1 (Step 000220): Train loss 5.333, Val loss 5.469\n",
      "Ep 1 (Step 000230): Train loss 5.286, Val loss 5.420\n",
      "Ep 1 (Step 000240): Train loss 5.310, Val loss 5.384\n",
      "Ep 1 (Step 000250): Train loss 5.277, Val loss 5.345\n",
      "Ep 1 (Step 000260): Train loss 5.327, Val loss 5.324\n",
      "Ep 1 (Step 000270): Train loss 5.269, Val loss 5.317\n",
      "Ep 1 (Step 000280): Train loss 5.274, Val loss 5.311\n",
      "Ep 1 (Step 000290): Train loss 5.394, Val loss 5.274\n",
      "Ep 1 (Step 000300): Train loss 5.235, Val loss 5.251\n",
      "Ep 1 (Step 000310): Train loss 5.233, Val loss 5.217\n",
      "Ep 1 (Step 000320): Train loss 5.197, Val loss 5.218\n",
      "Ep 1 (Step 000330): Train loss 5.222, Val loss 5.223\n",
      "Ep 1 (Step 000340): Train loss 5.263, Val loss 5.166\n",
      "Ep 1 (Step 000350): Train loss 5.068, Val loss 5.136\n",
      "Ep 1 (Step 000360): Train loss 5.246, Val loss 5.141\n",
      "Ep 1 (Step 000370): Train loss 5.211, Val loss 5.108\n",
      "Ep 1 (Step 000380): Train loss 5.129, Val loss 5.091\n",
      "Ep 1 (Step 000390): Train loss 5.063, Val loss 5.088\n",
      "Ep 1 (Step 000400): Train loss 5.053, Val loss 5.091\n",
      "Ep 1 (Step 000410): Train loss 5.084, Val loss 5.073\n",
      "Ep 1 (Step 000420): Train loss 5.035, Val loss 5.045\n",
      "Ep 1 (Step 000430): Train loss 5.056, Val loss 5.034\n",
      "Ep 1 (Step 000440): Train loss 5.131, Val loss 5.010\n",
      "Ep 1 (Step 000450): Train loss 5.167, Val loss 5.006\n",
      "Ep 1 (Step 000460): Train loss 4.995, Val loss 5.016\n",
      "Ep 1 (Step 000470): Train loss 4.967, Val loss 5.023\n",
      "Ep 1 (Step 000480): Train loss 5.026, Val loss 4.997\n",
      "Ep 1 (Step 000490): Train loss 4.971, Val loss 4.974\n",
      "Ep 1 (Step 000500): Train loss 4.961, Val loss 4.963\n",
      "Ep 1 (Step 000510): Train loss 4.931, Val loss 4.950\n",
      "Ep 1 (Step 000520): Train loss 4.993, Val loss 4.966\n",
      "Ep 1 (Step 000530): Train loss 4.927, Val loss 4.946\n",
      "Ep 1 (Step 000540): Train loss 5.103, Val loss 4.938\n",
      "Ep 1 (Step 000550): Train loss 4.957, Val loss 4.940\n",
      "Ep 1 (Step 000560): Train loss 4.956, Val loss 4.964\n",
      "Ep 1 (Step 000570): Train loss 4.965, Val loss 4.939\n",
      "Ep 1 (Step 000580): Train loss 5.004, Val loss 4.920\n",
      "Ep 1 (Step 000590): Train loss 4.943, Val loss 4.931\n",
      "Ep 1 (Step 000600): Train loss 5.034, Val loss 4.928\n",
      "Ep 1 (Step 000610): Train loss 5.001, Val loss 4.906\n",
      "Ep 1 (Step 000620): Train loss 4.861, Val loss 4.912\n",
      "Ep 1 (Step 000630): Train loss 4.837, Val loss 4.889\n",
      "Ep 1 (Step 000640): Train loss 4.859, Val loss 4.898\n",
      "Ep 1 (Step 000650): Train loss 4.901, Val loss 4.894\n",
      "Ep 1 (Step 000660): Train loss 4.782, Val loss 4.882\n",
      "Ep 1 (Step 000670): Train loss 4.900, Val loss 4.894\n",
      "Ep 1 (Step 000680): Train loss 4.832, Val loss 4.886\n",
      "Ep 1 (Step 000690): Train loss 4.821, Val loss 4.900\n",
      "Ep 1 (Step 000700): Train loss 4.877, Val loss 4.909\n",
      "Ep 1 (Step 000710): Train loss 4.833, Val loss 4.899\n",
      "Ep 1 (Step 000720): Train loss 4.792, Val loss 4.895\n",
      "Ep 1 (Step 000730): Train loss 4.887, Val loss 4.903\n",
      "Ep 1 (Step 000740): Train loss 4.850, Val loss 4.896\n",
      "Ep 1 (Step 000750): Train loss 4.927, Val loss 4.877\n",
      "Ep 1 (Step 000760): Train loss 4.724, Val loss 4.839\n",
      "Ep 1 (Step 000770): Train loss 4.810, Val loss 4.845\n",
      "Ep 1 (Step 000780): Train loss 4.832, Val loss 4.844\n",
      "Ep 1 (Step 000790): Train loss 4.783, Val loss 4.834\n",
      "Ep 1 (Step 000800): Train loss 4.776, Val loss 4.838\n",
      "Ep 1 (Step 000810): Train loss 4.790, Val loss 4.825\n",
      "Ep 1 (Step 000820): Train loss 4.837, Val loss 4.816\n",
      "Ep 2 (Step 000830): Train loss 4.805, Val loss 4.806\n",
      "Ep 2 (Step 000840): Train loss 4.774, Val loss 4.798\n",
      "Ep 2 (Step 000850): Train loss 4.794, Val loss 4.828\n",
      "Ep 2 (Step 000860): Train loss 4.792, Val loss 4.827\n",
      "Ep 2 (Step 000870): Train loss 4.701, Val loss 4.809\n",
      "Ep 2 (Step 000880): Train loss 4.726, Val loss 4.789\n",
      "Ep 2 (Step 000890): Train loss 4.710, Val loss 4.788\n",
      "Ep 2 (Step 000900): Train loss 4.682, Val loss 4.779\n",
      "Ep 2 (Step 000910): Train loss 4.726, Val loss 4.790\n",
      "Ep 2 (Step 000920): Train loss 4.687, Val loss 4.779\n",
      "Ep 2 (Step 000930): Train loss 4.650, Val loss 4.777\n",
      "Ep 2 (Step 000940): Train loss 4.702, Val loss 4.778\n",
      "Ep 2 (Step 000950): Train loss 4.595, Val loss 4.776\n",
      "Ep 2 (Step 000960): Train loss 4.618, Val loss 4.754\n",
      "Ep 2 (Step 000970): Train loss 4.691, Val loss 4.759\n",
      "Ep 2 (Step 000980): Train loss 4.678, Val loss 4.752\n",
      "Ep 2 (Step 000990): Train loss 4.776, Val loss 4.748\n",
      "Ep 2 (Step 001000): Train loss 4.586, Val loss 4.753\n",
      "Ep 2 (Step 001010): Train loss 4.618, Val loss 4.744\n",
      "Ep 2 (Step 001020): Train loss 4.787, Val loss 4.765\n",
      "Ep 2 (Step 001030): Train loss 4.678, Val loss 4.747\n",
      "Ep 2 (Step 001040): Train loss 4.624, Val loss 4.734\n",
      "Ep 2 (Step 001050): Train loss 4.716, Val loss 4.739\n",
      "Ep 2 (Step 001060): Train loss 4.580, Val loss 4.735\n",
      "Ep 2 (Step 001070): Train loss 4.603, Val loss 4.721\n",
      "Ep 2 (Step 001080): Train loss 4.581, Val loss 4.738\n",
      "Ep 2 (Step 001090): Train loss 4.640, Val loss 4.738\n",
      "Ep 2 (Step 001100): Train loss 4.644, Val loss 4.718\n",
      "Ep 2 (Step 001110): Train loss 4.681, Val loss 4.726\n",
      "Ep 2 (Step 001120): Train loss 4.617, Val loss 4.731\n",
      "Ep 2 (Step 001130): Train loss 4.593, Val loss 4.724\n",
      "Ep 2 (Step 001140): Train loss 4.635, Val loss 4.723\n",
      "Ep 2 (Step 001150): Train loss 4.544, Val loss 4.719\n",
      "Ep 2 (Step 001160): Train loss 4.532, Val loss 4.698\n",
      "Ep 2 (Step 001170): Train loss 4.499, Val loss 4.699\n",
      "Ep 2 (Step 001180): Train loss 4.569, Val loss 4.688\n",
      "Ep 2 (Step 001190): Train loss 4.604, Val loss 4.698\n",
      "Ep 2 (Step 001200): Train loss 4.601, Val loss 4.701\n",
      "Ep 2 (Step 001210): Train loss 4.552, Val loss 4.691\n",
      "Ep 2 (Step 001220): Train loss 4.540, Val loss 4.686\n",
      "Ep 2 (Step 001230): Train loss 4.567, Val loss 4.691\n",
      "Ep 2 (Step 001240): Train loss 4.499, Val loss 4.677\n",
      "Ep 2 (Step 001250): Train loss 4.498, Val loss 4.674\n",
      "Ep 2 (Step 001260): Train loss 4.534, Val loss 4.667\n",
      "Ep 2 (Step 001270): Train loss 4.514, Val loss 4.662\n",
      "Ep 2 (Step 001280): Train loss 4.443, Val loss 4.669\n",
      "Ep 2 (Step 001290): Train loss 4.541, Val loss 4.672\n",
      "Ep 2 (Step 001300): Train loss 4.507, Val loss 4.678\n",
      "Ep 2 (Step 001310): Train loss 4.520, Val loss 4.665\n",
      "Ep 2 (Step 001320): Train loss 4.589, Val loss 4.663\n",
      "Ep 2 (Step 001330): Train loss 4.521, Val loss 4.649\n",
      "Ep 2 (Step 001340): Train loss 4.475, Val loss 4.645\n",
      "Ep 2 (Step 001350): Train loss 4.498, Val loss 4.651\n",
      "Ep 2 (Step 001360): Train loss 4.513, Val loss 4.649\n",
      "Ep 2 (Step 001370): Train loss 4.539, Val loss 4.643\n",
      "Ep 2 (Step 001380): Train loss 4.526, Val loss 4.634\n",
      "Ep 2 (Step 001390): Train loss 4.556, Val loss 4.633\n",
      "Ep 2 (Step 001400): Train loss 4.470, Val loss 4.645\n",
      "Ep 2 (Step 001410): Train loss 4.528, Val loss 4.629\n",
      "Ep 2 (Step 001420): Train loss 4.564, Val loss 4.624\n",
      "Ep 2 (Step 001430): Train loss 4.548, Val loss 4.624\n",
      "Ep 2 (Step 001440): Train loss 4.500, Val loss 4.626\n",
      "Ep 2 (Step 001450): Train loss 4.440, Val loss 4.620\n",
      "Ep 2 (Step 001460): Train loss 4.436, Val loss 4.615\n",
      "Ep 2 (Step 001470): Train loss 4.407, Val loss 4.603\n",
      "Ep 2 (Step 001480): Train loss 4.483, Val loss 4.592\n",
      "Ep 2 (Step 001490): Train loss 4.401, Val loss 4.598\n",
      "Ep 2 (Step 001500): Train loss 4.383, Val loss 4.583\n",
      "Ep 2 (Step 001510): Train loss 4.412, Val loss 4.594\n",
      "Ep 2 (Step 001520): Train loss 4.435, Val loss 4.594\n",
      "Ep 2 (Step 001530): Train loss 4.451, Val loss 4.590\n",
      "Ep 2 (Step 001540): Train loss 4.433, Val loss 4.589\n",
      "Ep 2 (Step 001550): Train loss 4.403, Val loss 4.585\n",
      "Ep 2 (Step 001560): Train loss 4.401, Val loss 4.580\n",
      "Ep 2 (Step 001570): Train loss 4.376, Val loss 4.572\n",
      "Ep 2 (Step 001580): Train loss 4.361, Val loss 4.563\n",
      "Ep 2 (Step 001590): Train loss 4.412, Val loss 4.579\n",
      "Ep 2 (Step 001600): Train loss 4.435, Val loss 4.585\n",
      "Ep 2 (Step 001610): Train loss 4.472, Val loss 4.577\n",
      "Ep 2 (Step 001620): Train loss 4.427, Val loss 4.585\n",
      "Ep 2 (Step 001630): Train loss 4.430, Val loss 4.577\n",
      "Ep 2 (Step 001640): Train loss 4.462, Val loss 4.561\n",
      "Ep 3 (Step 001650): Train loss 4.357, Val loss 4.560\n",
      "Ep 3 (Step 001660): Train loss 4.412, Val loss 4.568\n",
      "Ep 3 (Step 001670): Train loss 4.424, Val loss 4.560\n",
      "Ep 3 (Step 001680): Train loss 4.356, Val loss 4.572\n",
      "Ep 3 (Step 001690): Train loss 4.354, Val loss 4.558\n",
      "Ep 3 (Step 001700): Train loss 4.418, Val loss 4.545\n",
      "Ep 3 (Step 001710): Train loss 4.367, Val loss 4.558\n",
      "Ep 3 (Step 001720): Train loss 4.451, Val loss 4.547\n",
      "Ep 3 (Step 001730): Train loss 4.412, Val loss 4.553\n",
      "Ep 3 (Step 001740): Train loss 4.346, Val loss 4.540\n",
      "Ep 3 (Step 001750): Train loss 4.359, Val loss 4.540\n",
      "Ep 3 (Step 001760): Train loss 4.312, Val loss 4.528\n",
      "Ep 3 (Step 001770): Train loss 4.292, Val loss 4.528\n",
      "Ep 3 (Step 001780): Train loss 4.353, Val loss 4.540\n",
      "Ep 3 (Step 001790): Train loss 4.368, Val loss 4.542\n",
      "Ep 3 (Step 001800): Train loss 4.254, Val loss 4.536\n",
      "Ep 3 (Step 001810): Train loss 4.383, Val loss 4.542\n",
      "Ep 3 (Step 001820): Train loss 4.311, Val loss 4.523\n",
      "Ep 3 (Step 001830): Train loss 4.356, Val loss 4.523\n",
      "Ep 3 (Step 001840): Train loss 4.270, Val loss 4.523\n",
      "Ep 3 (Step 001850): Train loss 4.350, Val loss 4.520\n",
      "Ep 3 (Step 001860): Train loss 4.407, Val loss 4.523\n",
      "Ep 3 (Step 001870): Train loss 4.398, Val loss 4.525\n",
      "Ep 3 (Step 001880): Train loss 4.202, Val loss 4.529\n",
      "Ep 3 (Step 001890): Train loss 4.382, Val loss 4.522\n",
      "Ep 3 (Step 001900): Train loss 4.212, Val loss 4.516\n",
      "Ep 3 (Step 001910): Train loss 4.231, Val loss 4.521\n",
      "Ep 3 (Step 001920): Train loss 4.290, Val loss 4.515\n",
      "Ep 3 (Step 001930): Train loss 4.256, Val loss 4.506\n",
      "Ep 3 (Step 001940): Train loss 4.235, Val loss 4.502\n",
      "Ep 3 (Step 001950): Train loss 4.276, Val loss 4.508\n",
      "Ep 3 (Step 001960): Train loss 4.258, Val loss 4.499\n",
      "Ep 3 (Step 001970): Train loss 4.234, Val loss 4.504\n",
      "Ep 3 (Step 001980): Train loss 4.225, Val loss 4.499\n",
      "Ep 3 (Step 001990): Train loss 4.368, Val loss 4.504\n",
      "Ep 3 (Step 002000): Train loss 4.188, Val loss 4.509\n",
      "Ep 3 (Step 002010): Train loss 4.312, Val loss 4.499\n",
      "Ep 3 (Step 002020): Train loss 4.278, Val loss 4.504\n",
      "Ep 3 (Step 002030): Train loss 4.207, Val loss 4.497\n",
      "Ep 3 (Step 002040): Train loss 4.316, Val loss 4.504\n",
      "Ep 3 (Step 002050): Train loss 4.235, Val loss 4.500\n",
      "Ep 3 (Step 002060): Train loss 4.193, Val loss 4.496\n",
      "Ep 3 (Step 002070): Train loss 4.276, Val loss 4.492\n",
      "Ep 3 (Step 002080): Train loss 4.235, Val loss 4.481\n",
      "Ep 3 (Step 002090): Train loss 4.246, Val loss 4.475\n",
      "Ep 3 (Step 002100): Train loss 4.257, Val loss 4.491\n",
      "Ep 3 (Step 002110): Train loss 4.228, Val loss 4.479\n",
      "Ep 3 (Step 002120): Train loss 4.281, Val loss 4.486\n",
      "Ep 3 (Step 002130): Train loss 4.273, Val loss 4.482\n",
      "Ep 3 (Step 002140): Train loss 4.262, Val loss 4.482\n",
      "Ep 3 (Step 002150): Train loss 4.216, Val loss 4.476\n",
      "Ep 3 (Step 002160): Train loss 4.137, Val loss 4.484\n",
      "Ep 3 (Step 002170): Train loss 4.180, Val loss 4.474\n",
      "Ep 3 (Step 002180): Train loss 4.279, Val loss 4.479\n",
      "Ep 3 (Step 002190): Train loss 4.207, Val loss 4.470\n",
      "Ep 3 (Step 002200): Train loss 4.182, Val loss 4.477\n",
      "Ep 3 (Step 002210): Train loss 4.191, Val loss 4.465\n",
      "Ep 3 (Step 002220): Train loss 4.157, Val loss 4.455\n",
      "Ep 3 (Step 002230): Train loss 4.140, Val loss 4.462\n",
      "Ep 3 (Step 002240): Train loss 4.225, Val loss 4.465\n",
      "Ep 3 (Step 002250): Train loss 4.202, Val loss 4.447\n",
      "Ep 3 (Step 002260): Train loss 4.068, Val loss 4.446\n",
      "Ep 3 (Step 002270): Train loss 4.271, Val loss 4.438\n",
      "Ep 3 (Step 002280): Train loss 4.215, Val loss 4.445\n",
      "Ep 3 (Step 002290): Train loss 4.152, Val loss 4.443\n",
      "Ep 3 (Step 002300): Train loss 4.163, Val loss 4.443\n",
      "Ep 3 (Step 002310): Train loss 4.255, Val loss 4.451\n",
      "Ep 3 (Step 002320): Train loss 4.150, Val loss 4.430\n",
      "Ep 3 (Step 002330): Train loss 4.211, Val loss 4.450\n",
      "Ep 3 (Step 002340): Train loss 4.181, Val loss 4.429\n",
      "Ep 3 (Step 002350): Train loss 4.204, Val loss 4.419\n",
      "Ep 3 (Step 002360): Train loss 4.166, Val loss 4.429\n",
      "Ep 3 (Step 002370): Train loss 4.171, Val loss 4.429\n",
      "Ep 3 (Step 002380): Train loss 4.184, Val loss 4.426\n",
      "Ep 3 (Step 002390): Train loss 4.169, Val loss 4.420\n",
      "Ep 3 (Step 002400): Train loss 4.111, Val loss 4.425\n",
      "Ep 3 (Step 002410): Train loss 4.082, Val loss 4.414\n",
      "Ep 3 (Step 002420): Train loss 4.168, Val loss 4.427\n",
      "Ep 3 (Step 002430): Train loss 4.142, Val loss 4.413\n",
      "Ep 3 (Step 002440): Train loss 4.189, Val loss 4.431\n",
      "Ep 3 (Step 002450): Train loss 4.116, Val loss 4.419\n",
      "Ep 3 (Step 002460): Train loss 4.084, Val loss 4.420\n",
      "Ep 4 (Step 002470): Train loss 4.077, Val loss 4.414\n",
      "Ep 4 (Step 002480): Train loss 4.130, Val loss 4.417\n",
      "Ep 4 (Step 002490): Train loss 4.098, Val loss 4.414\n",
      "Ep 4 (Step 002500): Train loss 4.256, Val loss 4.417\n",
      "Ep 4 (Step 002510): Train loss 4.158, Val loss 4.410\n",
      "Ep 4 (Step 002520): Train loss 4.132, Val loss 4.399\n",
      "Ep 4 (Step 002530): Train loss 4.016, Val loss 4.403\n",
      "Ep 4 (Step 002540): Train loss 4.109, Val loss 4.404\n",
      "Ep 4 (Step 002550): Train loss 4.141, Val loss 4.413\n",
      "Ep 4 (Step 002560): Train loss 4.125, Val loss 4.411\n",
      "Ep 4 (Step 002570): Train loss 4.056, Val loss 4.406\n",
      "Ep 4 (Step 002580): Train loss 4.081, Val loss 4.408\n",
      "Ep 4 (Step 002590): Train loss 4.073, Val loss 4.406\n",
      "Ep 4 (Step 002600): Train loss 4.155, Val loss 4.402\n",
      "Ep 4 (Step 002610): Train loss 4.054, Val loss 4.398\n",
      "Ep 4 (Step 002620): Train loss 4.033, Val loss 4.397\n",
      "Ep 4 (Step 002630): Train loss 4.054, Val loss 4.391\n",
      "Ep 4 (Step 002640): Train loss 4.103, Val loss 4.398\n",
      "Ep 4 (Step 002650): Train loss 4.010, Val loss 4.396\n",
      "Ep 4 (Step 002660): Train loss 4.069, Val loss 4.403\n",
      "Ep 4 (Step 002670): Train loss 4.062, Val loss 4.396\n",
      "Ep 4 (Step 002680): Train loss 4.029, Val loss 4.402\n",
      "Ep 4 (Step 002690): Train loss 4.001, Val loss 4.404\n",
      "Ep 4 (Step 002700): Train loss 4.035, Val loss 4.405\n",
      "Ep 4 (Step 002710): Train loss 4.037, Val loss 4.407\n",
      "Ep 4 (Step 002720): Train loss 4.065, Val loss 4.399\n",
      "Ep 4 (Step 002730): Train loss 4.073, Val loss 4.400\n",
      "Ep 4 (Step 002740): Train loss 4.049, Val loss 4.409\n",
      "Ep 4 (Step 002750): Train loss 3.960, Val loss 4.391\n",
      "Ep 4 (Step 002760): Train loss 3.967, Val loss 4.399\n",
      "Ep 4 (Step 002770): Train loss 4.008, Val loss 4.403\n",
      "Ep 4 (Step 002780): Train loss 4.006, Val loss 4.390\n",
      "Ep 4 (Step 002790): Train loss 4.116, Val loss 4.388\n",
      "Ep 4 (Step 002800): Train loss 4.039, Val loss 4.385\n",
      "Ep 4 (Step 002810): Train loss 4.032, Val loss 4.388\n",
      "Ep 4 (Step 002820): Train loss 4.066, Val loss 4.391\n",
      "Ep 4 (Step 002830): Train loss 3.985, Val loss 4.375\n",
      "Ep 4 (Step 002840): Train loss 4.020, Val loss 4.382\n",
      "Ep 4 (Step 002850): Train loss 3.945, Val loss 4.377\n",
      "Ep 4 (Step 002860): Train loss 4.021, Val loss 4.379\n",
      "Ep 4 (Step 002870): Train loss 4.045, Val loss 4.381\n",
      "Ep 4 (Step 002880): Train loss 3.976, Val loss 4.382\n",
      "Ep 4 (Step 002890): Train loss 4.066, Val loss 4.382\n",
      "Ep 4 (Step 002900): Train loss 3.991, Val loss 4.383\n",
      "Ep 4 (Step 002910): Train loss 4.046, Val loss 4.377\n",
      "Ep 4 (Step 002920): Train loss 3.906, Val loss 4.378\n",
      "Ep 4 (Step 002930): Train loss 3.996, Val loss 4.377\n",
      "Ep 4 (Step 002940): Train loss 3.910, Val loss 4.377\n",
      "Ep 4 (Step 002950): Train loss 4.023, Val loss 4.378\n",
      "Ep 4 (Step 002960): Train loss 4.060, Val loss 4.369\n",
      "Ep 4 (Step 002970): Train loss 4.012, Val loss 4.365\n",
      "Ep 4 (Step 002980): Train loss 4.031, Val loss 4.366\n",
      "Ep 4 (Step 002990): Train loss 3.933, Val loss 4.360\n",
      "Ep 4 (Step 003000): Train loss 3.890, Val loss 4.359\n",
      "Ep 4 (Step 003010): Train loss 4.012, Val loss 4.365\n",
      "Ep 4 (Step 003020): Train loss 3.998, Val loss 4.360\n",
      "Ep 4 (Step 003030): Train loss 3.875, Val loss 4.352\n",
      "Ep 4 (Step 003040): Train loss 3.979, Val loss 4.363\n",
      "Ep 4 (Step 003050): Train loss 3.945, Val loss 4.359\n",
      "Ep 4 (Step 003060): Train loss 4.005, Val loss 4.366\n",
      "Ep 4 (Step 003070): Train loss 3.916, Val loss 4.363\n",
      "Ep 4 (Step 003080): Train loss 3.999, Val loss 4.362\n",
      "Ep 4 (Step 003090): Train loss 3.953, Val loss 4.356\n",
      "Ep 4 (Step 003100): Train loss 3.911, Val loss 4.356\n",
      "Ep 4 (Step 003110): Train loss 3.961, Val loss 4.349\n",
      "Ep 4 (Step 003120): Train loss 3.906, Val loss 4.355\n",
      "Ep 4 (Step 003130): Train loss 4.035, Val loss 4.355\n",
      "Ep 4 (Step 003140): Train loss 3.991, Val loss 4.347\n",
      "Ep 4 (Step 003150): Train loss 4.047, Val loss 4.351\n",
      "Ep 4 (Step 003160): Train loss 3.997, Val loss 4.354\n",
      "Ep 4 (Step 003170): Train loss 3.929, Val loss 4.351\n",
      "Ep 4 (Step 003180): Train loss 3.998, Val loss 4.348\n",
      "Ep 4 (Step 003190): Train loss 3.842, Val loss 4.352\n",
      "Ep 4 (Step 003200): Train loss 3.923, Val loss 4.337\n",
      "Ep 4 (Step 003210): Train loss 3.928, Val loss 4.340\n",
      "Ep 4 (Step 003220): Train loss 3.963, Val loss 4.341\n",
      "Ep 4 (Step 003230): Train loss 4.083, Val loss 4.339\n",
      "Ep 4 (Step 003240): Train loss 3.996, Val loss 4.335\n",
      "Ep 4 (Step 003250): Train loss 3.879, Val loss 4.345\n",
      "Ep 4 (Step 003260): Train loss 3.891, Val loss 4.348\n",
      "Ep 4 (Step 003270): Train loss 3.910, Val loss 4.339\n",
      "Ep 4 (Step 003280): Train loss 3.945, Val loss 4.339\n",
      "Ep 5 (Step 003290): Train loss 3.914, Val loss 4.341\n",
      "Ep 5 (Step 003300): Train loss 3.920, Val loss 4.341\n",
      "Ep 5 (Step 003310): Train loss 3.837, Val loss 4.342\n",
      "Ep 5 (Step 003320): Train loss 3.901, Val loss 4.336\n",
      "Ep 5 (Step 003330): Train loss 3.944, Val loss 4.336\n",
      "Ep 5 (Step 003340): Train loss 3.994, Val loss 4.340\n",
      "Ep 5 (Step 003350): Train loss 3.890, Val loss 4.342\n",
      "Ep 5 (Step 003360): Train loss 3.920, Val loss 4.341\n",
      "Ep 5 (Step 003370): Train loss 3.879, Val loss 4.338\n",
      "Ep 5 (Step 003380): Train loss 3.916, Val loss 4.341\n",
      "Ep 5 (Step 003390): Train loss 3.805, Val loss 4.344\n",
      "Ep 5 (Step 003400): Train loss 3.869, Val loss 4.340\n",
      "Ep 5 (Step 003410): Train loss 3.891, Val loss 4.335\n",
      "Ep 5 (Step 003420): Train loss 3.894, Val loss 4.343\n",
      "Ep 5 (Step 003430): Train loss 3.872, Val loss 4.346\n",
      "Ep 5 (Step 003440): Train loss 3.857, Val loss 4.341\n",
      "Ep 5 (Step 003450): Train loss 3.913, Val loss 4.342\n",
      "Ep 5 (Step 003460): Train loss 3.851, Val loss 4.338\n",
      "Ep 5 (Step 003470): Train loss 3.833, Val loss 4.342\n",
      "Ep 5 (Step 003480): Train loss 3.938, Val loss 4.343\n",
      "Ep 5 (Step 003490): Train loss 3.876, Val loss 4.343\n",
      "Ep 5 (Step 003500): Train loss 3.821, Val loss 4.339\n",
      "Ep 5 (Step 003510): Train loss 3.873, Val loss 4.345\n",
      "Ep 5 (Step 003520): Train loss 3.872, Val loss 4.341\n",
      "Ep 5 (Step 003530): Train loss 3.858, Val loss 4.338\n",
      "Ep 5 (Step 003540): Train loss 3.805, Val loss 4.337\n",
      "Ep 5 (Step 003550): Train loss 3.902, Val loss 4.337\n",
      "Ep 5 (Step 003560): Train loss 3.917, Val loss 4.339\n",
      "Ep 5 (Step 003570): Train loss 3.945, Val loss 4.343\n",
      "Ep 5 (Step 003580): Train loss 3.799, Val loss 4.342\n",
      "Ep 5 (Step 003590): Train loss 3.864, Val loss 4.331\n",
      "Ep 5 (Step 003600): Train loss 3.937, Val loss 4.331\n",
      "Ep 5 (Step 003610): Train loss 3.917, Val loss 4.328\n",
      "Ep 5 (Step 003620): Train loss 3.811, Val loss 4.329\n",
      "Ep 5 (Step 003630): Train loss 3.878, Val loss 4.327\n",
      "Ep 5 (Step 003640): Train loss 3.884, Val loss 4.324\n",
      "Ep 5 (Step 003650): Train loss 3.896, Val loss 4.331\n",
      "Ep 5 (Step 003660): Train loss 3.924, Val loss 4.331\n",
      "Ep 5 (Step 003670): Train loss 3.869, Val loss 4.330\n",
      "Ep 5 (Step 003680): Train loss 3.812, Val loss 4.326\n",
      "Ep 5 (Step 003690): Train loss 3.916, Val loss 4.329\n",
      "Ep 5 (Step 003700): Train loss 3.813, Val loss 4.330\n",
      "Ep 5 (Step 003710): Train loss 3.864, Val loss 4.331\n",
      "Ep 5 (Step 003720): Train loss 3.953, Val loss 4.323\n",
      "Ep 5 (Step 003730): Train loss 3.891, Val loss 4.320\n",
      "Ep 5 (Step 003740): Train loss 3.870, Val loss 4.321\n",
      "Ep 5 (Step 003750): Train loss 3.887, Val loss 4.321\n",
      "Ep 5 (Step 003760): Train loss 3.801, Val loss 4.327\n",
      "Ep 5 (Step 003770): Train loss 3.774, Val loss 4.324\n",
      "Ep 5 (Step 003780): Train loss 3.940, Val loss 4.320\n",
      "Ep 5 (Step 003790): Train loss 3.905, Val loss 4.323\n",
      "Ep 5 (Step 003800): Train loss 3.819, Val loss 4.323\n",
      "Ep 5 (Step 003810): Train loss 3.802, Val loss 4.318\n",
      "Ep 5 (Step 003820): Train loss 3.818, Val loss 4.316\n",
      "Ep 5 (Step 003830): Train loss 3.809, Val loss 4.320\n",
      "Ep 5 (Step 003840): Train loss 3.897, Val loss 4.319\n",
      "Ep 5 (Step 003850): Train loss 3.822, Val loss 4.317\n",
      "Ep 5 (Step 003860): Train loss 3.832, Val loss 4.316\n",
      "Ep 5 (Step 003870): Train loss 3.839, Val loss 4.318\n",
      "Ep 5 (Step 003880): Train loss 3.818, Val loss 4.323\n",
      "Ep 5 (Step 003890): Train loss 3.849, Val loss 4.319\n",
      "Ep 5 (Step 003900): Train loss 3.849, Val loss 4.319\n",
      "Ep 5 (Step 003910): Train loss 3.861, Val loss 4.322\n",
      "Ep 5 (Step 003920): Train loss 3.801, Val loss 4.317\n",
      "Ep 5 (Step 003930): Train loss 3.789, Val loss 4.320\n",
      "Ep 5 (Step 003940): Train loss 3.869, Val loss 4.316\n",
      "Ep 5 (Step 003950): Train loss 3.840, Val loss 4.314\n",
      "Ep 5 (Step 003960): Train loss 3.795, Val loss 4.315\n",
      "Ep 5 (Step 003970): Train loss 3.808, Val loss 4.314\n",
      "Ep 5 (Step 003980): Train loss 3.791, Val loss 4.316\n",
      "Ep 5 (Step 003990): Train loss 3.879, Val loss 4.315\n",
      "Ep 5 (Step 004000): Train loss 3.916, Val loss 4.312\n",
      "Ep 5 (Step 004010): Train loss 3.764, Val loss 4.310\n",
      "Ep 5 (Step 004020): Train loss 3.821, Val loss 4.313\n",
      "Ep 5 (Step 004030): Train loss 3.795, Val loss 4.316\n",
      "Ep 5 (Step 004040): Train loss 3.830, Val loss 4.313\n",
      "Ep 5 (Step 004050): Train loss 3.788, Val loss 4.312\n",
      "Ep 5 (Step 004060): Train loss 3.776, Val loss 4.312\n",
      "Ep 5 (Step 004070): Train loss 3.794, Val loss 4.310\n",
      "Ep 5 (Step 004080): Train loss 3.774, Val loss 4.306\n",
      "Ep 5 (Step 004090): Train loss 3.813, Val loss 4.304\n",
      "Ep 5 (Step 004100): Train loss 3.842, Val loss 4.310\n",
      "Ep 6 (Step 004110): Train loss 3.855, Val loss 4.308\n",
      "Ep 6 (Step 004120): Train loss 3.845, Val loss 4.304\n",
      "Ep 6 (Step 004130): Train loss 3.778, Val loss 4.304\n",
      "Ep 6 (Step 004140): Train loss 3.787, Val loss 4.306\n",
      "Ep 6 (Step 004150): Train loss 3.763, Val loss 4.305\n",
      "Ep 6 (Step 004160): Train loss 3.820, Val loss 4.307\n",
      "Ep 6 (Step 004170): Train loss 3.795, Val loss 4.307\n",
      "Ep 6 (Step 004180): Train loss 3.810, Val loss 4.310\n",
      "Ep 6 (Step 004190): Train loss 3.770, Val loss 4.305\n",
      "Ep 6 (Step 004200): Train loss 3.763, Val loss 4.302\n",
      "Ep 6 (Step 004210): Train loss 3.764, Val loss 4.303\n",
      "Ep 6 (Step 004220): Train loss 3.851, Val loss 4.305\n",
      "Ep 6 (Step 004230): Train loss 3.771, Val loss 4.306\n",
      "Ep 6 (Step 004240): Train loss 3.820, Val loss 4.307\n",
      "Ep 6 (Step 004250): Train loss 3.855, Val loss 4.308\n",
      "Ep 6 (Step 004260): Train loss 3.822, Val loss 4.307\n",
      "Ep 6 (Step 004270): Train loss 3.828, Val loss 4.307\n",
      "Ep 6 (Step 004280): Train loss 3.884, Val loss 4.309\n",
      "Ep 6 (Step 004290): Train loss 3.816, Val loss 4.308\n",
      "Ep 6 (Step 004300): Train loss 3.823, Val loss 4.307\n",
      "Ep 6 (Step 004310): Train loss 3.817, Val loss 4.308\n",
      "Ep 6 (Step 004320): Train loss 3.789, Val loss 4.308\n",
      "Ep 6 (Step 004330): Train loss 3.800, Val loss 4.308\n",
      "Ep 6 (Step 004340): Train loss 3.851, Val loss 4.309\n",
      "Ep 6 (Step 004350): Train loss 3.807, Val loss 4.310\n",
      "Ep 6 (Step 004360): Train loss 3.781, Val loss 4.310\n",
      "Ep 6 (Step 004370): Train loss 3.765, Val loss 4.309\n",
      "Ep 6 (Step 004380): Train loss 3.779, Val loss 4.309\n",
      "Ep 6 (Step 004390): Train loss 3.667, Val loss 4.308\n",
      "Ep 6 (Step 004400): Train loss 3.799, Val loss 4.308\n",
      "Ep 6 (Step 004410): Train loss 3.829, Val loss 4.308\n",
      "Ep 6 (Step 004420): Train loss 3.788, Val loss 4.309\n",
      "Ep 6 (Step 004430): Train loss 3.817, Val loss 4.309\n",
      "Ep 6 (Step 004440): Train loss 3.701, Val loss 4.308\n",
      "Ep 6 (Step 004450): Train loss 3.763, Val loss 4.308\n",
      "Ep 6 (Step 004460): Train loss 3.900, Val loss 4.308\n",
      "Ep 6 (Step 004470): Train loss 3.767, Val loss 4.308\n",
      "Ep 6 (Step 004480): Train loss 3.735, Val loss 4.307\n",
      "Ep 6 (Step 004490): Train loss 3.874, Val loss 4.306\n",
      "Ep 6 (Step 004500): Train loss 3.836, Val loss 4.307\n",
      "Ep 6 (Step 004510): Train loss 3.733, Val loss 4.307\n",
      "Ep 6 (Step 004520): Train loss 3.851, Val loss 4.307\n",
      "Ep 6 (Step 004530): Train loss 3.832, Val loss 4.308\n",
      "Ep 6 (Step 004540): Train loss 3.790, Val loss 4.309\n",
      "Ep 6 (Step 004550): Train loss 3.715, Val loss 4.309\n",
      "Ep 6 (Step 004560): Train loss 3.753, Val loss 4.308\n",
      "Ep 6 (Step 004570): Train loss 3.744, Val loss 4.307\n",
      "Ep 6 (Step 004580): Train loss 3.871, Val loss 4.307\n",
      "Ep 6 (Step 004590): Train loss 3.748, Val loss 4.307\n",
      "Ep 6 (Step 004600): Train loss 3.757, Val loss 4.307\n",
      "Ep 6 (Step 004610): Train loss 3.705, Val loss 4.307\n",
      "Ep 6 (Step 004620): Train loss 3.826, Val loss 4.307\n",
      "Ep 6 (Step 004630): Train loss 3.832, Val loss 4.307\n",
      "Ep 6 (Step 004640): Train loss 3.853, Val loss 4.306\n",
      "Ep 6 (Step 004650): Train loss 3.741, Val loss 4.306\n",
      "Ep 6 (Step 004660): Train loss 3.735, Val loss 4.306\n",
      "Ep 6 (Step 004670): Train loss 3.769, Val loss 4.306\n",
      "Ep 6 (Step 004680): Train loss 3.846, Val loss 4.307\n",
      "Ep 6 (Step 004690): Train loss 3.849, Val loss 4.307\n",
      "Ep 6 (Step 004700): Train loss 3.796, Val loss 4.307\n",
      "Ep 6 (Step 004710): Train loss 3.757, Val loss 4.307\n",
      "Ep 6 (Step 004720): Train loss 3.833, Val loss 4.307\n",
      "Ep 6 (Step 004730): Train loss 3.845, Val loss 4.307\n",
      "Ep 6 (Step 004740): Train loss 3.723, Val loss 4.307\n",
      "Ep 6 (Step 004750): Train loss 3.813, Val loss 4.307\n",
      "Ep 6 (Step 004760): Train loss 3.807, Val loss 4.307\n",
      "Ep 6 (Step 004770): Train loss 3.766, Val loss 4.307\n",
      "Ep 6 (Step 004780): Train loss 3.854, Val loss 4.307\n",
      "Ep 6 (Step 004790): Train loss 3.779, Val loss 4.307\n",
      "Ep 6 (Step 004800): Train loss 3.714, Val loss 4.307\n",
      "Ep 6 (Step 004810): Train loss 3.781, Val loss 4.307\n",
      "Ep 6 (Step 004820): Train loss 3.733, Val loss 4.307\n",
      "Ep 6 (Step 004830): Train loss 3.750, Val loss 4.307\n",
      "Ep 6 (Step 004840): Train loss 3.806, Val loss 4.307\n",
      "Ep 6 (Step 004850): Train loss 3.814, Val loss 4.307\n",
      "Ep 6 (Step 004860): Train loss 3.778, Val loss 4.307\n",
      "Ep 6 (Step 004870): Train loss 3.886, Val loss 4.307\n",
      "Ep 6 (Step 004880): Train loss 3.761, Val loss 4.307\n",
      "Ep 6 (Step 004890): Train loss 3.731, Val loss 4.307\n",
      "Ep 6 (Step 004900): Train loss 3.807, Val loss 4.307\n",
      "Ep 6 (Step 004910): Train loss 3.763, Val loss 4.307\n",
      "Ep 6 (Step 004920): Train loss 3.803, Val loss 4.307\n",
      "Training completed in 14.31 minutes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_49791/679252246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model on all works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train(train_loader, val_loader, num_epochs=10,\n\u001b[0m\u001b[1;32m      4\u001b[0m       eval_iter=10, checkpoint_path=\"model_and_optimizer_best_old_tok.pth\");\n",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_49791/584259918.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, num_epochs, eval_iter, lr, generate_sample_text, sample_text, checkpoint_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Pass train_losses and val_losses as references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     train_model_simple(\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/Build GPT from scratch/pre_train.py\u001b[0m in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs, eval_iter, start_context, cfg, train_losses, val_losses, track_tokens_seen, generate_sample_text, checkpoint_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reset gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=6,\n",
    "      eval_iter=10, model_prefix=\"model_768_12_12_old_tok\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6651aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0002, weight_decay=0.05)\n",
    "\n",
    "checkpoint = torch.load(\"model_768_12_12_old_tok.pth\", weights_only=True, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ecee23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from itertools import combinations\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d523e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)  # Forward pass\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * target_ids.numel()\n",
    "            total_tokens += target_ids.numel()\n",
    "\n",
    "    perplexity = np.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "837f7534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(90.55419439493252)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44c61225-1077-4019-984a-564aa7ba6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "def weat_score(model, target_words_1, target_words_2, attribute_words_1, attribute_words_2, tokenizer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Measures bias by comparing how close different groups of words are in embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding(word):\n",
    "        token_id = tokenizer.encode(word, allowed_special={'<|endoftext|>'})[0]\n",
    "        with torch.no_grad():\n",
    "            embed = model.tok_emb(torch.tensor([token_id], device=device)).cpu().numpy()\n",
    "        return embed.flatten()\n",
    "\n",
    "    # Get embeddings\n",
    "    target_1_embs = [get_embedding(w) for w in target_words_1]\n",
    "    target_2_embs = [get_embedding(w) for w in target_words_2]\n",
    "    attr_1_embs = [get_embedding(w) for w in attribute_words_1]\n",
    "    attr_2_embs = [get_embedding(w) for w in attribute_words_2]\n",
    "\n",
    "    def association(t, A, B):\n",
    "        return np.mean([cosine_similarity(t, a) for a in A]) - np.mean([cosine_similarity(t, b) for b in B])\n",
    "\n",
    "    # Compute WEAT score\n",
    "    s1 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_1_embs])\n",
    "    s2 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_2_embs])\n",
    "    \n",
    "    weat_score = s1 - s2\n",
    "    return weat_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd7a1406-0bd9-409a-9fd8-8b4f343d2c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.027121741)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_male = [\"gentleman\", \"officer\", \"clergyman\", \"husband\", \"captain\"]\n",
    "target_female = [\"lady\", \"governess\", \"girl\", \"wife\", \"widow\"]\n",
    "\n",
    "attribute_male = [\"honour\", \"duty\", \"wisdom\", \"fortitude\", \"independence\"]\n",
    "attribute_female = [\"grace\", \"affection\", \"beauty\", \"delicacy\", \"modesty\"]\n",
    "\n",
    "weat_score(model, target_male, target_female, attribute_male, attribute_female, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cdcce2e-326f-4cc6-8033-e77e0a2ba270",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d845171-c8f1-47a0-a60b-1f9847a22590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import re\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_bleu_rouge_from_val(model, device=\"cpu\"):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    # Step 1: Load the validation set\n",
    "    with open('val_text_data_all_txt.txt', 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Step 2: Split into sentences & filter\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', data)\n",
    "    filtered_sentences = [s.strip() for s in sentences if 5 <= len(s.split()) <= 60]\n",
    "    filtered_sentences = filtered_sentences[:1000]\n",
    "\n",
    "    # Step 3: Split each sentence into two halves and store as tuples\n",
    "    sentence_tuples = []\n",
    "    for sent in filtered_sentences:\n",
    "        words = sent.split()\n",
    "        mid = len(words) // 2\n",
    "        first_half = ' '.join(words[:mid])\n",
    "        second_half = ' '.join(words[mid:])\n",
    "        sentence_tuples.append((first_half, second_half))\n",
    "\n",
    "    # Step 4: For each (first_half, second_half), generate prediction\n",
    "    for first_half, second_half in sentence_tuples:\n",
    "        generated_text = generate(\n",
    "            model=model, prompt=first_half,\n",
    "            max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "            device=device,\n",
    "            temperature=0.7,\n",
    "            top_k=50\n",
    "        )\n",
    "\n",
    "        # Build reference and prediction\n",
    "        reference = first_half + \" \" + second_half\n",
    "        prediction = generated_text\n",
    "\n",
    "        references.append(reference)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Step 5-6: Compute BLEU and ROUGE\n",
    "    # Format references correctly for BLEU\n",
    "    references_formatted = [[ref] for ref in references]\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references_formatted)['bleu']\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}, ROUGE-L Score: {rouge_score['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0404bbea-7821-4b52-a5e9-f240bf5c01d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.3110, ROUGE-L Score: 0.4049\n"
     ]
    }
   ],
   "source": [
    "compute_bleu_rouge_from_val(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adb0c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss Bennet has inherited the estate from her aunt, so she must not be at this, that she would not have thought of her own, she is not to her to have her own account. Miss Crawford would have been ashamed of her. She would have been the right to her. She has been so long ago\n",
      "==================================================\n",
      "Mr. Darcy has inherited the estate from his aunt, so he must have been lessened by his coming to his house, and, and by his father's being in the country, and would be so much of her own.\n",
      "<|endoftext|>\n",
      "<|endoftext|>\n",
      "It was the very much the evening. He did not have been\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Miss Bennet has inherited the estate from her aunt, so she must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Mr. Darcy has inherited the estate from his aunt, so he must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81220f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wife is a very pretty girl, and I think I should not be a very good thing. I would be so very well, and I am sure I am\n",
      "==================================================\n",
      "A husband is very good-morrow. I have a most intimate friends in the world; but I am sure I am sure I should not quite a very soon.\"\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"A wife is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"A husband is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "714f6606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I shall now go to be at Mansfield, and I know, I may say that if there is no better time to be. I am not an opportunity of the\n",
      "==================================================\n",
      "He said, \"I cannot be the very good fortune; and I am very agreeable man, that Mr. and Mrs. Churchill is very fond of Mr.\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"I shall now go\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"He said\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78dc249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She was sure, she was a great deal of a very kind of her situation. She was very kind to know how far he had been the idea of the most affection in the world; and when she was sorry to see him, that he should be in her, that he was very much better than she had been used to be the first meeting. He was very long enough for the day, and there being a little of a letter.\n",
      "\"I can think I have not a very great deal of it,\" said she, as \"I am not at the very well, and I have not so much more to think of it. I am sure you have been so happy to have had been there by the matter.\"\n",
      "\"But I am sure of you may know--but it does not be no doubt of you could not have been so many years ago.\"\n",
      "\"This is a very agreeable man, indeed! I never was not so sure to be sure. I have been to be very\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"She was\",\n",
    "    max_new_tokens=200, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb810-cfc6-49fe-bfd0-66c035e0707e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78fd8b69-ae98-4300-b876-93bf84fa0d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to be the very day, and I am sure I am sure I am sure I should have been a very much obliged to be very happy. I am'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "583fd4d9-4273-435a-a2fe-651162acb64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to go and Mrs. Weston.\\n\"I am very glad to think of your own family.\"\\n\"I will not like you. I am afraid'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2c8a060b-6d8a-4dc8-ade9-bfde78b38900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she is wild to get married from me to do the best of them. I think it is quite forgot to write to be a great deal. You must own. There is nothing'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"she is wild to get married\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=1,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba961e03-e482-4601-bef7-daab0c1dac77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f99b81-a4be-4994-9c00-8898a408f06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
