{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d339",
   "metadata": {},
   "source": [
    "### Detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d281a",
   "metadata": {},
   "source": [
    "### Set up model configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72561797-a3f0-4d84-9883-64c447482389",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.2,       # Dropout rate\n",
    "    \"qkv_bias\": False,      # Query-Key-Value bias\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fbf11",
   "metadata": {},
   "source": [
    "### Load training and validation data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b562de-efe1-40d2-a5ba-350b1edb7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data.txt'\n",
    "val_file_path = 'val_text_data.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf608bf",
   "metadata": {},
   "source": [
    "### Initialize data loaders for training\n",
    "Data loaders implementation can be found in `./data_loader_v1.py`.\n",
    "\n",
    "This implementation follows the omplementation detailed in _Raschka, Sebastian. Build a Large Language Model (From Scratch). Manning Publications, 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bddf6dae-302d-4fc7-853b-2806a0c7d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=16,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=16,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860e138",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a472b92-340a-46c8-8526-d0ab1a59fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 4132331\n",
      "Tokens: 935517\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "total_characters = len(train_data + val_data)\n",
    "total_tokens = len(tokenizer.encode(train_data + val_data, allowed_special={'<|endoftext|>'}))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb273e3a-602f-45b0-a404-89fa88b6aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f969edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean(): \n",
    "    \"\"\"\n",
    "    This is a function for GPU data claening before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    \n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.mps.empty_cache()  # Attempt to release MPS memory\n",
    "    \n",
    "    # Move tensors to CPU\n",
    "    for tensor in list(globals().values()):\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device == torch.device(\"mps\"):\n",
    "            tensor.to(\"cpu\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    del tensor\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "    print(\"Allocated Memory:\", torch.mps.current_allocated_memory() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da82d2c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f2c9bfd-5c57-4af6-98e8-5da47988d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train import train_model_simple\n",
    "import time\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\n",
    "def train(train_loader, val_loader,\n",
    "          num_epochs=10, eval_iter=5, \n",
    "          sample_text=\"Every effort moves you\",\n",
    "          checkpoint_path=\"model_and_optimizer.pth\"):\n",
    "\n",
    "    global train_losses, val_losses, track_tokens_seen  # Ensure these are updated globally\n",
    "\n",
    "    if device == \"mps\":\n",
    "        clean()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.05)\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=num_epochs, eval_iter=eval_iter,\n",
    "        start_context=sample_text, cfg=GPT_CONFIG_124M,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        print(50 * \"=\")\n",
    "        clean()\n",
    "    if device == \"cuda\":\n",
    "        print(50 * \"=\")\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d594966-9781-4ea2-9a68-01196f5111b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae6fc",
   "metadata": {},
   "source": [
    "### Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dda45148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.423, Val loss 10.416\n",
      "Ep 1 (Step 000020): Train loss 7.801, Val loss 7.793\n",
      "Ep 1 (Step 000040): Train loss 6.614, Val loss 6.624\n",
      "Ep 1 (Step 000060): Train loss 6.217, Val loss 6.256\n",
      "Ep 1 (Step 000080): Train loss 5.963, Val loss 6.021\n",
      "Ep 1 (Step 000100): Train loss 5.791, Val loss 5.847\n",
      "Ep 1 (Step 000120): Train loss 5.622, Val loss 5.724\n",
      "Ep 1 (Step 000140): Train loss 5.524, Val loss 5.606\n",
      "Ep 1 (Step 000160): Train loss 5.428, Val loss 5.505\n",
      "Ep 1 (Step 000180): Train loss 5.339, Val loss 5.432\n",
      "Ep 1 (Step 000200): Train loss 5.248, Val loss 5.356\n",
      "Ep 2 (Step 000220): Train loss 5.249, Val loss 5.304\n",
      "Ep 2 (Step 000240): Train loss 5.170, Val loss 5.266\n",
      "Ep 2 (Step 000260): Train loss 5.101, Val loss 5.215\n",
      "Ep 2 (Step 000280): Train loss 5.098, Val loss 5.188\n",
      "Ep 2 (Step 000300): Train loss 5.023, Val loss 5.148\n",
      "Ep 2 (Step 000320): Train loss 4.979, Val loss 5.120\n",
      "Ep 2 (Step 000340): Train loss 4.965, Val loss 5.073\n",
      "Ep 2 (Step 000360): Train loss 4.919, Val loss 5.063\n",
      "Ep 2 (Step 000380): Train loss 4.903, Val loss 5.035\n",
      "Ep 2 (Step 000400): Train loss 4.874, Val loss 5.007\n",
      "Ep 3 (Step 000420): Train loss 4.845, Val loss 4.982\n",
      "Ep 3 (Step 000440): Train loss 4.845, Val loss 4.977\n",
      "Ep 3 (Step 000460): Train loss 4.778, Val loss 4.952\n",
      "Ep 3 (Step 000480): Train loss 4.744, Val loss 4.931\n",
      "Ep 3 (Step 000500): Train loss 4.697, Val loss 4.915\n",
      "Ep 3 (Step 000520): Train loss 4.694, Val loss 4.905\n",
      "Ep 3 (Step 000540): Train loss 4.733, Val loss 4.900\n",
      "Ep 3 (Step 000560): Train loss 4.669, Val loss 4.882\n",
      "Ep 3 (Step 000580): Train loss 4.651, Val loss 4.864\n",
      "Ep 3 (Step 000600): Train loss 4.659, Val loss 4.848\n",
      "Ep 4 (Step 000620): Train loss 4.617, Val loss 4.839\n",
      "Ep 4 (Step 000640): Train loss 4.610, Val loss 4.834\n",
      "Ep 4 (Step 000660): Train loss 4.611, Val loss 4.819\n",
      "Ep 4 (Step 000680): Train loss 4.596, Val loss 4.827\n",
      "Ep 4 (Step 000700): Train loss 4.556, Val loss 4.807\n",
      "Ep 4 (Step 000720): Train loss 4.534, Val loss 4.804\n",
      "Ep 4 (Step 000740): Train loss 4.527, Val loss 4.791\n",
      "Ep 4 (Step 000760): Train loss 4.505, Val loss 4.774\n",
      "Ep 4 (Step 000780): Train loss 4.494, Val loss 4.758\n",
      "Ep 4 (Step 000800): Train loss 4.518, Val loss 4.755\n",
      "Ep 5 (Step 000820): Train loss 4.469, Val loss 4.747\n",
      "Ep 5 (Step 000840): Train loss 4.462, Val loss 4.754\n",
      "Ep 5 (Step 000860): Train loss 4.441, Val loss 4.743\n",
      "Ep 5 (Step 000880): Train loss 4.403, Val loss 4.733\n",
      "Ep 5 (Step 000900): Train loss 4.432, Val loss 4.721\n",
      "Ep 5 (Step 000920): Train loss 4.395, Val loss 4.714\n",
      "Ep 5 (Step 000940): Train loss 4.376, Val loss 4.712\n",
      "Ep 5 (Step 000960): Train loss 4.359, Val loss 4.715\n",
      "Ep 5 (Step 000980): Train loss 4.335, Val loss 4.690\n",
      "Ep 5 (Step 001000): Train loss 4.345, Val loss 4.688\n",
      "Ep 5 (Step 001020): Train loss 4.338, Val loss 4.680\n",
      "Ep 6 (Step 001040): Train loss 4.304, Val loss 4.664\n",
      "Ep 6 (Step 001060): Train loss 4.301, Val loss 4.665\n",
      "Ep 6 (Step 001080): Train loss 4.311, Val loss 4.661\n",
      "Ep 6 (Step 001100): Train loss 4.291, Val loss 4.663\n",
      "Ep 6 (Step 001120): Train loss 4.232, Val loss 4.654\n",
      "Ep 6 (Step 001140): Train loss 4.208, Val loss 4.646\n",
      "Ep 6 (Step 001160): Train loss 4.204, Val loss 4.636\n",
      "Ep 6 (Step 001180): Train loss 4.216, Val loss 4.636\n",
      "Ep 6 (Step 001200): Train loss 4.196, Val loss 4.630\n",
      "Ep 6 (Step 001220): Train loss 4.193, Val loss 4.620\n",
      "Ep 7 (Step 001240): Train loss 4.163, Val loss 4.624\n",
      "Ep 7 (Step 001260): Train loss 4.146, Val loss 4.618\n",
      "Ep 7 (Step 001280): Train loss 4.139, Val loss 4.612\n",
      "Ep 7 (Step 001300): Train loss 4.124, Val loss 4.598\n",
      "Ep 7 (Step 001320): Train loss 4.078, Val loss 4.595\n",
      "Ep 7 (Step 001340): Train loss 4.106, Val loss 4.595\n",
      "Ep 7 (Step 001360): Train loss 4.062, Val loss 4.586\n",
      "Ep 7 (Step 001380): Train loss 4.104, Val loss 4.587\n",
      "Ep 7 (Step 001400): Train loss 4.072, Val loss 4.594\n",
      "Ep 7 (Step 001420): Train loss 4.058, Val loss 4.569\n",
      "Ep 8 (Step 001440): Train loss 4.026, Val loss 4.576\n",
      "Ep 8 (Step 001460): Train loss 4.014, Val loss 4.574\n",
      "Ep 8 (Step 001480): Train loss 4.009, Val loss 4.566\n",
      "Ep 8 (Step 001500): Train loss 4.003, Val loss 4.561\n",
      "Ep 8 (Step 001520): Train loss 3.982, Val loss 4.556\n",
      "Ep 8 (Step 001540): Train loss 3.971, Val loss 4.550\n",
      "Ep 8 (Step 001560): Train loss 3.972, Val loss 4.558\n",
      "Ep 8 (Step 001580): Train loss 3.934, Val loss 4.558\n",
      "Ep 8 (Step 001600): Train loss 3.933, Val loss 4.549\n",
      "Ep 8 (Step 001620): Train loss 3.926, Val loss 4.552\n",
      "Ep 9 (Step 001640): Train loss 3.910, Val loss 4.540\n",
      "Ep 9 (Step 001660): Train loss 3.877, Val loss 4.545\n",
      "Ep 9 (Step 001680): Train loss 3.879, Val loss 4.541\n",
      "Ep 9 (Step 001700): Train loss 3.869, Val loss 4.534\n",
      "Ep 9 (Step 001720): Train loss 3.859, Val loss 4.529\n",
      "Ep 9 (Step 001740): Train loss 3.834, Val loss 4.530\n",
      "Ep 9 (Step 001760): Train loss 3.818, Val loss 4.526\n",
      "Ep 9 (Step 001780): Train loss 3.817, Val loss 4.529\n",
      "Ep 9 (Step 001800): Train loss 3.811, Val loss 4.520\n",
      "Ep 9 (Step 001820): Train loss 3.799, Val loss 4.515\n",
      "Ep 9 (Step 001840): Train loss 3.770, Val loss 4.519\n",
      "Ep 10 (Step 001860): Train loss 3.752, Val loss 4.528\n",
      "Ep 10 (Step 001880): Train loss 3.752, Val loss 4.528\n",
      "Ep 10 (Step 001900): Train loss 3.740, Val loss 4.520\n",
      "Ep 10 (Step 001920): Train loss 3.713, Val loss 4.519\n",
      "Ep 10 (Step 001940): Train loss 3.663, Val loss 4.513\n",
      "Ep 10 (Step 001960): Train loss 3.660, Val loss 4.505\n",
      "Ep 10 (Step 001980): Train loss 3.690, Val loss 4.515\n",
      "Ep 10 (Step 002000): Train loss 3.673, Val loss 4.510\n",
      "Ep 10 (Step 002020): Train loss 3.681, Val loss 4.506\n",
      "Ep 10 (Step 002040): Train loss 3.634, Val loss 4.496\n",
      "Training completed in 18.85 minutes.\n"
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=10,\n",
    "      eval_iter=20, sample_text=\"He inherited the estate\",\n",
    "      checkpoint_path=\"model_and_optimizer_all_txt.pth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6651aada",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0004\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_and_optimizer_all_txt.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1432\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1430\u001b[0m orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m   1431\u001b[0m overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1432\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m   1433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[0;32m   1434\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1435\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1436\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1437\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1438\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1439\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:763\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[1;34m(self, name_or_buffer)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer_all_txt.pth\", weights_only=True, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced793dd-5943-44e5-9168-9209eb2117d0",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdb50a2d-fb80-4789-a6f3-5e4a318799a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from itertools import combinations\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b5de5-a92f-47d5-acca-daed55ee18c8",
   "metadata": {},
   "source": [
    "#### 1. Perplexity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40a7fff1-8a66-4045-8300-283771d116e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)  # Forward pass\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * target_ids.numel()\n",
    "            total_tokens += target_ids.numel()\n",
    "\n",
    "    perplexity = np.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c1787b1-b933-4de3-a1b8-725a85897e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(82.51142259395502)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cee6a0-4e62-4f7d-95c0-b9f5c678ce26",
   "metadata": {},
   "source": [
    "#### 2. Word Embedding Association Test (WEAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d9fe34d-a728-4ecc-9843-4a47c7989cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "def weat_score(model, target_words_1, target_words_2, attribute_words_1, attribute_words_2, tokenizer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Measures bias by comparing how close different groups of words are in embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding(word):\n",
    "        token_id = tokenizer.encode(word, allowed_special={'<|endoftext|>'})[0]\n",
    "        with torch.no_grad():\n",
    "            embed = model.tok_emb(torch.tensor([token_id], device=device)).cpu().numpy()\n",
    "        return embed.flatten()\n",
    "\n",
    "    # Get embeddings\n",
    "    target_1_embs = [get_embedding(w) for w in target_words_1]\n",
    "    target_2_embs = [get_embedding(w) for w in target_words_2]\n",
    "    attr_1_embs = [get_embedding(w) for w in attribute_words_1]\n",
    "    attr_2_embs = [get_embedding(w) for w in attribute_words_2]\n",
    "\n",
    "    def association(t, A, B):\n",
    "        return np.mean([cosine_similarity(t, a) for a in A]) - np.mean([cosine_similarity(t, b) for b in B])\n",
    "\n",
    "    # Compute WEAT score\n",
    "    s1 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_1_embs])\n",
    "    s2 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_2_embs])\n",
    "    \n",
    "    weat_score = s1 - s2\n",
    "    return weat_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8be93d90-4ff0-435c-babb-b8231b279bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.027674114)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_male = [\"gentleman\", \"officer\", \"clergyman\", \"husband\", \"captain\"]\n",
    "target_female = [\"lady\", \"governess\", \"girl\", \"wife\", \"widow\"]\n",
    "\n",
    "attribute_male = [\"honour\", \"duty\", \"wisdom\", \"fortitude\", \"independence\"]\n",
    "attribute_female = [\"grace\", \"affection\", \"beauty\", \"delicacy\", \"modesty\"]\n",
    "\n",
    "weat_score(model, target_male, target_female, attribute_male, attribute_female, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fa951-0f72-4889-bc97-91f2ce17864d",
   "metadata": {},
   "source": [
    "#### 3. BLEU & ROUGE Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684122c-30ab-43e7-89dc-f83e4850b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd684f97-5fcd-48a5-a0ce-500df7dbfa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_rouge(references, predictions):\n",
    "    \"\"\"\n",
    "    Compute BLEU and ROUGE scores between reference texts and model-generated texts.\n",
    "    \"\"\"\n",
    "    references = [[ref.split()] for ref in references]  # BLEU requires tokenized reference list\n",
    "    predictions = [pred.split() for pred in predictions]\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references)['bleu']\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return bleu_score, rouge_score[\"rougeL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c1f13-bdc8-482c-a37a-8b6c474c3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_bleu_rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1a525-b977-4479-b39a-5249e7ba68b2",
   "metadata": {},
   "source": [
    "## Example text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adb0c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss Bennet has inherited the estate from her aunt, so she must have been the subject, and so much in love with her, as to her that Mr. and Mrs. Gardiner, that you should not be so disappointed, that his absence is not to be in his way. There is a very young man\n",
      "==================================================\n",
      "Mr. Darcy has inherited the estate from his aunt, so he must have done so much in all his own. It is not a very handsome young man, that is, I hope, to be very glad to be so in my power to be in the least. I am very glad that I hope it will be\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, prompt=\"Miss Bennet has inherited the estate from her aunt, so she must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, prompt=\"Mr. Darcy has inherited the estate from his aunt, so he must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81220f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wife is a young man, and is not a very agreeable young man, and the other people know. But it is a very different sort of man, and\n",
      "==================================================\n",
      "A husband is a man, and is a very pretty young man, and a very agreeable man, and a very good young man, and a very good man,\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, prompt=\"A wife is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, prompt=\"A husband is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "714f6606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good lady ought to be a very desirable thing. The two girls are a man, and every body ought to be allowed to be the best, and all must be a little\n",
      "==================================================\n",
      "A highly respectable man ought to be in town of him.\"\n",
      "\"I am sure,\" cried Catherine, \"for I always felt that I am so kind as to the match as possible\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, prompt=\"A good lady ought to be\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, prompt=\"A highly respectable man ought to be\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a25f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
