{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d339",
   "metadata": {},
   "source": [
    "### Detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d281a",
   "metadata": {},
   "source": [
    "### Set up model configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72561797-a3f0-4d84-9883-64c447482389",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 8,          # Number of attention heads\n",
    "    \"n_layers\": 8,         # Number of layers\n",
    "    \"drop_rate\": 0.2,       # Dropout rate\n",
    "    \"qkv_bias\": False,      # Query-Key-Value bias\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383eb92",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75227c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 1820039\n",
      "Tokens: 415577\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fbf11",
   "metadata": {},
   "source": [
    "### Load training and validation data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b562de-efe1-40d2-a5ba-350b1edb7a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data.txt'\n",
    "val_file_path = 'val_text_data.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf608bf",
   "metadata": {},
   "source": [
    "### Initialize data loaders for training\n",
    "Data loaders implementation can be found in `./data_loader_v1.py`.\n",
    "\n",
    "This implementation follows the omplementation detailed in _Raschka, Sebastian. Build a Large Language Model (From Scratch). Manning Publications, 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bddf6dae-302d-4fc7-853b-2806a0c7d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f915a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = len(train_data + val_data)\n",
    "total_tokens = len(tokenizer.encode(train_data + val_data, allowed_special={'<|endoftext|>'}))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f969edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean(): \n",
    "    \"\"\"\n",
    "    This is a function for GPU data claening before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    \n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.mps.empty_cache()  # Attempt to release MPS memory\n",
    "    \n",
    "    # Move tensors to CPU\n",
    "    for tensor in list(globals().values()):\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device == torch.device(\"mps\"):\n",
    "            tensor.to(\"cpu\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    del tensor\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "    print(\"Allocated Memory:\", torch.mps.current_allocated_memory() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da82d2c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f2c9bfd-5c57-4af6-98e8-5da47988d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train import train_model_simple\n",
    "import time\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\n",
    "def train(train_loader, val_loader,\n",
    "          num_epochs=10, eval_iter=5, lr=0.0002,\n",
    "          generate_sample_text=False,\n",
    "          sample_text=\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be\",\n",
    "          checkpoint_path=\"model_and_optimizer.pth\"):\n",
    "\n",
    "    global train_losses, val_losses, track_tokens_seen  # Ensure these are updated globally\n",
    "\n",
    "    if device == \"mps\":\n",
    "        clean()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-08, weight_decay=0.05)\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=num_epochs, eval_iter=eval_iter,\n",
    "        start_context=sample_text, cfg=GPT_CONFIG_124M,\n",
    "        generate_sample_text=generate_sample_text,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        print(50 * \"=\")\n",
    "        clean()\n",
    "    if device == \"cuda\":\n",
    "        print(50 * \"=\")\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d594966-9781-4ea2-9a68-01196f5111b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae6fc",
   "metadata": {},
   "source": [
    "### Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dda45148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.016, Val loss 9.021\n",
      "Ep 1 (Step 000010): Train loss 7.445, Val loss 7.414\n",
      "Ep 1 (Step 000020): Train loss 6.828, Val loss 6.787\n",
      "Ep 1 (Step 000030): Train loss 6.487, Val loss 6.478\n",
      "Ep 1 (Step 000040): Train loss 6.374, Val loss 6.376\n",
      "Ep 1 (Step 000050): Train loss 6.271, Val loss 6.323\n",
      "Ep 1 (Step 000060): Train loss 6.242, Val loss 6.206\n",
      "Ep 1 (Step 000070): Train loss 6.109, Val loss 6.111\n",
      "Ep 1 (Step 000080): Train loss 6.035, Val loss 6.028\n",
      "Ep 1 (Step 000090): Train loss 5.930, Val loss 5.948\n",
      "Ep 1 (Step 000100): Train loss 5.861, Val loss 5.889\n",
      "Ep 1 (Step 000110): Train loss 5.812, Val loss 5.833\n",
      "Ep 1 (Step 000120): Train loss 5.739, Val loss 5.800\n",
      "Ep 1 (Step 000130): Train loss 5.660, Val loss 5.743\n",
      "Ep 1 (Step 000140): Train loss 5.696, Val loss 5.708\n",
      "Ep 1 (Step 000150): Train loss 5.663, Val loss 5.672\n",
      "Ep 1 (Step 000160): Train loss 5.495, Val loss 5.634\n",
      "Ep 1 (Step 000170): Train loss 5.630, Val loss 5.605\n",
      "Ep 1 (Step 000180): Train loss 5.538, Val loss 5.586\n",
      "Ep 1 (Step 000190): Train loss 5.489, Val loss 5.565\n",
      "Ep 1 (Step 000200): Train loss 5.466, Val loss 5.543\n",
      "Ep 1 (Step 000210): Train loss 5.386, Val loss 5.524\n",
      "Ep 1 (Step 000220): Train loss 5.376, Val loss 5.489\n",
      "Ep 1 (Step 000230): Train loss 5.455, Val loss 5.475\n",
      "Ep 1 (Step 000240): Train loss 5.320, Val loss 5.453\n",
      "Ep 1 (Step 000250): Train loss 5.439, Val loss 5.443\n",
      "Ep 1 (Step 000260): Train loss 5.337, Val loss 5.435\n",
      "Ep 1 (Step 000270): Train loss 5.252, Val loss 5.409\n",
      "Ep 1 (Step 000280): Train loss 5.294, Val loss 5.388\n",
      "Ep 1 (Step 000290): Train loss 5.226, Val loss 5.389\n",
      "Ep 1 (Step 000300): Train loss 5.226, Val loss 5.368\n",
      "Ep 1 (Step 000310): Train loss 5.220, Val loss 5.356\n",
      "Ep 1 (Step 000320): Train loss 5.191, Val loss 5.347\n",
      "Ep 1 (Step 000330): Train loss 5.166, Val loss 5.326\n",
      "Ep 1 (Step 000340): Train loss 5.180, Val loss 5.316\n",
      "Ep 2 (Step 000350): Train loss 5.172, Val loss 5.304\n",
      "Ep 2 (Step 000360): Train loss 5.177, Val loss 5.287\n",
      "Ep 2 (Step 000370): Train loss 5.141, Val loss 5.279\n",
      "Ep 2 (Step 000380): Train loss 5.098, Val loss 5.275\n",
      "Ep 2 (Step 000390): Train loss 5.125, Val loss 5.265\n",
      "Ep 2 (Step 000400): Train loss 5.141, Val loss 5.252\n",
      "Ep 2 (Step 000410): Train loss 5.121, Val loss 5.247\n",
      "Ep 2 (Step 000420): Train loss 5.116, Val loss 5.233\n",
      "Ep 2 (Step 000430): Train loss 4.995, Val loss 5.230\n",
      "Ep 2 (Step 000440): Train loss 5.141, Val loss 5.231\n",
      "Ep 2 (Step 000450): Train loss 5.083, Val loss 5.235\n",
      "Ep 2 (Step 000460): Train loss 5.050, Val loss 5.205\n",
      "Ep 2 (Step 000470): Train loss 5.017, Val loss 5.200\n",
      "Ep 2 (Step 000480): Train loss 4.994, Val loss 5.200\n",
      "Ep 2 (Step 000490): Train loss 4.997, Val loss 5.183\n",
      "Ep 2 (Step 000500): Train loss 5.060, Val loss 5.183\n",
      "Ep 2 (Step 000510): Train loss 5.070, Val loss 5.174\n",
      "Ep 2 (Step 000520): Train loss 4.965, Val loss 5.173\n",
      "Ep 2 (Step 000530): Train loss 5.021, Val loss 5.176\n",
      "Ep 2 (Step 000540): Train loss 4.967, Val loss 5.159\n",
      "Ep 2 (Step 000550): Train loss 4.990, Val loss 5.158\n",
      "Ep 2 (Step 000560): Train loss 4.951, Val loss 5.148\n",
      "Ep 2 (Step 000570): Train loss 4.992, Val loss 5.150\n",
      "Ep 2 (Step 000580): Train loss 4.913, Val loss 5.144\n",
      "Ep 2 (Step 000590): Train loss 4.952, Val loss 5.147\n",
      "Ep 2 (Step 000600): Train loss 4.925, Val loss 5.145\n",
      "Ep 2 (Step 000610): Train loss 4.941, Val loss 5.134\n",
      "Ep 2 (Step 000620): Train loss 4.874, Val loss 5.131\n",
      "Ep 2 (Step 000630): Train loss 4.871, Val loss 5.127\n",
      "Ep 2 (Step 000640): Train loss 4.865, Val loss 5.110\n",
      "Ep 2 (Step 000650): Train loss 4.854, Val loss 5.106\n",
      "Ep 2 (Step 000660): Train loss 4.853, Val loss 5.103\n",
      "Ep 2 (Step 000670): Train loss 4.832, Val loss 5.091\n",
      "Ep 2 (Step 000680): Train loss 4.933, Val loss 5.083\n",
      "Ep 2 (Step 000690): Train loss 4.890, Val loss 5.086\n",
      "Ep 3 (Step 000700): Train loss 4.878, Val loss 5.083\n",
      "Ep 3 (Step 000710): Train loss 4.893, Val loss 5.077\n",
      "Ep 3 (Step 000720): Train loss 4.895, Val loss 5.083\n",
      "Ep 3 (Step 000730): Train loss 4.793, Val loss 5.081\n",
      "Ep 3 (Step 000740): Train loss 4.843, Val loss 5.089\n",
      "Ep 3 (Step 000750): Train loss 4.823, Val loss 5.064\n",
      "Ep 3 (Step 000760): Train loss 4.767, Val loss 5.066\n",
      "Ep 3 (Step 000770): Train loss 4.856, Val loss 5.073\n",
      "Ep 3 (Step 000780): Train loss 4.804, Val loss 5.062\n",
      "Ep 3 (Step 000790): Train loss 4.775, Val loss 5.058\n",
      "Ep 3 (Step 000800): Train loss 4.805, Val loss 5.098\n",
      "Ep 3 (Step 000810): Train loss 4.854, Val loss 5.062\n",
      "Ep 3 (Step 000820): Train loss 4.685, Val loss 5.051\n",
      "Ep 3 (Step 000830): Train loss 4.756, Val loss 5.039\n",
      "Ep 3 (Step 000840): Train loss 4.774, Val loss 5.048\n",
      "Ep 3 (Step 000850): Train loss 4.739, Val loss 5.041\n",
      "Ep 3 (Step 000860): Train loss 4.737, Val loss 5.040\n",
      "Ep 3 (Step 000870): Train loss 4.732, Val loss 5.040\n",
      "Ep 3 (Step 000880): Train loss 4.642, Val loss 5.029\n",
      "Ep 3 (Step 000890): Train loss 4.712, Val loss 5.038\n",
      "Ep 3 (Step 000900): Train loss 4.693, Val loss 5.032\n",
      "Ep 3 (Step 000910): Train loss 4.788, Val loss 5.027\n",
      "Ep 3 (Step 000920): Train loss 4.716, Val loss 5.021\n",
      "Ep 3 (Step 000930): Train loss 4.763, Val loss 5.029\n",
      "Ep 3 (Step 000940): Train loss 4.747, Val loss 5.015\n",
      "Ep 3 (Step 000950): Train loss 4.811, Val loss 5.022\n",
      "Ep 3 (Step 000960): Train loss 4.640, Val loss 5.009\n",
      "Ep 3 (Step 000970): Train loss 4.696, Val loss 5.010\n",
      "Ep 3 (Step 000980): Train loss 4.643, Val loss 5.001\n",
      "Ep 3 (Step 000990): Train loss 4.648, Val loss 4.993\n",
      "Ep 3 (Step 001000): Train loss 4.646, Val loss 4.998\n",
      "Ep 3 (Step 001010): Train loss 4.648, Val loss 5.003\n",
      "Ep 3 (Step 001020): Train loss 4.646, Val loss 4.985\n",
      "Ep 3 (Step 001030): Train loss 4.708, Val loss 5.003\n",
      "Ep 3 (Step 001040): Train loss 4.645, Val loss 4.987\n",
      "Ep 4 (Step 001050): Train loss 4.703, Val loss 4.991\n",
      "Ep 4 (Step 001060): Train loss 4.608, Val loss 4.987\n",
      "Ep 4 (Step 001070): Train loss 4.661, Val loss 4.978\n",
      "Ep 4 (Step 001080): Train loss 4.641, Val loss 4.991\n",
      "Ep 4 (Step 001090): Train loss 4.585, Val loss 4.982\n",
      "Ep 4 (Step 001100): Train loss 4.683, Val loss 4.985\n",
      "Ep 4 (Step 001110): Train loss 4.603, Val loss 4.984\n",
      "Ep 4 (Step 001120): Train loss 4.642, Val loss 4.980\n",
      "Ep 4 (Step 001130): Train loss 4.629, Val loss 4.984\n",
      "Ep 4 (Step 001140): Train loss 4.576, Val loss 4.979\n",
      "Ep 4 (Step 001150): Train loss 4.617, Val loss 4.982\n",
      "Ep 4 (Step 001160): Train loss 4.504, Val loss 4.960\n",
      "Ep 4 (Step 001170): Train loss 4.594, Val loss 4.950\n",
      "Ep 4 (Step 001180): Train loss 4.530, Val loss 4.961\n",
      "Ep 4 (Step 001190): Train loss 4.524, Val loss 4.960\n",
      "Ep 4 (Step 001200): Train loss 4.577, Val loss 4.974\n",
      "Ep 4 (Step 001210): Train loss 4.525, Val loss 4.944\n",
      "Ep 4 (Step 001220): Train loss 4.581, Val loss 4.953\n",
      "Ep 4 (Step 001230): Train loss 4.518, Val loss 4.944\n",
      "Ep 4 (Step 001240): Train loss 4.538, Val loss 4.957\n",
      "Ep 4 (Step 001250): Train loss 4.529, Val loss 4.953\n",
      "Ep 4 (Step 001260): Train loss 4.603, Val loss 4.957\n",
      "Ep 4 (Step 001270): Train loss 4.534, Val loss 4.956\n",
      "Ep 4 (Step 001280): Train loss 4.512, Val loss 4.946\n",
      "Ep 4 (Step 001290): Train loss 4.589, Val loss 4.950\n",
      "Ep 4 (Step 001300): Train loss 4.460, Val loss 4.935\n",
      "Ep 4 (Step 001310): Train loss 4.559, Val loss 4.944\n",
      "Ep 4 (Step 001320): Train loss 4.502, Val loss 4.937\n",
      "Ep 4 (Step 001330): Train loss 4.531, Val loss 4.931\n",
      "Ep 4 (Step 001340): Train loss 4.446, Val loss 4.929\n",
      "Ep 4 (Step 001350): Train loss 4.347, Val loss 4.926\n",
      "Ep 4 (Step 001360): Train loss 4.488, Val loss 4.928\n",
      "Ep 4 (Step 001370): Train loss 4.498, Val loss 4.932\n",
      "Ep 4 (Step 001380): Train loss 4.413, Val loss 4.927\n",
      "Ep 4 (Step 001390): Train loss 4.459, Val loss 4.928\n",
      "Ep 5 (Step 001400): Train loss 4.497, Val loss 4.935\n",
      "Ep 5 (Step 001410): Train loss 4.464, Val loss 4.949\n",
      "Ep 5 (Step 001420): Train loss 4.455, Val loss 4.928\n",
      "Ep 5 (Step 001430): Train loss 4.542, Val loss 4.931\n",
      "Ep 5 (Step 001440): Train loss 4.498, Val loss 4.937\n",
      "Ep 5 (Step 001450): Train loss 4.494, Val loss 4.929\n",
      "Ep 5 (Step 001460): Train loss 4.460, Val loss 4.927\n",
      "Ep 5 (Step 001470): Train loss 4.497, Val loss 4.923\n",
      "Ep 5 (Step 001480): Train loss 4.424, Val loss 4.915\n",
      "Ep 5 (Step 001490): Train loss 4.502, Val loss 4.931\n",
      "Ep 5 (Step 001500): Train loss 4.465, Val loss 4.928\n",
      "Ep 5 (Step 001510): Train loss 4.461, Val loss 4.934\n",
      "Ep 5 (Step 001520): Train loss 4.449, Val loss 4.919\n",
      "Ep 5 (Step 001530): Train loss 4.421, Val loss 4.917\n",
      "Ep 5 (Step 001540): Train loss 4.470, Val loss 4.917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5 (Step 001550): Train loss 4.456, Val loss 4.919\n",
      "Ep 5 (Step 001560): Train loss 4.445, Val loss 4.908\n",
      "Ep 5 (Step 001570): Train loss 4.367, Val loss 4.906\n",
      "Ep 5 (Step 001580): Train loss 4.366, Val loss 4.909\n",
      "Ep 5 (Step 001590): Train loss 4.380, Val loss 4.910\n",
      "Ep 5 (Step 001600): Train loss 4.356, Val loss 4.903\n",
      "Ep 5 (Step 001610): Train loss 4.371, Val loss 4.902\n",
      "Ep 5 (Step 001620): Train loss 4.306, Val loss 4.902\n",
      "Ep 5 (Step 001630): Train loss 4.389, Val loss 4.903\n",
      "Ep 5 (Step 001640): Train loss 4.357, Val loss 4.905\n",
      "Ep 5 (Step 001650): Train loss 4.403, Val loss 4.895\n",
      "Ep 5 (Step 001660): Train loss 4.406, Val loss 4.891\n",
      "Ep 5 (Step 001670): Train loss 4.330, Val loss 4.893\n",
      "Ep 5 (Step 001680): Train loss 4.322, Val loss 4.888\n",
      "Ep 5 (Step 001690): Train loss 4.289, Val loss 4.887\n",
      "Ep 5 (Step 001700): Train loss 4.349, Val loss 4.888\n",
      "Ep 5 (Step 001710): Train loss 4.316, Val loss 4.887\n",
      "Ep 5 (Step 001720): Train loss 4.380, Val loss 4.886\n",
      "Ep 5 (Step 001730): Train loss 4.310, Val loss 4.886\n",
      "Ep 5 (Step 001740): Train loss 4.365, Val loss 4.886\n",
      "Ep 6 (Step 001750): Train loss 4.334, Val loss 4.883\n",
      "Ep 6 (Step 001760): Train loss 4.299, Val loss 4.874\n",
      "Ep 6 (Step 001770): Train loss 4.281, Val loss 4.888\n",
      "Ep 6 (Step 001780): Train loss 4.327, Val loss 4.882\n",
      "Ep 6 (Step 001790): Train loss 4.354, Val loss 4.883\n",
      "Ep 6 (Step 001800): Train loss 4.263, Val loss 4.890\n",
      "Ep 6 (Step 001810): Train loss 4.248, Val loss 4.894\n",
      "Ep 6 (Step 001820): Train loss 4.342, Val loss 4.894\n",
      "Ep 6 (Step 001830): Train loss 4.356, Val loss 4.892\n",
      "Ep 6 (Step 001840): Train loss 4.325, Val loss 4.883\n",
      "Ep 6 (Step 001850): Train loss 4.371, Val loss 4.882\n",
      "Ep 6 (Step 001860): Train loss 4.247, Val loss 4.880\n",
      "Ep 6 (Step 001870): Train loss 4.258, Val loss 4.885\n",
      "Ep 6 (Step 001880): Train loss 4.308, Val loss 4.890\n",
      "Ep 6 (Step 001890): Train loss 4.278, Val loss 4.887\n",
      "Ep 6 (Step 001900): Train loss 4.278, Val loss 4.883\n",
      "Ep 6 (Step 001910): Train loss 4.268, Val loss 4.890\n",
      "Ep 6 (Step 001920): Train loss 4.298, Val loss 4.891\n",
      "Ep 6 (Step 001930): Train loss 4.237, Val loss 4.881\n",
      "Ep 6 (Step 001940): Train loss 4.239, Val loss 4.881\n",
      "Ep 6 (Step 001950): Train loss 4.267, Val loss 4.877\n",
      "Ep 6 (Step 001960): Train loss 4.323, Val loss 4.878\n",
      "Ep 6 (Step 001970): Train loss 4.300, Val loss 4.873\n",
      "Ep 6 (Step 001980): Train loss 4.189, Val loss 4.873\n",
      "Ep 6 (Step 001990): Train loss 4.220, Val loss 4.867\n",
      "Ep 6 (Step 002000): Train loss 4.262, Val loss 4.870\n",
      "Ep 6 (Step 002010): Train loss 4.212, Val loss 4.863\n",
      "Ep 6 (Step 002020): Train loss 4.212, Val loss 4.862\n",
      "Ep 6 (Step 002030): Train loss 4.260, Val loss 4.879\n",
      "Ep 6 (Step 002040): Train loss 4.276, Val loss 4.870\n",
      "Ep 6 (Step 002050): Train loss 4.264, Val loss 4.861\n",
      "Ep 6 (Step 002060): Train loss 4.210, Val loss 4.867\n",
      "Ep 6 (Step 002070): Train loss 4.272, Val loss 4.860\n",
      "Ep 6 (Step 002080): Train loss 4.249, Val loss 4.855\n",
      "Ep 6 (Step 002090): Train loss 4.216, Val loss 4.861\n",
      "Ep 7 (Step 002100): Train loss 4.139, Val loss 4.867\n",
      "Ep 7 (Step 002110): Train loss 4.221, Val loss 4.863\n",
      "Ep 7 (Step 002120): Train loss 4.182, Val loss 4.862\n",
      "Ep 7 (Step 002130): Train loss 4.188, Val loss 4.864\n",
      "Ep 7 (Step 002140): Train loss 4.229, Val loss 4.869\n",
      "Ep 7 (Step 002150): Train loss 4.246, Val loss 4.867\n",
      "Ep 7 (Step 002160): Train loss 4.227, Val loss 4.871\n",
      "Ep 7 (Step 002170): Train loss 4.211, Val loss 4.865\n",
      "Ep 7 (Step 002180): Train loss 4.190, Val loss 4.860\n",
      "Ep 7 (Step 002190): Train loss 4.081, Val loss 4.863\n",
      "Ep 7 (Step 002200): Train loss 4.150, Val loss 4.867\n",
      "Ep 7 (Step 002210): Train loss 4.199, Val loss 4.862\n",
      "Ep 7 (Step 002220): Train loss 4.233, Val loss 4.859\n",
      "Ep 7 (Step 002230): Train loss 4.155, Val loss 4.862\n",
      "Ep 7 (Step 002240): Train loss 4.155, Val loss 4.863\n",
      "Ep 7 (Step 002250): Train loss 4.155, Val loss 4.859\n",
      "Ep 7 (Step 002260): Train loss 4.138, Val loss 4.855\n",
      "Ep 7 (Step 002270): Train loss 4.219, Val loss 4.861\n",
      "Ep 7 (Step 002280): Train loss 4.182, Val loss 4.859\n",
      "Ep 7 (Step 002290): Train loss 4.206, Val loss 4.857\n",
      "Ep 7 (Step 002300): Train loss 4.125, Val loss 4.860\n",
      "Ep 7 (Step 002310): Train loss 4.209, Val loss 4.856\n",
      "Ep 7 (Step 002320): Train loss 4.232, Val loss 4.862\n",
      "Ep 7 (Step 002330): Train loss 4.145, Val loss 4.854\n",
      "Ep 7 (Step 002340): Train loss 4.177, Val loss 4.857\n",
      "Ep 7 (Step 002350): Train loss 4.130, Val loss 4.859\n",
      "Ep 7 (Step 002360): Train loss 4.156, Val loss 4.856\n",
      "Ep 7 (Step 002370): Train loss 4.185, Val loss 4.857\n",
      "Ep 7 (Step 002380): Train loss 4.180, Val loss 4.854\n",
      "Ep 7 (Step 002390): Train loss 4.140, Val loss 4.861\n",
      "Ep 7 (Step 002400): Train loss 4.096, Val loss 4.855\n",
      "Ep 7 (Step 002410): Train loss 4.162, Val loss 4.849\n",
      "Ep 7 (Step 002420): Train loss 4.132, Val loss 4.850\n",
      "Ep 7 (Step 002430): Train loss 4.123, Val loss 4.850\n",
      "Ep 7 (Step 002440): Train loss 4.233, Val loss 4.851\n",
      "Ep 8 (Step 002450): Train loss 4.162, Val loss 4.852\n",
      "Ep 8 (Step 002460): Train loss 4.147, Val loss 4.853\n",
      "Ep 8 (Step 002470): Train loss 4.190, Val loss 4.856\n",
      "Ep 8 (Step 002480): Train loss 4.196, Val loss 4.853\n",
      "Ep 8 (Step 002490): Train loss 4.196, Val loss 4.853\n",
      "Ep 8 (Step 002500): Train loss 4.061, Val loss 4.856\n",
      "Ep 8 (Step 002510): Train loss 4.091, Val loss 4.857\n",
      "Ep 8 (Step 002520): Train loss 4.160, Val loss 4.858\n",
      "Ep 8 (Step 002530): Train loss 4.113, Val loss 4.854\n",
      "Ep 8 (Step 002540): Train loss 4.094, Val loss 4.854\n",
      "Ep 8 (Step 002550): Train loss 4.205, Val loss 4.857\n",
      "Ep 8 (Step 002560): Train loss 4.156, Val loss 4.860\n",
      "Ep 8 (Step 002570): Train loss 4.078, Val loss 4.858\n",
      "Ep 8 (Step 002580): Train loss 4.117, Val loss 4.856\n",
      "Ep 8 (Step 002590): Train loss 4.078, Val loss 4.861\n",
      "Ep 8 (Step 002600): Train loss 4.090, Val loss 4.856\n",
      "Ep 8 (Step 002610): Train loss 4.109, Val loss 4.853\n",
      "Ep 8 (Step 002620): Train loss 4.167, Val loss 4.855\n",
      "Ep 8 (Step 002630): Train loss 4.084, Val loss 4.853\n",
      "Ep 8 (Step 002640): Train loss 4.118, Val loss 4.851\n",
      "Ep 8 (Step 002650): Train loss 4.094, Val loss 4.851\n",
      "Ep 8 (Step 002660): Train loss 4.102, Val loss 4.851\n",
      "Ep 8 (Step 002670): Train loss 4.122, Val loss 4.849\n",
      "Ep 8 (Step 002680): Train loss 4.129, Val loss 4.850\n",
      "Ep 8 (Step 002690): Train loss 4.075, Val loss 4.848\n",
      "Ep 8 (Step 002700): Train loss 4.104, Val loss 4.849\n",
      "Ep 8 (Step 002710): Train loss 4.124, Val loss 4.850\n",
      "Ep 8 (Step 002720): Train loss 4.056, Val loss 4.847\n",
      "Ep 8 (Step 002730): Train loss 4.084, Val loss 4.847\n",
      "Ep 8 (Step 002740): Train loss 4.087, Val loss 4.848\n",
      "Ep 8 (Step 002750): Train loss 4.025, Val loss 4.847\n",
      "Ep 8 (Step 002760): Train loss 4.095, Val loss 4.847\n",
      "Ep 8 (Step 002770): Train loss 4.041, Val loss 4.847\n",
      "Ep 8 (Step 002780): Train loss 4.096, Val loss 4.847\n",
      "Ep 8 (Step 002790): Train loss 4.116, Val loss 4.847\n",
      "Ep 9 (Step 002800): Train loss 4.081, Val loss 4.847\n",
      "Ep 9 (Step 002810): Train loss 4.134, Val loss 4.848\n",
      "Ep 9 (Step 002820): Train loss 4.080, Val loss 4.849\n",
      "Ep 9 (Step 002830): Train loss 4.080, Val loss 4.851\n",
      "Ep 9 (Step 002840): Train loss 4.112, Val loss 4.849\n",
      "Ep 9 (Step 002850): Train loss 4.119, Val loss 4.846\n",
      "Ep 9 (Step 002860): Train loss 4.071, Val loss 4.846\n",
      "Ep 9 (Step 002870): Train loss 4.085, Val loss 4.847\n",
      "Ep 9 (Step 002880): Train loss 4.073, Val loss 4.850\n",
      "Ep 9 (Step 002890): Train loss 4.095, Val loss 4.849\n",
      "Ep 9 (Step 002900): Train loss 4.064, Val loss 4.848\n",
      "Ep 9 (Step 002910): Train loss 4.126, Val loss 4.849\n",
      "Ep 9 (Step 002920): Train loss 4.055, Val loss 4.850\n",
      "Ep 9 (Step 002930): Train loss 4.081, Val loss 4.849\n",
      "Ep 9 (Step 002940): Train loss 4.011, Val loss 4.849\n",
      "Ep 9 (Step 002950): Train loss 4.071, Val loss 4.848\n",
      "Ep 9 (Step 002960): Train loss 4.037, Val loss 4.848\n",
      "Ep 9 (Step 002970): Train loss 4.167, Val loss 4.847\n",
      "Ep 9 (Step 002980): Train loss 4.077, Val loss 4.848\n",
      "Ep 9 (Step 002990): Train loss 4.050, Val loss 4.849\n",
      "Ep 9 (Step 003000): Train loss 4.076, Val loss 4.849\n",
      "Ep 9 (Step 003010): Train loss 4.097, Val loss 4.848\n",
      "Ep 9 (Step 003020): Train loss 4.029, Val loss 4.848\n",
      "Ep 9 (Step 003030): Train loss 4.083, Val loss 4.848\n",
      "Ep 9 (Step 003040): Train loss 4.120, Val loss 4.847\n",
      "Ep 9 (Step 003050): Train loss 4.132, Val loss 4.847\n",
      "Ep 9 (Step 003060): Train loss 4.038, Val loss 4.848\n",
      "Ep 9 (Step 003070): Train loss 4.089, Val loss 4.849\n",
      "Ep 9 (Step 003080): Train loss 4.073, Val loss 4.849\n",
      "Ep 9 (Step 003090): Train loss 4.064, Val loss 4.848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9 (Step 003100): Train loss 4.029, Val loss 4.847\n",
      "Ep 9 (Step 003110): Train loss 4.046, Val loss 4.846\n",
      "Ep 9 (Step 003120): Train loss 4.093, Val loss 4.846\n",
      "Ep 9 (Step 003130): Train loss 4.077, Val loss 4.846\n",
      "Ep 9 (Step 003140): Train loss 4.074, Val loss 4.846\n",
      "Ep 10 (Step 003150): Train loss 4.054, Val loss 4.846\n",
      "Ep 10 (Step 003160): Train loss 4.029, Val loss 4.846\n",
      "Ep 10 (Step 003170): Train loss 4.140, Val loss 4.846\n",
      "Ep 10 (Step 003180): Train loss 4.067, Val loss 4.847\n",
      "Ep 10 (Step 003190): Train loss 4.024, Val loss 4.847\n",
      "Ep 10 (Step 003200): Train loss 4.107, Val loss 4.847\n",
      "Ep 10 (Step 003210): Train loss 4.073, Val loss 4.847\n",
      "Ep 10 (Step 003220): Train loss 4.061, Val loss 4.847\n",
      "Ep 10 (Step 003230): Train loss 4.097, Val loss 4.847\n",
      "Ep 10 (Step 003240): Train loss 4.026, Val loss 4.847\n",
      "Ep 10 (Step 003250): Train loss 4.027, Val loss 4.847\n",
      "Ep 10 (Step 003260): Train loss 4.107, Val loss 4.847\n",
      "Ep 10 (Step 003270): Train loss 4.066, Val loss 4.847\n",
      "Ep 10 (Step 003280): Train loss 4.108, Val loss 4.847\n",
      "Ep 10 (Step 003290): Train loss 4.126, Val loss 4.847\n",
      "Ep 10 (Step 003300): Train loss 4.085, Val loss 4.847\n",
      "Ep 10 (Step 003310): Train loss 4.078, Val loss 4.847\n",
      "Ep 10 (Step 003320): Train loss 4.028, Val loss 4.847\n",
      "Ep 10 (Step 003330): Train loss 4.067, Val loss 4.847\n",
      "Ep 10 (Step 003340): Train loss 4.114, Val loss 4.847\n",
      "Ep 10 (Step 003350): Train loss 4.051, Val loss 4.847\n",
      "Ep 10 (Step 003360): Train loss 4.046, Val loss 4.847\n",
      "Ep 10 (Step 003370): Train loss 4.087, Val loss 4.847\n",
      "Ep 10 (Step 003380): Train loss 4.042, Val loss 4.847\n",
      "Ep 10 (Step 003390): Train loss 4.055, Val loss 4.847\n",
      "Ep 10 (Step 003400): Train loss 4.069, Val loss 4.847\n",
      "Ep 10 (Step 003410): Train loss 4.044, Val loss 4.847\n",
      "Ep 10 (Step 003420): Train loss 4.103, Val loss 4.847\n",
      "Ep 10 (Step 003430): Train loss 4.009, Val loss 4.847\n",
      "Ep 10 (Step 003440): Train loss 4.032, Val loss 4.847\n",
      "Ep 10 (Step 003450): Train loss 4.065, Val loss 4.847\n",
      "Ep 10 (Step 003460): Train loss 4.093, Val loss 4.847\n",
      "Ep 10 (Step 003470): Train loss 4.055, Val loss 4.847\n",
      "Ep 10 (Step 003480): Train loss 4.106, Val loss 4.847\n",
      "Training completed in 15.73 minutes.\n"
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=10,\n",
    "      eval_iter=10, checkpoint_path=\"model_and_optimizer_best.pth\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8539769d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.484, Val loss 10.461\n",
      "Ep 1 (Step 000010): Train loss 8.235, Val loss 8.203\n",
      "Ep 1 (Step 000020): Train loss 7.011, Val loss 6.983\n",
      "Ep 1 (Step 000030): Train loss 6.659, Val loss 6.617\n",
      "Ep 1 (Step 000040): Train loss 6.569, Val loss 6.562\n",
      "Ep 1 (Step 000050): Train loss 6.469, Val loss 6.395\n",
      "Ep 1 (Step 000060): Train loss 6.120, Val loss 6.250\n",
      "Ep 1 (Step 000070): Train loss 5.919, Val loss 6.099\n",
      "Ep 1 (Step 000080): Train loss 5.901, Val loss 6.016\n",
      "Ep 1 (Step 000090): Train loss 5.774, Val loss 5.884\n",
      "Ep 1 (Step 000100): Train loss 5.647, Val loss 5.786\n",
      "Ep 1 (Step 000110): Train loss 5.595, Val loss 5.718\n",
      "Ep 1 (Step 000120): Train loss 5.488, Val loss 5.658\n",
      "Ep 1 (Step 000130): Train loss 5.502, Val loss 5.618\n",
      "Ep 1 (Step 000140): Train loss 5.392, Val loss 5.549\n",
      "Ep 1 (Step 000150): Train loss 5.318, Val loss 5.516\n",
      "Ep 1 (Step 000160): Train loss 5.285, Val loss 5.481\n",
      "Ep 1 (Step 000170): Train loss 5.227, Val loss 5.443\n",
      "Ep 1 (Step 000180): Train loss 5.306, Val loss 5.396\n",
      "Ep 1 (Step 000190): Train loss 5.247, Val loss 5.377\n",
      "Ep 1 (Step 000200): Train loss 5.217, Val loss 5.341\n",
      "Ep 1 (Step 000210): Train loss 5.176, Val loss 5.298\n",
      "Ep 1 (Step 000220): Train loss 5.142, Val loss 5.294\n",
      "Ep 1 (Step 000230): Train loss 5.034, Val loss 5.271\n",
      "Ep 1 (Step 000240): Train loss 4.985, Val loss 5.246\n",
      "Ep 1 (Step 000250): Train loss 5.101, Val loss 5.250\n",
      "Ep 1 (Step 000260): Train loss 5.071, Val loss 5.218\n",
      "Ep 1 (Step 000270): Train loss 4.969, Val loss 5.199\n",
      "Ep 1 (Step 000280): Train loss 4.883, Val loss 5.187\n",
      "Ep 1 (Step 000290): Train loss 4.936, Val loss 5.175\n",
      "Ep 1 (Step 000300): Train loss 4.807, Val loss 5.161\n",
      "Ep 1 (Step 000310): Train loss 5.005, Val loss 5.135\n",
      "Ep 1 (Step 000320): Train loss 4.994, Val loss 5.121\n",
      "Ep 1 (Step 000330): Train loss 4.881, Val loss 5.115\n",
      "Ep 1 (Step 000340): Train loss 4.900, Val loss 5.090\n",
      "Ep 1 (Step 000350): Train loss 4.848, Val loss 5.086\n",
      "Ep 1 (Step 000360): Train loss 4.834, Val loss 5.077\n",
      "Ep 2 (Step 000370): Train loss 4.873, Val loss 5.068\n",
      "Ep 2 (Step 000380): Train loss 4.761, Val loss 5.057\n",
      "Ep 2 (Step 000390): Train loss 4.721, Val loss 5.048\n",
      "Ep 2 (Step 000400): Train loss 4.733, Val loss 5.047\n",
      "Ep 2 (Step 000410): Train loss 4.797, Val loss 5.034\n",
      "Ep 2 (Step 000420): Train loss 4.706, Val loss 5.028\n",
      "Ep 2 (Step 000430): Train loss 4.739, Val loss 5.026\n",
      "Ep 2 (Step 000440): Train loss 4.692, Val loss 5.009\n",
      "Ep 2 (Step 000450): Train loss 4.779, Val loss 5.010\n",
      "Ep 2 (Step 000460): Train loss 4.692, Val loss 4.986\n",
      "Ep 2 (Step 000470): Train loss 4.721, Val loss 5.002\n",
      "Ep 2 (Step 000480): Train loss 4.804, Val loss 4.990\n",
      "Ep 2 (Step 000490): Train loss 4.616, Val loss 4.969\n",
      "Ep 2 (Step 000500): Train loss 4.645, Val loss 4.963\n",
      "Ep 2 (Step 000510): Train loss 4.606, Val loss 4.966\n",
      "Ep 2 (Step 000520): Train loss 4.591, Val loss 4.967\n",
      "Ep 2 (Step 000530): Train loss 4.634, Val loss 4.952\n",
      "Ep 2 (Step 000540): Train loss 4.548, Val loss 4.938\n",
      "Ep 2 (Step 000550): Train loss 4.511, Val loss 4.943\n",
      "Ep 2 (Step 000560): Train loss 4.516, Val loss 4.937\n",
      "Ep 2 (Step 000570): Train loss 4.621, Val loss 4.916\n",
      "Ep 2 (Step 000580): Train loss 4.530, Val loss 4.927\n",
      "Ep 2 (Step 000590): Train loss 4.500, Val loss 4.934\n",
      "Ep 2 (Step 000600): Train loss 4.532, Val loss 4.936\n",
      "Ep 2 (Step 000610): Train loss 4.565, Val loss 4.917\n",
      "Ep 2 (Step 000620): Train loss 4.582, Val loss 4.921\n",
      "Ep 2 (Step 000630): Train loss 4.416, Val loss 4.910\n",
      "Ep 2 (Step 000640): Train loss 4.507, Val loss 4.908\n",
      "Ep 2 (Step 000650): Train loss 4.495, Val loss 4.882\n",
      "Ep 2 (Step 000660): Train loss 4.565, Val loss 4.882\n",
      "Ep 2 (Step 000670): Train loss 4.466, Val loss 4.881\n",
      "Ep 2 (Step 000680): Train loss 4.459, Val loss 4.863\n",
      "Ep 2 (Step 000690): Train loss 4.456, Val loss 4.867\n",
      "Ep 2 (Step 000700): Train loss 4.483, Val loss 4.864\n",
      "Ep 2 (Step 000710): Train loss 4.411, Val loss 4.859\n",
      "Ep 2 (Step 000720): Train loss 4.316, Val loss 4.867\n",
      "Ep 3 (Step 000730): Train loss 4.358, Val loss 4.836\n",
      "Ep 3 (Step 000740): Train loss 4.385, Val loss 4.843\n",
      "Ep 3 (Step 000750): Train loss 4.416, Val loss 4.839\n",
      "Ep 3 (Step 000760): Train loss 4.240, Val loss 4.856\n",
      "Ep 3 (Step 000770): Train loss 4.377, Val loss 4.852\n",
      "Ep 3 (Step 000780): Train loss 4.465, Val loss 4.862\n",
      "Ep 3 (Step 000790): Train loss 4.390, Val loss 4.840\n",
      "Ep 3 (Step 000800): Train loss 4.274, Val loss 4.820\n",
      "Ep 3 (Step 000810): Train loss 4.292, Val loss 4.827\n",
      "Ep 3 (Step 000820): Train loss 4.381, Val loss 4.808\n",
      "Ep 3 (Step 000830): Train loss 4.316, Val loss 4.809\n",
      "Ep 3 (Step 000840): Train loss 4.357, Val loss 4.803\n",
      "Ep 3 (Step 000850): Train loss 4.401, Val loss 4.795\n",
      "Ep 3 (Step 000860): Train loss 4.158, Val loss 4.806\n",
      "Ep 3 (Step 000870): Train loss 4.228, Val loss 4.798\n",
      "Ep 3 (Step 000880): Train loss 4.387, Val loss 4.809\n",
      "Ep 3 (Step 000890): Train loss 4.297, Val loss 4.804\n",
      "Ep 3 (Step 000900): Train loss 4.235, Val loss 4.795\n",
      "Ep 3 (Step 000910): Train loss 4.404, Val loss 4.796\n",
      "Ep 3 (Step 000920): Train loss 4.248, Val loss 4.784\n",
      "Ep 3 (Step 000930): Train loss 4.329, Val loss 4.782\n",
      "Ep 3 (Step 000940): Train loss 4.209, Val loss 4.794\n",
      "Ep 3 (Step 000950): Train loss 4.237, Val loss 4.788\n",
      "Ep 3 (Step 000960): Train loss 4.255, Val loss 4.780\n",
      "Ep 3 (Step 000970): Train loss 4.320, Val loss 4.776\n",
      "Ep 3 (Step 000980): Train loss 4.135, Val loss 4.765\n",
      "Ep 3 (Step 000990): Train loss 4.280, Val loss 4.766\n",
      "Ep 3 (Step 001000): Train loss 4.206, Val loss 4.777\n",
      "Ep 3 (Step 001010): Train loss 4.135, Val loss 4.747\n",
      "Ep 3 (Step 001020): Train loss 4.227, Val loss 4.757\n",
      "Ep 3 (Step 001030): Train loss 4.136, Val loss 4.759\n",
      "Ep 3 (Step 001040): Train loss 4.173, Val loss 4.750\n",
      "Ep 3 (Step 001050): Train loss 4.167, Val loss 4.742\n",
      "Ep 3 (Step 001060): Train loss 4.228, Val loss 4.743\n",
      "Ep 3 (Step 001070): Train loss 4.194, Val loss 4.744\n",
      "Ep 3 (Step 001080): Train loss 4.149, Val loss 4.760\n",
      "Ep 3 (Step 001090): Train loss 4.108, Val loss 4.735\n",
      "Ep 4 (Step 001100): Train loss 4.138, Val loss 4.731\n",
      "Ep 4 (Step 001110): Train loss 4.160, Val loss 4.748\n",
      "Ep 4 (Step 001120): Train loss 4.109, Val loss 4.755\n",
      "Ep 4 (Step 001130): Train loss 4.184, Val loss 4.750\n",
      "Ep 4 (Step 001140): Train loss 4.127, Val loss 4.749\n",
      "Ep 4 (Step 001150): Train loss 4.147, Val loss 4.741\n",
      "Ep 4 (Step 001160): Train loss 4.093, Val loss 4.743\n",
      "Ep 4 (Step 001170): Train loss 4.147, Val loss 4.736\n",
      "Ep 4 (Step 001180): Train loss 4.098, Val loss 4.733\n",
      "Ep 4 (Step 001190): Train loss 4.039, Val loss 4.722\n",
      "Ep 4 (Step 001200): Train loss 4.071, Val loss 4.731\n",
      "Ep 4 (Step 001210): Train loss 3.924, Val loss 4.727\n",
      "Ep 4 (Step 001220): Train loss 4.106, Val loss 4.729\n",
      "Ep 4 (Step 001230): Train loss 4.132, Val loss 4.733\n",
      "Ep 4 (Step 001240): Train loss 3.987, Val loss 4.712\n",
      "Ep 4 (Step 001250): Train loss 4.012, Val loss 4.711\n",
      "Ep 4 (Step 001260): Train loss 4.097, Val loss 4.726\n",
      "Ep 4 (Step 001270): Train loss 4.011, Val loss 4.725\n",
      "Ep 4 (Step 001280): Train loss 4.021, Val loss 4.725\n",
      "Ep 4 (Step 001290): Train loss 4.038, Val loss 4.718\n",
      "Ep 4 (Step 001300): Train loss 4.047, Val loss 4.725\n",
      "Ep 4 (Step 001310): Train loss 3.957, Val loss 4.713\n",
      "Ep 4 (Step 001320): Train loss 3.999, Val loss 4.714\n",
      "Ep 4 (Step 001330): Train loss 3.906, Val loss 4.710\n",
      "Ep 4 (Step 001340): Train loss 4.015, Val loss 4.699\n",
      "Ep 4 (Step 001350): Train loss 3.994, Val loss 4.707\n",
      "Ep 4 (Step 001360): Train loss 3.981, Val loss 4.694\n",
      "Ep 4 (Step 001370): Train loss 3.921, Val loss 4.701\n",
      "Ep 4 (Step 001380): Train loss 3.939, Val loss 4.691\n",
      "Ep 4 (Step 001390): Train loss 3.924, Val loss 4.700\n",
      "Ep 4 (Step 001400): Train loss 3.878, Val loss 4.691\n",
      "Ep 4 (Step 001410): Train loss 3.927, Val loss 4.691\n",
      "Ep 4 (Step 001420): Train loss 3.957, Val loss 4.712\n",
      "Ep 4 (Step 001430): Train loss 3.901, Val loss 4.699\n",
      "Ep 4 (Step 001440): Train loss 3.913, Val loss 4.697\n",
      "Ep 4 (Step 001450): Train loss 3.846, Val loss 4.690\n",
      "Ep 5 (Step 001460): Train loss 3.930, Val loss 4.700\n",
      "Ep 5 (Step 001470): Train loss 3.898, Val loss 4.689\n",
      "Ep 5 (Step 001480): Train loss 3.831, Val loss 4.699\n",
      "Ep 5 (Step 001490): Train loss 3.866, Val loss 4.692\n",
      "Ep 5 (Step 001500): Train loss 3.867, Val loss 4.698\n",
      "Ep 5 (Step 001510): Train loss 3.874, Val loss 4.698\n",
      "Ep 5 (Step 001520): Train loss 3.861, Val loss 4.699\n",
      "Ep 5 (Step 001530): Train loss 3.831, Val loss 4.697\n",
      "Ep 5 (Step 001540): Train loss 3.894, Val loss 4.701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5 (Step 001550): Train loss 3.853, Val loss 4.701\n",
      "Ep 5 (Step 001560): Train loss 3.830, Val loss 4.711\n",
      "Ep 5 (Step 001570): Train loss 3.903, Val loss 4.702\n",
      "Ep 5 (Step 001580): Train loss 3.810, Val loss 4.697\n",
      "Ep 5 (Step 001590): Train loss 3.739, Val loss 4.700\n",
      "Ep 5 (Step 001600): Train loss 3.831, Val loss 4.685\n",
      "Ep 5 (Step 001610): Train loss 3.745, Val loss 4.690\n",
      "Ep 5 (Step 001620): Train loss 3.849, Val loss 4.693\n",
      "Ep 5 (Step 001630): Train loss 3.725, Val loss 4.681\n",
      "Ep 5 (Step 001640): Train loss 3.857, Val loss 4.701\n",
      "Ep 5 (Step 001650): Train loss 3.746, Val loss 4.691\n",
      "Ep 5 (Step 001660): Train loss 3.781, Val loss 4.686\n",
      "Ep 5 (Step 001670): Train loss 3.831, Val loss 4.691\n",
      "Ep 5 (Step 001680): Train loss 3.774, Val loss 4.696\n",
      "Ep 5 (Step 001690): Train loss 3.775, Val loss 4.697\n",
      "Ep 5 (Step 001700): Train loss 3.673, Val loss 4.691\n",
      "Ep 5 (Step 001710): Train loss 3.683, Val loss 4.689\n",
      "Ep 5 (Step 001720): Train loss 3.720, Val loss 4.691\n",
      "Ep 5 (Step 001730): Train loss 3.750, Val loss 4.689\n",
      "Ep 5 (Step 001740): Train loss 3.762, Val loss 4.686\n",
      "Ep 5 (Step 001750): Train loss 3.750, Val loss 4.678\n",
      "Ep 5 (Step 001760): Train loss 3.736, Val loss 4.674\n",
      "Ep 5 (Step 001770): Train loss 3.704, Val loss 4.681\n",
      "Ep 5 (Step 001780): Train loss 3.678, Val loss 4.678\n",
      "Ep 5 (Step 001790): Train loss 3.712, Val loss 4.670\n",
      "Ep 5 (Step 001800): Train loss 3.636, Val loss 4.669\n",
      "Ep 5 (Step 001810): Train loss 3.692, Val loss 4.670\n",
      "Ep 5 (Step 001820): Train loss 3.640, Val loss 4.679\n",
      "Ep 6 (Step 001830): Train loss 3.597, Val loss 4.675\n",
      "Ep 6 (Step 001840): Train loss 3.683, Val loss 4.682\n",
      "Ep 6 (Step 001850): Train loss 3.626, Val loss 4.672\n",
      "Ep 6 (Step 001860): Train loss 3.574, Val loss 4.682\n",
      "Ep 6 (Step 001870): Train loss 3.657, Val loss 4.688\n",
      "Ep 6 (Step 001880): Train loss 3.600, Val loss 4.687\n",
      "Ep 6 (Step 001890): Train loss 3.646, Val loss 4.689\n",
      "Ep 6 (Step 001900): Train loss 3.659, Val loss 4.684\n",
      "Ep 6 (Step 001910): Train loss 3.547, Val loss 4.700\n",
      "Ep 6 (Step 001920): Train loss 3.611, Val loss 4.699\n",
      "Ep 6 (Step 001930): Train loss 3.547, Val loss 4.698\n",
      "Ep 6 (Step 001940): Train loss 3.617, Val loss 4.697\n",
      "Ep 6 (Step 001950): Train loss 3.590, Val loss 4.699\n",
      "Ep 6 (Step 001960): Train loss 3.649, Val loss 4.698\n",
      "Ep 6 (Step 001970): Train loss 3.649, Val loss 4.698\n",
      "Ep 6 (Step 001980): Train loss 3.632, Val loss 4.695\n",
      "Ep 6 (Step 001990): Train loss 3.543, Val loss 4.693\n",
      "Ep 6 (Step 002000): Train loss 3.641, Val loss 4.697\n",
      "Ep 6 (Step 002010): Train loss 3.528, Val loss 4.703\n",
      "Ep 6 (Step 002020): Train loss 3.499, Val loss 4.695\n",
      "Ep 6 (Step 002030): Train loss 3.605, Val loss 4.695\n",
      "Ep 6 (Step 002040): Train loss 3.497, Val loss 4.690\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_49791/679252246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model on all works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train(train_loader, val_loader, num_epochs=10,\n\u001b[0m\u001b[1;32m      4\u001b[0m       eval_iter=10, checkpoint_path=\"model_and_optimizer_best_old_tok.pth\");\n",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_49791/584259918.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, num_epochs, eval_iter, lr, generate_sample_text, sample_text, checkpoint_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Pass train_losses and val_losses as references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     train_model_simple(\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/Build GPT from scratch/pre_train.py\u001b[0m in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs, eval_iter, start_context, cfg, train_losses, val_losses, track_tokens_seen, generate_sample_text, checkpoint_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reset gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=10,\n",
    "      eval_iter=10, checkpoint_path=\"model_and_optimizer_best_old_tok.pth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6651aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0002, weight_decay=0.05)\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer_best_old_tok.pth\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ecee23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from itertools import combinations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d523e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)  # Forward pass\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * target_ids.numel()\n",
    "            total_tokens += target_ids.numel()\n",
    "\n",
    "    perplexity = np.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "837f7534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.75698927127588"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adb0c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss Bennet has inherited the estate from her aunt, so she must have been rather.\n",
      "\"And I think you have no longer to be sure. My poor mother will be so very well known to be sure of Edward? And will not be able to tell you.\"\n",
      "\"Yes. You have given me in\n",
      "==================================================\n",
      "Mr. Darcy has inherited the estate from his aunt, so he must have no less well as he, though in his behaviour, and the country. Mr. Bennet was obliged to be a way by his behaviour to the room, as he was a very often wished to have happened; and Mr. Bennet,\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    prompt=\"Miss Bennet has inherited the estate from her aunt, so she must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model,  tokenizer=tokenizer,\n",
    "    prompt=\"Mr. Darcy has inherited the estate from his aunt, so he must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81220f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wife is not to her.\n",
      "“And you are very well.”\n",
      "“I have not know, “I am not you,\n",
      "==================================================\n",
      "A husband is not to be in the of this, I am sure I shall not mean to be sure I am sure to say to the very little to be so\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    prompt=\"A wife is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    prompt=\"A husband is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "714f6606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good lady ought to be done. He saw him in his manner of his own judgment. He was not to him, he had a man, he had he had not been\n",
      "==================================================\n",
      "A highly respectable man ought to be quite as she had seen him to take it. The house had passed, the greatest part of the house was not been the whole party of his own\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    prompt=\"A good lady ought to be\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    prompt=\"A highly respectable man ought to be\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67a25f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['state', 'param_groups'])\n",
      "Learning Rate (lr): 0.0002\n",
      "Weight Decay: 0.01\n",
      "Betas: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('model_and_optimizer_all_txt_updated.pth', map_location='cpu')\n",
    "\n",
    "# Extract optimizer state dict\n",
    "optimizer_state = checkpoint['optimizer_state_dict']\n",
    "\n",
    "# Optional: print all optimizer keys to explore\n",
    "print(optimizer_state.keys())\n",
    "\n",
    "# Extract settings (if AdamW or Adam)\n",
    "for param_group in optimizer_state['param_groups']:\n",
    "    print(\"Learning Rate (lr):\", param_group['lr'])\n",
    "    print(\"Weight Decay:\", param_group['weight_decay'])\n",
    "    print(\"Betas:\", optimizer_state['state'][list(optimizer_state['state'].keys())[0]]['exp_avg'])  # Optional state content\n",
    "    print(\"Eps (may not be saved):\", 'Check model code, not always stored')\n",
    "    print(param_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc249e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
