{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d339",
   "metadata": {},
   "source": [
    "### Detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d281a",
   "metadata": {},
   "source": [
    "### Set up model configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72561797-a3f0-4d84-9883-64c447482389",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 384,  # Context length\n",
    "    \"emb_dim\": 640,         # Embedding dimension\n",
    "    \"n_heads\": 8,          # Number of attention heads\n",
    "    \"n_layers\": 10,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": True,      # Query-Key-Value bias\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383eb92",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d1c16",
   "metadata": {},
   "source": [
    "#### GPT-2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75227c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88bc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(full_text):\n",
    "    return tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fbf11",
   "metadata": {},
   "source": [
    "### Load training and validation data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2b562de-efe1-40d2-a5ba-350b1edb7a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data.txt'\n",
    "val_file_path = 'val_text_data.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf608bf",
   "metadata": {},
   "source": [
    "### Initialize data loaders for training\n",
    "Data loaders implementation can be found in `./data_loader_v1.py`.\n",
    "\n",
    "This implementation follows the omplementation detailed in _Raschka, Sebastian. Build a Large Language Model (From Scratch). Manning Publications, 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bddf6dae-302d-4fc7-853b-2806a0c7d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f915a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 5789730\n",
      "Characters: 32372094\n",
      "Tokens: 7608098\n",
      "Unique Tokens Used: 28960\n"
     ]
    }
   ],
   "source": [
    "full_text = train_data + val_data\n",
    "\n",
    "word_count = len(full_text.split())\n",
    "char_count = len(full_text)\n",
    "\n",
    "tokens = tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "token_count = len(tokens)\n",
    "unique_token_count = len(set(tokens))\n",
    "\n",
    "print(\"Words:\", word_count)\n",
    "print(\"Characters:\", char_count)\n",
    "print(\"Tokens:\", token_count)\n",
    "print(\"Unique Tokens Used:\", unique_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f969edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean(): \n",
    "    \"\"\"\n",
    "    This is a function for GPU data claening before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    \n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.mps.empty_cache()  # Attempt to release MPS memory\n",
    "    \n",
    "    # Move tensors to CPU\n",
    "    for tensor in list(globals().values()):\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device == torch.device(\"mps\"):\n",
    "            tensor.to(\"cpu\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    del tensor\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "    print(\"Allocated Memory:\", torch.mps.current_allocated_memory() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da82d2c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f2c9bfd-5c57-4af6-98e8-5da47988d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train import train_model_simple\n",
    "import time\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\n",
    "def train(train_loader, val_loader,\n",
    "          num_epochs=10, eval_iter=5, lr=0.0002,\n",
    "          generate_sample_text=False,\n",
    "          sample_text=\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be\",\n",
    "          model_prefix=\"model_and_optimizer\"):\n",
    "\n",
    "    global train_losses, val_losses, track_tokens_seen  # Ensure these are updated globally\n",
    "\n",
    "    if device == \"mps\":\n",
    "        clean()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-08, weight_decay=0.05)\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=num_epochs, eval_iter=eval_iter,\n",
    "        start_context=sample_text, cfg=GPT_CONFIG_124M,\n",
    "        generate_sample_text=generate_sample_text,\n",
    "        model_prefix=model_prefix,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        print(50 * \"=\")\n",
    "        clean()\n",
    "    if device == \"cuda\":\n",
    "        print(50 * \"=\")\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d594966-9781-4ea2-9a68-01196f5111b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae6fc",
   "metadata": {},
   "source": [
    "### Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda45148",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.449, Val loss 10.457\n",
      "Ep 1 (Step 000150): Train loss 6.141, Val loss 6.075\n",
      "Ep 1 (Step 000300): Train loss 5.762, Val loss 5.716\n",
      "Ep 1 (Step 000450): Train loss 5.631, Val loss 5.572\n",
      "Ep 1 (Step 000600): Train loss 5.530, Val loss 5.458\n",
      "Ep 1 (Step 000750): Train loss 5.393, Val loss 5.399\n",
      "Ep 1 (Step 000900): Train loss 5.349, Val loss 5.337\n",
      "Ep 1 (Step 001050): Train loss 5.309, Val loss 5.282\n",
      "Ep 1 (Step 001200): Train loss 5.212, Val loss 5.193\n",
      "Ep 1 (Step 001350): Train loss 5.194, Val loss 5.169\n",
      "Ep 1 (Step 001500): Train loss 5.145, Val loss 5.067\n",
      "Ep 1 (Step 001650): Train loss 5.045, Val loss 5.060\n",
      "Ep 1 (Step 001800): Train loss 5.056, Val loss 4.991\n",
      "Ep 1 (Step 001950): Train loss 4.967, Val loss 4.930\n",
      "Ep 1 (Step 002100): Train loss 4.948, Val loss 4.903\n",
      "Ep 1 (Step 002250): Train loss 4.875, Val loss 4.900\n",
      "Ep 1 (Step 002400): Train loss 4.897, Val loss 4.862\n",
      "Ep 1 (Step 002550): Train loss 4.870, Val loss 4.860\n",
      "Ep 1 (Step 002700): Train loss 4.812, Val loss 4.812\n",
      "Ep 1 (Step 002850): Train loss 4.801, Val loss 4.808\n",
      "Ep 1 (Step 003000): Train loss 4.756, Val loss 4.759\n",
      "Ep 1 (Step 003150): Train loss 4.751, Val loss 4.735\n",
      "Ep 1 (Step 003300): Train loss 4.739, Val loss 4.707\n",
      "Ep 1 (Step 003450): Train loss 4.729, Val loss 4.693\n",
      "Ep 1 (Step 003600): Train loss 4.678, Val loss 4.699\n",
      "Ep 1 (Step 003750): Train loss 4.663, Val loss 4.667\n",
      "Ep 1 (Step 003900): Train loss 4.595, Val loss 4.634\n",
      "Ep 1 (Step 004050): Train loss 4.586, Val loss 4.636\n",
      "Ep 1 (Step 004200): Train loss 4.573, Val loss 4.609\n",
      "Ep 1 (Step 004350): Train loss 4.544, Val loss 4.604\n",
      "Ep 2 (Step 004500): Train loss 4.540, Val loss 4.599\n",
      "Ep 2 (Step 004650): Train loss 4.544, Val loss 4.563\n",
      "Ep 2 (Step 004800): Train loss 4.496, Val loss 4.547\n",
      "Ep 2 (Step 004950): Train loss 4.478, Val loss 4.560\n",
      "Ep 2 (Step 005100): Train loss 4.478, Val loss 4.530\n",
      "Ep 2 (Step 005250): Train loss 4.448, Val loss 4.530\n",
      "Ep 2 (Step 005400): Train loss 4.437, Val loss 4.516\n",
      "Ep 2 (Step 005550): Train loss 4.461, Val loss 4.504\n",
      "Ep 2 (Step 005700): Train loss 4.435, Val loss 4.477\n",
      "Ep 2 (Step 005850): Train loss 4.413, Val loss 4.461\n",
      "Ep 2 (Step 006000): Train loss 4.377, Val loss 4.442\n",
      "Ep 2 (Step 006150): Train loss 4.372, Val loss 4.432\n",
      "Ep 2 (Step 006300): Train loss 4.374, Val loss 4.416\n",
      "Ep 2 (Step 006450): Train loss 4.349, Val loss 4.401\n",
      "Ep 2 (Step 006600): Train loss 4.332, Val loss 4.395\n",
      "Ep 2 (Step 006750): Train loss 4.321, Val loss 4.396\n",
      "Ep 2 (Step 006900): Train loss 4.287, Val loss 4.401\n",
      "Ep 2 (Step 007050): Train loss 4.309, Val loss 4.391\n",
      "Ep 2 (Step 007200): Train loss 4.269, Val loss 4.389\n",
      "Ep 2 (Step 007350): Train loss 4.243, Val loss 4.384\n",
      "Ep 2 (Step 007500): Train loss 4.257, Val loss 4.371\n",
      "Ep 2 (Step 007650): Train loss 4.278, Val loss 4.368\n",
      "Ep 2 (Step 007800): Train loss 4.228, Val loss 4.348\n",
      "Ep 2 (Step 007950): Train loss 4.211, Val loss 4.339\n",
      "Ep 2 (Step 008100): Train loss 4.181, Val loss 4.332\n",
      "Ep 2 (Step 008250): Train loss 4.186, Val loss 4.320\n",
      "Ep 2 (Step 008400): Train loss 4.194, Val loss 4.309\n",
      "Ep 2 (Step 008550): Train loss 4.198, Val loss 4.280\n",
      "Ep 2 (Step 008700): Train loss 4.175, Val loss 4.281\n",
      "Ep 2 (Step 008850): Train loss 4.179, Val loss 4.268\n",
      "Ep 3 (Step 009000): Train loss 4.184, Val loss 4.274\n",
      "Ep 3 (Step 009150): Train loss 4.142, Val loss 4.257\n",
      "Ep 3 (Step 009300): Train loss 4.159, Val loss 4.236\n",
      "Ep 3 (Step 009450): Train loss 4.145, Val loss 4.247\n",
      "Ep 3 (Step 009600): Train loss 4.099, Val loss 4.245\n",
      "Ep 3 (Step 009750): Train loss 4.117, Val loss 4.247\n",
      "Ep 3 (Step 009900): Train loss 4.084, Val loss 4.231\n",
      "Ep 3 (Step 010050): Train loss 4.075, Val loss 4.227\n",
      "Ep 3 (Step 010200): Train loss 4.074, Val loss 4.229\n",
      "Ep 3 (Step 010350): Train loss 4.054, Val loss 4.233\n",
      "Ep 3 (Step 010500): Train loss 4.046, Val loss 4.216\n",
      "Ep 3 (Step 010650): Train loss 4.050, Val loss 4.213\n",
      "Ep 3 (Step 010800): Train loss 4.043, Val loss 4.203\n",
      "Ep 3 (Step 010950): Train loss 4.055, Val loss 4.207\n",
      "Ep 3 (Step 011100): Train loss 4.014, Val loss 4.210\n",
      "Ep 3 (Step 011250): Train loss 4.000, Val loss 4.201\n",
      "Ep 3 (Step 011400): Train loss 4.008, Val loss 4.205\n",
      "Ep 3 (Step 011550): Train loss 4.007, Val loss 4.193\n",
      "Ep 3 (Step 011700): Train loss 3.987, Val loss 4.186\n",
      "Ep 3 (Step 011850): Train loss 3.991, Val loss 4.194\n",
      "Ep 3 (Step 012000): Train loss 4.007, Val loss 4.190\n",
      "Ep 3 (Step 012150): Train loss 3.964, Val loss 4.180\n",
      "Ep 3 (Step 012300): Train loss 3.943, Val loss 4.160\n",
      "Ep 3 (Step 012450): Train loss 3.950, Val loss 4.166\n",
      "Ep 3 (Step 012600): Train loss 3.960, Val loss 4.155\n",
      "Ep 3 (Step 012750): Train loss 3.949, Val loss 4.153\n",
      "Ep 3 (Step 012900): Train loss 3.975, Val loss 4.153\n",
      "Ep 3 (Step 013050): Train loss 3.925, Val loss 4.154\n",
      "Ep 3 (Step 013200): Train loss 3.898, Val loss 4.158\n",
      "Ep 3 (Step 013350): Train loss 3.897, Val loss 4.152\n",
      "Ep 4 (Step 013500): Train loss 3.918, Val loss 4.172\n",
      "Ep 4 (Step 013650): Train loss 3.895, Val loss 4.167\n",
      "Ep 4 (Step 013800): Train loss 3.916, Val loss 4.160\n",
      "Ep 4 (Step 013950): Train loss 3.894, Val loss 4.149\n",
      "Ep 4 (Step 014100): Train loss 3.858, Val loss 4.146\n",
      "Ep 4 (Step 014250): Train loss 3.847, Val loss 4.134\n",
      "Ep 4 (Step 014400): Train loss 3.869, Val loss 4.127\n",
      "Ep 4 (Step 014550): Train loss 3.850, Val loss 4.128\n",
      "Ep 4 (Step 014700): Train loss 3.857, Val loss 4.120\n",
      "Ep 4 (Step 014850): Train loss 3.846, Val loss 4.119\n",
      "Ep 4 (Step 015000): Train loss 3.844, Val loss 4.112\n",
      "Ep 4 (Step 015150): Train loss 3.831, Val loss 4.094\n",
      "Ep 4 (Step 015300): Train loss 3.792, Val loss 4.103\n",
      "Ep 4 (Step 015450): Train loss 3.794, Val loss 4.096\n",
      "Ep 4 (Step 015600): Train loss 3.838, Val loss 4.098\n",
      "Ep 4 (Step 015750): Train loss 3.835, Val loss 4.089\n",
      "Ep 4 (Step 015900): Train loss 3.803, Val loss 4.087\n",
      "Ep 4 (Step 016050): Train loss 3.832, Val loss 4.091\n",
      "Ep 4 (Step 016200): Train loss 3.814, Val loss 4.090\n",
      "Ep 4 (Step 016350): Train loss 3.804, Val loss 4.088\n",
      "Ep 4 (Step 016500): Train loss 3.781, Val loss 4.084\n",
      "Ep 4 (Step 016650): Train loss 3.774, Val loss 4.087\n",
      "Ep 4 (Step 016800): Train loss 3.752, Val loss 4.080\n",
      "Ep 4 (Step 016950): Train loss 3.764, Val loss 4.075\n",
      "Ep 4 (Step 017100): Train loss 3.745, Val loss 4.073\n",
      "Ep 4 (Step 017250): Train loss 3.734, Val loss 4.072\n",
      "Ep 4 (Step 017400): Train loss 3.744, Val loss 4.066\n",
      "Ep 4 (Step 017550): Train loss 3.723, Val loss 4.072\n",
      "Ep 4 (Step 017700): Train loss 3.746, Val loss 4.066\n",
      "Ep 5 (Step 017850): Train loss 3.749, Val loss 4.072\n",
      "Ep 5 (Step 018000): Train loss 3.750, Val loss 4.074\n",
      "Ep 5 (Step 018150): Train loss 3.736, Val loss 4.075\n",
      "Ep 5 (Step 018300): Train loss 3.704, Val loss 4.078\n",
      "Ep 5 (Step 018450): Train loss 3.750, Val loss 4.075\n",
      "Ep 5 (Step 018600): Train loss 3.714, Val loss 4.072\n",
      "Ep 5 (Step 018750): Train loss 3.726, Val loss 4.072\n",
      "Ep 5 (Step 018900): Train loss 3.731, Val loss 4.061\n",
      "Ep 5 (Step 019050): Train loss 3.723, Val loss 4.064\n",
      "Ep 5 (Step 019200): Train loss 3.738, Val loss 4.063\n",
      "Ep 5 (Step 019350): Train loss 3.677, Val loss 4.060\n",
      "Ep 5 (Step 019500): Train loss 3.693, Val loss 4.057\n",
      "Ep 5 (Step 019650): Train loss 3.653, Val loss 4.052\n",
      "Ep 5 (Step 019800): Train loss 3.683, Val loss 4.055\n",
      "Ep 5 (Step 019950): Train loss 3.680, Val loss 4.046\n",
      "Ep 5 (Step 020100): Train loss 3.681, Val loss 4.047\n",
      "Ep 5 (Step 020250): Train loss 3.676, Val loss 4.044\n",
      "Ep 5 (Step 020400): Train loss 3.684, Val loss 4.043\n",
      "Ep 5 (Step 020550): Train loss 3.645, Val loss 4.041\n",
      "Ep 5 (Step 020700): Train loss 3.665, Val loss 4.041\n",
      "Ep 5 (Step 020850): Train loss 3.643, Val loss 4.039\n",
      "Ep 5 (Step 021000): Train loss 3.675, Val loss 4.041\n",
      "Ep 5 (Step 021150): Train loss 3.657, Val loss 4.038\n",
      "Ep 5 (Step 021300): Train loss 3.666, Val loss 4.034\n",
      "Ep 5 (Step 021450): Train loss 3.652, Val loss 4.035\n",
      "Ep 5 (Step 021600): Train loss 3.661, Val loss 4.032\n",
      "Ep 5 (Step 021750): Train loss 3.678, Val loss 4.035\n",
      "Ep 5 (Step 021900): Train loss 3.644, Val loss 4.035\n",
      "Ep 5 (Step 022050): Train loss 3.636, Val loss 4.032\n",
      "Ep 5 (Step 022200): Train loss 3.635, Val loss 4.030\n",
      "Ep 6 (Step 022350): Train loss 3.645, Val loss 4.032\n",
      "Ep 6 (Step 022500): Train loss 3.637, Val loss 4.034\n",
      "Ep 6 (Step 022650): Train loss 3.648, Val loss 4.037\n",
      "Ep 6 (Step 022800): Train loss 3.644, Val loss 4.033\n",
      "Ep 6 (Step 022950): Train loss 3.617, Val loss 4.035\n",
      "Ep 6 (Step 023100): Train loss 3.597, Val loss 4.034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6 (Step 023250): Train loss 3.627, Val loss 4.034\n",
      "Ep 6 (Step 023400): Train loss 3.616, Val loss 4.032\n",
      "Ep 6 (Step 023550): Train loss 3.626, Val loss 4.031\n",
      "Ep 6 (Step 023700): Train loss 3.645, Val loss 4.034\n",
      "Ep 6 (Step 023850): Train loss 3.635, Val loss 4.036\n",
      "Ep 6 (Step 024000): Train loss 3.629, Val loss 4.032\n",
      "Ep 6 (Step 024150): Train loss 3.625, Val loss 4.031\n",
      "Ep 6 (Step 024300): Train loss 3.601, Val loss 4.032\n",
      "Ep 6 (Step 024450): Train loss 3.619, Val loss 4.031\n",
      "Ep 6 (Step 024600): Train loss 3.620, Val loss 4.031\n",
      "Ep 6 (Step 024750): Train loss 3.614, Val loss 4.033\n",
      "Ep 6 (Step 024900): Train loss 3.649, Val loss 4.032\n",
      "Ep 6 (Step 025050): Train loss 3.629, Val loss 4.032\n",
      "Ep 6 (Step 025200): Train loss 3.619, Val loss 4.031\n",
      "Ep 6 (Step 025350): Train loss 3.599, Val loss 4.031\n",
      "Ep 6 (Step 025500): Train loss 3.652, Val loss 4.032\n",
      "Ep 6 (Step 025650): Train loss 3.641, Val loss 4.032\n",
      "Ep 6 (Step 025800): Train loss 3.603, Val loss 4.031\n",
      "Ep 6 (Step 025950): Train loss 3.624, Val loss 4.031\n",
      "Ep 6 (Step 026100): Train loss 3.651, Val loss 4.031\n",
      "Ep 6 (Step 026250): Train loss 3.617, Val loss 4.031\n",
      "Ep 6 (Step 026400): Train loss 3.618, Val loss 4.031\n",
      "Ep 6 (Step 026550): Train loss 3.614, Val loss 4.031\n",
      "Ep 6 (Step 026700): Train loss 3.602, Val loss 4.031\n",
      "Training completed in 400.43 minutes.\n"
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=6,\n",
    "      eval_iter=150, model_prefix=\"model_640_10_8_0_1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6651aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0002, weight_decay=0.05)\n",
    "\n",
    "checkpoint = torch.load(\"model_768_12_12_512.pth\", weights_only=True, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc47f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ecee23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 23:19:26.056087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. This error is frequently encountered on macOS when running an x86 Python installation on ARM hardware. In this case, try installing an ARM build of Python. Otherwise, you may be able work around this issue by building jaxlib from source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_81582/843098088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/evaluate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluation_suite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationSuite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m from .evaluator import (\n\u001b[1;32m     31\u001b[0m     \u001b[0mAudioClassificationEvaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/evaluate/evaluation_suite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluation_module_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/evaluate/evaluator/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSUPPORTED_TASKS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSUPPORTED_PIPELINE_TASKS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTASK_ALIASES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_task\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcheck_pipeline_task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_class_from_dynamic_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseImageProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFEATURE_EXTRACTOR_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_processing_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageProcessingMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_image_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpreter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpHint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauthoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelAnalyzer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpResolverType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/authoring/authoring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverter_error_data_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrap_toco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_phase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxla_computation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_xla_computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0m_xla_computation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Set Cloud TPU env vars if necessary before transitively loading C++ backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud_tpu_init\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcloud_tpu_init\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_cloud_tpu_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0m_cloud_tpu_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/cloud_tpu_init.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardware_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransfer_guard_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# uses instructions that are present on this machine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_feature_guard\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcpu_feature_guard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mcpu_feature_guard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_cpu_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. This error is frequently encountered on macOS when running an x86 Python installation on ARM hardware. In this case, try installing an ARM build of Python. Otherwise, you may be able work around this issue by building jaxlib from source."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from itertools import combinations\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d523e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)  # Forward pass\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * target_ids.numel()\n",
    "            total_tokens += target_ids.numel()\n",
    "\n",
    "    perplexity = np.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "837f7534",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_81582/1973960337.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/c1/qdg0q3b92ys5hzf9t6h4xvf40000gn/T/ipykernel_81582/160537806.py\u001b[0m in \u001b[0;36mcompute_perplexity\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "compute_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c61225-1077-4019-984a-564aa7ba6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "def weat_score(model, target_words_1, target_words_2, attribute_words_1, attribute_words_2, tokenizer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Measures bias by comparing how close different groups of words are in embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding(word):\n",
    "        token_id = tokenizer.encode(word, allowed_special={'<|endoftext|>'})[0]\n",
    "        with torch.no_grad():\n",
    "            embed = model.tok_emb(torch.tensor([token_id], device=device)).cpu().numpy()\n",
    "        return embed.flatten()\n",
    "\n",
    "    # Get embeddings\n",
    "    target_1_embs = [get_embedding(w) for w in target_words_1]\n",
    "    target_2_embs = [get_embedding(w) for w in target_words_2]\n",
    "    attr_1_embs = [get_embedding(w) for w in attribute_words_1]\n",
    "    attr_2_embs = [get_embedding(w) for w in attribute_words_2]\n",
    "\n",
    "    def association(t, A, B):\n",
    "        return np.mean([cosine_similarity(t, a) for a in A]) - np.mean([cosine_similarity(t, b) for b in B])\n",
    "\n",
    "    # Compute WEAT score\n",
    "    s1 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_1_embs])\n",
    "    s2 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_2_embs])\n",
    "    \n",
    "    weat_score = s1 - s2\n",
    "    return weat_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a1406-0bd9-409a-9fd8-8b4f343d2c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_male = [\"gentleman\", \"officer\", \"clergyman\", \"husband\", \"captain\"]\n",
    "target_female = [\"lady\", \"governess\", \"girl\", \"wife\", \"widow\"]\n",
    "\n",
    "attribute_male = [\"honour\", \"duty\", \"wisdom\", \"fortitude\", \"independence\"]\n",
    "attribute_female = [\"grace\", \"affection\", \"beauty\", \"delicacy\", \"modesty\"]\n",
    "\n",
    "weat_score(model, target_male, target_female, attribute_male, attribute_female, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcce2e-326f-4cc6-8033-e77e0a2ba270",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d845171-c8f1-47a0-a60b-1f9847a22590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import re\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_bleu_rouge_from_val(model, device=\"cpu\"):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    # Step 1: Load the validation set\n",
    "    with open('val_text_data_all_txt.txt', 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Step 2: Split into sentences & filter\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', data)\n",
    "    filtered_sentences = [s.strip() for s in sentences if 5 <= len(s.split()) <= 60]\n",
    "    filtered_sentences = filtered_sentences[:1000]\n",
    "\n",
    "    # Step 3: Split each sentence into two halves and store as tuples\n",
    "    sentence_tuples = []\n",
    "    for sent in filtered_sentences:\n",
    "        words = sent.split()\n",
    "        mid = len(words) // 2\n",
    "        first_half = ' '.join(words[:mid])\n",
    "        second_half = ' '.join(words[mid:])\n",
    "        sentence_tuples.append((first_half, second_half))\n",
    "\n",
    "    # Step 4: For each (first_half, second_half), generate prediction\n",
    "    for first_half, second_half in sentence_tuples:\n",
    "        generated_text = generate(\n",
    "            model=model, prompt=first_half,\n",
    "            max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "            device=device,\n",
    "            temperature=0.7,\n",
    "            top_k=50\n",
    "        )\n",
    "\n",
    "        # Build reference and prediction\n",
    "        reference = first_half + \" \" + second_half\n",
    "        prediction = generated_text\n",
    "\n",
    "        references.append(reference)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Step 5-6: Compute BLEU and ROUGE\n",
    "    # Format references correctly for BLEU\n",
    "    references_formatted = [[ref] for ref in references]\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references_formatted)['bleu']\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}, ROUGE-L Score: {rouge_score['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404bbea-7821-4b52-a5e9-f240bf5c01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_bleu_rouge_from_val(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Miss Bennet has inherited the estate from her aunt, so she must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Mr. Darcy has inherited the estate from his aunt, so he must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81220f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"A wife is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"A husband is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"I shall now go\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"He said\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"She was\",\n",
    "    max_new_tokens=200, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb810-cfc6-49fe-bfd0-66c035e0707e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78fd8b69-ae98-4300-b876-93bf84fa0d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to be the very day, and I am sure I am sure I am sure I should have been a very much obliged to be very happy. I am'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "583fd4d9-4273-435a-a2fe-651162acb64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to go and Mrs. Weston.\\n\"I am very glad to think of your own family.\"\\n\"I will not like you. I am afraid'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2c8a060b-6d8a-4dc8-ade9-bfde78b38900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she is wild to get married from me to do the best of them. I think it is quite forgot to write to be a great deal. You must own. There is nothing'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"she is wild to get married\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=1,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba961e03-e482-4601-bef7-daab0c1dac77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f99b81-a4be-4994-9c00-8898a408f06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
