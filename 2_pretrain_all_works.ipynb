{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794e4f44-970d-4f30-a0d9-58c5df31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from gpt_model import GPTModel\n",
    "from data_loader_v1 import create_dataloader_v1\n",
    "from generate_text import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d339",
   "metadata": {},
   "source": [
    "### Detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e6d70-0358-4455-b556-01f4283ac928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d281a",
   "metadata": {},
   "source": [
    "### Set up model configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72561797-a3f0-4d84-9883-64c447482389",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 896,         # Embedding dimension\n",
    "    \"n_heads\": 14,          # Number of attention heads\n",
    "    \"n_layers\": 8,         # Number of layers\n",
    "    \"drop_rate\": 0.2,       # Dropout rate\n",
    "    \"qkv_bias\": True,      # Query-Key-Value bias\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383eb92",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d1c16",
   "metadata": {},
   "source": [
    "#### GPT-2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75227c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88bc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(full_text):\n",
    "    return tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fbf11",
   "metadata": {},
   "source": [
    "### Load training and validation data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2b562de-efe1-40d2-a5ba-350b1edb7a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_file_path = 'train_text_data.txt'\n",
    "val_file_path = 'val_text_data.txt'\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.read()\n",
    "with open(val_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf608bf",
   "metadata": {},
   "source": [
    "### Initialize data loaders for training\n",
    "Data loaders implementation can be found in `./data_loader_v1.py`.\n",
    "\n",
    "This implementation follows the omplementation detailed in _Raschka, Sebastian. Build a Large Language Model (From Scratch). Manning Publications, 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bddf6dae-302d-4fc7-853b-2806a0c7d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    encode=encode,\n",
    "    batch_size=4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f915a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 5789730\n",
      "Characters: 32372094\n",
      "Tokens: 7608098\n",
      "Unique Tokens Used: 28960\n"
     ]
    }
   ],
   "source": [
    "full_text = train_data + val_data\n",
    "\n",
    "word_count = len(full_text.split())\n",
    "char_count = len(full_text)\n",
    "\n",
    "tokens = tokenizer.encode(full_text, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "token_count = len(tokens)\n",
    "unique_token_count = len(set(tokens))\n",
    "\n",
    "print(\"Words:\", word_count)\n",
    "print(\"Characters:\", char_count)\n",
    "print(\"Tokens:\", token_count)\n",
    "print(\"Unique Tokens Used:\", unique_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f969edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean(): \n",
    "    \"\"\"\n",
    "    This is a function for GPU data claening before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    \n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.mps.empty_cache()  # Attempt to release MPS memory\n",
    "    \n",
    "    # Move tensors to CPU\n",
    "    for tensor in list(globals().values()):\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device == torch.device(\"mps\"):\n",
    "            tensor.to(\"cpu\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    del tensor\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "    print(\"Allocated Memory:\", torch.mps.current_allocated_memory() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da82d2c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f2c9bfd-5c57-4af6-98e8-5da47988d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train import train_model_simple\n",
    "import time\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\n",
    "def train(train_loader, val_loader,\n",
    "          num_epochs=10, eval_iter=5, lr=0.0002,\n",
    "          generate_sample_text=False,\n",
    "          sample_text=\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be\",\n",
    "          model_prefix=\"model_and_optimizer\"):\n",
    "\n",
    "    global train_losses, val_losses, track_tokens_seen  # Ensure these are updated globally\n",
    "\n",
    "    if device == \"mps\":\n",
    "        clean()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "        print(50 * \"=\")\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-08, weight_decay=0.05)\n",
    "\n",
    "    # Pass train_losses and val_losses as references\n",
    "    train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        num_epochs=num_epochs, eval_iter=eval_iter,\n",
    "        start_context=sample_text, cfg=GPT_CONFIG_124M,\n",
    "        generate_sample_text=generate_sample_text,\n",
    "        model_prefix=model_prefix,\n",
    "        train_losses=train_losses, val_losses=val_losses,\n",
    "        track_tokens_seen=track_tokens_seen,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        print(50 * \"=\")\n",
    "        clean()\n",
    "    if device == \"cuda\":\n",
    "        print(50 * \"=\")\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.memory_summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d594966-9781-4ea2-9a68-01196f5111b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae6fc",
   "metadata": {},
   "source": [
    "### Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda45148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.369, Val loss 10.398\n",
      "Ep 1 (Step 000150): Train loss 6.153, Val loss 6.124\n",
      "Ep 1 (Step 000300): Train loss 5.855, Val loss 5.788\n",
      "Ep 1 (Step 000450): Train loss 5.681, Val loss 5.622\n",
      "Ep 1 (Step 000600): Train loss 5.562, Val loss 5.517\n",
      "Ep 1 (Step 000750): Train loss 5.500, Val loss 5.448\n",
      "Ep 1 (Step 000900): Train loss 5.415, Val loss 5.313\n",
      "Ep 1 (Step 001050): Train loss 5.358, Val loss 5.258\n",
      "Ep 1 (Step 001200): Train loss 5.286, Val loss 5.186\n",
      "Ep 1 (Step 001350): Train loss 5.222, Val loss 5.160\n",
      "Ep 1 (Step 001500): Train loss 5.204, Val loss 5.098\n",
      "Ep 1 (Step 001650): Train loss 5.166, Val loss 5.030\n",
      "Ep 1 (Step 001800): Train loss 5.141, Val loss 5.005\n",
      "Ep 1 (Step 001950): Train loss 5.070, Val loss 4.971\n",
      "Ep 1 (Step 002100): Train loss 5.018, Val loss 4.932\n",
      "Ep 1 (Step 002250): Train loss 5.019, Val loss 4.964\n",
      "Ep 1 (Step 002400): Train loss 4.979, Val loss 4.851\n",
      "Ep 1 (Step 002550): Train loss 4.978, Val loss 4.872\n",
      "Ep 1 (Step 002700): Train loss 4.950, Val loss 4.850\n",
      "Ep 1 (Step 002850): Train loss 4.910, Val loss 4.813\n",
      "Ep 1 (Step 003000): Train loss 4.899, Val loss 4.811\n",
      "Ep 1 (Step 003150): Train loss 4.829, Val loss 4.801\n",
      "Ep 1 (Step 003300): Train loss 4.839, Val loss 4.794\n",
      "Ep 1 (Step 003450): Train loss 4.833, Val loss 4.769\n",
      "Ep 1 (Step 003600): Train loss 4.828, Val loss 4.727\n",
      "Ep 1 (Step 003750): Train loss 4.759, Val loss 4.719\n",
      "Ep 1 (Step 003900): Train loss 4.735, Val loss 4.730\n",
      "Ep 1 (Step 004050): Train loss 4.727, Val loss 4.685\n",
      "Ep 1 (Step 004200): Train loss 4.741, Val loss 4.695\n",
      "Ep 1 (Step 004350): Train loss 4.704, Val loss 4.684\n",
      "Ep 1 (Step 004500): Train loss 4.672, Val loss 4.686\n",
      "Ep 1 (Step 004650): Train loss 4.693, Val loss 4.669\n",
      "Ep 1 (Step 004800): Train loss 4.671, Val loss 4.658\n",
      "Ep 1 (Step 004950): Train loss 4.632, Val loss 4.601\n",
      "Ep 1 (Step 005100): Train loss 4.613, Val loss 4.599\n",
      "Ep 1 (Step 005250): Train loss 4.581, Val loss 4.601\n",
      "Ep 1 (Step 005400): Train loss 4.604, Val loss 4.605\n",
      "Ep 1 (Step 005550): Train loss 4.605, Val loss 4.599\n",
      "Ep 1 (Step 005700): Train loss 4.612, Val loss 4.612\n",
      "Ep 1 (Step 005850): Train loss 4.594, Val loss 4.572\n",
      "Ep 1 (Step 006000): Train loss 4.533, Val loss 4.559\n",
      "Ep 1 (Step 006150): Train loss 4.524, Val loss 4.520\n",
      "Ep 1 (Step 006300): Train loss 4.513, Val loss 4.505\n",
      "Ep 1 (Step 006450): Train loss 4.498, Val loss 4.499\n",
      "Ep 1 (Step 006600): Train loss 4.508, Val loss 4.483\n",
      "Ep 2 (Step 006750): Train loss 4.455, Val loss 4.480\n",
      "Ep 2 (Step 006900): Train loss 4.482, Val loss 4.464\n",
      "Ep 2 (Step 007050): Train loss 4.484, Val loss 4.467\n",
      "Ep 2 (Step 007200): Train loss 4.447, Val loss 4.478\n",
      "Ep 2 (Step 007350): Train loss 4.495, Val loss 4.473\n",
      "Ep 2 (Step 007500): Train loss 4.459, Val loss 4.461\n",
      "Ep 2 (Step 007650): Train loss 4.425, Val loss 4.450\n",
      "Ep 2 (Step 007800): Train loss 4.464, Val loss 4.451\n",
      "Ep 2 (Step 007950): Train loss 4.374, Val loss 4.414\n",
      "Ep 2 (Step 008100): Train loss 4.448, Val loss 4.421\n",
      "Ep 2 (Step 008250): Train loss 4.378, Val loss 4.412\n",
      "Ep 2 (Step 008400): Train loss 4.382, Val loss 4.393\n",
      "Ep 2 (Step 008550): Train loss 4.372, Val loss 4.371\n",
      "Ep 2 (Step 008700): Train loss 4.369, Val loss 4.377\n",
      "Ep 2 (Step 008850): Train loss 4.352, Val loss 4.408\n",
      "Ep 2 (Step 009000): Train loss 4.340, Val loss 4.382\n",
      "Ep 2 (Step 009150): Train loss 4.313, Val loss 4.359\n",
      "Ep 2 (Step 009300): Train loss 4.329, Val loss 4.355\n",
      "Ep 2 (Step 009450): Train loss 4.347, Val loss 4.356\n",
      "Ep 2 (Step 009600): Train loss 4.360, Val loss 4.340\n",
      "Ep 2 (Step 009750): Train loss 4.322, Val loss 4.345\n",
      "Ep 2 (Step 009900): Train loss 4.302, Val loss 4.332\n",
      "Ep 2 (Step 010050): Train loss 4.309, Val loss 4.339\n",
      "Ep 2 (Step 010200): Train loss 4.285, Val loss 4.336\n",
      "Ep 2 (Step 010350): Train loss 4.295, Val loss 4.326\n",
      "Ep 2 (Step 010500): Train loss 4.282, Val loss 4.329\n",
      "Ep 2 (Step 010650): Train loss 4.241, Val loss 4.316\n",
      "Ep 2 (Step 010800): Train loss 4.247, Val loss 4.284\n",
      "Ep 2 (Step 010950): Train loss 4.256, Val loss 4.290\n",
      "Ep 2 (Step 011100): Train loss 4.255, Val loss 4.285\n",
      "Ep 2 (Step 011250): Train loss 4.219, Val loss 4.277\n",
      "Ep 2 (Step 011400): Train loss 4.238, Val loss 4.274\n",
      "Ep 2 (Step 011550): Train loss 4.226, Val loss 4.275\n",
      "Ep 2 (Step 011700): Train loss 4.205, Val loss 4.269\n",
      "Ep 2 (Step 011850): Train loss 4.220, Val loss 4.249\n",
      "Ep 2 (Step 012000): Train loss 4.186, Val loss 4.259\n",
      "Ep 2 (Step 012150): Train loss 4.216, Val loss 4.246\n",
      "Ep 2 (Step 012300): Train loss 4.188, Val loss 4.266\n",
      "Ep 2 (Step 012450): Train loss 4.160, Val loss 4.253\n",
      "Ep 2 (Step 012600): Train loss 4.149, Val loss 4.240\n",
      "Ep 2 (Step 012750): Train loss 4.163, Val loss 4.229\n",
      "Ep 2 (Step 012900): Train loss 4.146, Val loss 4.219\n",
      "Ep 2 (Step 013050): Train loss 4.177, Val loss 4.225\n",
      "Ep 2 (Step 013200): Train loss 4.149, Val loss 4.225\n",
      "Ep 2 (Step 013350): Train loss 4.160, Val loss 4.227\n",
      "Ep 3 (Step 013500): Train loss 4.156, Val loss 4.233\n",
      "Ep 3 (Step 013650): Train loss 4.117, Val loss 4.219\n",
      "Ep 3 (Step 013800): Train loss 4.123, Val loss 4.193\n",
      "Ep 3 (Step 013950): Train loss 4.113, Val loss 4.200\n",
      "Ep 3 (Step 014100): Train loss 4.137, Val loss 4.204\n",
      "Ep 3 (Step 014250): Train loss 4.118, Val loss 4.202\n",
      "Ep 3 (Step 014400): Train loss 4.127, Val loss 4.206\n",
      "Ep 3 (Step 014550): Train loss 4.101, Val loss 4.187\n",
      "Ep 3 (Step 014700): Train loss 4.107, Val loss 4.196\n",
      "Ep 3 (Step 014850): Train loss 4.086, Val loss 4.188\n",
      "Ep 3 (Step 015000): Train loss 4.067, Val loss 4.183\n",
      "Ep 3 (Step 015150): Train loss 4.070, Val loss 4.172\n",
      "Ep 3 (Step 015300): Train loss 4.108, Val loss 4.184\n",
      "Ep 3 (Step 015450): Train loss 4.062, Val loss 4.187\n",
      "Ep 3 (Step 015600): Train loss 4.085, Val loss 4.182\n",
      "Ep 3 (Step 015750): Train loss 4.031, Val loss 4.172\n",
      "Ep 3 (Step 015900): Train loss 4.054, Val loss 4.172\n",
      "Ep 3 (Step 016050): Train loss 4.063, Val loss 4.159\n",
      "Ep 3 (Step 016200): Train loss 4.035, Val loss 4.162\n",
      "Ep 3 (Step 016350): Train loss 4.052, Val loss 4.165\n",
      "Ep 3 (Step 016500): Train loss 4.032, Val loss 4.156\n",
      "Ep 3 (Step 016650): Train loss 4.032, Val loss 4.144\n",
      "Ep 3 (Step 016800): Train loss 4.031, Val loss 4.141\n",
      "Ep 3 (Step 016950): Train loss 3.983, Val loss 4.130\n",
      "Ep 3 (Step 017100): Train loss 3.977, Val loss 4.128\n",
      "Ep 3 (Step 017250): Train loss 4.006, Val loss 4.126\n",
      "Ep 3 (Step 017400): Train loss 4.024, Val loss 4.140\n",
      "Ep 3 (Step 017550): Train loss 3.996, Val loss 4.128\n",
      "Ep 3 (Step 017700): Train loss 3.975, Val loss 4.122\n",
      "Ep 3 (Step 017850): Train loss 3.985, Val loss 4.102\n",
      "Ep 3 (Step 018000): Train loss 4.015, Val loss 4.113\n",
      "Ep 3 (Step 018150): Train loss 3.975, Val loss 4.116\n",
      "Ep 3 (Step 018300): Train loss 3.957, Val loss 4.096\n",
      "Ep 3 (Step 018450): Train loss 3.990, Val loss 4.092\n",
      "Ep 3 (Step 018600): Train loss 3.934, Val loss 4.097\n",
      "Ep 3 (Step 018750): Train loss 3.959, Val loss 4.084\n",
      "Ep 3 (Step 018900): Train loss 3.964, Val loss 4.076\n",
      "Ep 3 (Step 019050): Train loss 3.943, Val loss 4.090\n",
      "Ep 3 (Step 019200): Train loss 3.945, Val loss 4.091\n",
      "Ep 3 (Step 019350): Train loss 3.922, Val loss 4.085\n",
      "Ep 3 (Step 019500): Train loss 3.946, Val loss 4.073\n",
      "Ep 3 (Step 019650): Train loss 3.888, Val loss 4.063\n",
      "Ep 3 (Step 019800): Train loss 3.932, Val loss 4.060\n",
      "Ep 3 (Step 019950): Train loss 3.942, Val loss 4.056\n",
      "Ep 4 (Step 020100): Train loss 3.923, Val loss 4.056\n",
      "Ep 4 (Step 020250): Train loss 3.931, Val loss 4.059\n",
      "Ep 4 (Step 020400): Train loss 3.917, Val loss 4.063\n",
      "Ep 4 (Step 020550): Train loss 3.896, Val loss 4.062\n",
      "Ep 4 (Step 020700): Train loss 3.874, Val loss 4.042\n",
      "Ep 4 (Step 020850): Train loss 3.913, Val loss 4.062\n",
      "Ep 4 (Step 021000): Train loss 3.873, Val loss 4.068\n",
      "Ep 4 (Step 021150): Train loss 3.871, Val loss 4.056\n",
      "Ep 4 (Step 021300): Train loss 3.861, Val loss 4.056\n",
      "Ep 4 (Step 021450): Train loss 3.837, Val loss 4.057\n",
      "Ep 4 (Step 021600): Train loss 3.855, Val loss 4.061\n",
      "Ep 4 (Step 021750): Train loss 3.868, Val loss 4.054\n",
      "Ep 4 (Step 021900): Train loss 3.824, Val loss 4.053\n",
      "Ep 4 (Step 022050): Train loss 3.838, Val loss 4.061\n",
      "Ep 4 (Step 022200): Train loss 3.870, Val loss 4.053\n",
      "Ep 4 (Step 022350): Train loss 3.823, Val loss 4.038\n",
      "Ep 4 (Step 022500): Train loss 3.805, Val loss 4.042\n",
      "Ep 4 (Step 022650): Train loss 3.844, Val loss 4.049\n",
      "Ep 4 (Step 022800): Train loss 3.842, Val loss 4.047\n",
      "Ep 4 (Step 022950): Train loss 3.836, Val loss 4.047\n",
      "Ep 4 (Step 023100): Train loss 3.804, Val loss 4.044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 (Step 023250): Train loss 3.839, Val loss 4.029\n",
      "Ep 4 (Step 023400): Train loss 3.814, Val loss 4.040\n",
      "Ep 4 (Step 023550): Train loss 3.843, Val loss 4.028\n",
      "Ep 4 (Step 023700): Train loss 3.808, Val loss 4.034\n",
      "Ep 4 (Step 023850): Train loss 3.771, Val loss 4.026\n",
      "Ep 4 (Step 024000): Train loss 3.807, Val loss 4.021\n",
      "Ep 4 (Step 024150): Train loss 3.803, Val loss 4.017\n",
      "Ep 4 (Step 024300): Train loss 3.834, Val loss 4.019\n",
      "Ep 4 (Step 024450): Train loss 3.795, Val loss 4.007\n",
      "Ep 4 (Step 024600): Train loss 3.801, Val loss 4.013\n",
      "Ep 4 (Step 024750): Train loss 3.793, Val loss 3.998\n",
      "Ep 4 (Step 024900): Train loss 3.769, Val loss 4.000\n",
      "Ep 4 (Step 025050): Train loss 3.774, Val loss 3.986\n",
      "Ep 4 (Step 025200): Train loss 3.770, Val loss 4.000\n",
      "Ep 4 (Step 025350): Train loss 3.790, Val loss 4.003\n",
      "Ep 4 (Step 025500): Train loss 3.777, Val loss 3.994\n",
      "Ep 4 (Step 025650): Train loss 3.805, Val loss 3.990\n",
      "Ep 4 (Step 025800): Train loss 3.823, Val loss 3.997\n",
      "Ep 4 (Step 025950): Train loss 3.767, Val loss 3.986\n",
      "Ep 4 (Step 026100): Train loss 3.757, Val loss 3.985\n",
      "Ep 4 (Step 026250): Train loss 3.775, Val loss 3.974\n",
      "Ep 4 (Step 026400): Train loss 3.757, Val loss 3.977\n",
      "Ep 4 (Step 026550): Train loss 3.749, Val loss 3.974\n",
      "Ep 4 (Step 026700): Train loss 3.744, Val loss 3.973\n",
      "Ep 5 (Step 026850): Train loss 3.735, Val loss 3.985\n",
      "Ep 5 (Step 027000): Train loss 3.773, Val loss 3.984\n",
      "Ep 5 (Step 027150): Train loss 3.711, Val loss 3.981\n",
      "Ep 5 (Step 027300): Train loss 3.743, Val loss 3.982\n",
      "Ep 5 (Step 027450): Train loss 3.742, Val loss 3.980\n",
      "Ep 5 (Step 027600): Train loss 3.724, Val loss 3.977\n",
      "Ep 5 (Step 027750): Train loss 3.697, Val loss 3.973\n",
      "Ep 5 (Step 027900): Train loss 3.745, Val loss 3.971\n",
      "Ep 5 (Step 028050): Train loss 3.723, Val loss 3.974\n",
      "Ep 5 (Step 028200): Train loss 3.709, Val loss 3.973\n",
      "Ep 5 (Step 028350): Train loss 3.680, Val loss 3.975\n",
      "Ep 5 (Step 028500): Train loss 3.684, Val loss 3.978\n",
      "Ep 5 (Step 028650): Train loss 3.696, Val loss 3.974\n",
      "Ep 5 (Step 028800): Train loss 3.689, Val loss 3.973\n",
      "Ep 5 (Step 028950): Train loss 3.670, Val loss 3.965\n",
      "Ep 5 (Step 029100): Train loss 3.683, Val loss 3.971\n",
      "Ep 5 (Step 029250): Train loss 3.666, Val loss 3.981\n",
      "Ep 5 (Step 029400): Train loss 3.702, Val loss 3.980\n",
      "Ep 5 (Step 029550): Train loss 3.741, Val loss 3.983\n",
      "Ep 5 (Step 029700): Train loss 3.682, Val loss 3.971\n",
      "Ep 5 (Step 029850): Train loss 3.696, Val loss 3.975\n",
      "Ep 5 (Step 030000): Train loss 3.681, Val loss 3.976\n",
      "Ep 5 (Step 030150): Train loss 3.707, Val loss 3.968\n",
      "Ep 5 (Step 030300): Train loss 3.688, Val loss 3.968\n",
      "Ep 5 (Step 030450): Train loss 3.643, Val loss 3.965\n",
      "Ep 5 (Step 030600): Train loss 3.689, Val loss 3.963\n",
      "Ep 5 (Step 030750): Train loss 3.647, Val loss 3.967\n",
      "Ep 5 (Step 030900): Train loss 3.654, Val loss 3.965\n",
      "Ep 5 (Step 031050): Train loss 3.642, Val loss 3.960\n",
      "Ep 5 (Step 031200): Train loss 3.659, Val loss 3.958\n",
      "Ep 5 (Step 031350): Train loss 3.667, Val loss 3.961\n",
      "Ep 5 (Step 031500): Train loss 3.671, Val loss 3.961\n",
      "Ep 5 (Step 031650): Train loss 3.671, Val loss 3.961\n",
      "Ep 5 (Step 031800): Train loss 3.650, Val loss 3.958\n",
      "Ep 5 (Step 031950): Train loss 3.647, Val loss 3.961\n",
      "Ep 5 (Step 032100): Train loss 3.650, Val loss 3.956\n",
      "Ep 5 (Step 032250): Train loss 3.639, Val loss 3.953\n",
      "Ep 5 (Step 032400): Train loss 3.672, Val loss 3.953\n",
      "Ep 5 (Step 032550): Train loss 3.639, Val loss 3.952\n",
      "Ep 5 (Step 032700): Train loss 3.619, Val loss 3.950\n",
      "Ep 5 (Step 032850): Train loss 3.631, Val loss 3.953\n",
      "Ep 5 (Step 033000): Train loss 3.652, Val loss 3.951\n",
      "Ep 5 (Step 033150): Train loss 3.616, Val loss 3.945\n",
      "Ep 5 (Step 033300): Train loss 3.667, Val loss 3.950\n",
      "Ep 6 (Step 033450): Train loss 3.670, Val loss 3.948\n",
      "Ep 6 (Step 033600): Train loss 3.648, Val loss 3.950\n",
      "Ep 6 (Step 033750): Train loss 3.638, Val loss 3.950\n",
      "Ep 6 (Step 033900): Train loss 3.622, Val loss 3.951\n",
      "Ep 6 (Step 034050): Train loss 3.645, Val loss 3.950\n",
      "Ep 6 (Step 034200): Train loss 3.627, Val loss 3.949\n",
      "Ep 6 (Step 034350): Train loss 3.623, Val loss 3.949\n",
      "Ep 6 (Step 034500): Train loss 3.662, Val loss 3.949\n",
      "Ep 6 (Step 034650): Train loss 3.640, Val loss 3.949\n",
      "Ep 6 (Step 034800): Train loss 3.621, Val loss 3.950\n",
      "Ep 6 (Step 034950): Train loss 3.636, Val loss 3.950\n",
      "Ep 6 (Step 035100): Train loss 3.599, Val loss 3.951\n",
      "Ep 6 (Step 035250): Train loss 3.644, Val loss 3.949\n",
      "Ep 6 (Step 035400): Train loss 3.639, Val loss 3.947\n",
      "Ep 6 (Step 035550): Train loss 3.655, Val loss 3.949\n",
      "Ep 6 (Step 035700): Train loss 3.639, Val loss 3.947\n",
      "Ep 6 (Step 035850): Train loss 3.599, Val loss 3.947\n",
      "Ep 6 (Step 036000): Train loss 3.602, Val loss 3.947\n",
      "Ep 6 (Step 036150): Train loss 3.637, Val loss 3.945\n",
      "Ep 6 (Step 036300): Train loss 3.612, Val loss 3.947\n",
      "Ep 6 (Step 036450): Train loss 3.631, Val loss 3.947\n",
      "Ep 6 (Step 036600): Train loss 3.617, Val loss 3.947\n",
      "Ep 6 (Step 036750): Train loss 3.628, Val loss 3.947\n",
      "Ep 6 (Step 036900): Train loss 3.624, Val loss 3.947\n",
      "Ep 6 (Step 037050): Train loss 3.641, Val loss 3.946\n",
      "Ep 6 (Step 037200): Train loss 3.605, Val loss 3.945\n",
      "Ep 6 (Step 037350): Train loss 3.617, Val loss 3.945\n",
      "Ep 6 (Step 037500): Train loss 3.634, Val loss 3.945\n",
      "Ep 6 (Step 037650): Train loss 3.629, Val loss 3.945\n",
      "Ep 6 (Step 037800): Train loss 3.645, Val loss 3.945\n",
      "Ep 6 (Step 037950): Train loss 3.649, Val loss 3.945\n",
      "Ep 6 (Step 038100): Train loss 3.635, Val loss 3.945\n",
      "Ep 6 (Step 038250): Train loss 3.635, Val loss 3.945\n",
      "Ep 6 (Step 038400): Train loss 3.598, Val loss 3.944\n",
      "Ep 6 (Step 038550): Train loss 3.618, Val loss 3.944\n",
      "Ep 6 (Step 038700): Train loss 3.633, Val loss 3.944\n",
      "Ep 6 (Step 038850): Train loss 3.630, Val loss 3.944\n",
      "Ep 6 (Step 039000): Train loss 3.610, Val loss 3.944\n",
      "Ep 6 (Step 039150): Train loss 3.651, Val loss 3.944\n",
      "Ep 6 (Step 039300): Train loss 3.598, Val loss 3.944\n",
      "Ep 6 (Step 039450): Train loss 3.617, Val loss 3.944\n",
      "Ep 6 (Step 039600): Train loss 3.595, Val loss 3.944\n",
      "Ep 6 (Step 039750): Train loss 3.597, Val loss 3.944\n",
      "Ep 6 (Step 039900): Train loss 3.623, Val loss 3.944\n",
      "Ep 6 (Step 040050): Train loss 3.626, Val loss 3.944\n",
      "Training completed in 485.76 minutes.\n"
     ]
    }
   ],
   "source": [
    "# train model on all works\n",
    "\n",
    "train(train_loader, val_loader, num_epochs=6,\n",
    "      eval_iter=150, model_prefix=\"model_896_14_8\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c4e59",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6651aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0002, weight_decay=0.05)\n",
    "\n",
    "checkpoint = torch.load(\"model_896_14_8_256.pth\", weights_only=True, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc47f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ecee23b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py:128\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[1;34m(coro)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3286\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[1;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[0;32m   3284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_history:\n\u001b[0;32m   3285\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3286\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[0;32m   3288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(cell, raw_cell)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\history.py:793\u001b[0m, in \u001b[0;36mHistoryManager.store_inputs\u001b[1;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_hist_raw\u001b[38;5;241m.\u001b[39mappend(source_raw)\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_input_cache_lock:\n\u001b[1;32m--> 793\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb_input_cache\u001b[49m\u001b[38;5;241m.\u001b[39mappend((line_num, source, source_raw))\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;66;03m# Trigger to flush cache and write to DB.\u001b[39;00m\n\u001b[0;32m    795\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_input_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_cache_size:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\traitlets\\traitlets.py:676\u001b[0m, in \u001b[0;36mTraitType.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;129m@t\u001b[39m\u001b[38;5;241m.\u001b[39moverload\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[t\u001b[38;5;241m.\u001b[39mAny]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m G:\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m--> 676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: HasTraits \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[t\u001b[38;5;241m.\u001b[39mAny]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self \u001b[38;5;241m|\u001b[39m G:\n\u001b[0;32m    677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the value of the trait by self.name for the instance.\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03m    Default values are instantiated when :meth:`HasTraits.__new__`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;124;03m    is in the :class:`HasTraits` instance.\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from itertools import combinations\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)  # Forward pass\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * target_ids.numel()\n",
    "            total_tokens += target_ids.numel()\n",
    "\n",
    "    perplexity = np.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c61225-1077-4019-984a-564aa7ba6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "def weat_score(model, target_words_1, target_words_2, attribute_words_1, attribute_words_2, tokenizer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Measures bias by comparing how close different groups of words are in embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding(word):\n",
    "        token_id = tokenizer.encode(word, allowed_special={'<|endoftext|>'})[0]\n",
    "        with torch.no_grad():\n",
    "            embed = model.tok_emb(torch.tensor([token_id], device=device)).cpu().numpy()\n",
    "        return embed.flatten()\n",
    "\n",
    "    # Get embeddings\n",
    "    target_1_embs = [get_embedding(w) for w in target_words_1]\n",
    "    target_2_embs = [get_embedding(w) for w in target_words_2]\n",
    "    attr_1_embs = [get_embedding(w) for w in attribute_words_1]\n",
    "    attr_2_embs = [get_embedding(w) for w in attribute_words_2]\n",
    "\n",
    "    def association(t, A, B):\n",
    "        return np.mean([cosine_similarity(t, a) for a in A]) - np.mean([cosine_similarity(t, b) for b in B])\n",
    "\n",
    "    # Compute WEAT score\n",
    "    s1 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_1_embs])\n",
    "    s2 = np.sum([association(t, attr_1_embs, attr_2_embs) for t in target_2_embs])\n",
    "    \n",
    "    weat_score = s1 - s2\n",
    "    return weat_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a1406-0bd9-409a-9fd8-8b4f343d2c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_male = [\"gentleman\", \"officer\", \"clergyman\", \"husband\", \"captain\"]\n",
    "target_female = [\"lady\", \"governess\", \"girl\", \"wife\", \"widow\"]\n",
    "\n",
    "attribute_male = [\"honour\", \"duty\", \"wisdom\", \"fortitude\", \"independence\"]\n",
    "attribute_female = [\"grace\", \"affection\", \"beauty\", \"delicacy\", \"modesty\"]\n",
    "\n",
    "weat_score(model, target_male, target_female, attribute_male, attribute_female, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcce2e-326f-4cc6-8033-e77e0a2ba270",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d845171-c8f1-47a0-a60b-1f9847a22590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import re\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_bleu_rouge_from_val(model, device=\"cpu\"):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    # Step 1: Load the validation set\n",
    "    with open('val_text_data_all_txt.txt', 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Step 2: Split into sentences & filter\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', data)\n",
    "    filtered_sentences = [s.strip() for s in sentences if 5 <= len(s.split()) <= 60]\n",
    "    filtered_sentences = filtered_sentences[:1000]\n",
    "\n",
    "    # Step 3: Split each sentence into two halves and store as tuples\n",
    "    sentence_tuples = []\n",
    "    for sent in filtered_sentences:\n",
    "        words = sent.split()\n",
    "        mid = len(words) // 2\n",
    "        first_half = ' '.join(words[:mid])\n",
    "        second_half = ' '.join(words[mid:])\n",
    "        sentence_tuples.append((first_half, second_half))\n",
    "\n",
    "    # Step 4: For each (first_half, second_half), generate prediction\n",
    "    for first_half, second_half in sentence_tuples:\n",
    "        generated_text = generate(\n",
    "            model=model, prompt=first_half,\n",
    "            max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "            device=device,\n",
    "            temperature=0.7,\n",
    "            top_k=50\n",
    "        )\n",
    "\n",
    "        # Build reference and prediction\n",
    "        reference = first_half + \" \" + second_half\n",
    "        prediction = generated_text\n",
    "\n",
    "        references.append(reference)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Step 5-6: Compute BLEU and ROUGE\n",
    "    # Format references correctly for BLEU\n",
    "    references_formatted = [[ref] for ref in references]\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references_formatted)['bleu']\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}, ROUGE-L Score: {rouge_score['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404bbea-7821-4b52-a5e9-f240bf5c01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_bleu_rouge_from_val(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adb0c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss Bennet has inherited the estate from her aunt, so she must have been in the world to have been a great deal better acquainted with him than to have seen him. If she had been a very old wife, she would have had her own opinion on the very last Sunday; but then, the money was very\n",
      "==================================================\n",
      "Mr. Darcy has inherited the estate from his aunt, so he must be assured of the poor people that I am sure you will be the happiest of men.\"\n",
      "\"I do not mean to give up Mrs. Bennet,\" said Elizabeth, \"that the only person who can make her amends for the family,\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Miss Bennet has inherited the estate from her aunt, so she must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"Mr. Darcy has inherited the estate from his aunt, so he must\",\n",
    "    max_new_tokens=50, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81220f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wife is as good as her husband: and that is all I can say. But I don't think she is a good wife to me; and I have\n",
      "==================================================\n",
      "A husband is a man of my own, and I am sure I shall be glad to be married.\"\n",
      "\"And if you please, sir, you will do\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"A wife is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"A husband is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "714f6606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I shall now go on with her on the morrow. You will not be surprised to be on the way of making any change of manner in this way; you will\n",
      "==================================================\n",
      "He said, \"I am not sure of any consequence. I am sure you would not have been angry with me, if I had not been so ungr\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"I shall now go\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)\n",
    "    \n",
    "print(50*\"=\")\n",
    "    \n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"He said\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78dc249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She was so much obliged to leave her to go, that she would not stay till night.\n",
      "<|endoftext|>\n",
      "Mrs. Jennings was not a creature to be known to her, nor had she been able to see her friends, nor of her family; nor could she possibly have found a place in the world more than any thing else, in any of the family. She was a young woman, a very sensible woman, and of her husband; but she was not one to herself, except a young woman in a family, who, as her husband, she had been brought home with them, she had not been in her company with him, and had not been long so much obliged to her, or with her aunt, to have taken up a great deal of conversation with her; but she was now a little surprised, and had the satisfaction of seeing her sister, and of her being so long as to have seen a sister so very young, and so very good-humoured, that she was so\n"
     ]
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model, \n",
    "    prompt=\"She was\",\n",
    "    max_new_tokens=200, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.7,\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "splitted = text.split(\"\\n\")\n",
    "for txt in splitted:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dacdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb810-cfc6-49fe-bfd0-66c035e0707e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78fd8b69-ae98-4300-b876-93bf84fa0d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to be the only one to whom I am so much obliged to you. I am sure I am not so glad to hear it. But I am very'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "583fd4d9-4273-435a-a2fe-651162acb64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a duty to be a relief to me, and I am no longer so happy as I am now to be. I am not so happy as I am, that'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"a duty to\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c8a060b-6d8a-4dc8-ade9-bfde78b38900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is a great deal more pleased with him,\" said Will, rather annoyed at his habitual appeal. \"Ah! there is a great difference in him--but'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_text import generate\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "text = generate(\n",
    "    model=model,\n",
    "    prompt=\"he is\",\n",
    "    max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'],\n",
    "    device=\"cpu\",\n",
    "    temperature=1,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba961e03-e482-4601-bef7-daab0c1dac77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f99b81-a4be-4994-9c00-8898a408f06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
